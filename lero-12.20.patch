diff --git a/src/backend/Makefile b/src/backend/Makefile
index b03d5e5..2f30661 100644
--- a/src/backend/Makefile
+++ b/src/backend/Makefile
@@ -19,7 +19,7 @@ include $(top_builddir)/src/Makefile.global
 
 SUBDIRS = access bootstrap catalog parser commands executor foreign lib libpq \
 	main nodes optimizer partitioning port postmaster \
-	regex replication rewrite \
+	lero regex replication rewrite \
 	statistics storage tcop tsearch utils $(top_builddir)/src/timezone \
 	jit
 
diff --git a/src/backend/commands/extension.c b/src/backend/commands/extension.c
index 0f9edf6..0c0bb52 100644
--- a/src/backend/commands/extension.c
+++ b/src/backend/commands/extension.c
@@ -728,7 +728,7 @@ execute_sql_string(const char *sql)
 										   NULL,
 										   0,
 										   NULL);
-		stmt_list = pg_plan_queries(stmt_list, CURSOR_OPT_PARALLEL_OK, NULL);
+		stmt_list = pg_plan_queries(stmt_list, sql, CURSOR_OPT_PARALLEL_OK, NULL);
 
 		foreach(lc2, stmt_list)
 		{
diff --git a/src/backend/lero/Makefile b/src/backend/lero/Makefile
new file mode 100644
index 0000000..e33cf58
--- /dev/null
+++ b/src/backend/lero/Makefile
@@ -0,0 +1,20 @@
+#-------------------------------------------------------------------------
+#
+# Makefile--
+#    Makefile for lero
+#
+# IDENTIFICATION
+#    src/backend/lero/Makefile
+#
+#-------------------------------------------------------------------------
+
+subdir = src/backend/lero
+top_builddir = ../../..
+include $(top_builddir)/src/Makefile.global
+
+OBJS = \
+	utils.o \
+	yyjson.o \
+	lero_extension.o
+
+include $(top_srcdir)/src/backend/common.mk
diff --git a/src/backend/lero/lero_extension.c b/src/backend/lero/lero_extension.c
new file mode 100644
index 0000000..3cb5a6c
--- /dev/null
+++ b/src/backend/lero/lero_extension.c
@@ -0,0 +1,373 @@
+#include "lero/lero_extension.h"
+#include "miscadmin.h"
+#include "optimizer/appendinfo.h"
+#include "optimizer/joininfo.h"
+#include "optimizer/pathnode.h"
+#include "optimizer/planner.h"
+#include "optimizer/paths.h"
+#include "partitioning/partbounds.h"
+#include "nodes/bitmapset.h"
+#include "utils/memutils.h"
+#include "lero/utils.h"
+#include "lero/yyjson.h"
+#include <string.h>
+#include <stdlib.h>
+
+#define PLAN_MAX_SAMPLES 1024
+
+#define CARD_MAX_NUM 25000
+
+bool enable_lero = false;
+
+// lero server configuration
+int lero_server_port = 14567;
+char *lero_server_host = "localhost";
+
+// the number of join cardinalities to be calculated in this query
+int join_card_num = -1;
+// the original join cardinalities without any reduction
+double original_card_list[CARD_MAX_NUM] = {-1.0};
+// indicates whether to record the original join cardinalities or read the new join cardinalities after zooming
+bool record_original_card_phase = false;
+// the new join cardinalities after zooming given by lero
+double lero_card_list[CARD_MAX_NUM] = {-1.0};
+// the number of join cardinalities given by lero
+int max_lero_join_card_idx = -1;
+// recored the names of the input tables involved in each join
+RelatedTable *join_input_tables[CARD_MAX_NUM] = {NULL};
+
+int cur_card_idx = 0;
+
+char* query_unique_id = NULL;
+
+
+static
+LeroPlan *get_lero_plan(int i, Query *parse, const char *queryString,
+					  int cursorOptions,
+					  ParamListInfo boundParams, int *early_stop);
+static 
+double predict_plan_score(yyjson_mut_doc *json_doc, yyjson_mut_val *json_root, int *early_stop);
+
+static 
+void send_default_rows(const char *queryString);
+
+static 
+void get_join_card_list();
+
+static 
+void remove_opt_state();
+
+void lero_pgsysml_set_joinrel_size_estimates(PlannerInfo *root, RelOptInfo *rel,
+											RelOptInfo *outer_rel,
+											RelOptInfo *inner_rel,
+											SpecialJoinInfo *sjinfo,
+											List *restrictlist)
+{
+	static double rows;
+	if (record_original_card_phase)
+	{
+		rows = rel->rows;
+		original_card_list[join_card_num] = rows;
+
+		RelatedTable *related_table = (RelatedTable *) palloc(sizeof(RelatedTable));
+		related_table->tables = NIL;
+		add_join_input_tables(root, outer_rel->cheapest_total_path, related_table);
+		add_join_input_tables(root, inner_rel->cheapest_total_path, related_table);
+		join_input_tables[join_card_num] = related_table;
+		join_card_num += 1;
+	}
+	else
+	{
+		if (cur_card_idx <= max_lero_join_card_idx) {
+			rows = lero_card_list[cur_card_idx];
+		} else {
+			rows = rel->rows;
+		}
+		cur_card_idx += 1;
+	}
+	rel->rows = rows;
+}
+
+PlannedStmt *lero_pgsysml_hook_planner(Query *parse, const char *queryString,
+									  int cursorOptions,
+									  ParamListInfo boundParams)
+{
+	if (!enable_lero)
+	{
+		return standard_planner(parse, queryString, cursorOptions, boundParams);
+	}
+
+	query_unique_id = get_query_unique_id(queryString);
+
+	LeroPlan *plan_for_card[PLAN_MAX_SAMPLES];
+	Query *query_copy;
+	int conn_fd;
+	record_original_card_phase = false;
+
+	LeroPlan* best = NULL;
+	double best_latency;
+	int best_idx = 0;
+	join_card_num = 0;
+	int plan_num = PLAN_MAX_SAMPLES;
+	int early_stop = 0;
+	for (int i = 0; i < PLAN_MAX_SAMPLES; i++)
+	{
+		// Plan the query for this card list.
+		query_copy = copyObject(parse);
+		LeroPlan* p = get_lero_plan(i, query_copy,
+										queryString, cursorOptions, boundParams, &early_stop);
+		if (best == NULL || p->latency < best_latency) {
+			best = p;
+			best_latency = p->latency;
+			best_idx = i;
+		}
+		plan_for_card[i] = p;
+
+		if (early_stop) {
+			plan_num = i + 1;
+			break;
+		}
+	}
+
+	for (int i = 0; i < plan_num; i++) {
+		elog(WARNING, "%d-th plan's latency is %f", i, plan_for_card[i]->latency);
+	}
+
+	remove_opt_state();
+	pfree(query_unique_id);
+	elog(WARNING, "best plan is %d", best_idx);
+	return best->plan;
+}
+
+static
+LeroPlan *get_lero_plan(int i, Query *parse, const char *queryString,
+					  int cursorOptions,
+					  ParamListInfo boundParams, int* early_stop)
+{
+	// do not change the cardinality list for the first query planning
+	// and send the default cardinality list to the server
+	record_original_card_phase = i == 0;
+	cur_card_idx = 0;
+	LeroPlan *p = (LeroPlan *)palloc(sizeof(LeroPlan));
+	elog(WARNING, "Query string:%s", queryString);
+
+	if (!record_original_card_phase) {
+		get_join_card_list();
+	}
+
+	PlannedStmt *plan = standard_planner(parse, queryString, cursorOptions, boundParams);
+	p->plan = plan;
+
+	if (record_original_card_phase)
+	{
+		send_default_rows(queryString);
+	}
+
+	yyjson_mut_doc *json_doc = yyjson_mut_doc_new(NULL);
+	yyjson_mut_val *root = yyjson_mut_obj(json_doc);	
+	yyjson_mut_doc_set_root(json_doc, root);
+	yyjson_mut_val *plan_json = plan_to_json(plan, plan->planTree, json_doc);
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, "Plan"), plan_json);
+	p->latency = predict_plan_score(json_doc, root, early_stop);
+	yyjson_mut_doc_free(json_doc);
+	return p;
+}
+
+static 
+void send_default_rows(const char *queryString)
+{
+	int conn_fd = connect_to_server(lero_server_host, lero_server_port);
+	if (conn_fd == -1)
+	{
+		elog(WARNING, "%s:%d", lero_server_host, lero_server_port);
+		elog(ERROR, "Unable to connect to Lero server.");
+		return;
+	}
+
+	yyjson_mut_doc *json_doc = yyjson_mut_doc_new(NULL);
+	yyjson_mut_val *root = yyjson_mut_obj(json_doc);
+	yyjson_mut_doc_set_root(json_doc, root);
+
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_TYPE), yyjson_mut_strcpy(json_doc, MSG_INIT));
+
+	yyjson_mut_val *row_arr = yyjson_mut_arr(json_doc);
+	for (int i = 0; i < join_card_num; i++) {
+		yyjson_mut_arr_append(row_arr, yyjson_mut_real(json_doc, original_card_list[i]));
+	}
+
+	yyjson_mut_val *table_arr = yyjson_mut_arr(json_doc);
+	for (int i = 0; i < join_card_num; i++) {
+		yyjson_mut_val *arr = yyjson_mut_arr(json_doc);
+		RelatedTable *related_table = join_input_tables[i];
+		ListCell   *lc;
+		foreach(lc, related_table->tables) {
+			char* table_name = (char *) lfirst(lc);
+			yyjson_mut_arr_append(arr, yyjson_mut_strcpy(json_doc, table_name));
+		}
+		list_free(related_table->tables);
+		pfree(related_table);
+
+		yyjson_mut_arr_append(table_arr, arr);
+	}
+
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, "rows_array"), row_arr);
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, "table_array"), table_arr);
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, "max_samples"),
+					   yyjson_mut_uint(json_doc, PLAN_MAX_SAMPLES));
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_QUERY_ID), 
+					   yyjson_mut_strcpy(json_doc, query_unique_id));		   
+
+	char *json = yyjson_mut_write(json_doc, YYJSON_WRITE_PRETTY, NULL);
+	char *send_json = concat_str(json, MSG_END_FLAG);
+	if (json) {
+		free(json);
+	}
+
+	// check whether Lero is initialized
+	char *msg = send_and_receive_msg(conn_fd, send_json);
+	yyjson_doc *msg_doc = parse_json_str(msg);
+	pfree(msg);
+
+	yyjson_val *msg_json_obj = yyjson_doc_get_root(msg_doc);
+	yyjson_val *msg_type = yyjson_obj_get(msg_json_obj, MSG_TYPE);
+	char *msg_char = yyjson_get_str(msg_type);
+	if (strcmp(msg_char, MSG_ERROR) == 0)
+	{
+		yyjson_doc_free(msg_doc);
+		elog(ERROR, "fail to init Lero");
+		return;
+	}
+
+	if (send_json) {
+		pfree((void *) send_json);
+	}
+	yyjson_doc_free(msg_doc);
+	yyjson_mut_doc_free(json_doc);
+	close(conn_fd);
+}
+
+static 
+double predict_plan_score(yyjson_mut_doc *json_doc, yyjson_mut_val *json_root, int *early_stop) {
+	yyjson_mut_obj_put(json_root, yyjson_mut_strcpy(json_doc, MSG_TYPE), yyjson_mut_strcpy(json_doc, MSG_PREDICT));
+	yyjson_mut_obj_put(json_root, yyjson_mut_strcpy(json_doc, MSG_QUERY_ID), yyjson_mut_strcpy(json_doc, query_unique_id));
+	char *json = yyjson_mut_write(json_doc, YYJSON_WRITE_PRETTY, NULL);
+	json = concat_str(json, MSG_END_FLAG);
+
+	int conn_fd = connect_to_server(lero_server_host, lero_server_port);
+	if (conn_fd == -1)
+	{
+		elog(ERROR, "Unable to connect to Lero server.");
+	}
+
+	char *msg = send_and_receive_msg(conn_fd, json);
+	yyjson_doc *msg_doc = parse_json_str(msg);
+	pfree(msg);
+
+	yyjson_val *msg_json_obj = yyjson_doc_get_root(msg_doc);
+	yyjson_val *msg_type = yyjson_obj_get(msg_json_obj, MSG_TYPE);
+	char *msg_char = yyjson_get_str(msg_type);
+
+	if (strcmp(msg_char, MSG_ERROR) == 0)
+	{
+		yyjson_doc_free(msg_doc);
+		close(conn_fd);
+		elog(ERROR, "fail to get score from Lero");
+	} else {
+		*early_stop = yyjson_get_int(yyjson_obj_get(msg_json_obj, MSG_FINISH));
+		double score = yyjson_get_real(yyjson_obj_get(msg_json_obj, MSG_SCORE));
+		yyjson_doc_free(msg_doc);
+		close(conn_fd);
+		return score;
+	}
+}
+
+static 
+void get_join_card_list() {
+	yyjson_mut_doc *json_doc = yyjson_mut_doc_new(NULL);
+	yyjson_mut_val *root = yyjson_mut_obj(json_doc);
+	yyjson_mut_doc_set_root(json_doc, root);
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_TYPE), yyjson_mut_strcpy(json_doc, "join_card"));
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_QUERY_ID), yyjson_mut_strcpy(json_doc, query_unique_id));
+	
+	char *json = yyjson_mut_write(json_doc, YYJSON_WRITE_PRETTY, NULL);
+	char *send_json = concat_str(json, MSG_END_FLAG);
+	if (json) {
+		free(json);
+	}
+
+	int conn_fd = connect_to_server(lero_server_host, lero_server_port);
+	if (conn_fd == -1)
+	{
+		elog(ERROR, "Unable to connect to Lero server.");
+	}
+	char *msg = send_and_receive_msg(conn_fd, send_json);
+	yyjson_doc *msg_doc = parse_json_str(msg);
+	
+	close(conn_fd);
+	yyjson_mut_doc_free(json_doc);
+	pfree(msg);
+	pfree(send_json);
+	
+	yyjson_val *msg_json_obj = yyjson_doc_get_root(msg_doc);
+	yyjson_val *msg_type = yyjson_obj_get(msg_json_obj, MSG_TYPE);
+	char *msg_char = yyjson_get_str(msg_type);
+
+	if (strcmp(msg_char, MSG_ERROR) == 0)
+	{
+		yyjson_doc_free(msg_doc);
+		elog(ERROR, "fail to get latency from Lero");
+	} else {
+		yyjson_val *joinrel_card_list_val = yyjson_obj_get(msg_json_obj, "join_card");		
+		yyjson_val *val;
+		yyjson_arr_iter iter;
+		yyjson_arr_iter_init(joinrel_card_list_val, &iter);
+		int i = -1;
+		while ((val = yyjson_arr_iter_next(&iter))) {
+			i++;
+			double real = yyjson_get_real(val);
+			printf("%f", real);
+			lero_card_list[i] = real;
+		}
+
+		max_lero_join_card_idx = i;
+		yyjson_doc_free(msg_doc);
+	}
+}
+
+static 
+void remove_opt_state() {
+	yyjson_mut_doc *json_doc = yyjson_mut_doc_new(NULL);
+	yyjson_mut_val *root = yyjson_mut_obj(json_doc);
+	yyjson_mut_doc_set_root(json_doc, root);
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_TYPE), yyjson_mut_strcpy(json_doc, "remove_state"));
+	yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_QUERY_ID), yyjson_mut_strcpy(json_doc, query_unique_id));
+	
+	char *json = yyjson_mut_write(json_doc, YYJSON_WRITE_PRETTY, NULL);
+	char *send_json = concat_str(json, MSG_END_FLAG);
+	if (json) {
+		free(json);
+	}
+
+	int conn_fd = connect_to_server(lero_server_host, lero_server_port);
+	if (conn_fd == -1)
+	{
+		elog(ERROR, "Unable to connect to Lero server.");
+	}
+	char *msg = send_and_receive_msg(conn_fd, send_json);
+	yyjson_doc *msg_doc = parse_json_str(msg);
+	
+	close(conn_fd);
+	yyjson_mut_doc_free(json_doc);
+	pfree(msg);
+	pfree(send_json);
+	
+	yyjson_val *msg_json_obj = yyjson_doc_get_root(msg_doc);
+	yyjson_val *msg_type = yyjson_obj_get(msg_json_obj, MSG_TYPE);
+	char *msg_char = yyjson_get_str(msg_type);
+
+	if (strcmp(msg_char, MSG_ERROR) == 0)
+	{
+		elog(WARNING, "fail to remove state");
+	}
+	yyjson_doc_free(msg_doc);
+}
\ No newline at end of file
diff --git a/src/backend/lero/utils.c b/src/backend/lero/utils.c
new file mode 100644
index 0000000..9ed54d5
--- /dev/null
+++ b/src/backend/lero/utils.c
@@ -0,0 +1,385 @@
+#include "postgres.h"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <netinet/in.h>
+#include <unistd.h>
+#include "nodes/pg_list.h"
+#include "lero/utils.h"
+#include "c.h"
+#include "utils/lsyscache.h"
+#include "optimizer/planmain.h"
+#include "optimizer/planner.h"
+#include "optimizer/cost.h"
+#include "nodes/pathnodes.h"
+#include "miscadmin.h"
+#include "parser/parsetree.h"
+
+#define MSG_SIZE 524288
+#define SOCKET_ERR -1
+#define SOCKET_SUCC 0
+
+// Connect to the server.
+int 
+connect_to_server(const char* host, int port) {
+  int ret, conn_fd;
+  struct sockaddr_in server_addr = { 0 };
+
+  server_addr.sin_family = AF_INET;
+  server_addr.sin_port = htons(port);
+  inet_pton(AF_INET, host, &server_addr.sin_addr);
+  conn_fd = socket(AF_INET, SOCK_STREAM, 0);
+  if (conn_fd < 0) {
+    return conn_fd;
+  }
+  
+  ret = connect(conn_fd, (struct sockaddr*)&server_addr, sizeof(server_addr));
+  if (ret == -1) {
+    return ret;
+  }
+
+  return conn_fd;
+}
+
+// Write the entire string to the given socket.
+void 
+write_all_to_socket(int conn_fd, const char* str) 
+{
+  size_t str_length;
+  ssize_t written, written_total;
+  str_length = strlen(str);
+  written_total = 0;
+  
+  while (written_total != str_length) {
+    written = write(conn_fd,
+                    str + written_total,
+                    str_length - written_total);
+    written_total += written;
+  }
+}
+
+char*
+concat_str(char* a, char *b)
+{
+	Assert(a != NULL && b != NULL);
+	int size_a = sizeof(a);
+
+	int len_a = strlen(a);
+	int len_b = strlen(b);
+	char* output = (char *) palloc(len_a + len_b + 1);
+	
+	memset(output, '\0', sizeof(output));
+	strcpy(output, a);
+	strcat(output, b);
+	return output;
+}
+
+char*
+send_and_receive_msg(int conn_fd, char* json_str) 
+{
+	if (conn_fd < 0) {
+      elog(WARNING, "Unable to connect to server.");
+      return;
+    }
+
+	write_all_to_socket(conn_fd, json_str);
+	shutdown(conn_fd, SHUT_WR);
+
+	int current_msg_size = MSG_SIZE;
+	char *msg = (char *) palloc(current_msg_size * sizeof(char));
+	memset(msg, '\0', current_msg_size);
+    int ret = 0;
+	int offset = 0;
+
+	while ((ret = read(conn_fd, msg + offset, MSG_SIZE - 1)) > 0) {
+		offset += ret;
+		if (offset >= current_msg_size) {
+			int new_msg_size = current_msg_size;
+			while(offset >= new_msg_size) {
+				new_msg_size *= 2;
+			}
+
+			msg = (char *) repalloc(msg, new_msg_size * sizeof(char));
+			current_msg_size = new_msg_size;
+		}
+	}
+	msg[offset] = '\0';
+
+	if (SOCKET_ERR == ret) {
+		elog(WARNING, "can not read the response from the server.");
+	}
+	return msg;
+}
+
+/**
+ * A tricky method to create an unique id of a given query.
+ */
+char*
+get_query_unique_id(const char *queryString)
+{
+	char* unique_id = (char *) palloc(17);
+	memset(unique_id, '\0', 17);
+	int query_len = strlen(queryString);
+	for (int i = 0; i < 16; i++) 
+	{
+		char ith = queryString[(rand() % query_len)];
+		while (!((ith >= 'a' && ith <='z') || (ith >= 'A' && ith <= 'Z')))
+		{
+			ith = queryString[(rand() % query_len)];
+		}
+		unique_id[i] = ith;
+	}
+	return unique_id;
+}
+
+yyjson_doc*
+parse_json_str(const char* json)
+{
+    if (json == NULL || strlen(json) == 0)
+    {
+        yyjson_mut_doc *json_doc = yyjson_mut_doc_new(NULL);
+	    yyjson_mut_val *root = yyjson_mut_obj(json_doc);
+        yyjson_mut_doc_set_root(json_doc, root);
+	    yyjson_mut_obj_put(root, yyjson_mut_strcpy(json_doc, MSG_TYPE), "error");
+        return json_doc;
+    }
+
+    return yyjson_read(json, strlen(json), 0);
+}
+
+yyjson_mut_val*
+double_list_to_json_arr(double l[], int n, yyjson_mut_doc *json_doc)
+{
+    yyjson_mut_val *arr = yyjson_mut_arr(json_doc);
+
+	for (int i = 0; i < n; i++) {
+		yyjson_mut_arr_append(arr, yyjson_mut_real(json_doc, l[i]));
+	}
+    return arr;
+}
+
+yyjson_mut_val*
+int_list_to_json_arr(int l[], int n, yyjson_mut_doc *json_doc)
+{
+    yyjson_mut_val *arr = yyjson_mut_arr(json_doc);
+
+	for (int i = 0; i < n; i++) {
+		yyjson_mut_arr_append(arr, yyjson_mut_uint(json_doc, l[i]));
+	}
+    return arr;
+}
+
+yyjson_mut_val*
+plan_to_json(PlannedStmt* stmt, Plan *plan, yyjson_mut_doc *json_doc)
+{
+    yyjson_mut_val *op = yyjson_mut_obj(json_doc);
+    yyjson_mut_val *inputs = yyjson_mut_arr(json_doc);
+    
+    char *op_name;
+	char *table_name = NULL;
+	char *index_name = NULL;
+	char *refname = NULL;
+	switch (plan->type)
+	{
+		case T_SeqScan:
+            op_name = "Seq Scan";
+			Index seqscan_idx = ((Scan*) plan)->scanrelid;
+			table_name = get_rel_name(rt_fetch(seqscan_idx, stmt->rtable)->relid);
+			refname = rt_fetch(seqscan_idx, stmt->rtable)->eref->aliasname;
+            break;
+		case T_IndexScan:
+            op_name = "Index Scan";
+			Index idxscan_idx = ((Scan*) plan)->scanrelid;
+			Index idxscan_index_id = ((IndexScan*) plan)->indexid;
+
+			table_name = get_rel_name(rt_fetch(idxscan_idx, stmt->rtable)->relid);
+			index_name = get_rel_name(idxscan_index_id);
+			refname = rt_fetch(idxscan_idx, stmt->rtable)->eref->aliasname;
+            break;
+		case T_IndexOnlyScan:
+			op_name = "Index Only Scan";
+			Index idxonlyscan_idx = ((Scan*) plan)->scanrelid;
+			Index idxonlyscan_index_id = ((IndexOnlyScan*) plan)->indexid;
+
+			table_name = get_rel_name(rt_fetch(idxonlyscan_idx, stmt->rtable)->relid);
+			index_name = get_rel_name(idxonlyscan_index_id);
+			refname = rt_fetch(idxonlyscan_idx, stmt->rtable)->eref->aliasname;
+            break;
+		case T_BitmapIndexScan:
+			op_name = "Bitmap Index Scan";
+			// Index bit_indexscan_idx = ((Scan*) plan)->scanrelid;
+			Index bit_indexscan_index_id = ((BitmapIndexScan*) plan)->indexid;
+
+			index_name = get_rel_name(bit_indexscan_index_id);
+			// Index bit_idxscan_idx = ((Scan*) plan)->scanrelid;
+			// table_name = get_rel_name(rt_fetch(bit_idxscan_idx, stmt->rtable)->relid);
+            break;
+		case T_BitmapHeapScan:
+			op_name = "Bitmap Heap Scan";
+			Index bit_heapscan_idx = ((Scan*) plan)->scanrelid;
+			table_name = get_rel_name(rt_fetch(bit_heapscan_idx, stmt->rtable)->relid);
+			refname = rt_fetch(bit_heapscan_idx, stmt->rtable)->eref->aliasname;
+            break;
+		case T_HashJoin:
+		case T_MergeJoin:
+		case T_NestLoop:;
+			if (plan->type == T_HashJoin) {
+				op_name = "Hash Join";
+			} else if (plan->type == T_MergeJoin) {
+				op_name = "Merge Join";
+			} else {
+				op_name = "Nested Loop";
+			}
+
+			yyjson_mut_val *inner = plan_to_json(stmt, plan->righttree, json_doc);
+			yyjson_mut_val *outer = plan_to_json(stmt, plan->lefttree, json_doc);
+			yyjson_mut_arr_append(inputs, outer);
+            yyjson_mut_arr_append(inputs, inner);
+			break;
+		case T_Hash:
+            op_name = "Hash";
+			yyjson_mut_val *hash_input = plan_to_json(stmt, plan->lefttree, json_doc);
+            yyjson_mut_arr_append(inputs, hash_input);
+			break;
+		case T_Material:
+            op_name = "Materialize";
+			yyjson_mut_val *mat_input = plan_to_json(stmt, plan->lefttree, json_doc);
+            yyjson_mut_arr_append(inputs, mat_input);
+			break;
+		case T_Sort:
+            op_name = "Sort";
+			yyjson_mut_val *sort_input = plan_to_json(stmt, plan->lefttree, json_doc);
+            yyjson_mut_arr_append(inputs, sort_input);
+			break;
+		case T_Agg:
+            op_name = "Aggregate";
+			yyjson_mut_val *agg_input = plan_to_json(stmt, plan->lefttree, json_doc);
+            yyjson_mut_arr_append(inputs, agg_input);
+			break;
+		case T_Limit:
+			op_name = "Limit";
+			yyjson_mut_val *limit_input = plan_to_json(stmt, plan->lefttree, json_doc);
+            yyjson_mut_arr_append(inputs, limit_input);
+			break;
+		case T_SampleScan:
+		case T_TidScan:
+		case T_SubqueryScan:
+		case T_FunctionScan:
+		case T_TableFuncScan:
+		case T_ValuesScan:
+		case T_CteScan:
+		case T_WorkTableScan:
+		case T_NamedTuplestoreScan:
+		case T_ForeignScan:
+		case T_CustomScan:
+		case T_Append:
+		case T_MergeAppend:
+		case T_Result:
+		case T_ProjectSet:
+		case T_Unique:
+		case T_Gather:
+		case T_Group:
+		case T_WindowAgg:
+		case T_RecursiveUnion:
+		case T_LockRows:
+		case T_ModifyTable:
+		case T_GatherMerge:
+			elog(WARNING, "unrecognized node type: %d",
+				 (int) plan->type);
+			break;
+		default:
+			elog(ERROR, "unrecognized node type: %d",
+				 (int) plan->type);
+			break;
+	}
+
+    yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Node Type"), yyjson_mut_strcpy(json_doc, op_name));
+	if (table_name != NULL) {
+		yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Relation Name"), yyjson_mut_strcpy(json_doc, table_name));
+	}
+	if (refname != NULL) {
+		yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Alias"), yyjson_mut_strcpy(json_doc, refname));
+	}
+	if (index_name != NULL) {
+		yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Index Name"), yyjson_mut_strcpy(json_doc, index_name));
+	}
+
+	if (yyjson_arr_size(inputs)) {
+    	yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Plans"), inputs);
+	}
+    yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Plan Rows"), yyjson_mut_real(json_doc, plan->plan_rows));
+    yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Plan Width"), yyjson_mut_sint(json_doc, plan->plan_width));
+    yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Startup Cost"), yyjson_mut_real(json_doc, plan->startup_cost));
+    yyjson_mut_obj_put(op, yyjson_mut_strcpy(json_doc, "Total Cost"), yyjson_mut_real(json_doc, plan->total_cost));
+
+    return op;
+}
+
+void 
+add_join_input_tables(PlannerInfo *root, Path *path, RelatedTable *related_table)
+{
+	switch (path->pathtype)
+	{
+		case T_SeqScan:
+		case T_IndexScan:
+		case T_IndexOnlyScan:
+		case T_BitmapHeapScan:;
+			Index table_relid = path->parent->relid;
+ 			char *table_name = get_rel_name(root->simple_rte_array[table_relid]->relid);
+			related_table->tables = lappend(related_table->tables, table_name);
+            break;
+		case T_HashJoin:
+		case T_MergeJoin:
+		case T_NestLoop:;
+			JoinPath *join_path = (JoinPath *) path;
+			Path *inner_path = join_path->innerjoinpath;
+			add_join_input_tables(root, inner_path, related_table);
+			Path *outer_path = join_path->outerjoinpath;
+			add_join_input_tables(root, outer_path, related_table);
+			break;
+		case T_Material:;
+			MaterialPath *material_path = (MaterialPath *) path;
+			add_join_input_tables(root, material_path->subpath, related_table);
+			break;
+		case T_Sort:;
+			SortPath *sort_path = (SortPath *) path;
+			add_join_input_tables(root, sort_path->subpath, related_table);
+			break;
+		case T_Agg:;
+			AggPath *agg_path = (AggPath *) path;
+			add_join_input_tables(root, agg_path->subpath, related_table);
+			break;
+		case T_SampleScan:
+		case T_TidScan:
+		case T_SubqueryScan:
+		case T_FunctionScan:
+		case T_TableFuncScan:
+		case T_ValuesScan:
+		case T_CteScan:
+		case T_WorkTableScan:
+		case T_NamedTuplestoreScan:
+		case T_ForeignScan:
+		case T_CustomScan:
+		case T_Append:
+		case T_MergeAppend:
+		case T_Result:
+		case T_ProjectSet:
+		case T_Unique:
+		case T_Gather:
+		case T_Group:
+		case T_WindowAgg:
+		case T_RecursiveUnion:
+		case T_LockRows:
+		case T_ModifyTable:
+		case T_Limit:
+		case T_GatherMerge:
+			elog(WARNING, "unrecognized node type: %d",
+				 (int) path->pathtype);
+			break;
+		default:
+			elog(ERROR, "unrecognized node type: %d",
+				 (int) path->pathtype);
+			break;
+	}
+}
\ No newline at end of file
diff --git a/src/backend/lero/yyjson.c b/src/backend/lero/yyjson.c
new file mode 100644
index 0000000..2b34f19
--- /dev/null
+++ b/src/backend/lero/yyjson.c
@@ -0,0 +1,7448 @@
+/*==============================================================================
+ * Created by Yaoyuan on 2019/3/9.
+ * Copyright (C) 2019 Yaoyuan <ibireme@gmail.com>.
+ *
+ * Released under the MIT License:
+ * https://github.com/ibireme/yyjson/blob/master/LICENSE
+ *============================================================================*/
+
+#include "lero/yyjson.h"
+#include <stdio.h>
+#include <math.h>
+
+
+
+/*==============================================================================
+ * Compile Hint Begin
+ *============================================================================*/
+
+/* warning suppress begin */
+#if defined(__clang__)
+#   pragma clang diagnostic push
+#   pragma clang diagnostic ignored "-Wunused-function"
+#   pragma clang diagnostic ignored "-Wunused-parameter"
+#   pragma clang diagnostic ignored "-Wunused-label"
+#   pragma clang diagnostic ignored "-Wunused-macros"
+#elif defined(__GNUC__)
+#   if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6)
+#   pragma GCC diagnostic push
+#   endif
+#   pragma GCC diagnostic ignored "-Wunused-function"
+#   pragma GCC diagnostic ignored "-Wunused-parameter"
+#   pragma GCC diagnostic ignored "-Wunused-label"
+#   pragma GCC diagnostic ignored "-Wunused-macros"
+#elif defined(_MSC_VER)
+#   pragma warning(push)
+#   pragma warning(disable:4100) /* unreferenced formal parameter */
+#   pragma warning(disable:4102) /* unreferenced label */
+#   pragma warning(disable:4127) /* conditional expression is constant */
+#   pragma warning(disable:4706) /* assignment within conditional expression */
+#endif
+
+/* version, same as YYJSON_VERSION_HEX */
+yyjson_api uint32_t yyjson_version(void) {
+    return YYJSON_VERSION_HEX;
+}
+
+
+
+/*==============================================================================
+ * Flags
+ *============================================================================*/
+
+/* gcc version check */
+#if defined(__GNUC__)
+#   if defined(__GNUC_MINOR__) && defined(__GNUC_PATCHLEVEL__)
+#       define yyjson_gcc_available(major, minor, patch) \
+            ((__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__) \
+            >= (major * 10000 + minor * 100 + patch))
+#   elif defined(__GNUC_MINOR__)
+#       define yyjson_gcc_available(major, minor, patch) \
+            ((__GNUC__ * 10000 + __GNUC_MINOR__ * 100) \
+            >= (major * 10000 + minor * 100 + patch))
+#   else
+#       define yyjson_gcc_available(major, minor, patch) \
+            ((__GNUC__ * 10000) >= (major * 10000 + minor * 100 + patch))
+#   endif
+#else
+#       define yyjson_gcc_available(major, minor, patch) 0
+#endif
+
+/* real gcc check */
+#if !defined(__clang__) && !defined(__INTEL_COMPILER) && !defined(__ICC) && \
+    defined(__GNUC__) && defined(__GNUC_MINOR__)
+#   define YYJSON_IS_REAL_GCC 1
+#else
+#   define YYJSON_IS_REAL_GCC 0
+#endif
+
+/* msvc intrinsic */
+#if YYJSON_MSC_VER >= 1400
+#   include <intrin.h>
+#   if defined(_M_AMD64) || defined(_M_ARM64)
+#       define MSC_HAS_BIT_SCAN_64 1
+#       pragma intrinsic(_BitScanForward64)
+#       pragma intrinsic(_BitScanReverse64)
+#   else
+#       define MSC_HAS_BIT_SCAN_64 0
+#   endif
+#   if defined(_M_AMD64) || defined(_M_ARM64) || \
+        defined(_M_IX86) || defined(_M_ARM)
+#       define MSC_HAS_BIT_SCAN 1
+#       pragma intrinsic(_BitScanForward)
+#       pragma intrinsic(_BitScanReverse)
+#   else
+#       define MSC_HAS_BIT_SCAN 0
+#   endif
+#   if defined(_M_AMD64)
+#       define MSC_HAS_UMUL128 1
+#       pragma intrinsic(_umul128)
+#   else
+#       define MSC_HAS_UMUL128 0
+#   endif
+#else
+#   define MSC_HAS_BIT_SCAN_64 0
+#   define MSC_HAS_BIT_SCAN 0
+#   define MSC_HAS_UMUL128 0
+#endif
+
+/* gcc builtin */
+#if yyjson_has_builtin(__builtin_clzll) || yyjson_gcc_available(3, 4, 0)
+#   define GCC_HAS_CLZLL 1
+#else
+#   define GCC_HAS_CLZLL 0
+#endif
+
+#if yyjson_has_builtin(__builtin_ctzll) || yyjson_gcc_available(3, 4, 0)
+#   define GCC_HAS_CTZLL 1
+#else
+#   define GCC_HAS_CTZLL 0
+#endif
+
+/* int128 type */
+#if defined(__SIZEOF_INT128__) && (__SIZEOF_INT128__ == 16) && \
+    (defined(__GNUC__) || defined(__clang__) || defined(__INTEL_COMPILER))
+#    define YYJSON_HAS_INT128 1
+#else
+#    define YYJSON_HAS_INT128 0
+#endif
+
+/* IEEE 754 floating-point binary representation */
+#if defined(__STDC_IEC_559__) || defined(__STDC_IEC_60559_BFP__)
+#   define YYJSON_HAS_IEEE_754 1
+#elif (FLT_RADIX == 2) && (DBL_MANT_DIG == 53) && (DBL_DIG == 15) && \
+     (DBL_MIN_EXP == -1021) && (DBL_MAX_EXP == 1024) && \
+     (DBL_MIN_10_EXP == -307) && (DBL_MAX_10_EXP == 308)
+#   define YYJSON_HAS_IEEE_754 1
+#else
+#   define YYJSON_HAS_IEEE_754 0
+#endif
+
+/*
+ Correct rounding in double number computations.
+ 
+ On the x86 architecture, some compilers may use x87 FPU instructions for
+ floating-point arithmetic. The x87 FPU loads all floating point number as
+ 80-bit double-extended precision internally, then rounds the result to original
+ precision, which may produce inaccurate results. For a more detailed
+ explanation, see the paper: https://arxiv.org/abs/cs/0701192
+ 
+ Here are some examples of double precision calculation error:
+ 
+     2877.0 / 1e6   == 0.002877,  but x87 returns 0.0028770000000000002
+     43683.0 * 1e21 == 4.3683e25, but x87 returns 4.3683000000000004e25
+ 
+ Here are some examples of compiler flags to generate x87 instructions on x86:
+ 
+     clang -m32 -mno-sse
+     gcc/icc -m32 -mfpmath=387
+     msvc /arch:SSE or /arch:IA32
+ 
+ If we are sure that there's no similar error described above, we can define the
+ YYJSON_DOUBLE_MATH_CORRECT as 1 to enable the fast path calculation. This is
+ not an accurate detection, it's just try to avoid the error at compiler time.
+ An accurate detection can be done at runtime:
+ 
+     bool is_double_math_correct(void) {
+         volatile double r = 43683.0;
+         r *= 1e21;
+         return r == 4.3683e25;
+     }
+ 
+ */
+#if !defined(FLT_EVAL_METHOD) && defined(__FLT_EVAL_METHOD__)
+#    define FLT_EVAL_METHOD __FLT_EVAL_METHOD__
+#endif
+
+#if defined(FLT_EVAL_METHOD) && FLT_EVAL_METHOD != 0 && FLT_EVAL_METHOD != 1
+#    define YYJSON_DOUBLE_MATH_CORRECT 0
+#elif defined(i386) || defined(__i386) || defined(__i386__) || \
+    defined(_X86_) || defined(__X86__) || defined(_M_IX86) || \
+    defined(__I86__) || defined(__IA32__) || defined(__THW_INTEL)
+#   if (defined(_MSC_VER) && defined(_M_IX86_FP) && _M_IX86_FP == 2) || \
+        (defined(__SSE2_MATH__) && __SSE2_MATH__)
+#       define YYJSON_DOUBLE_MATH_CORRECT 1
+#   else
+#       define YYJSON_DOUBLE_MATH_CORRECT 0
+#   endif
+#elif defined(__x86_64) || defined(__x86_64__) || \
+    defined(__amd64) || defined(__amd64__) || \
+    defined(_M_AMD64) || defined(_M_X64) || \
+    defined(__ia64) || defined(_IA64) || defined(__IA64__) ||  \
+    defined(__ia64__) || defined(_M_IA64) || defined(__itanium__) || \
+    defined(__arm64) || defined(__arm64__) || \
+    defined(__aarch64__) || defined(_M_ARM64) || \
+    defined(__arm) || defined(__arm__) || defined(_ARM_) || \
+    defined(_ARM) || defined(_M_ARM) || defined(__TARGET_ARCH_ARM) || \
+    defined(mips) || defined(__mips) || defined(__mips__) || \
+    defined(MIPS) || defined(_MIPS_) || defined(__MIPS__) || \
+    defined(_ARCH_PPC64) || defined(__PPC64__) || \
+    defined(__ppc64__) || defined(__powerpc64__) || \
+    defined(__powerpc) || defined(__powerpc__) || defined(__POWERPC__) || \
+    defined(__ppc__) || defined(__ppc) || defined(__PPC__) || \
+    defined(__sparcv9) || defined(__sparc_v9__) || \
+    defined(__sparc) || defined(__sparc__) || defined(__sparc64__) || \
+    defined(__alpha) || defined(__alpha__) || defined(_M_ALPHA) || \
+    defined(__or1k__) || defined(__OR1K__) || defined(OR1K) || \
+    defined(__hppa) || defined(__hppa__) || defined(__HPPA__) || \
+    defined(__riscv) || defined(__riscv__) || \
+    defined(__s390__) || defined(__avr32__) || defined(__SH4__) || \
+    defined(__e2k__) || defined(__arc__) || defined(__loongarch__) || \
+    defined(__EMSCRIPTEN__) || defined(__wasm__)
+#   define YYJSON_DOUBLE_MATH_CORRECT 1
+#else
+#   define YYJSON_DOUBLE_MATH_CORRECT 0 /* unknown, disable fast path */
+#endif
+
+/*
+ Microsoft Visual C++ 6.0 doesn't support converting number from u64 to f64:
+ error C2520: conversion from unsigned __int64 to double not implemented.
+ */
+#ifndef YYJSON_U64_TO_F64_NO_IMPL
+#   if (0 < YYJSON_MSC_VER) && (YYJSON_MSC_VER <= 1200)
+#       define YYJSON_U64_TO_F64_NO_IMPL 1
+#   else
+#       define YYJSON_U64_TO_F64_NO_IMPL 0
+#   endif
+#endif
+
+/* endian */
+#if yyjson_has_include(<sys/types.h>)
+#    include <sys/types.h>
+#endif
+
+#if yyjson_has_include(<endian.h>)
+#    include <endian.h>
+#elif yyjson_has_include(<machine/endian.h>)
+#    include <machine/endian.h>
+#elif yyjson_has_include(<sys/endian.h>)
+#    include <sys/endian.h>
+#endif
+
+#define YYJSON_BIG_ENDIAN       4321
+#define YYJSON_LITTLE_ENDIAN    1234
+
+#if defined(__BYTE_ORDER__) && __BYTE_ORDER__
+#   if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+#       define YYJSON_ENDIAN YYJSON_BIG_ENDIAN
+#   elif __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
+#       define YYJSON_ENDIAN YYJSON_LITTLE_ENDIAN
+#   endif
+
+#elif defined(__BYTE_ORDER) && __BYTE_ORDER
+#   if __BYTE_ORDER == __BIG_ENDIAN
+#       define YYJSON_ENDIAN YYJSON_BIG_ENDIAN
+#   elif __BYTE_ORDER == __LITTLE_ENDIAN
+#       define YYJSON_ENDIAN YYJSON_LITTLE_ENDIAN
+#   endif
+
+#elif defined(BYTE_ORDER) && BYTE_ORDER
+#   if BYTE_ORDER == BIG_ENDIAN
+#       define YYJSON_ENDIAN YYJSON_BIG_ENDIAN
+#   elif BYTE_ORDER == LITTLE_ENDIAN
+#       define YYJSON_ENDIAN YYJSON_LITTLE_ENDIAN
+#   endif
+
+#elif (defined(__LITTLE_ENDIAN__) && __LITTLE_ENDIAN__ == 1) || \
+    defined(__i386) || defined(__i386__) || \
+    defined(_X86_) || defined(__X86__) || \
+    defined(_M_IX86) || defined(__THW_INTEL__) || \
+    defined(__x86_64) || defined(__x86_64__) || \
+    defined(__amd64) || defined(__amd64__) || \
+    defined(_M_AMD64) || defined(_M_X64) || \
+    defined(__ia64) || defined(_IA64) || defined(__IA64__) ||  \
+    defined(__ia64__) || defined(_M_IA64) || defined(__itanium__) || \
+    defined(__ARMEL__) || defined(__THUMBEL__) || defined(__AARCH64EL__) || \
+    defined(__alpha) || defined(__alpha__) || defined(_M_ALPHA) || \
+    defined(__riscv) || defined(__riscv__) || \
+    defined(_MIPSEL) || defined(__MIPSEL) || defined(__MIPSEL__) || \
+    defined(__EMSCRIPTEN__) || defined(__wasm__)
+#   define YYJSON_ENDIAN YYJSON_LITTLE_ENDIAN
+
+#elif (defined(__BIG_ENDIAN__) && __BIG_ENDIAN__ == 1) || \
+    defined(__ARMEB__) || defined(__THUMBEB__) || defined(__AARCH64EB__) || \
+    defined(_MIPSEB) || defined(__MIPSEB) || defined(__MIPSEB__) || \
+    defined(_ARCH_PPC) || defined(_ARCH_PPC64) || \
+    defined(__ppc) || defined(__ppc__) || \
+    defined(__sparc) || defined(__sparc__) || defined(__sparc64__) || \
+    defined(__or1k__) || defined(__OR1K__)
+#   define YYJSON_ENDIAN YYJSON_BIG_ENDIAN
+
+#else
+#   define YYJSON_ENDIAN 0 /* unknown endian, detect at runtime */
+#endif
+
+/*
+ Unaligned memory access detection.
+ 
+ Some architectures are unable to perform unaligned memory accesses, and the
+ unaligned access may cause processor exceptions.
+ 
+ Modern compilers can make some optimizations for unaligned access.
+ For example: https://godbolt.org/z/Ejo3Pa
+ 
+    typedef struct { char c[2] } vec2;
+    void copy_vec2(vec2 *dst, vec2 *src) {
+        *dst = *src;
+    }
+ 
+ Compiler may generate `load/store` or `move` instruction if target architecture
+ supports unaligned access, otherwise it may generate `call memcpy` instruction.
+ 
+ We want to avoid `memcpy` calls, so we should disable unaligned access by
+ define `YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS` as 1 on these architectures.
+ */
+#ifndef YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+#   if defined(i386) || defined(__i386) || defined(__i386__) || \
+        defined(__i486__) || defined(__i586__) || defined(__i686__) || \
+        defined(_X86_) || defined(__X86__) || defined(_M_IX86) || \
+        defined(__I86__) || defined(__IA32__) || \
+        defined(__THW_INTEL) || defined(__THW_INTEL__) || \
+        defined(__x86_64) || defined(__x86_64__) || \
+        defined(__amd64) || defined(__amd64__) || \
+        defined(_M_AMD64) || defined(_M_X64)
+#       define YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS 0 /* x86 */
+
+#   elif defined(__ia64) || defined(_IA64) || defined(__IA64__) ||  \
+        defined(__ia64__) || defined(_M_IA64) || defined(__itanium__)
+#       define YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS 1 /* Itanium */
+
+#   elif defined(__arm64) || defined(__arm64__) || \
+        defined(__AARCH64EL__) || defined(__AARCH64EB__) || \
+        defined(__aarch64__) || defined(_M_ARM64)
+#       define YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS 0 /* ARM64 */
+
+#   elif defined(__ARM_ARCH_4__) || defined(__ARM_ARCH_4T__) || \
+        defined(__ARM_ARCH_5TEJ__) || defined(__ARM_ARCH_5TE__) || \
+        defined(__ARM_ARCH_6T2__) || defined(__ARM_ARCH_6KZ__) || \
+        defined(__ARM_ARCH_6Z__) || defined(__ARM_ARCH_6K__)
+#       define YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS 1 /* ARM */
+
+#   elif defined(__ppc64__) || defined(__PPC64__) || \
+        defined(__powerpc64__) || defined(_ARCH_PPC64) || \
+        defined(__ppc) || defined(__ppc__) || defined(__PPC__) || \
+        defined(__powerpc) || defined(__powerpc__) || defined(__POWERPC__) || \
+        defined(_ARCH_PPC) || defined(_M_PPC) || \
+        defined(__PPCGECKO__) || defined(__PPCBROADWAY__) || defined(_XENON)
+#       define YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS 0 /* PowerPC */
+
+#   else
+#       define YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS 0 /* Unknown */
+#   endif
+
+#endif
+
+/* Some estimated initial ratio of the JSON data (data_size / value_count).
+   These values are used to avoid frequent memory allocation calls. */
+#define YYJSON_READER_ESTIMATED_PRETTY_RATIO 16
+#define YYJSON_READER_ESTIMATED_MINIFY_RATIO 6
+#define YYJSON_WRITER_ESTIMATED_PRETTY_RATIO 32
+#define YYJSON_WRITER_ESTIMATED_MINIFY_RATIO 18
+
+/* Default value for public flags. */
+#ifndef YYJSON_DISABLE_READER
+#define YYJSON_DISABLE_READER 0
+#endif
+#ifndef YYJSON_DISABLE_WRITER
+#define YYJSON_DISABLE_WRITER 0
+#endif
+#ifndef YYJSON_DISABLE_FAST_FP_CONV
+#define YYJSON_DISABLE_FAST_FP_CONV 0
+#endif
+#ifndef YYJSON_DISABLE_NON_STANDARD
+#define YYJSON_DISABLE_NON_STANDARD 0
+#endif
+
+
+
+/*==============================================================================
+ * Macros
+ *============================================================================*/
+
+/* Macros used for loop unrolling and other purpose. */
+#define repeat2(x)  { x x }
+#define repeat3(x)  { x x x }
+#define repeat4(x)  { x x x x }
+#define repeat8(x)  { x x x x x x x x }
+#define repeat16(x) { x x x x x x x x x x x x x x x x }
+
+#define repeat4_incr(x)  { x(0) x(1) x(2) x(3) }
+
+#define repeat8_incr(x)  { x(0) x(1) x(2) x(3) x(4) x(5) x(6) x(7) }
+
+#define repeat16_incr(x) { x(0) x(1) x(2) x(3) x(4) x(5) x(6) x(7) \
+                           x(8) x(9) x(10) x(11) x(12) x(13) x(14) x(15) }
+
+#define repeat_in_1_18(x) { x(1) x(2) x(3) x(4) x(5) x(6) x(7) \
+                            x(8) x(9) x(10) x(11) x(12) x(13) x(14) x(15) \
+                            x(16) x(17) x(18) }
+
+/* Macros used to provide branch prediction information for compiler. */
+#undef  likely
+#define likely(x)       yyjson_likely(x)
+#undef  unlikely
+#define unlikely(x)     yyjson_unlikely(x)
+
+/* Macros used to provide inline information for compiler. */
+#undef  static_inline
+#define static_inline   static yyjson_inline
+#undef  static_noinline
+#define static_noinline static yyjson_noinline
+
+/* Macros for min and max. */
+#undef  yyjson_min
+#define yyjson_min(x, y) ((x) < (y) ? (x) : (y))
+#undef  yyjson_max
+#define yyjson_max(x, y) ((x) > (y) ? (x) : (y))
+
+/* Used to write u64 literal for C89 which doesn't support "ULL" suffix. */
+#undef  U64
+#define U64(hi, lo) ((((u64)hi##UL) << 32U) + lo##UL)
+
+
+
+/*==============================================================================
+ * Integer Constants
+ *============================================================================*/
+
+/* U64 constant values */
+#undef  U64_MAX
+#define U64_MAX         U64(0xFFFFFFFF, 0xFFFFFFFF)
+#undef  I64_MAX
+#define I64_MAX         U64(0x7FFFFFFF, 0xFFFFFFFF)
+#undef  USIZE_MAX
+#define USIZE_MAX       ((usize)(~(usize)0))
+
+/* Maximum number of digits for reading u64 safety. */
+#undef  U64_SAFE_DIG
+#define U64_SAFE_DIG    19
+
+
+
+/*==============================================================================
+ * IEEE-754 Double Number Constants
+ *============================================================================*/
+
+/* Inf raw value (positive) */
+#define F64_RAW_INF U64(0x7FF00000, 0x00000000)
+
+/* NaN raw value (positive, without payload) */
+#define F64_RAW_NAN U64(0x7FF80000, 0x00000000)
+
+/* double number bits */
+#define F64_BITS 64
+
+/* double number exponent part bits */
+#define F64_EXP_BITS 11
+
+/* double number significand part bits */
+#define F64_SIG_BITS 52
+
+/* double number significand part bits (with 1 hidden bit) */
+#define F64_SIG_FULL_BITS 53
+
+/* double number significand bit mask */
+#define F64_SIG_MASK U64(0x000FFFFF, 0xFFFFFFFF)
+
+/* double number exponent bit mask */
+#define F64_EXP_MASK U64(0x7FF00000, 0x00000000)
+
+/* double number exponent bias */
+#define F64_EXP_BIAS 1023
+
+/* double number significant digits count in decimal */
+#define F64_DEC_DIG 17
+
+/* max significant digits count in decimal when reading double number */
+#define F64_MAX_DEC_DIG 768
+
+/* maximum decimal power of double number (1.7976931348623157e308) */
+#define F64_MAX_DEC_EXP 308
+
+/* minimum decimal power of double number (4.9406564584124654e-324) */
+#define F64_MIN_DEC_EXP (-324)
+
+/* maximum binary power of double number */
+#define F64_MAX_BIN_EXP 1024
+
+/* minimum binary power of double number */
+#define F64_MIN_BIN_EXP (-1021)
+
+
+
+/*==============================================================================
+ * Types
+ *============================================================================*/
+
+/** Type define for primitive types. */
+typedef float       f32;
+typedef double      f64;
+typedef int8_t      i8;
+typedef uint8_t     u8;
+typedef int16_t     i16;
+typedef uint16_t    u16;
+typedef int32_t     i32;
+typedef uint32_t    u32;
+typedef int64_t     i64;
+typedef uint64_t    u64;
+typedef size_t      usize;
+
+/** 128-bit integer, used by floating-point number reader and writer. */
+#if YYJSON_HAS_INT128
+__extension__ typedef __int128          i128;
+__extension__ typedef unsigned __int128 u128;
+#endif
+
+/** 16/32/64-bit vector */
+typedef struct v16 { char c1, c2; } v16;
+typedef struct v32 { char c1, c2, c3, c4; } v32;
+typedef struct v64 { char c1, c2, c3, c4, c5, c6, c7, c8; } v64;
+
+/** 16/32/64-bit vector union, used for unaligned memory access on modern CPU */
+typedef union v16_uni { v16 v; u16 u; } v16_uni;
+typedef union v32_uni { v32 v; u32 u; } v32_uni;
+typedef union v64_uni { v64 v; u64 u; } v64_uni;
+
+
+
+/*==============================================================================
+ * Character Utils
+ *============================================================================*/
+
+static_inline void byte_move_2(void *dst, const void *src) {
+#if YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+    ((u8 *)dst)[0] = ((u8 *)src)[0];
+    ((u8 *)dst)[1] = ((u8 *)src)[1];
+#else
+    memmove(dst, src, 2);
+#endif
+}
+
+static_inline void byte_move_4(void *dst, const void *src) {
+#if YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+    ((u8 *)dst)[0] = ((u8 *)src)[0];
+    ((u8 *)dst)[1] = ((u8 *)src)[1];
+    ((u8 *)dst)[2] = ((u8 *)src)[2];
+    ((u8 *)dst)[3] = ((u8 *)src)[3];
+#else
+    memmove(dst, src, 4);
+#endif
+}
+
+static_inline void byte_move_8(void *dst, const void *src) {
+#if YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+    ((u8 *)dst)[0] = ((u8 *)src)[0];
+    ((u8 *)dst)[1] = ((u8 *)src)[1];
+    ((u8 *)dst)[2] = ((u8 *)src)[2];
+    ((u8 *)dst)[3] = ((u8 *)src)[3];
+    ((u8 *)dst)[4] = ((u8 *)src)[4];
+    ((u8 *)dst)[5] = ((u8 *)src)[5];
+    ((u8 *)dst)[6] = ((u8 *)src)[6];
+    ((u8 *)dst)[7] = ((u8 *)src)[7];
+#else
+    memmove(dst, src, 8);
+#endif
+}
+
+static_inline void byte_move_16(void *dst, const void *src) {
+#if YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+    ((u8 *)dst)[0] = ((u8 *)src)[0];
+    ((u8 *)dst)[1] = ((u8 *)src)[1];
+    ((u8 *)dst)[2] = ((u8 *)src)[2];
+    ((u8 *)dst)[3] = ((u8 *)src)[3];
+    ((u8 *)dst)[4] = ((u8 *)src)[4];
+    ((u8 *)dst)[5] = ((u8 *)src)[5];
+    ((u8 *)dst)[6] = ((u8 *)src)[6];
+    ((u8 *)dst)[7] = ((u8 *)src)[7];
+    ((u8 *)dst)[8] = ((u8 *)src)[8];
+    ((u8 *)dst)[9] = ((u8 *)src)[9];
+    ((u8 *)dst)[10] = ((u8 *)src)[10];
+    ((u8 *)dst)[11] = ((u8 *)src)[11];
+    ((u8 *)dst)[12] = ((u8 *)src)[12];
+    ((u8 *)dst)[13] = ((u8 *)src)[13];
+    ((u8 *)dst)[14] = ((u8 *)src)[14];
+    ((u8 *)dst)[15] = ((u8 *)src)[15];
+#else
+    memmove(dst, src, 16);
+#endif
+}
+
+static_inline bool byte_match_1(void *buf, const char *pat) {
+    return *(u8 *)buf == (u8)*pat;
+}
+
+static_inline bool byte_match_2(void *buf, const char *pat) {
+#if YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+    return
+    ((u8 *)buf)[0] == ((const u8 *)pat)[0] &&
+    ((u8 *)buf)[1] == ((const u8 *)pat)[1];
+#else
+    v16_uni u1, u2;
+    u1.v = *(const v16 *)pat;
+    u2.v = *(const v16 *)buf;
+    return u1.u == u2.u;
+#endif
+}
+
+static_inline bool byte_match_4(void *buf, const char *pat) {
+#if YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+    return
+    ((u8 *)buf)[0] == ((const u8 *)pat)[0] &&
+    ((u8 *)buf)[1] == ((const u8 *)pat)[1] &&
+    ((u8 *)buf)[2] == ((const u8 *)pat)[2] &&
+    ((u8 *)buf)[3] == ((const u8 *)pat)[3];
+#else
+    v32_uni u1, u2;
+    u1.v = *(const v32 *)pat;
+    u2.v = *(const v32 *)buf;
+    return u1.u == u2.u;
+#endif
+}
+
+static_inline u32 byte_load_4(void *src) {
+    v32_uni uni;
+    uni.v = *(v32 *)src;
+    return uni.u;
+}
+
+/** Compound Literals, C99 only. */
+#if YYJSON_STDC_VER >= 199901L && !defined(__cplusplus)
+
+#define v16_make(c1, c2) \
+    ((v16){ c1, c2 })
+
+#define v32_make(c1, c2, c3, c4) \
+    ((v32){ c1, c2, c3, c4 })
+
+#define v64_make(c1, c2, c3, c4, c5, c6, c7, c8) \
+    ((v64){ c1, c2, c3, c4, c5, c6, c7, c8 })
+
+#else
+
+static_inline v16 v16_make(char c1, char c2) {
+    v16 v;
+    v.c1 = c1;
+    v.c2 = c2;
+    return v;
+}
+
+static_inline v32 v32_make(char c1, char c2, char c3, char c4) {
+    v32 v;
+    v.c1 = c1;
+    v.c2 = c2;
+    v.c3 = c3;
+    v.c4 = c4;
+    return v;
+}
+
+static_inline v64 v64_make(char c1, char c2, char c3, char c4,
+                           char c5, char c6, char c7, char c8) {
+    v64 v;
+    v.c1 = c1;
+    v.c2 = c2;
+    v.c3 = c3;
+    v.c4 = c4;
+    v.c5 = c5;
+    v.c6 = c6;
+    v.c7 = c7;
+    v.c8 = c8;
+    return v;
+}
+
+#endif
+
+
+
+/*==============================================================================
+ * Number Utils
+ * These functions are used to detect and convert NaN and Inf numbers.
+ *============================================================================*/
+
+/**
+ This union is used to avoid violating the strict aliasing rule in C.
+ `memcpy` can be used in both C and C++, but it may reduce performance without
+ compiler optimization.
+ */
+typedef union { u64 u; f64 f; } f64_uni;
+
+/** Convert raw binary to double. */
+static_inline f64 f64_from_raw(u64 u) {
+#ifndef __cplusplus
+    f64_uni uni;
+    uni.u = u;
+    return uni.f;
+#else
+    f64 f;
+    memcpy(&f, &u, 8);
+    return f;
+#endif
+}
+
+/** Convert double to raw binary. */
+static_inline u64 f64_to_raw(f64 f) {
+#ifndef __cplusplus
+    f64_uni uni;
+    uni.f = f;
+    return uni.u;
+#else
+    u64 u;
+    memcpy(&u, &f, 8);
+    return u;
+#endif
+}
+
+/** Get raw 'infinity' with sign. */
+static_inline u64 f64_raw_get_inf(bool sign) {
+#if YYJSON_HAS_IEEE_754
+    return F64_RAW_INF | ((u64)sign << 63);
+#elif defined(INFINITY)
+    return f64_to_raw(sign ? -INFINITY : INFINITY);
+#else
+    return f64_to_raw(sign ? -HUGE_VAL : HUGE_VAL);
+#endif
+}
+
+/** Get raw 'nan' with sign. */
+static_inline u64 f64_raw_get_nan(bool sign) {
+#if YYJSON_HAS_IEEE_754
+    return F64_RAW_NAN | ((u64)sign << 63);
+#elif defined(NAN)
+    return f64_to_raw(sign ? (f64)-NAN : (f64)NAN);
+#else
+    return f64_to_raw((sign ? -0.0 : 0.0) / 0.0);
+#endif
+}
+
+/**
+ Convert normalized u64 (highest bit is 1) to f64 raw.
+ 
+ Some compiler (such as Microsoft Visual C++ 6.0) do not support converting
+ number from u64 to f64. This function will first convert u64 to i64 and then
+ to f64, with `to nearest` rounding mode.
+ */
+static_inline f64 normalized_u64_to_f64(u64 val) {
+#if YYJSON_U64_TO_F64_NO_IMPL
+    i64 sig = (i64)((val >> 1) | (val & 1));
+    return ((f64)sig) * (f64)2.0;
+#else
+    return (f64)val;
+#endif
+}
+
+
+
+/*==============================================================================
+ * Size Utils
+ * These functions are used for memory allocation.
+ *============================================================================*/
+
+/** Returns whether the size is overflow after increment. */
+static_inline bool size_add_is_overflow(usize size, usize add) {
+    usize val = size + add;
+    return (val < size) | (val < add);
+}
+
+/** Returns whether the size is power of 2 (size should not be 0). */
+static_inline bool size_is_pow2(usize size) {
+    return (size & (size - 1)) == 0;
+}
+
+/** Align size upwards (may overflow). */
+static_inline usize size_align_up(usize size, usize align) {
+    if (size_is_pow2(align)) {
+        return (size + (align - 1)) & ~(align - 1);
+    } else {
+        return size + align - (size + align - 1) % align - 1;
+    }
+}
+
+/** Align size downwards. */
+static_inline usize size_align_down(usize size, usize align) {
+    if (size_is_pow2(align)) {
+        return size & ~(align - 1);
+    } else {
+        return size - (size % align);
+    }
+}
+
+/** Align address upwards (may overflow). */
+static_inline void *mem_align_up(void *mem, usize align) {
+    usize size;
+    memcpy(&size, &mem, sizeof(usize));
+    size = size_align_up(size, align);
+    memcpy(&mem, &size, sizeof(usize));
+    return mem;
+}
+
+/** Align address downwards. */
+static_inline void *mem_align_down(void *mem, usize align) {
+    usize size;
+    memcpy(&size, &mem, sizeof(usize));
+    size = size_align_down(size, align);
+    memcpy(&mem, &size, sizeof(usize));
+    return mem;
+}
+
+
+
+/*==============================================================================
+ * Bits Utils
+ * These functions are used by the floating-point number reader and writer.
+ *============================================================================*/
+
+/** Returns the number of leading 0-bits in value (input should not be 0). */
+static_inline u32 u64_lz_bits(u64 v) {
+#if GCC_HAS_CLZLL
+    return (u32)__builtin_clzll(v);
+#elif MSC_HAS_BIT_SCAN_64
+    unsigned long r;
+    _BitScanReverse64(&r, v);
+    return (u32)63 - (u32)r;
+#elif MSC_HAS_BIT_SCAN
+    unsigned long hi, lo;
+    bool hi_set = _BitScanReverse(&hi, (u32)(v >> 32)) != 0;
+    _BitScanReverse(&lo, (u32)v);
+    hi |= 32;
+    return (u32)63 - (u32)(hi_set ? hi : lo);
+#else
+    /* branchless, use de Bruijn sequences */
+    const u8 table[64] = {
+        63, 16, 62,  7, 15, 36, 61,  3,  6, 14, 22, 26, 35, 47, 60,  2,
+         9,  5, 28, 11, 13, 21, 42, 19, 25, 31, 34, 40, 46, 52, 59,  1,
+        17,  8, 37,  4, 23, 27, 48, 10, 29, 12, 43, 20, 32, 41, 53, 18,
+        38, 24, 49, 30, 44, 33, 54, 39, 50, 45, 55, 51, 56, 57, 58,  0
+    };
+    v |= v >> 1;
+    v |= v >> 2;
+    v |= v >> 4;
+    v |= v >> 8;
+    v |= v >> 16;
+    v |= v >> 32;
+    return table[(v * U64(0x03F79D71, 0xB4CB0A89)) >> 58];
+#endif
+}
+
+/** Returns the number of trailing 0-bits in value (input should not be 0). */
+static_inline u32 u64_tz_bits(u64 v) {
+#if GCC_HAS_CTZLL
+    return (u32)__builtin_ctzll(v);
+#elif MSC_HAS_BIT_SCAN_64
+    unsigned long r;
+    _BitScanForward64(&r, v);
+    return (u32)r;
+#elif MSC_HAS_BIT_SCAN
+    unsigned long lo, hi;
+    bool lo_set = _BitScanForward(&lo, (u32)(v)) != 0;
+    _BitScanForward(&hi, (u32)(v >> 32));
+    hi += 32;
+    return lo_set ? lo : hi;
+#else
+    /* branchless, use de Bruijn sequences */
+    const u8 table[64] = {
+         0,  1,  2, 53,  3,  7, 54, 27,  4, 38, 41,  8, 34, 55, 48, 28,
+        62,  5, 39, 46, 44, 42, 22,  9, 24, 35, 59, 56, 49, 18, 29, 11,
+        63, 52,  6, 26, 37, 40, 33, 47, 61, 45, 43, 21, 23, 58, 17, 10,
+        51, 25, 36, 32, 60, 20, 57, 16, 50, 31, 19, 15, 30, 14, 13, 12
+    };
+    return table[((v & (~v + 1)) * U64(0x022FDD63, 0xCC95386D)) >> 58];
+#endif
+}
+
+
+
+/*==============================================================================
+ * 128-bit Integer Utils
+ * These functions are used by the floating-point number reader and writer.
+ *============================================================================*/
+
+/** Multiplies two 64-bit unsigned integers (a * b),
+    returns the 128-bit result as 'hi' and 'lo'. */
+static_inline void u128_mul(u64 a, u64 b, u64 *hi, u64 *lo) {
+#if YYJSON_HAS_INT128
+    u128 m = (u128)a * b;
+    *hi = (u64)(m >> 64);
+    *lo = (u64)(m);
+#elif MSC_HAS_UMUL128
+    *lo = _umul128(a, b, hi);
+#else
+    u32 a0 = (u32)(a), a1 = (u32)(a >> 32);
+    u32 b0 = (u32)(b), b1 = (u32)(b >> 32);
+    u64 p00 = (u64)a0 * b0, p01 = (u64)a0 * b1;
+    u64 p10 = (u64)a1 * b0, p11 = (u64)a1 * b1;
+    u64 m0 = p01 + (p00 >> 32);
+    u32 m00 = (u32)(m0), m01 = (u32)(m0 >> 32);
+    u64 m1 = p10 + m00;
+    u32 m10 = (u32)(m1), m11 = (u32)(m1 >> 32);
+    *hi = p11 + m01 + m11;
+    *lo = ((u64)m10 << 32) | (u32)p00;
+#endif
+}
+
+/** Multiplies two 64-bit unsigned integers and add a value (a * b + c),
+    returns the 128-bit result as 'hi' and 'lo'. */
+static_inline void u128_mul_add(u64 a, u64 b, u64 c, u64 *hi, u64 *lo) {
+#if YYJSON_HAS_INT128
+    u128 m = (u128)a * b + c;
+    *hi = (u64)(m >> 64);
+    *lo = (u64)(m);
+#else
+    u64 h, l, t;
+    u128_mul(a, b, &h, &l);
+    t = l + c;
+    h += ((t < l) | (t < c));
+    *hi = h;
+    *lo = t;
+#endif
+}
+
+
+
+/*==============================================================================
+ * File Utils
+ * These functions are used to read and write JSON files.
+ *============================================================================*/
+
+#define YYJSON_FOPEN_EXT
+#if !defined(_MSC_VER) && defined(__GLIBC__) && defined(__GLIBC_PREREQ)
+#   if __GLIBC_PREREQ(2, 7)
+#       undef YYJSON_FOPEN_EXT
+#       define YYJSON_FOPEN_EXT "e" /* glibc extension to enable O_CLOEXEC */
+#   endif
+#endif
+
+static_inline FILE *fopen_safe(const char *path, const char *mode) {
+#if YYJSON_MSC_VER >= 1400
+    FILE *file = NULL;
+    if (fopen_s(&file, path, mode) != 0) return NULL;
+    return file;
+#else
+    return fopen(path, mode);
+#endif
+}
+
+static_inline FILE *fopen_readonly(const char *path) {
+    return fopen_safe(path, "rb" YYJSON_FOPEN_EXT);
+}
+
+static_inline FILE *fopen_writeonly(const char *path) {
+    return fopen_safe(path, "wb" YYJSON_FOPEN_EXT);
+}
+
+static_inline usize fread_safe(void *buf, usize size, FILE *file) {
+#if YYJSON_MSC_VER >= 1400
+    return fread_s(buf, size, 1, size, file);
+#else
+    return fread(buf, 1, size, file);
+#endif
+}
+
+
+
+/*==============================================================================
+ * Default Memory Allocator
+ * This is a simple libc memory allocator wrapper.
+ *============================================================================*/
+
+static void *default_malloc(void *ctx, usize size) {
+    return malloc(size);
+}
+
+static void *default_realloc(void *ctx, void *ptr, usize size) {
+    return realloc(ptr, size);
+}
+
+static void default_free(void *ctx, void *ptr) {
+    free(ptr);
+}
+
+static const yyjson_alc YYJSON_DEFAULT_ALC = {
+    default_malloc,
+    default_realloc,
+    default_free,
+    NULL
+};
+
+
+
+/*==============================================================================
+ * Pool Memory Allocator
+ * This is a simple memory allocator that uses linked list memory chunk.
+ * The following code will be executed only when the library user creates
+ * this allocator manually.
+ *============================================================================*/
+
+/** chunk header */
+typedef struct pool_chunk {
+    usize size; /* chunk memory size (include chunk header) */
+    struct pool_chunk *next;
+} pool_chunk;
+
+/** ctx header */
+typedef struct pool_ctx {
+    usize size; /* total memory size (include ctx header) */
+    pool_chunk *free_list;
+} pool_ctx;
+
+static void *pool_malloc(void *ctx_ptr, usize size) {
+    pool_ctx *ctx = (pool_ctx *)ctx_ptr;
+    pool_chunk *next, *prev = NULL, *cur = ctx->free_list;
+    
+    if (unlikely(size == 0 || size >= ctx->size)) return NULL;
+    size = size_align_up(size, sizeof(pool_chunk)) + sizeof(pool_chunk);
+    
+    while (cur) {
+        if (cur->size < size) {
+            /* not enough space, try next chunk */
+            prev = cur;
+            cur = cur->next;
+            continue;
+        }
+        if (cur->size >= size + sizeof(pool_chunk) * 2) {
+            /* too much space, split this chunk */
+            next = (pool_chunk *)(void *)((u8 *)cur + size);
+            next->size = cur->size - size;
+            next->next = cur->next;
+            cur->size = size;
+        } else {
+            /* just enough space, use whole chunk */
+            next = cur->next;
+        }
+        if (prev) prev->next = next;
+        else ctx->free_list = next;
+        return (void *)(cur + 1);
+    }
+    return NULL;
+}
+
+static void pool_free(void *ctx_ptr, void *ptr) {
+    pool_ctx *ctx = (pool_ctx *)ctx_ptr;
+    pool_chunk *cur = ((pool_chunk *)ptr) - 1;
+    pool_chunk *prev = NULL, *next = ctx->free_list;
+    
+    while (next && next < cur) {
+        prev = next;
+        next = next->next;
+    }
+    if (prev) prev->next = cur;
+    else ctx->free_list = cur;
+    cur->next = next;
+    
+    if (next && ((u8 *)cur + cur->size) == (u8 *)next) {
+        /* merge cur to higher chunk */
+        cur->size += next->size;
+        cur->next = next->next;
+    }
+    if (prev && ((u8 *)prev + prev->size) == (u8 *)cur) {
+        /* merge cur to lower chunk */
+        prev->size += cur->size;
+        prev->next = cur->next;
+    }
+}
+
+static void *pool_realloc(void *ctx_ptr, void *ptr, usize size) {
+    pool_ctx *ctx = (pool_ctx *)ctx_ptr;
+    pool_chunk *cur = ((pool_chunk *)ptr) - 1, *prev, *next, *tmp;
+    usize free_size;
+    void *new_ptr;
+    
+    if (unlikely(size == 0 || size >= ctx->size)) return NULL;
+    size = size_align_up(size, sizeof(pool_chunk)) + sizeof(pool_chunk);
+    
+    /* reduce size */
+    if (unlikely(size <= cur->size)) {
+        free_size = cur->size - size;
+        if (free_size >= sizeof(pool_chunk) * 2) {
+            tmp = (pool_chunk *)(void *)((u8 *)cur + cur->size - free_size);
+            tmp->size = free_size;
+            pool_free(ctx_ptr, (void *)(tmp + 1));
+            cur->size -= free_size;
+        }
+        return ptr;
+    }
+    
+    /* find next and prev chunk */
+    prev = NULL;
+    next = ctx->free_list;
+    while (next && next < cur) {
+        prev = next;
+        next = next->next;
+    }
+    
+    /* merge to higher chunk if they are contiguous */
+    if ((u8 *)cur + cur->size == (u8 *)next &&
+        cur->size + next->size >= size) {
+        free_size = cur->size + next->size - size;
+        if (free_size > sizeof(pool_chunk) * 2) {
+            tmp = (pool_chunk *)(void *)((u8 *)cur + size);
+            if (prev) prev->next = tmp;
+            else ctx->free_list = tmp;
+            tmp->next = next->next;
+            tmp->size = free_size;
+            cur->size = size;
+        } else {
+            if (prev) prev->next = next->next;
+            else ctx->free_list = next->next;
+            cur->size += next->size;
+        }
+        return ptr;
+    }
+    
+    /* fallback to malloc and memcpy */
+    new_ptr = pool_malloc(ctx_ptr, size - sizeof(pool_chunk));
+    if (new_ptr) {
+        memcpy(new_ptr, ptr, cur->size - sizeof(pool_chunk));
+        pool_free(ctx_ptr, ptr);
+    }
+    return new_ptr;
+}
+
+bool yyjson_alc_pool_init(yyjson_alc *alc, void *buf, usize size) {
+    pool_chunk *chunk;
+    pool_ctx *ctx;
+    
+    if (unlikely(!alc || size < sizeof(pool_ctx) * 4)) return false;
+    ctx = (pool_ctx *)mem_align_up(buf, sizeof(pool_ctx));
+    if (unlikely(!ctx)) return false;
+    size -= (usize)((u8 *)ctx - (u8 *)buf);
+    size = size_align_down(size, sizeof(pool_ctx));
+    
+    chunk = (pool_chunk *)(ctx + 1);
+    chunk->size = size - sizeof(pool_ctx);
+    chunk->next = NULL;
+    ctx->size = size;
+    ctx->free_list = chunk;
+    
+    alc->malloc = pool_malloc;
+    alc->realloc = pool_realloc;
+    alc->free = pool_free;
+    alc->ctx = (void *)ctx;
+    return true;
+}
+
+
+
+/*==============================================================================
+ * JSON document and value
+ *============================================================================*/
+
+static_inline void unsafe_yyjson_str_pool_release(yyjson_str_pool *pool,
+                                                  yyjson_alc *alc) {
+    yyjson_str_chunk *chunk = pool->chunks, *next;
+    while (chunk) {
+        next = chunk->next;
+        alc->free(alc->ctx, chunk);
+        chunk = next;
+    }
+}
+
+static_inline void unsafe_yyjson_val_pool_release(yyjson_val_pool *pool,
+                                                  yyjson_alc *alc) {
+    yyjson_val_chunk *chunk = pool->chunks, *next;
+    while (chunk) {
+        next = chunk->next;
+        alc->free(alc->ctx, chunk);
+        chunk = next;
+    }
+}
+
+bool unsafe_yyjson_str_pool_grow(yyjson_str_pool *pool,
+                                 yyjson_alc *alc, usize len) {
+    yyjson_str_chunk *chunk;
+    usize size = len + sizeof(yyjson_str_chunk);
+    size = yyjson_max(pool->chunk_size, size);
+    chunk = (yyjson_str_chunk *)alc->malloc(alc->ctx, size);
+    if (yyjson_unlikely(!chunk)) return false;
+    
+    chunk->next = pool->chunks;
+    pool->chunks = chunk;
+    pool->cur = (char *)chunk + sizeof(yyjson_str_chunk);
+    pool->end = (char *)chunk + size;
+    
+    size = yyjson_min(pool->chunk_size * 2, pool->chunk_size_max);
+    pool->chunk_size = size;
+    return true;
+}
+
+bool unsafe_yyjson_val_pool_grow(yyjson_val_pool *pool,
+                                 yyjson_alc *alc, usize count) {
+    yyjson_val_chunk *chunk;
+    usize size;
+    
+    if (count >= USIZE_MAX / sizeof(yyjson_mut_val) - 16) return false;
+    size = (count + 1) * sizeof(yyjson_mut_val);
+    size = yyjson_max(pool->chunk_size, size);
+    chunk = (yyjson_val_chunk *)alc->malloc(alc->ctx, size);
+    if (yyjson_unlikely(!chunk)) return false;
+    
+    chunk->next = pool->chunks;
+    pool->chunks = chunk;
+    pool->cur = (yyjson_mut_val *)(void *)((u8 *)chunk
+                                           + sizeof(yyjson_mut_val));
+    pool->end = (yyjson_mut_val *)(void *)((u8 *)chunk + size);
+    
+    size = yyjson_min(pool->chunk_size * 2, pool->chunk_size_max);
+    pool->chunk_size = size;
+    return true;
+}
+
+void yyjson_mut_doc_free(yyjson_mut_doc *doc) {
+    if (doc) {
+        yyjson_alc alc = doc->alc;
+        unsafe_yyjson_str_pool_release(&doc->str_pool, &alc);
+        unsafe_yyjson_val_pool_release(&doc->val_pool, &alc);
+        alc.free(alc.ctx, doc);
+    }
+}
+
+yyjson_mut_doc *yyjson_mut_doc_new(const yyjson_alc *alc) {
+    yyjson_mut_doc *doc;
+    if (!alc) alc = &YYJSON_DEFAULT_ALC;
+    doc = (yyjson_mut_doc *)alc->malloc(alc->ctx, sizeof(yyjson_mut_doc));
+    if (!doc) return NULL;
+    memset(doc, 0, sizeof(yyjson_mut_doc));
+    
+    doc->alc = *alc;
+    doc->str_pool.chunk_size = 0x100;
+    doc->str_pool.chunk_size_max = 0x10000000;
+    doc->val_pool.chunk_size = 0x10 * sizeof(yyjson_mut_val);
+    doc->val_pool.chunk_size_max = 0x1000000 * sizeof(yyjson_mut_val);
+    return doc;
+}
+
+yyjson_api yyjson_mut_doc *yyjson_doc_mut_copy(yyjson_doc *i_doc,
+                                               const yyjson_alc *alc) {
+    yyjson_mut_doc *m_doc;
+    yyjson_mut_val *m_val;
+    
+    if (!i_doc) return NULL;
+    m_doc = yyjson_mut_doc_new(alc);
+    if (!m_doc) return NULL;
+    m_val = yyjson_val_mut_copy(m_doc, i_doc->root);
+    if (!m_val) {
+        yyjson_mut_doc_free(m_doc);
+        return NULL;
+    }
+    yyjson_mut_doc_set_root(m_doc, m_val);
+    return m_doc;
+}
+
+yyjson_api yyjson_mut_val *yyjson_val_mut_copy(yyjson_mut_doc *m_doc,
+                                               yyjson_val *i_vals) {
+    /*
+     The immutable object or array stores all sub-value in a contiguous memory,
+     We copy them to another contiguous memory as mutable values,
+     then reconnect the mutable values with the original relationship.
+     */
+    
+    usize i_vals_len;
+    yyjson_mut_val *m_vals, *m_val;
+    yyjson_val *i_val, *i_end;
+    
+    if (!m_doc || !i_vals) return NULL;
+    i_end = unsafe_yyjson_get_next(i_vals);
+    i_vals_len = (usize)(unsafe_yyjson_get_next(i_vals) - i_vals);
+    m_vals = unsafe_yyjson_mut_val(m_doc, i_vals_len);
+    if (!m_vals) return NULL;
+    i_val = i_vals;
+    m_val = m_vals;
+    
+    for (; i_val < i_end; i_val++, m_val++) {
+        yyjson_type type = unsafe_yyjson_get_type(i_val);
+        m_val->tag = i_val->tag;
+        m_val->uni.u64 = i_val->uni.u64;
+        if (type == YYJSON_TYPE_STR) {
+            const char *str = i_val->uni.str;
+            usize str_len = unsafe_yyjson_get_len(i_val);
+            m_val->uni.str = unsafe_yyjson_mut_strncpy(m_doc, str, str_len);
+            if (!m_val->uni.str) return NULL;
+            
+        } else if (type == YYJSON_TYPE_ARR) {
+            usize len = unsafe_yyjson_get_len(i_val);
+            if (len > 0) {
+                yyjson_val *ii_val = i_val + 1, *ii_next;
+                yyjson_mut_val *mm_val = m_val + 1, *mm_ctn = m_val, *mm_next;
+                while (len-- > 1) {
+                    ii_next = unsafe_yyjson_get_next(ii_val);
+                    mm_next = mm_val + (ii_next - ii_val);
+                    mm_val->next = mm_next;
+                    ii_val = ii_next;
+                    mm_val = mm_next;
+                }
+                mm_val->next = mm_ctn + 1;
+                mm_ctn->uni.ptr = mm_val;
+            }
+        } else if (type == YYJSON_TYPE_OBJ) {
+            usize len = unsafe_yyjson_get_len(i_val);
+            if (len > 0) {
+                yyjson_val *ii_key = i_val + 1, *ii_nextkey;
+                yyjson_mut_val *mm_key = m_val + 1, *mm_ctn = m_val;
+                yyjson_mut_val *mm_nextkey;
+                while (len-- > 1) {
+                    ii_nextkey = unsafe_yyjson_get_next(ii_key + 1);
+                    mm_nextkey = mm_key + (ii_nextkey - ii_key);
+                    mm_key->next = mm_key + 1;
+                    mm_key->next->next = mm_nextkey;
+                    ii_key = ii_nextkey;
+                    mm_key = mm_nextkey;
+                }
+                mm_key->next = mm_key + 1;
+                mm_key->next->next = mm_ctn + 1;
+                mm_ctn->uni.ptr = mm_key;
+            }
+        }
+    }
+    
+    return m_vals;
+}
+
+bool yyjson_mut_equals(yyjson_mut_val* lhs, yyjson_mut_val* rhs) {
+    yyjson_mut_val *key, *lhs_val, *rhs_val;
+    yyjson_mut_arr_iter lhs_arr_iter, rhs_arr_iter;
+    yyjson_mut_obj_iter lhs_obj_iter;
+    size_t size;
+
+    if (yyjson_mut_get_type(lhs) != yyjson_mut_get_type(rhs)) {
+        return false;
+    }
+    if (yyjson_mut_is_null(lhs)) {
+        return true;
+    }
+    if (yyjson_mut_is_bool(lhs)) {
+        return yyjson_mut_get_bool(lhs) == yyjson_mut_get_bool(rhs);
+    }
+    if (yyjson_mut_is_uint(lhs)) {
+        return yyjson_mut_get_uint(lhs) == yyjson_mut_get_uint(rhs);
+    }
+    if (yyjson_mut_is_sint(lhs)) {
+        return yyjson_mut_get_sint(lhs) == yyjson_mut_get_sint(rhs);
+    }
+    if (yyjson_mut_is_int(lhs)) {
+        return yyjson_mut_get_int(lhs) == yyjson_mut_get_int(rhs);
+    }
+    if (yyjson_mut_is_real(lhs)) {
+        return yyjson_mut_get_real(lhs) == yyjson_mut_get_real(rhs);
+    }
+    if (yyjson_mut_is_str(lhs)) {
+        return !strcmp(yyjson_mut_get_str(lhs), yyjson_mut_get_str(rhs));
+    }
+    if (yyjson_mut_is_arr(lhs)) {
+        size = yyjson_mut_arr_size(lhs);
+        if (yyjson_mut_arr_size(rhs) != size) {
+            return false;
+        }
+        if (!size) {
+            return true;
+        }
+
+        yyjson_mut_arr_iter_init(lhs, &lhs_arr_iter);
+        yyjson_mut_arr_iter_init(rhs, &rhs_arr_iter);
+        while ((lhs_val = yyjson_mut_arr_iter_next(&lhs_arr_iter)) &&
+               (rhs_val = yyjson_mut_arr_iter_next(&rhs_arr_iter))) {
+            if (!yyjson_mut_equals(lhs_val, rhs_val)) {
+                return false;
+            }
+        }
+        return true;
+    }
+    if (yyjson_mut_is_obj(lhs)) {
+        size = yyjson_mut_obj_size(lhs);
+        if (yyjson_mut_obj_size(rhs) != size) {
+            return false;
+        }
+        if (!size) {
+            return true;
+        }
+
+        yyjson_mut_obj_iter_init(lhs, &lhs_obj_iter);
+        while ((key = yyjson_mut_obj_iter_next(&lhs_obj_iter))) {
+            lhs_val = yyjson_mut_obj_iter_get_val(key);
+            rhs_val = yyjson_mut_obj_get(rhs, yyjson_mut_get_str(key));
+            if (!rhs_val) {
+                return false;
+            }
+            if (!yyjson_mut_equals(lhs_val, rhs_val)) {
+                return false;
+            }
+        }
+        return true;
+    }
+
+    return false;
+}
+
+/*==============================================================================
+ * JSON Pointer
+ *============================================================================*/
+
+/* only 0x0, 0x2F ('/') and 0x7E ('~') are excluded from this table */
+static const bool pointer_char_table[] = {
+    0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
+};
+
+static_inline bool pointer_read_index(const char *ptr,
+                                      const char **end,
+                                      usize *idx) {
+    u64 num;
+    u32 i;
+    u8 add;
+    
+    add = (u8)(*ptr++ - '0');
+    if (add == 0) {
+        *idx = 0;
+        *end = ptr;
+        return true;
+    }
+    if (add > 9) return false;
+    
+    num = add;
+    for (i = 0; i < U64_SAFE_DIG; i++) {
+        if ((add = (u8)(ptr[i] - '0')) <= 9) {
+            num = add + num * 10;
+        } else {
+            *idx = (usize)num;
+            *end = ptr + i;
+            return num < (u64)USIZE_MAX;
+        }
+    }
+    return false;
+}
+
+static_inline bool pointer_read_str(const char *ptr, const char **end,
+                                    char *buf, const char **str, usize *len) {
+    const char *hdr = ptr;
+    char *dst = buf;
+    usize distance;
+    
+    /* skip unescaped characters, do not validate encoding */
+    while (true) {
+#define skip_expr(i) \
+        if (likely(pointer_char_table[(u8)ptr[i]])) {} else { ptr += i; break; }
+        repeat8_incr(skip_expr)
+#undef skip_expr
+        ptr += 8;
+    }
+    distance = (usize)(ptr - hdr);
+    if (likely(*ptr != '~')) {
+        *end = ptr;
+        *str = hdr;
+        *len = distance;
+        return true;
+    }
+    
+    /* copy escaped string */
+    memcpy(dst, hdr, distance);
+    dst += distance;
+    ptr += 1;
+    if (*ptr != '0' && *ptr != '1') return false;
+    *dst++ = (char)(*ptr++ == '0' ? '~' : '/');
+    while (true) {
+        if (pointer_char_table[(u8)*ptr]) {
+            *dst++ = *ptr++;
+        } else if (*ptr != '~') {
+            *end = ptr;
+            *str = buf;
+            *len = (usize)(dst - buf);
+            return true;
+        } else {
+            ptr++;
+            if (*ptr != '0' && *ptr != '1') return false;
+            *dst++ = (char)(*ptr++ == '0' ? '~' : '/');
+        }
+    }
+}
+
+yyjson_api yyjson_val *unsafe_yyjson_get_pointer(yyjson_val *val,
+                                                 const char *ptr,
+                                                 size_t len) {
+    char tmp[512];
+    char *buf;
+    
+    if (likely(len <= sizeof(tmp))) {
+        buf = tmp;
+    } else {
+        buf = (char *)malloc(len);
+        if (!buf) return NULL;
+    }
+    
+    ptr++;
+    while (true) {
+        if (yyjson_is_obj(val)) {
+            const char *key;
+            usize key_len;
+            if (pointer_read_str(ptr, &ptr, buf, &key, &key_len)) {
+                val = yyjson_obj_getn(val, key, key_len);
+            } else {
+                val = NULL;
+            }
+        } else if (yyjson_is_arr(val)) {
+            usize idx;
+            if (pointer_read_index(ptr, &ptr, &idx)) {
+                val = yyjson_arr_get(val, idx);
+            } else {
+                val = NULL;
+            }
+        } else {
+            val = NULL;
+        }
+        if (val && *ptr == '\0') {
+            if (unlikely(len > sizeof(tmp))) free(buf);
+            return val;
+        }
+        if (*ptr++ == '/') continue;
+        if (unlikely(len > sizeof(tmp))) free(buf);
+        return NULL;
+    }
+}
+
+yyjson_api yyjson_mut_val *unsafe_yyjson_mut_get_pointer(yyjson_mut_val *val,
+                                                         const char *ptr,
+                                                         size_t len) {
+    char tmp[512];
+    char *buf;
+    
+    if (likely(len <= sizeof(tmp))) {
+        buf = tmp;
+    } else {
+        buf = (char *)malloc(len);
+        if (!buf) return NULL;
+    }
+    
+    ptr++;
+    while (true) {
+        if (yyjson_mut_is_obj(val)) {
+            const char *key;
+            usize key_len;
+            if (pointer_read_str(ptr, &ptr, buf, &key, &key_len)) {
+                val = yyjson_mut_obj_getn(val, key, key_len);
+            } else {
+                val = NULL;
+            }
+        } else if (yyjson_mut_is_arr(val)) {
+            usize idx;
+            if (pointer_read_index(ptr, &ptr, &idx)) {
+                val = yyjson_mut_arr_get(val, idx);
+            } else {
+                val = NULL;
+            }
+        } else {
+            val = NULL;
+        }
+        if (val && *ptr == '\0') {
+            if (unlikely(len > sizeof(tmp))) free(buf);
+            return val;
+        }
+        if (*ptr++ == '/') continue;
+        if (unlikely(len > sizeof(tmp))) free(buf);
+        return NULL;
+    }
+}
+
+
+
+/*==============================================================================
+ * JSON Merge-Patch
+ *============================================================================*/
+
+yyjson_api yyjson_mut_val *yyjson_merge_patch(yyjson_mut_doc *doc,
+                                              yyjson_val *orig,
+                                              yyjson_val *patch) {
+    size_t idx, max;
+    yyjson_val *key, *orig_val, *patch_val, local_orig;
+    yyjson_mut_val *builder, *mut_key, *mut_val, *merged_val;
+    
+    if (unlikely(!yyjson_is_obj(patch))) {
+        return yyjson_val_mut_copy(doc, patch);
+    }
+    
+    builder = yyjson_mut_obj(doc);
+    if (unlikely(!builder)) return NULL;
+    
+    if (!yyjson_is_obj(orig)) {
+        orig = &local_orig;
+        orig->tag = builder->tag;
+        orig->uni = builder->uni;
+    }
+    
+    /* Merge items modified by the patch. */
+    yyjson_obj_foreach(patch, idx, max, key, patch_val) {
+        /* null indicates the field is removed. */
+        if (unsafe_yyjson_is_null(patch_val)) {
+            continue;
+        }
+        mut_key = yyjson_val_mut_copy(doc, key);
+        orig_val = yyjson_obj_getn(orig,
+                                   unsafe_yyjson_get_str(key),
+                                   unsafe_yyjson_get_len(key));
+        merged_val = yyjson_merge_patch(doc, orig_val, patch_val);
+        if (unlikely(!yyjson_mut_obj_add(builder, mut_key, merged_val))) {
+            return NULL;
+        }
+    }
+    
+    /* Exit early, if orig is not contributing to the final result. */
+    if (orig == &local_orig) {
+        return builder;
+    }
+    
+    /* Copy over any items that weren't modified by the patch. */
+    yyjson_obj_foreach(orig, idx, max, key, orig_val) {
+        patch_val = yyjson_obj_getn(patch,
+                                    unsafe_yyjson_get_str(key),
+                                    unsafe_yyjson_get_len(key));
+        if (!patch_val) {
+            mut_key = yyjson_val_mut_copy(doc, key);
+            mut_val = yyjson_val_mut_copy(doc, orig_val);
+            if (unlikely(!yyjson_mut_obj_add(builder, mut_key, mut_val))) {
+                return NULL;
+            }
+        }
+    }
+    
+    return builder;
+}
+
+
+
+/*==============================================================================
+ * Power10 Lookup Table
+ * These data are used by the floating-point number reader and writer.
+ *============================================================================*/
+
+#if (!YYJSON_DISABLE_READER || !YYJSON_DISABLE_WRITER) && \
+    (!YYJSON_DISABLE_FAST_FP_CONV)
+
+/** Minimum decimal exponent in pow10_sig_table. */
+#define POW10_SIG_TABLE_MIN_EXP -343
+
+/** Maximum decimal exponent in pow10_sig_table. */
+#define POW10_SIG_TABLE_MAX_EXP 324
+
+/** Minimum exact decimal exponent in pow10_sig_table */
+#define POW10_SIG_TABLE_MIN_EXACT_EXP 0
+
+/** Maximum exact decimal exponent in pow10_sig_table */
+#define POW10_SIG_TABLE_MAX_EXACT_EXP 55
+
+/** Normalized significant 128 bits of pow10, no rounded up (size: 10.4KB).
+    This lookup table is used by both the double number reader and writer.
+    (generate with misc/make_tables.c) */
+static const u64 pow10_sig_table[] = {
+    U64(0xBF29DCAB, 0xA82FDEAE), U64(0x7432EE87, 0x3880FC33), /* ~= 10^-343 */
+    U64(0xEEF453D6, 0x923BD65A), U64(0x113FAA29, 0x06A13B3F), /* ~= 10^-342 */
+    U64(0x9558B466, 0x1B6565F8), U64(0x4AC7CA59, 0xA424C507), /* ~= 10^-341 */
+    U64(0xBAAEE17F, 0xA23EBF76), U64(0x5D79BCF0, 0x0D2DF649), /* ~= 10^-340 */
+    U64(0xE95A99DF, 0x8ACE6F53), U64(0xF4D82C2C, 0x107973DC), /* ~= 10^-339 */
+    U64(0x91D8A02B, 0xB6C10594), U64(0x79071B9B, 0x8A4BE869), /* ~= 10^-338 */
+    U64(0xB64EC836, 0xA47146F9), U64(0x9748E282, 0x6CDEE284), /* ~= 10^-337 */
+    U64(0xE3E27A44, 0x4D8D98B7), U64(0xFD1B1B23, 0x08169B25), /* ~= 10^-336 */
+    U64(0x8E6D8C6A, 0xB0787F72), U64(0xFE30F0F5, 0xE50E20F7), /* ~= 10^-335 */
+    U64(0xB208EF85, 0x5C969F4F), U64(0xBDBD2D33, 0x5E51A935), /* ~= 10^-334 */
+    U64(0xDE8B2B66, 0xB3BC4723), U64(0xAD2C7880, 0x35E61382), /* ~= 10^-333 */
+    U64(0x8B16FB20, 0x3055AC76), U64(0x4C3BCB50, 0x21AFCC31), /* ~= 10^-332 */
+    U64(0xADDCB9E8, 0x3C6B1793), U64(0xDF4ABE24, 0x2A1BBF3D), /* ~= 10^-331 */
+    U64(0xD953E862, 0x4B85DD78), U64(0xD71D6DAD, 0x34A2AF0D), /* ~= 10^-330 */
+    U64(0x87D4713D, 0x6F33AA6B), U64(0x8672648C, 0x40E5AD68), /* ~= 10^-329 */
+    U64(0xA9C98D8C, 0xCB009506), U64(0x680EFDAF, 0x511F18C2), /* ~= 10^-328 */
+    U64(0xD43BF0EF, 0xFDC0BA48), U64(0x0212BD1B, 0x2566DEF2), /* ~= 10^-327 */
+    U64(0x84A57695, 0xFE98746D), U64(0x014BB630, 0xF7604B57), /* ~= 10^-326 */
+    U64(0xA5CED43B, 0x7E3E9188), U64(0x419EA3BD, 0x35385E2D), /* ~= 10^-325 */
+    U64(0xCF42894A, 0x5DCE35EA), U64(0x52064CAC, 0x828675B9), /* ~= 10^-324 */
+    U64(0x818995CE, 0x7AA0E1B2), U64(0x7343EFEB, 0xD1940993), /* ~= 10^-323 */
+    U64(0xA1EBFB42, 0x19491A1F), U64(0x1014EBE6, 0xC5F90BF8), /* ~= 10^-322 */
+    U64(0xCA66FA12, 0x9F9B60A6), U64(0xD41A26E0, 0x77774EF6), /* ~= 10^-321 */
+    U64(0xFD00B897, 0x478238D0), U64(0x8920B098, 0x955522B4), /* ~= 10^-320 */
+    U64(0x9E20735E, 0x8CB16382), U64(0x55B46E5F, 0x5D5535B0), /* ~= 10^-319 */
+    U64(0xC5A89036, 0x2FDDBC62), U64(0xEB2189F7, 0x34AA831D), /* ~= 10^-318 */
+    U64(0xF712B443, 0xBBD52B7B), U64(0xA5E9EC75, 0x01D523E4), /* ~= 10^-317 */
+    U64(0x9A6BB0AA, 0x55653B2D), U64(0x47B233C9, 0x2125366E), /* ~= 10^-316 */
+    U64(0xC1069CD4, 0xEABE89F8), U64(0x999EC0BB, 0x696E840A), /* ~= 10^-315 */
+    U64(0xF148440A, 0x256E2C76), U64(0xC00670EA, 0x43CA250D), /* ~= 10^-314 */
+    U64(0x96CD2A86, 0x5764DBCA), U64(0x38040692, 0x6A5E5728), /* ~= 10^-313 */
+    U64(0xBC807527, 0xED3E12BC), U64(0xC6050837, 0x04F5ECF2), /* ~= 10^-312 */
+    U64(0xEBA09271, 0xE88D976B), U64(0xF7864A44, 0xC633682E), /* ~= 10^-311 */
+    U64(0x93445B87, 0x31587EA3), U64(0x7AB3EE6A, 0xFBE0211D), /* ~= 10^-310 */
+    U64(0xB8157268, 0xFDAE9E4C), U64(0x5960EA05, 0xBAD82964), /* ~= 10^-309 */
+    U64(0xE61ACF03, 0x3D1A45DF), U64(0x6FB92487, 0x298E33BD), /* ~= 10^-308 */
+    U64(0x8FD0C162, 0x06306BAB), U64(0xA5D3B6D4, 0x79F8E056), /* ~= 10^-307 */
+    U64(0xB3C4F1BA, 0x87BC8696), U64(0x8F48A489, 0x9877186C), /* ~= 10^-306 */
+    U64(0xE0B62E29, 0x29ABA83C), U64(0x331ACDAB, 0xFE94DE87), /* ~= 10^-305 */
+    U64(0x8C71DCD9, 0xBA0B4925), U64(0x9FF0C08B, 0x7F1D0B14), /* ~= 10^-304 */
+    U64(0xAF8E5410, 0x288E1B6F), U64(0x07ECF0AE, 0x5EE44DD9), /* ~= 10^-303 */
+    U64(0xDB71E914, 0x32B1A24A), U64(0xC9E82CD9, 0xF69D6150), /* ~= 10^-302 */
+    U64(0x892731AC, 0x9FAF056E), U64(0xBE311C08, 0x3A225CD2), /* ~= 10^-301 */
+    U64(0xAB70FE17, 0xC79AC6CA), U64(0x6DBD630A, 0x48AAF406), /* ~= 10^-300 */
+    U64(0xD64D3D9D, 0xB981787D), U64(0x092CBBCC, 0xDAD5B108), /* ~= 10^-299 */
+    U64(0x85F04682, 0x93F0EB4E), U64(0x25BBF560, 0x08C58EA5), /* ~= 10^-298 */
+    U64(0xA76C5823, 0x38ED2621), U64(0xAF2AF2B8, 0x0AF6F24E), /* ~= 10^-297 */
+    U64(0xD1476E2C, 0x07286FAA), U64(0x1AF5AF66, 0x0DB4AEE1), /* ~= 10^-296 */
+    U64(0x82CCA4DB, 0x847945CA), U64(0x50D98D9F, 0xC890ED4D), /* ~= 10^-295 */
+    U64(0xA37FCE12, 0x6597973C), U64(0xE50FF107, 0xBAB528A0), /* ~= 10^-294 */
+    U64(0xCC5FC196, 0xFEFD7D0C), U64(0x1E53ED49, 0xA96272C8), /* ~= 10^-293 */
+    U64(0xFF77B1FC, 0xBEBCDC4F), U64(0x25E8E89C, 0x13BB0F7A), /* ~= 10^-292 */
+    U64(0x9FAACF3D, 0xF73609B1), U64(0x77B19161, 0x8C54E9AC), /* ~= 10^-291 */
+    U64(0xC795830D, 0x75038C1D), U64(0xD59DF5B9, 0xEF6A2417), /* ~= 10^-290 */
+    U64(0xF97AE3D0, 0xD2446F25), U64(0x4B057328, 0x6B44AD1D), /* ~= 10^-289 */
+    U64(0x9BECCE62, 0x836AC577), U64(0x4EE367F9, 0x430AEC32), /* ~= 10^-288 */
+    U64(0xC2E801FB, 0x244576D5), U64(0x229C41F7, 0x93CDA73F), /* ~= 10^-287 */
+    U64(0xF3A20279, 0xED56D48A), U64(0x6B435275, 0x78C1110F), /* ~= 10^-286 */
+    U64(0x9845418C, 0x345644D6), U64(0x830A1389, 0x6B78AAA9), /* ~= 10^-285 */
+    U64(0xBE5691EF, 0x416BD60C), U64(0x23CC986B, 0xC656D553), /* ~= 10^-284 */
+    U64(0xEDEC366B, 0x11C6CB8F), U64(0x2CBFBE86, 0xB7EC8AA8), /* ~= 10^-283 */
+    U64(0x94B3A202, 0xEB1C3F39), U64(0x7BF7D714, 0x32F3D6A9), /* ~= 10^-282 */
+    U64(0xB9E08A83, 0xA5E34F07), U64(0xDAF5CCD9, 0x3FB0CC53), /* ~= 10^-281 */
+    U64(0xE858AD24, 0x8F5C22C9), U64(0xD1B3400F, 0x8F9CFF68), /* ~= 10^-280 */
+    U64(0x91376C36, 0xD99995BE), U64(0x23100809, 0xB9C21FA1), /* ~= 10^-279 */
+    U64(0xB5854744, 0x8FFFFB2D), U64(0xABD40A0C, 0x2832A78A), /* ~= 10^-278 */
+    U64(0xE2E69915, 0xB3FFF9F9), U64(0x16C90C8F, 0x323F516C), /* ~= 10^-277 */
+    U64(0x8DD01FAD, 0x907FFC3B), U64(0xAE3DA7D9, 0x7F6792E3), /* ~= 10^-276 */
+    U64(0xB1442798, 0xF49FFB4A), U64(0x99CD11CF, 0xDF41779C), /* ~= 10^-275 */
+    U64(0xDD95317F, 0x31C7FA1D), U64(0x40405643, 0xD711D583), /* ~= 10^-274 */
+    U64(0x8A7D3EEF, 0x7F1CFC52), U64(0x482835EA, 0x666B2572), /* ~= 10^-273 */
+    U64(0xAD1C8EAB, 0x5EE43B66), U64(0xDA324365, 0x0005EECF), /* ~= 10^-272 */
+    U64(0xD863B256, 0x369D4A40), U64(0x90BED43E, 0x40076A82), /* ~= 10^-271 */
+    U64(0x873E4F75, 0xE2224E68), U64(0x5A7744A6, 0xE804A291), /* ~= 10^-270 */
+    U64(0xA90DE353, 0x5AAAE202), U64(0x711515D0, 0xA205CB36), /* ~= 10^-269 */
+    U64(0xD3515C28, 0x31559A83), U64(0x0D5A5B44, 0xCA873E03), /* ~= 10^-268 */
+    U64(0x8412D999, 0x1ED58091), U64(0xE858790A, 0xFE9486C2), /* ~= 10^-267 */
+    U64(0xA5178FFF, 0x668AE0B6), U64(0x626E974D, 0xBE39A872), /* ~= 10^-266 */
+    U64(0xCE5D73FF, 0x402D98E3), U64(0xFB0A3D21, 0x2DC8128F), /* ~= 10^-265 */
+    U64(0x80FA687F, 0x881C7F8E), U64(0x7CE66634, 0xBC9D0B99), /* ~= 10^-264 */
+    U64(0xA139029F, 0x6A239F72), U64(0x1C1FFFC1, 0xEBC44E80), /* ~= 10^-263 */
+    U64(0xC9874347, 0x44AC874E), U64(0xA327FFB2, 0x66B56220), /* ~= 10^-262 */
+    U64(0xFBE91419, 0x15D7A922), U64(0x4BF1FF9F, 0x0062BAA8), /* ~= 10^-261 */
+    U64(0x9D71AC8F, 0xADA6C9B5), U64(0x6F773FC3, 0x603DB4A9), /* ~= 10^-260 */
+    U64(0xC4CE17B3, 0x99107C22), U64(0xCB550FB4, 0x384D21D3), /* ~= 10^-259 */
+    U64(0xF6019DA0, 0x7F549B2B), U64(0x7E2A53A1, 0x46606A48), /* ~= 10^-258 */
+    U64(0x99C10284, 0x4F94E0FB), U64(0x2EDA7444, 0xCBFC426D), /* ~= 10^-257 */
+    U64(0xC0314325, 0x637A1939), U64(0xFA911155, 0xFEFB5308), /* ~= 10^-256 */
+    U64(0xF03D93EE, 0xBC589F88), U64(0x793555AB, 0x7EBA27CA), /* ~= 10^-255 */
+    U64(0x96267C75, 0x35B763B5), U64(0x4BC1558B, 0x2F3458DE), /* ~= 10^-254 */
+    U64(0xBBB01B92, 0x83253CA2), U64(0x9EB1AAED, 0xFB016F16), /* ~= 10^-253 */
+    U64(0xEA9C2277, 0x23EE8BCB), U64(0x465E15A9, 0x79C1CADC), /* ~= 10^-252 */
+    U64(0x92A1958A, 0x7675175F), U64(0x0BFACD89, 0xEC191EC9), /* ~= 10^-251 */
+    U64(0xB749FAED, 0x14125D36), U64(0xCEF980EC, 0x671F667B), /* ~= 10^-250 */
+    U64(0xE51C79A8, 0x5916F484), U64(0x82B7E127, 0x80E7401A), /* ~= 10^-249 */
+    U64(0x8F31CC09, 0x37AE58D2), U64(0xD1B2ECB8, 0xB0908810), /* ~= 10^-248 */
+    U64(0xB2FE3F0B, 0x8599EF07), U64(0x861FA7E6, 0xDCB4AA15), /* ~= 10^-247 */
+    U64(0xDFBDCECE, 0x67006AC9), U64(0x67A791E0, 0x93E1D49A), /* ~= 10^-246 */
+    U64(0x8BD6A141, 0x006042BD), U64(0xE0C8BB2C, 0x5C6D24E0), /* ~= 10^-245 */
+    U64(0xAECC4991, 0x4078536D), U64(0x58FAE9F7, 0x73886E18), /* ~= 10^-244 */
+    U64(0xDA7F5BF5, 0x90966848), U64(0xAF39A475, 0x506A899E), /* ~= 10^-243 */
+    U64(0x888F9979, 0x7A5E012D), U64(0x6D8406C9, 0x52429603), /* ~= 10^-242 */
+    U64(0xAAB37FD7, 0xD8F58178), U64(0xC8E5087B, 0xA6D33B83), /* ~= 10^-241 */
+    U64(0xD5605FCD, 0xCF32E1D6), U64(0xFB1E4A9A, 0x90880A64), /* ~= 10^-240 */
+    U64(0x855C3BE0, 0xA17FCD26), U64(0x5CF2EEA0, 0x9A55067F), /* ~= 10^-239 */
+    U64(0xA6B34AD8, 0xC9DFC06F), U64(0xF42FAA48, 0xC0EA481E), /* ~= 10^-238 */
+    U64(0xD0601D8E, 0xFC57B08B), U64(0xF13B94DA, 0xF124DA26), /* ~= 10^-237 */
+    U64(0x823C1279, 0x5DB6CE57), U64(0x76C53D08, 0xD6B70858), /* ~= 10^-236 */
+    U64(0xA2CB1717, 0xB52481ED), U64(0x54768C4B, 0x0C64CA6E), /* ~= 10^-235 */
+    U64(0xCB7DDCDD, 0xA26DA268), U64(0xA9942F5D, 0xCF7DFD09), /* ~= 10^-234 */
+    U64(0xFE5D5415, 0x0B090B02), U64(0xD3F93B35, 0x435D7C4C), /* ~= 10^-233 */
+    U64(0x9EFA548D, 0x26E5A6E1), U64(0xC47BC501, 0x4A1A6DAF), /* ~= 10^-232 */
+    U64(0xC6B8E9B0, 0x709F109A), U64(0x359AB641, 0x9CA1091B), /* ~= 10^-231 */
+    U64(0xF867241C, 0x8CC6D4C0), U64(0xC30163D2, 0x03C94B62), /* ~= 10^-230 */
+    U64(0x9B407691, 0xD7FC44F8), U64(0x79E0DE63, 0x425DCF1D), /* ~= 10^-229 */
+    U64(0xC2109436, 0x4DFB5636), U64(0x985915FC, 0x12F542E4), /* ~= 10^-228 */
+    U64(0xF294B943, 0xE17A2BC4), U64(0x3E6F5B7B, 0x17B2939D), /* ~= 10^-227 */
+    U64(0x979CF3CA, 0x6CEC5B5A), U64(0xA705992C, 0xEECF9C42), /* ~= 10^-226 */
+    U64(0xBD8430BD, 0x08277231), U64(0x50C6FF78, 0x2A838353), /* ~= 10^-225 */
+    U64(0xECE53CEC, 0x4A314EBD), U64(0xA4F8BF56, 0x35246428), /* ~= 10^-224 */
+    U64(0x940F4613, 0xAE5ED136), U64(0x871B7795, 0xE136BE99), /* ~= 10^-223 */
+    U64(0xB9131798, 0x99F68584), U64(0x28E2557B, 0x59846E3F), /* ~= 10^-222 */
+    U64(0xE757DD7E, 0xC07426E5), U64(0x331AEADA, 0x2FE589CF), /* ~= 10^-221 */
+    U64(0x9096EA6F, 0x3848984F), U64(0x3FF0D2C8, 0x5DEF7621), /* ~= 10^-220 */
+    U64(0xB4BCA50B, 0x065ABE63), U64(0x0FED077A, 0x756B53A9), /* ~= 10^-219 */
+    U64(0xE1EBCE4D, 0xC7F16DFB), U64(0xD3E84959, 0x12C62894), /* ~= 10^-218 */
+    U64(0x8D3360F0, 0x9CF6E4BD), U64(0x64712DD7, 0xABBBD95C), /* ~= 10^-217 */
+    U64(0xB080392C, 0xC4349DEC), U64(0xBD8D794D, 0x96AACFB3), /* ~= 10^-216 */
+    U64(0xDCA04777, 0xF541C567), U64(0xECF0D7A0, 0xFC5583A0), /* ~= 10^-215 */
+    U64(0x89E42CAA, 0xF9491B60), U64(0xF41686C4, 0x9DB57244), /* ~= 10^-214 */
+    U64(0xAC5D37D5, 0xB79B6239), U64(0x311C2875, 0xC522CED5), /* ~= 10^-213 */
+    U64(0xD77485CB, 0x25823AC7), U64(0x7D633293, 0x366B828B), /* ~= 10^-212 */
+    U64(0x86A8D39E, 0xF77164BC), U64(0xAE5DFF9C, 0x02033197), /* ~= 10^-211 */
+    U64(0xA8530886, 0xB54DBDEB), U64(0xD9F57F83, 0x0283FDFC), /* ~= 10^-210 */
+    U64(0xD267CAA8, 0x62A12D66), U64(0xD072DF63, 0xC324FD7B), /* ~= 10^-209 */
+    U64(0x8380DEA9, 0x3DA4BC60), U64(0x4247CB9E, 0x59F71E6D), /* ~= 10^-208 */
+    U64(0xA4611653, 0x8D0DEB78), U64(0x52D9BE85, 0xF074E608), /* ~= 10^-207 */
+    U64(0xCD795BE8, 0x70516656), U64(0x67902E27, 0x6C921F8B), /* ~= 10^-206 */
+    U64(0x806BD971, 0x4632DFF6), U64(0x00BA1CD8, 0xA3DB53B6), /* ~= 10^-205 */
+    U64(0xA086CFCD, 0x97BF97F3), U64(0x80E8A40E, 0xCCD228A4), /* ~= 10^-204 */
+    U64(0xC8A883C0, 0xFDAF7DF0), U64(0x6122CD12, 0x8006B2CD), /* ~= 10^-203 */
+    U64(0xFAD2A4B1, 0x3D1B5D6C), U64(0x796B8057, 0x20085F81), /* ~= 10^-202 */
+    U64(0x9CC3A6EE, 0xC6311A63), U64(0xCBE33036, 0x74053BB0), /* ~= 10^-201 */
+    U64(0xC3F490AA, 0x77BD60FC), U64(0xBEDBFC44, 0x11068A9C), /* ~= 10^-200 */
+    U64(0xF4F1B4D5, 0x15ACB93B), U64(0xEE92FB55, 0x15482D44), /* ~= 10^-199 */
+    U64(0x99171105, 0x2D8BF3C5), U64(0x751BDD15, 0x2D4D1C4A), /* ~= 10^-198 */
+    U64(0xBF5CD546, 0x78EEF0B6), U64(0xD262D45A, 0x78A0635D), /* ~= 10^-197 */
+    U64(0xEF340A98, 0x172AACE4), U64(0x86FB8971, 0x16C87C34), /* ~= 10^-196 */
+    U64(0x9580869F, 0x0E7AAC0E), U64(0xD45D35E6, 0xAE3D4DA0), /* ~= 10^-195 */
+    U64(0xBAE0A846, 0xD2195712), U64(0x89748360, 0x59CCA109), /* ~= 10^-194 */
+    U64(0xE998D258, 0x869FACD7), U64(0x2BD1A438, 0x703FC94B), /* ~= 10^-193 */
+    U64(0x91FF8377, 0x5423CC06), U64(0x7B6306A3, 0x4627DDCF), /* ~= 10^-192 */
+    U64(0xB67F6455, 0x292CBF08), U64(0x1A3BC84C, 0x17B1D542), /* ~= 10^-191 */
+    U64(0xE41F3D6A, 0x7377EECA), U64(0x20CABA5F, 0x1D9E4A93), /* ~= 10^-190 */
+    U64(0x8E938662, 0x882AF53E), U64(0x547EB47B, 0x7282EE9C), /* ~= 10^-189 */
+    U64(0xB23867FB, 0x2A35B28D), U64(0xE99E619A, 0x4F23AA43), /* ~= 10^-188 */
+    U64(0xDEC681F9, 0xF4C31F31), U64(0x6405FA00, 0xE2EC94D4), /* ~= 10^-187 */
+    U64(0x8B3C113C, 0x38F9F37E), U64(0xDE83BC40, 0x8DD3DD04), /* ~= 10^-186 */
+    U64(0xAE0B158B, 0x4738705E), U64(0x9624AB50, 0xB148D445), /* ~= 10^-185 */
+    U64(0xD98DDAEE, 0x19068C76), U64(0x3BADD624, 0xDD9B0957), /* ~= 10^-184 */
+    U64(0x87F8A8D4, 0xCFA417C9), U64(0xE54CA5D7, 0x0A80E5D6), /* ~= 10^-183 */
+    U64(0xA9F6D30A, 0x038D1DBC), U64(0x5E9FCF4C, 0xCD211F4C), /* ~= 10^-182 */
+    U64(0xD47487CC, 0x8470652B), U64(0x7647C320, 0x0069671F), /* ~= 10^-181 */
+    U64(0x84C8D4DF, 0xD2C63F3B), U64(0x29ECD9F4, 0x0041E073), /* ~= 10^-180 */
+    U64(0xA5FB0A17, 0xC777CF09), U64(0xF4681071, 0x00525890), /* ~= 10^-179 */
+    U64(0xCF79CC9D, 0xB955C2CC), U64(0x7182148D, 0x4066EEB4), /* ~= 10^-178 */
+    U64(0x81AC1FE2, 0x93D599BF), U64(0xC6F14CD8, 0x48405530), /* ~= 10^-177 */
+    U64(0xA21727DB, 0x38CB002F), U64(0xB8ADA00E, 0x5A506A7C), /* ~= 10^-176 */
+    U64(0xCA9CF1D2, 0x06FDC03B), U64(0xA6D90811, 0xF0E4851C), /* ~= 10^-175 */
+    U64(0xFD442E46, 0x88BD304A), U64(0x908F4A16, 0x6D1DA663), /* ~= 10^-174 */
+    U64(0x9E4A9CEC, 0x15763E2E), U64(0x9A598E4E, 0x043287FE), /* ~= 10^-173 */
+    U64(0xC5DD4427, 0x1AD3CDBA), U64(0x40EFF1E1, 0x853F29FD), /* ~= 10^-172 */
+    U64(0xF7549530, 0xE188C128), U64(0xD12BEE59, 0xE68EF47C), /* ~= 10^-171 */
+    U64(0x9A94DD3E, 0x8CF578B9), U64(0x82BB74F8, 0x301958CE), /* ~= 10^-170 */
+    U64(0xC13A148E, 0x3032D6E7), U64(0xE36A5236, 0x3C1FAF01), /* ~= 10^-169 */
+    U64(0xF18899B1, 0xBC3F8CA1), U64(0xDC44E6C3, 0xCB279AC1), /* ~= 10^-168 */
+    U64(0x96F5600F, 0x15A7B7E5), U64(0x29AB103A, 0x5EF8C0B9), /* ~= 10^-167 */
+    U64(0xBCB2B812, 0xDB11A5DE), U64(0x7415D448, 0xF6B6F0E7), /* ~= 10^-166 */
+    U64(0xEBDF6617, 0x91D60F56), U64(0x111B495B, 0x3464AD21), /* ~= 10^-165 */
+    U64(0x936B9FCE, 0xBB25C995), U64(0xCAB10DD9, 0x00BEEC34), /* ~= 10^-164 */
+    U64(0xB84687C2, 0x69EF3BFB), U64(0x3D5D514F, 0x40EEA742), /* ~= 10^-163 */
+    U64(0xE65829B3, 0x046B0AFA), U64(0x0CB4A5A3, 0x112A5112), /* ~= 10^-162 */
+    U64(0x8FF71A0F, 0xE2C2E6DC), U64(0x47F0E785, 0xEABA72AB), /* ~= 10^-161 */
+    U64(0xB3F4E093, 0xDB73A093), U64(0x59ED2167, 0x65690F56), /* ~= 10^-160 */
+    U64(0xE0F218B8, 0xD25088B8), U64(0x306869C1, 0x3EC3532C), /* ~= 10^-159 */
+    U64(0x8C974F73, 0x83725573), U64(0x1E414218, 0xC73A13FB), /* ~= 10^-158 */
+    U64(0xAFBD2350, 0x644EEACF), U64(0xE5D1929E, 0xF90898FA), /* ~= 10^-157 */
+    U64(0xDBAC6C24, 0x7D62A583), U64(0xDF45F746, 0xB74ABF39), /* ~= 10^-156 */
+    U64(0x894BC396, 0xCE5DA772), U64(0x6B8BBA8C, 0x328EB783), /* ~= 10^-155 */
+    U64(0xAB9EB47C, 0x81F5114F), U64(0x066EA92F, 0x3F326564), /* ~= 10^-154 */
+    U64(0xD686619B, 0xA27255A2), U64(0xC80A537B, 0x0EFEFEBD), /* ~= 10^-153 */
+    U64(0x8613FD01, 0x45877585), U64(0xBD06742C, 0xE95F5F36), /* ~= 10^-152 */
+    U64(0xA798FC41, 0x96E952E7), U64(0x2C481138, 0x23B73704), /* ~= 10^-151 */
+    U64(0xD17F3B51, 0xFCA3A7A0), U64(0xF75A1586, 0x2CA504C5), /* ~= 10^-150 */
+    U64(0x82EF8513, 0x3DE648C4), U64(0x9A984D73, 0xDBE722FB), /* ~= 10^-149 */
+    U64(0xA3AB6658, 0x0D5FDAF5), U64(0xC13E60D0, 0xD2E0EBBA), /* ~= 10^-148 */
+    U64(0xCC963FEE, 0x10B7D1B3), U64(0x318DF905, 0x079926A8), /* ~= 10^-147 */
+    U64(0xFFBBCFE9, 0x94E5C61F), U64(0xFDF17746, 0x497F7052), /* ~= 10^-146 */
+    U64(0x9FD561F1, 0xFD0F9BD3), U64(0xFEB6EA8B, 0xEDEFA633), /* ~= 10^-145 */
+    U64(0xC7CABA6E, 0x7C5382C8), U64(0xFE64A52E, 0xE96B8FC0), /* ~= 10^-144 */
+    U64(0xF9BD690A, 0x1B68637B), U64(0x3DFDCE7A, 0xA3C673B0), /* ~= 10^-143 */
+    U64(0x9C1661A6, 0x51213E2D), U64(0x06BEA10C, 0xA65C084E), /* ~= 10^-142 */
+    U64(0xC31BFA0F, 0xE5698DB8), U64(0x486E494F, 0xCFF30A62), /* ~= 10^-141 */
+    U64(0xF3E2F893, 0xDEC3F126), U64(0x5A89DBA3, 0xC3EFCCFA), /* ~= 10^-140 */
+    U64(0x986DDB5C, 0x6B3A76B7), U64(0xF8962946, 0x5A75E01C), /* ~= 10^-139 */
+    U64(0xBE895233, 0x86091465), U64(0xF6BBB397, 0xF1135823), /* ~= 10^-138 */
+    U64(0xEE2BA6C0, 0x678B597F), U64(0x746AA07D, 0xED582E2C), /* ~= 10^-137 */
+    U64(0x94DB4838, 0x40B717EF), U64(0xA8C2A44E, 0xB4571CDC), /* ~= 10^-136 */
+    U64(0xBA121A46, 0x50E4DDEB), U64(0x92F34D62, 0x616CE413), /* ~= 10^-135 */
+    U64(0xE896A0D7, 0xE51E1566), U64(0x77B020BA, 0xF9C81D17), /* ~= 10^-134 */
+    U64(0x915E2486, 0xEF32CD60), U64(0x0ACE1474, 0xDC1D122E), /* ~= 10^-133 */
+    U64(0xB5B5ADA8, 0xAAFF80B8), U64(0x0D819992, 0x132456BA), /* ~= 10^-132 */
+    U64(0xE3231912, 0xD5BF60E6), U64(0x10E1FFF6, 0x97ED6C69), /* ~= 10^-131 */
+    U64(0x8DF5EFAB, 0xC5979C8F), U64(0xCA8D3FFA, 0x1EF463C1), /* ~= 10^-130 */
+    U64(0xB1736B96, 0xB6FD83B3), U64(0xBD308FF8, 0xA6B17CB2), /* ~= 10^-129 */
+    U64(0xDDD0467C, 0x64BCE4A0), U64(0xAC7CB3F6, 0xD05DDBDE), /* ~= 10^-128 */
+    U64(0x8AA22C0D, 0xBEF60EE4), U64(0x6BCDF07A, 0x423AA96B), /* ~= 10^-127 */
+    U64(0xAD4AB711, 0x2EB3929D), U64(0x86C16C98, 0xD2C953C6), /* ~= 10^-126 */
+    U64(0xD89D64D5, 0x7A607744), U64(0xE871C7BF, 0x077BA8B7), /* ~= 10^-125 */
+    U64(0x87625F05, 0x6C7C4A8B), U64(0x11471CD7, 0x64AD4972), /* ~= 10^-124 */
+    U64(0xA93AF6C6, 0xC79B5D2D), U64(0xD598E40D, 0x3DD89BCF), /* ~= 10^-123 */
+    U64(0xD389B478, 0x79823479), U64(0x4AFF1D10, 0x8D4EC2C3), /* ~= 10^-122 */
+    U64(0x843610CB, 0x4BF160CB), U64(0xCEDF722A, 0x585139BA), /* ~= 10^-121 */
+    U64(0xA54394FE, 0x1EEDB8FE), U64(0xC2974EB4, 0xEE658828), /* ~= 10^-120 */
+    U64(0xCE947A3D, 0xA6A9273E), U64(0x733D2262, 0x29FEEA32), /* ~= 10^-119 */
+    U64(0x811CCC66, 0x8829B887), U64(0x0806357D, 0x5A3F525F), /* ~= 10^-118 */
+    U64(0xA163FF80, 0x2A3426A8), U64(0xCA07C2DC, 0xB0CF26F7), /* ~= 10^-117 */
+    U64(0xC9BCFF60, 0x34C13052), U64(0xFC89B393, 0xDD02F0B5), /* ~= 10^-116 */
+    U64(0xFC2C3F38, 0x41F17C67), U64(0xBBAC2078, 0xD443ACE2), /* ~= 10^-115 */
+    U64(0x9D9BA783, 0x2936EDC0), U64(0xD54B944B, 0x84AA4C0D), /* ~= 10^-114 */
+    U64(0xC5029163, 0xF384A931), U64(0x0A9E795E, 0x65D4DF11), /* ~= 10^-113 */
+    U64(0xF64335BC, 0xF065D37D), U64(0x4D4617B5, 0xFF4A16D5), /* ~= 10^-112 */
+    U64(0x99EA0196, 0x163FA42E), U64(0x504BCED1, 0xBF8E4E45), /* ~= 10^-111 */
+    U64(0xC06481FB, 0x9BCF8D39), U64(0xE45EC286, 0x2F71E1D6), /* ~= 10^-110 */
+    U64(0xF07DA27A, 0x82C37088), U64(0x5D767327, 0xBB4E5A4C), /* ~= 10^-109 */
+    U64(0x964E858C, 0x91BA2655), U64(0x3A6A07F8, 0xD510F86F), /* ~= 10^-108 */
+    U64(0xBBE226EF, 0xB628AFEA), U64(0x890489F7, 0x0A55368B), /* ~= 10^-107 */
+    U64(0xEADAB0AB, 0xA3B2DBE5), U64(0x2B45AC74, 0xCCEA842E), /* ~= 10^-106 */
+    U64(0x92C8AE6B, 0x464FC96F), U64(0x3B0B8BC9, 0x0012929D), /* ~= 10^-105 */
+    U64(0xB77ADA06, 0x17E3BBCB), U64(0x09CE6EBB, 0x40173744), /* ~= 10^-104 */
+    U64(0xE5599087, 0x9DDCAABD), U64(0xCC420A6A, 0x101D0515), /* ~= 10^-103 */
+    U64(0x8F57FA54, 0xC2A9EAB6), U64(0x9FA94682, 0x4A12232D), /* ~= 10^-102 */
+    U64(0xB32DF8E9, 0xF3546564), U64(0x47939822, 0xDC96ABF9), /* ~= 10^-101 */
+    U64(0xDFF97724, 0x70297EBD), U64(0x59787E2B, 0x93BC56F7), /* ~= 10^-100 */
+    U64(0x8BFBEA76, 0xC619EF36), U64(0x57EB4EDB, 0x3C55B65A), /* ~= 10^-99 */
+    U64(0xAEFAE514, 0x77A06B03), U64(0xEDE62292, 0x0B6B23F1), /* ~= 10^-98 */
+    U64(0xDAB99E59, 0x958885C4), U64(0xE95FAB36, 0x8E45ECED), /* ~= 10^-97 */
+    U64(0x88B402F7, 0xFD75539B), U64(0x11DBCB02, 0x18EBB414), /* ~= 10^-96 */
+    U64(0xAAE103B5, 0xFCD2A881), U64(0xD652BDC2, 0x9F26A119), /* ~= 10^-95 */
+    U64(0xD59944A3, 0x7C0752A2), U64(0x4BE76D33, 0x46F0495F), /* ~= 10^-94 */
+    U64(0x857FCAE6, 0x2D8493A5), U64(0x6F70A440, 0x0C562DDB), /* ~= 10^-93 */
+    U64(0xA6DFBD9F, 0xB8E5B88E), U64(0xCB4CCD50, 0x0F6BB952), /* ~= 10^-92 */
+    U64(0xD097AD07, 0xA71F26B2), U64(0x7E2000A4, 0x1346A7A7), /* ~= 10^-91 */
+    U64(0x825ECC24, 0xC873782F), U64(0x8ED40066, 0x8C0C28C8), /* ~= 10^-90 */
+    U64(0xA2F67F2D, 0xFA90563B), U64(0x72890080, 0x2F0F32FA), /* ~= 10^-89 */
+    U64(0xCBB41EF9, 0x79346BCA), U64(0x4F2B40A0, 0x3AD2FFB9), /* ~= 10^-88 */
+    U64(0xFEA126B7, 0xD78186BC), U64(0xE2F610C8, 0x4987BFA8), /* ~= 10^-87 */
+    U64(0x9F24B832, 0xE6B0F436), U64(0x0DD9CA7D, 0x2DF4D7C9), /* ~= 10^-86 */
+    U64(0xC6EDE63F, 0xA05D3143), U64(0x91503D1C, 0x79720DBB), /* ~= 10^-85 */
+    U64(0xF8A95FCF, 0x88747D94), U64(0x75A44C63, 0x97CE912A), /* ~= 10^-84 */
+    U64(0x9B69DBE1, 0xB548CE7C), U64(0xC986AFBE, 0x3EE11ABA), /* ~= 10^-83 */
+    U64(0xC24452DA, 0x229B021B), U64(0xFBE85BAD, 0xCE996168), /* ~= 10^-82 */
+    U64(0xF2D56790, 0xAB41C2A2), U64(0xFAE27299, 0x423FB9C3), /* ~= 10^-81 */
+    U64(0x97C560BA, 0x6B0919A5), U64(0xDCCD879F, 0xC967D41A), /* ~= 10^-80 */
+    U64(0xBDB6B8E9, 0x05CB600F), U64(0x5400E987, 0xBBC1C920), /* ~= 10^-79 */
+    U64(0xED246723, 0x473E3813), U64(0x290123E9, 0xAAB23B68), /* ~= 10^-78 */
+    U64(0x9436C076, 0x0C86E30B), U64(0xF9A0B672, 0x0AAF6521), /* ~= 10^-77 */
+    U64(0xB9447093, 0x8FA89BCE), U64(0xF808E40E, 0x8D5B3E69), /* ~= 10^-76 */
+    U64(0xE7958CB8, 0x7392C2C2), U64(0xB60B1D12, 0x30B20E04), /* ~= 10^-75 */
+    U64(0x90BD77F3, 0x483BB9B9), U64(0xB1C6F22B, 0x5E6F48C2), /* ~= 10^-74 */
+    U64(0xB4ECD5F0, 0x1A4AA828), U64(0x1E38AEB6, 0x360B1AF3), /* ~= 10^-73 */
+    U64(0xE2280B6C, 0x20DD5232), U64(0x25C6DA63, 0xC38DE1B0), /* ~= 10^-72 */
+    U64(0x8D590723, 0x948A535F), U64(0x579C487E, 0x5A38AD0E), /* ~= 10^-71 */
+    U64(0xB0AF48EC, 0x79ACE837), U64(0x2D835A9D, 0xF0C6D851), /* ~= 10^-70 */
+    U64(0xDCDB1B27, 0x98182244), U64(0xF8E43145, 0x6CF88E65), /* ~= 10^-69 */
+    U64(0x8A08F0F8, 0xBF0F156B), U64(0x1B8E9ECB, 0x641B58FF), /* ~= 10^-68 */
+    U64(0xAC8B2D36, 0xEED2DAC5), U64(0xE272467E, 0x3D222F3F), /* ~= 10^-67 */
+    U64(0xD7ADF884, 0xAA879177), U64(0x5B0ED81D, 0xCC6ABB0F), /* ~= 10^-66 */
+    U64(0x86CCBB52, 0xEA94BAEA), U64(0x98E94712, 0x9FC2B4E9), /* ~= 10^-65 */
+    U64(0xA87FEA27, 0xA539E9A5), U64(0x3F2398D7, 0x47B36224), /* ~= 10^-64 */
+    U64(0xD29FE4B1, 0x8E88640E), U64(0x8EEC7F0D, 0x19A03AAD), /* ~= 10^-63 */
+    U64(0x83A3EEEE, 0xF9153E89), U64(0x1953CF68, 0x300424AC), /* ~= 10^-62 */
+    U64(0xA48CEAAA, 0xB75A8E2B), U64(0x5FA8C342, 0x3C052DD7), /* ~= 10^-61 */
+    U64(0xCDB02555, 0x653131B6), U64(0x3792F412, 0xCB06794D), /* ~= 10^-60 */
+    U64(0x808E1755, 0x5F3EBF11), U64(0xE2BBD88B, 0xBEE40BD0), /* ~= 10^-59 */
+    U64(0xA0B19D2A, 0xB70E6ED6), U64(0x5B6ACEAE, 0xAE9D0EC4), /* ~= 10^-58 */
+    U64(0xC8DE0475, 0x64D20A8B), U64(0xF245825A, 0x5A445275), /* ~= 10^-57 */
+    U64(0xFB158592, 0xBE068D2E), U64(0xEED6E2F0, 0xF0D56712), /* ~= 10^-56 */
+    U64(0x9CED737B, 0xB6C4183D), U64(0x55464DD6, 0x9685606B), /* ~= 10^-55 */
+    U64(0xC428D05A, 0xA4751E4C), U64(0xAA97E14C, 0x3C26B886), /* ~= 10^-54 */
+    U64(0xF5330471, 0x4D9265DF), U64(0xD53DD99F, 0x4B3066A8), /* ~= 10^-53 */
+    U64(0x993FE2C6, 0xD07B7FAB), U64(0xE546A803, 0x8EFE4029), /* ~= 10^-52 */
+    U64(0xBF8FDB78, 0x849A5F96), U64(0xDE985204, 0x72BDD033), /* ~= 10^-51 */
+    U64(0xEF73D256, 0xA5C0F77C), U64(0x963E6685, 0x8F6D4440), /* ~= 10^-50 */
+    U64(0x95A86376, 0x27989AAD), U64(0xDDE70013, 0x79A44AA8), /* ~= 10^-49 */
+    U64(0xBB127C53, 0xB17EC159), U64(0x5560C018, 0x580D5D52), /* ~= 10^-48 */
+    U64(0xE9D71B68, 0x9DDE71AF), U64(0xAAB8F01E, 0x6E10B4A6), /* ~= 10^-47 */
+    U64(0x92267121, 0x62AB070D), U64(0xCAB39613, 0x04CA70E8), /* ~= 10^-46 */
+    U64(0xB6B00D69, 0xBB55C8D1), U64(0x3D607B97, 0xC5FD0D22), /* ~= 10^-45 */
+    U64(0xE45C10C4, 0x2A2B3B05), U64(0x8CB89A7D, 0xB77C506A), /* ~= 10^-44 */
+    U64(0x8EB98A7A, 0x9A5B04E3), U64(0x77F3608E, 0x92ADB242), /* ~= 10^-43 */
+    U64(0xB267ED19, 0x40F1C61C), U64(0x55F038B2, 0x37591ED3), /* ~= 10^-42 */
+    U64(0xDF01E85F, 0x912E37A3), U64(0x6B6C46DE, 0xC52F6688), /* ~= 10^-41 */
+    U64(0x8B61313B, 0xBABCE2C6), U64(0x2323AC4B, 0x3B3DA015), /* ~= 10^-40 */
+    U64(0xAE397D8A, 0xA96C1B77), U64(0xABEC975E, 0x0A0D081A), /* ~= 10^-39 */
+    U64(0xD9C7DCED, 0x53C72255), U64(0x96E7BD35, 0x8C904A21), /* ~= 10^-38 */
+    U64(0x881CEA14, 0x545C7575), U64(0x7E50D641, 0x77DA2E54), /* ~= 10^-37 */
+    U64(0xAA242499, 0x697392D2), U64(0xDDE50BD1, 0xD5D0B9E9), /* ~= 10^-36 */
+    U64(0xD4AD2DBF, 0xC3D07787), U64(0x955E4EC6, 0x4B44E864), /* ~= 10^-35 */
+    U64(0x84EC3C97, 0xDA624AB4), U64(0xBD5AF13B, 0xEF0B113E), /* ~= 10^-34 */
+    U64(0xA6274BBD, 0xD0FADD61), U64(0xECB1AD8A, 0xEACDD58E), /* ~= 10^-33 */
+    U64(0xCFB11EAD, 0x453994BA), U64(0x67DE18ED, 0xA5814AF2), /* ~= 10^-32 */
+    U64(0x81CEB32C, 0x4B43FCF4), U64(0x80EACF94, 0x8770CED7), /* ~= 10^-31 */
+    U64(0xA2425FF7, 0x5E14FC31), U64(0xA1258379, 0xA94D028D), /* ~= 10^-30 */
+    U64(0xCAD2F7F5, 0x359A3B3E), U64(0x096EE458, 0x13A04330), /* ~= 10^-29 */
+    U64(0xFD87B5F2, 0x8300CA0D), U64(0x8BCA9D6E, 0x188853FC), /* ~= 10^-28 */
+    U64(0x9E74D1B7, 0x91E07E48), U64(0x775EA264, 0xCF55347D), /* ~= 10^-27 */
+    U64(0xC6120625, 0x76589DDA), U64(0x95364AFE, 0x032A819D), /* ~= 10^-26 */
+    U64(0xF79687AE, 0xD3EEC551), U64(0x3A83DDBD, 0x83F52204), /* ~= 10^-25 */
+    U64(0x9ABE14CD, 0x44753B52), U64(0xC4926A96, 0x72793542), /* ~= 10^-24 */
+    U64(0xC16D9A00, 0x95928A27), U64(0x75B7053C, 0x0F178293), /* ~= 10^-23 */
+    U64(0xF1C90080, 0xBAF72CB1), U64(0x5324C68B, 0x12DD6338), /* ~= 10^-22 */
+    U64(0x971DA050, 0x74DA7BEE), U64(0xD3F6FC16, 0xEBCA5E03), /* ~= 10^-21 */
+    U64(0xBCE50864, 0x92111AEA), U64(0x88F4BB1C, 0xA6BCF584), /* ~= 10^-20 */
+    U64(0xEC1E4A7D, 0xB69561A5), U64(0x2B31E9E3, 0xD06C32E5), /* ~= 10^-19 */
+    U64(0x9392EE8E, 0x921D5D07), U64(0x3AFF322E, 0x62439FCF), /* ~= 10^-18 */
+    U64(0xB877AA32, 0x36A4B449), U64(0x09BEFEB9, 0xFAD487C2), /* ~= 10^-17 */
+    U64(0xE69594BE, 0xC44DE15B), U64(0x4C2EBE68, 0x7989A9B3), /* ~= 10^-16 */
+    U64(0x901D7CF7, 0x3AB0ACD9), U64(0x0F9D3701, 0x4BF60A10), /* ~= 10^-15 */
+    U64(0xB424DC35, 0x095CD80F), U64(0x538484C1, 0x9EF38C94), /* ~= 10^-14 */
+    U64(0xE12E1342, 0x4BB40E13), U64(0x2865A5F2, 0x06B06FB9), /* ~= 10^-13 */
+    U64(0x8CBCCC09, 0x6F5088CB), U64(0xF93F87B7, 0x442E45D3), /* ~= 10^-12 */
+    U64(0xAFEBFF0B, 0xCB24AAFE), U64(0xF78F69A5, 0x1539D748), /* ~= 10^-11 */
+    U64(0xDBE6FECE, 0xBDEDD5BE), U64(0xB573440E, 0x5A884D1B), /* ~= 10^-10 */
+    U64(0x89705F41, 0x36B4A597), U64(0x31680A88, 0xF8953030), /* ~= 10^-9 */
+    U64(0xABCC7711, 0x8461CEFC), U64(0xFDC20D2B, 0x36BA7C3D), /* ~= 10^-8 */
+    U64(0xD6BF94D5, 0xE57A42BC), U64(0x3D329076, 0x04691B4C), /* ~= 10^-7 */
+    U64(0x8637BD05, 0xAF6C69B5), U64(0xA63F9A49, 0xC2C1B10F), /* ~= 10^-6 */
+    U64(0xA7C5AC47, 0x1B478423), U64(0x0FCF80DC, 0x33721D53), /* ~= 10^-5 */
+    U64(0xD1B71758, 0xE219652B), U64(0xD3C36113, 0x404EA4A8), /* ~= 10^-4 */
+    U64(0x83126E97, 0x8D4FDF3B), U64(0x645A1CAC, 0x083126E9), /* ~= 10^-3 */
+    U64(0xA3D70A3D, 0x70A3D70A), U64(0x3D70A3D7, 0x0A3D70A3), /* ~= 10^-2 */
+    U64(0xCCCCCCCC, 0xCCCCCCCC), U64(0xCCCCCCCC, 0xCCCCCCCC), /* ~= 10^-1 */
+    U64(0x80000000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^0 */
+    U64(0xA0000000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^1 */
+    U64(0xC8000000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^2 */
+    U64(0xFA000000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^3 */
+    U64(0x9C400000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^4 */
+    U64(0xC3500000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^5 */
+    U64(0xF4240000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^6 */
+    U64(0x98968000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^7 */
+    U64(0xBEBC2000, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^8 */
+    U64(0xEE6B2800, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^9 */
+    U64(0x9502F900, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^10 */
+    U64(0xBA43B740, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^11 */
+    U64(0xE8D4A510, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^12 */
+    U64(0x9184E72A, 0x00000000), U64(0x00000000, 0x00000000), /* == 10^13 */
+    U64(0xB5E620F4, 0x80000000), U64(0x00000000, 0x00000000), /* == 10^14 */
+    U64(0xE35FA931, 0xA0000000), U64(0x00000000, 0x00000000), /* == 10^15 */
+    U64(0x8E1BC9BF, 0x04000000), U64(0x00000000, 0x00000000), /* == 10^16 */
+    U64(0xB1A2BC2E, 0xC5000000), U64(0x00000000, 0x00000000), /* == 10^17 */
+    U64(0xDE0B6B3A, 0x76400000), U64(0x00000000, 0x00000000), /* == 10^18 */
+    U64(0x8AC72304, 0x89E80000), U64(0x00000000, 0x00000000), /* == 10^19 */
+    U64(0xAD78EBC5, 0xAC620000), U64(0x00000000, 0x00000000), /* == 10^20 */
+    U64(0xD8D726B7, 0x177A8000), U64(0x00000000, 0x00000000), /* == 10^21 */
+    U64(0x87867832, 0x6EAC9000), U64(0x00000000, 0x00000000), /* == 10^22 */
+    U64(0xA968163F, 0x0A57B400), U64(0x00000000, 0x00000000), /* == 10^23 */
+    U64(0xD3C21BCE, 0xCCEDA100), U64(0x00000000, 0x00000000), /* == 10^24 */
+    U64(0x84595161, 0x401484A0), U64(0x00000000, 0x00000000), /* == 10^25 */
+    U64(0xA56FA5B9, 0x9019A5C8), U64(0x00000000, 0x00000000), /* == 10^26 */
+    U64(0xCECB8F27, 0xF4200F3A), U64(0x00000000, 0x00000000), /* == 10^27 */
+    U64(0x813F3978, 0xF8940984), U64(0x40000000, 0x00000000), /* == 10^28 */
+    U64(0xA18F07D7, 0x36B90BE5), U64(0x50000000, 0x00000000), /* == 10^29 */
+    U64(0xC9F2C9CD, 0x04674EDE), U64(0xA4000000, 0x00000000), /* == 10^30 */
+    U64(0xFC6F7C40, 0x45812296), U64(0x4D000000, 0x00000000), /* == 10^31 */
+    U64(0x9DC5ADA8, 0x2B70B59D), U64(0xF0200000, 0x00000000), /* == 10^32 */
+    U64(0xC5371912, 0x364CE305), U64(0x6C280000, 0x00000000), /* == 10^33 */
+    U64(0xF684DF56, 0xC3E01BC6), U64(0xC7320000, 0x00000000), /* == 10^34 */
+    U64(0x9A130B96, 0x3A6C115C), U64(0x3C7F4000, 0x00000000), /* == 10^35 */
+    U64(0xC097CE7B, 0xC90715B3), U64(0x4B9F1000, 0x00000000), /* == 10^36 */
+    U64(0xF0BDC21A, 0xBB48DB20), U64(0x1E86D400, 0x00000000), /* == 10^37 */
+    U64(0x96769950, 0xB50D88F4), U64(0x13144480, 0x00000000), /* == 10^38 */
+    U64(0xBC143FA4, 0xE250EB31), U64(0x17D955A0, 0x00000000), /* == 10^39 */
+    U64(0xEB194F8E, 0x1AE525FD), U64(0x5DCFAB08, 0x00000000), /* == 10^40 */
+    U64(0x92EFD1B8, 0xD0CF37BE), U64(0x5AA1CAE5, 0x00000000), /* == 10^41 */
+    U64(0xB7ABC627, 0x050305AD), U64(0xF14A3D9E, 0x40000000), /* == 10^42 */
+    U64(0xE596B7B0, 0xC643C719), U64(0x6D9CCD05, 0xD0000000), /* == 10^43 */
+    U64(0x8F7E32CE, 0x7BEA5C6F), U64(0xE4820023, 0xA2000000), /* == 10^44 */
+    U64(0xB35DBF82, 0x1AE4F38B), U64(0xDDA2802C, 0x8A800000), /* == 10^45 */
+    U64(0xE0352F62, 0xA19E306E), U64(0xD50B2037, 0xAD200000), /* == 10^46 */
+    U64(0x8C213D9D, 0xA502DE45), U64(0x4526F422, 0xCC340000), /* == 10^47 */
+    U64(0xAF298D05, 0x0E4395D6), U64(0x9670B12B, 0x7F410000), /* == 10^48 */
+    U64(0xDAF3F046, 0x51D47B4C), U64(0x3C0CDD76, 0x5F114000), /* == 10^49 */
+    U64(0x88D8762B, 0xF324CD0F), U64(0xA5880A69, 0xFB6AC800), /* == 10^50 */
+    U64(0xAB0E93B6, 0xEFEE0053), U64(0x8EEA0D04, 0x7A457A00), /* == 10^51 */
+    U64(0xD5D238A4, 0xABE98068), U64(0x72A49045, 0x98D6D880), /* == 10^52 */
+    U64(0x85A36366, 0xEB71F041), U64(0x47A6DA2B, 0x7F864750), /* == 10^53 */
+    U64(0xA70C3C40, 0xA64E6C51), U64(0x999090B6, 0x5F67D924), /* == 10^54 */
+    U64(0xD0CF4B50, 0xCFE20765), U64(0xFFF4B4E3, 0xF741CF6D), /* == 10^55 */
+    U64(0x82818F12, 0x81ED449F), U64(0xBFF8F10E, 0x7A8921A4), /* ~= 10^56 */
+    U64(0xA321F2D7, 0x226895C7), U64(0xAFF72D52, 0x192B6A0D), /* ~= 10^57 */
+    U64(0xCBEA6F8C, 0xEB02BB39), U64(0x9BF4F8A6, 0x9F764490), /* ~= 10^58 */
+    U64(0xFEE50B70, 0x25C36A08), U64(0x02F236D0, 0x4753D5B4), /* ~= 10^59 */
+    U64(0x9F4F2726, 0x179A2245), U64(0x01D76242, 0x2C946590), /* ~= 10^60 */
+    U64(0xC722F0EF, 0x9D80AAD6), U64(0x424D3AD2, 0xB7B97EF5), /* ~= 10^61 */
+    U64(0xF8EBAD2B, 0x84E0D58B), U64(0xD2E08987, 0x65A7DEB2), /* ~= 10^62 */
+    U64(0x9B934C3B, 0x330C8577), U64(0x63CC55F4, 0x9F88EB2F), /* ~= 10^63 */
+    U64(0xC2781F49, 0xFFCFA6D5), U64(0x3CBF6B71, 0xC76B25FB), /* ~= 10^64 */
+    U64(0xF316271C, 0x7FC3908A), U64(0x8BEF464E, 0x3945EF7A), /* ~= 10^65 */
+    U64(0x97EDD871, 0xCFDA3A56), U64(0x97758BF0, 0xE3CBB5AC), /* ~= 10^66 */
+    U64(0xBDE94E8E, 0x43D0C8EC), U64(0x3D52EEED, 0x1CBEA317), /* ~= 10^67 */
+    U64(0xED63A231, 0xD4C4FB27), U64(0x4CA7AAA8, 0x63EE4BDD), /* ~= 10^68 */
+    U64(0x945E455F, 0x24FB1CF8), U64(0x8FE8CAA9, 0x3E74EF6A), /* ~= 10^69 */
+    U64(0xB975D6B6, 0xEE39E436), U64(0xB3E2FD53, 0x8E122B44), /* ~= 10^70 */
+    U64(0xE7D34C64, 0xA9C85D44), U64(0x60DBBCA8, 0x7196B616), /* ~= 10^71 */
+    U64(0x90E40FBE, 0xEA1D3A4A), U64(0xBC8955E9, 0x46FE31CD), /* ~= 10^72 */
+    U64(0xB51D13AE, 0xA4A488DD), U64(0x6BABAB63, 0x98BDBE41), /* ~= 10^73 */
+    U64(0xE264589A, 0x4DCDAB14), U64(0xC696963C, 0x7EED2DD1), /* ~= 10^74 */
+    U64(0x8D7EB760, 0x70A08AEC), U64(0xFC1E1DE5, 0xCF543CA2), /* ~= 10^75 */
+    U64(0xB0DE6538, 0x8CC8ADA8), U64(0x3B25A55F, 0x43294BCB), /* ~= 10^76 */
+    U64(0xDD15FE86, 0xAFFAD912), U64(0x49EF0EB7, 0x13F39EBE), /* ~= 10^77 */
+    U64(0x8A2DBF14, 0x2DFCC7AB), U64(0x6E356932, 0x6C784337), /* ~= 10^78 */
+    U64(0xACB92ED9, 0x397BF996), U64(0x49C2C37F, 0x07965404), /* ~= 10^79 */
+    U64(0xD7E77A8F, 0x87DAF7FB), U64(0xDC33745E, 0xC97BE906), /* ~= 10^80 */
+    U64(0x86F0AC99, 0xB4E8DAFD), U64(0x69A028BB, 0x3DED71A3), /* ~= 10^81 */
+    U64(0xA8ACD7C0, 0x222311BC), U64(0xC40832EA, 0x0D68CE0C), /* ~= 10^82 */
+    U64(0xD2D80DB0, 0x2AABD62B), U64(0xF50A3FA4, 0x90C30190), /* ~= 10^83 */
+    U64(0x83C7088E, 0x1AAB65DB), U64(0x792667C6, 0xDA79E0FA), /* ~= 10^84 */
+    U64(0xA4B8CAB1, 0xA1563F52), U64(0x577001B8, 0x91185938), /* ~= 10^85 */
+    U64(0xCDE6FD5E, 0x09ABCF26), U64(0xED4C0226, 0xB55E6F86), /* ~= 10^86 */
+    U64(0x80B05E5A, 0xC60B6178), U64(0x544F8158, 0x315B05B4), /* ~= 10^87 */
+    U64(0xA0DC75F1, 0x778E39D6), U64(0x696361AE, 0x3DB1C721), /* ~= 10^88 */
+    U64(0xC913936D, 0xD571C84C), U64(0x03BC3A19, 0xCD1E38E9), /* ~= 10^89 */
+    U64(0xFB587849, 0x4ACE3A5F), U64(0x04AB48A0, 0x4065C723), /* ~= 10^90 */
+    U64(0x9D174B2D, 0xCEC0E47B), U64(0x62EB0D64, 0x283F9C76), /* ~= 10^91 */
+    U64(0xC45D1DF9, 0x42711D9A), U64(0x3BA5D0BD, 0x324F8394), /* ~= 10^92 */
+    U64(0xF5746577, 0x930D6500), U64(0xCA8F44EC, 0x7EE36479), /* ~= 10^93 */
+    U64(0x9968BF6A, 0xBBE85F20), U64(0x7E998B13, 0xCF4E1ECB), /* ~= 10^94 */
+    U64(0xBFC2EF45, 0x6AE276E8), U64(0x9E3FEDD8, 0xC321A67E), /* ~= 10^95 */
+    U64(0xEFB3AB16, 0xC59B14A2), U64(0xC5CFE94E, 0xF3EA101E), /* ~= 10^96 */
+    U64(0x95D04AEE, 0x3B80ECE5), U64(0xBBA1F1D1, 0x58724A12), /* ~= 10^97 */
+    U64(0xBB445DA9, 0xCA61281F), U64(0x2A8A6E45, 0xAE8EDC97), /* ~= 10^98 */
+    U64(0xEA157514, 0x3CF97226), U64(0xF52D09D7, 0x1A3293BD), /* ~= 10^99 */
+    U64(0x924D692C, 0xA61BE758), U64(0x593C2626, 0x705F9C56), /* ~= 10^100 */
+    U64(0xB6E0C377, 0xCFA2E12E), U64(0x6F8B2FB0, 0x0C77836C), /* ~= 10^101 */
+    U64(0xE498F455, 0xC38B997A), U64(0x0B6DFB9C, 0x0F956447), /* ~= 10^102 */
+    U64(0x8EDF98B5, 0x9A373FEC), U64(0x4724BD41, 0x89BD5EAC), /* ~= 10^103 */
+    U64(0xB2977EE3, 0x00C50FE7), U64(0x58EDEC91, 0xEC2CB657), /* ~= 10^104 */
+    U64(0xDF3D5E9B, 0xC0F653E1), U64(0x2F2967B6, 0x6737E3ED), /* ~= 10^105 */
+    U64(0x8B865B21, 0x5899F46C), U64(0xBD79E0D2, 0x0082EE74), /* ~= 10^106 */
+    U64(0xAE67F1E9, 0xAEC07187), U64(0xECD85906, 0x80A3AA11), /* ~= 10^107 */
+    U64(0xDA01EE64, 0x1A708DE9), U64(0xE80E6F48, 0x20CC9495), /* ~= 10^108 */
+    U64(0x884134FE, 0x908658B2), U64(0x3109058D, 0x147FDCDD), /* ~= 10^109 */
+    U64(0xAA51823E, 0x34A7EEDE), U64(0xBD4B46F0, 0x599FD415), /* ~= 10^110 */
+    U64(0xD4E5E2CD, 0xC1D1EA96), U64(0x6C9E18AC, 0x7007C91A), /* ~= 10^111 */
+    U64(0x850FADC0, 0x9923329E), U64(0x03E2CF6B, 0xC604DDB0), /* ~= 10^112 */
+    U64(0xA6539930, 0xBF6BFF45), U64(0x84DB8346, 0xB786151C), /* ~= 10^113 */
+    U64(0xCFE87F7C, 0xEF46FF16), U64(0xE6126418, 0x65679A63), /* ~= 10^114 */
+    U64(0x81F14FAE, 0x158C5F6E), U64(0x4FCB7E8F, 0x3F60C07E), /* ~= 10^115 */
+    U64(0xA26DA399, 0x9AEF7749), U64(0xE3BE5E33, 0x0F38F09D), /* ~= 10^116 */
+    U64(0xCB090C80, 0x01AB551C), U64(0x5CADF5BF, 0xD3072CC5), /* ~= 10^117 */
+    U64(0xFDCB4FA0, 0x02162A63), U64(0x73D9732F, 0xC7C8F7F6), /* ~= 10^118 */
+    U64(0x9E9F11C4, 0x014DDA7E), U64(0x2867E7FD, 0xDCDD9AFA), /* ~= 10^119 */
+    U64(0xC646D635, 0x01A1511D), U64(0xB281E1FD, 0x541501B8), /* ~= 10^120 */
+    U64(0xF7D88BC2, 0x4209A565), U64(0x1F225A7C, 0xA91A4226), /* ~= 10^121 */
+    U64(0x9AE75759, 0x6946075F), U64(0x3375788D, 0xE9B06958), /* ~= 10^122 */
+    U64(0xC1A12D2F, 0xC3978937), U64(0x0052D6B1, 0x641C83AE), /* ~= 10^123 */
+    U64(0xF209787B, 0xB47D6B84), U64(0xC0678C5D, 0xBD23A49A), /* ~= 10^124 */
+    U64(0x9745EB4D, 0x50CE6332), U64(0xF840B7BA, 0x963646E0), /* ~= 10^125 */
+    U64(0xBD176620, 0xA501FBFF), U64(0xB650E5A9, 0x3BC3D898), /* ~= 10^126 */
+    U64(0xEC5D3FA8, 0xCE427AFF), U64(0xA3E51F13, 0x8AB4CEBE), /* ~= 10^127 */
+    U64(0x93BA47C9, 0x80E98CDF), U64(0xC66F336C, 0x36B10137), /* ~= 10^128 */
+    U64(0xB8A8D9BB, 0xE123F017), U64(0xB80B0047, 0x445D4184), /* ~= 10^129 */
+    U64(0xE6D3102A, 0xD96CEC1D), U64(0xA60DC059, 0x157491E5), /* ~= 10^130 */
+    U64(0x9043EA1A, 0xC7E41392), U64(0x87C89837, 0xAD68DB2F), /* ~= 10^131 */
+    U64(0xB454E4A1, 0x79DD1877), U64(0x29BABE45, 0x98C311FB), /* ~= 10^132 */
+    U64(0xE16A1DC9, 0xD8545E94), U64(0xF4296DD6, 0xFEF3D67A), /* ~= 10^133 */
+    U64(0x8CE2529E, 0x2734BB1D), U64(0x1899E4A6, 0x5F58660C), /* ~= 10^134 */
+    U64(0xB01AE745, 0xB101E9E4), U64(0x5EC05DCF, 0xF72E7F8F), /* ~= 10^135 */
+    U64(0xDC21A117, 0x1D42645D), U64(0x76707543, 0xF4FA1F73), /* ~= 10^136 */
+    U64(0x899504AE, 0x72497EBA), U64(0x6A06494A, 0x791C53A8), /* ~= 10^137 */
+    U64(0xABFA45DA, 0x0EDBDE69), U64(0x0487DB9D, 0x17636892), /* ~= 10^138 */
+    U64(0xD6F8D750, 0x9292D603), U64(0x45A9D284, 0x5D3C42B6), /* ~= 10^139 */
+    U64(0x865B8692, 0x5B9BC5C2), U64(0x0B8A2392, 0xBA45A9B2), /* ~= 10^140 */
+    U64(0xA7F26836, 0xF282B732), U64(0x8E6CAC77, 0x68D7141E), /* ~= 10^141 */
+    U64(0xD1EF0244, 0xAF2364FF), U64(0x3207D795, 0x430CD926), /* ~= 10^142 */
+    U64(0x8335616A, 0xED761F1F), U64(0x7F44E6BD, 0x49E807B8), /* ~= 10^143 */
+    U64(0xA402B9C5, 0xA8D3A6E7), U64(0x5F16206C, 0x9C6209A6), /* ~= 10^144 */
+    U64(0xCD036837, 0x130890A1), U64(0x36DBA887, 0xC37A8C0F), /* ~= 10^145 */
+    U64(0x80222122, 0x6BE55A64), U64(0xC2494954, 0xDA2C9789), /* ~= 10^146 */
+    U64(0xA02AA96B, 0x06DEB0FD), U64(0xF2DB9BAA, 0x10B7BD6C), /* ~= 10^147 */
+    U64(0xC83553C5, 0xC8965D3D), U64(0x6F928294, 0x94E5ACC7), /* ~= 10^148 */
+    U64(0xFA42A8B7, 0x3ABBF48C), U64(0xCB772339, 0xBA1F17F9), /* ~= 10^149 */
+    U64(0x9C69A972, 0x84B578D7), U64(0xFF2A7604, 0x14536EFB), /* ~= 10^150 */
+    U64(0xC38413CF, 0x25E2D70D), U64(0xFEF51385, 0x19684ABA), /* ~= 10^151 */
+    U64(0xF46518C2, 0xEF5B8CD1), U64(0x7EB25866, 0x5FC25D69), /* ~= 10^152 */
+    U64(0x98BF2F79, 0xD5993802), U64(0xEF2F773F, 0xFBD97A61), /* ~= 10^153 */
+    U64(0xBEEEFB58, 0x4AFF8603), U64(0xAAFB550F, 0xFACFD8FA), /* ~= 10^154 */
+    U64(0xEEAABA2E, 0x5DBF6784), U64(0x95BA2A53, 0xF983CF38), /* ~= 10^155 */
+    U64(0x952AB45C, 0xFA97A0B2), U64(0xDD945A74, 0x7BF26183), /* ~= 10^156 */
+    U64(0xBA756174, 0x393D88DF), U64(0x94F97111, 0x9AEEF9E4), /* ~= 10^157 */
+    U64(0xE912B9D1, 0x478CEB17), U64(0x7A37CD56, 0x01AAB85D), /* ~= 10^158 */
+    U64(0x91ABB422, 0xCCB812EE), U64(0xAC62E055, 0xC10AB33A), /* ~= 10^159 */
+    U64(0xB616A12B, 0x7FE617AA), U64(0x577B986B, 0x314D6009), /* ~= 10^160 */
+    U64(0xE39C4976, 0x5FDF9D94), U64(0xED5A7E85, 0xFDA0B80B), /* ~= 10^161 */
+    U64(0x8E41ADE9, 0xFBEBC27D), U64(0x14588F13, 0xBE847307), /* ~= 10^162 */
+    U64(0xB1D21964, 0x7AE6B31C), U64(0x596EB2D8, 0xAE258FC8), /* ~= 10^163 */
+    U64(0xDE469FBD, 0x99A05FE3), U64(0x6FCA5F8E, 0xD9AEF3BB), /* ~= 10^164 */
+    U64(0x8AEC23D6, 0x80043BEE), U64(0x25DE7BB9, 0x480D5854), /* ~= 10^165 */
+    U64(0xADA72CCC, 0x20054AE9), U64(0xAF561AA7, 0x9A10AE6A), /* ~= 10^166 */
+    U64(0xD910F7FF, 0x28069DA4), U64(0x1B2BA151, 0x8094DA04), /* ~= 10^167 */
+    U64(0x87AA9AFF, 0x79042286), U64(0x90FB44D2, 0xF05D0842), /* ~= 10^168 */
+    U64(0xA99541BF, 0x57452B28), U64(0x353A1607, 0xAC744A53), /* ~= 10^169 */
+    U64(0xD3FA922F, 0x2D1675F2), U64(0x42889B89, 0x97915CE8), /* ~= 10^170 */
+    U64(0x847C9B5D, 0x7C2E09B7), U64(0x69956135, 0xFEBADA11), /* ~= 10^171 */
+    U64(0xA59BC234, 0xDB398C25), U64(0x43FAB983, 0x7E699095), /* ~= 10^172 */
+    U64(0xCF02B2C2, 0x1207EF2E), U64(0x94F967E4, 0x5E03F4BB), /* ~= 10^173 */
+    U64(0x8161AFB9, 0x4B44F57D), U64(0x1D1BE0EE, 0xBAC278F5), /* ~= 10^174 */
+    U64(0xA1BA1BA7, 0x9E1632DC), U64(0x6462D92A, 0x69731732), /* ~= 10^175 */
+    U64(0xCA28A291, 0x859BBF93), U64(0x7D7B8F75, 0x03CFDCFE), /* ~= 10^176 */
+    U64(0xFCB2CB35, 0xE702AF78), U64(0x5CDA7352, 0x44C3D43E), /* ~= 10^177 */
+    U64(0x9DEFBF01, 0xB061ADAB), U64(0x3A088813, 0x6AFA64A7), /* ~= 10^178 */
+    U64(0xC56BAEC2, 0x1C7A1916), U64(0x088AAA18, 0x45B8FDD0), /* ~= 10^179 */
+    U64(0xF6C69A72, 0xA3989F5B), U64(0x8AAD549E, 0x57273D45), /* ~= 10^180 */
+    U64(0x9A3C2087, 0xA63F6399), U64(0x36AC54E2, 0xF678864B), /* ~= 10^181 */
+    U64(0xC0CB28A9, 0x8FCF3C7F), U64(0x84576A1B, 0xB416A7DD), /* ~= 10^182 */
+    U64(0xF0FDF2D3, 0xF3C30B9F), U64(0x656D44A2, 0xA11C51D5), /* ~= 10^183 */
+    U64(0x969EB7C4, 0x7859E743), U64(0x9F644AE5, 0xA4B1B325), /* ~= 10^184 */
+    U64(0xBC4665B5, 0x96706114), U64(0x873D5D9F, 0x0DDE1FEE), /* ~= 10^185 */
+    U64(0xEB57FF22, 0xFC0C7959), U64(0xA90CB506, 0xD155A7EA), /* ~= 10^186 */
+    U64(0x9316FF75, 0xDD87CBD8), U64(0x09A7F124, 0x42D588F2), /* ~= 10^187 */
+    U64(0xB7DCBF53, 0x54E9BECE), U64(0x0C11ED6D, 0x538AEB2F), /* ~= 10^188 */
+    U64(0xE5D3EF28, 0x2A242E81), U64(0x8F1668C8, 0xA86DA5FA), /* ~= 10^189 */
+    U64(0x8FA47579, 0x1A569D10), U64(0xF96E017D, 0x694487BC), /* ~= 10^190 */
+    U64(0xB38D92D7, 0x60EC4455), U64(0x37C981DC, 0xC395A9AC), /* ~= 10^191 */
+    U64(0xE070F78D, 0x3927556A), U64(0x85BBE253, 0xF47B1417), /* ~= 10^192 */
+    U64(0x8C469AB8, 0x43B89562), U64(0x93956D74, 0x78CCEC8E), /* ~= 10^193 */
+    U64(0xAF584166, 0x54A6BABB), U64(0x387AC8D1, 0x970027B2), /* ~= 10^194 */
+    U64(0xDB2E51BF, 0xE9D0696A), U64(0x06997B05, 0xFCC0319E), /* ~= 10^195 */
+    U64(0x88FCF317, 0xF22241E2), U64(0x441FECE3, 0xBDF81F03), /* ~= 10^196 */
+    U64(0xAB3C2FDD, 0xEEAAD25A), U64(0xD527E81C, 0xAD7626C3), /* ~= 10^197 */
+    U64(0xD60B3BD5, 0x6A5586F1), U64(0x8A71E223, 0xD8D3B074), /* ~= 10^198 */
+    U64(0x85C70565, 0x62757456), U64(0xF6872D56, 0x67844E49), /* ~= 10^199 */
+    U64(0xA738C6BE, 0xBB12D16C), U64(0xB428F8AC, 0x016561DB), /* ~= 10^200 */
+    U64(0xD106F86E, 0x69D785C7), U64(0xE13336D7, 0x01BEBA52), /* ~= 10^201 */
+    U64(0x82A45B45, 0x0226B39C), U64(0xECC00246, 0x61173473), /* ~= 10^202 */
+    U64(0xA34D7216, 0x42B06084), U64(0x27F002D7, 0xF95D0190), /* ~= 10^203 */
+    U64(0xCC20CE9B, 0xD35C78A5), U64(0x31EC038D, 0xF7B441F4), /* ~= 10^204 */
+    U64(0xFF290242, 0xC83396CE), U64(0x7E670471, 0x75A15271), /* ~= 10^205 */
+    U64(0x9F79A169, 0xBD203E41), U64(0x0F0062C6, 0xE984D386), /* ~= 10^206 */
+    U64(0xC75809C4, 0x2C684DD1), U64(0x52C07B78, 0xA3E60868), /* ~= 10^207 */
+    U64(0xF92E0C35, 0x37826145), U64(0xA7709A56, 0xCCDF8A82), /* ~= 10^208 */
+    U64(0x9BBCC7A1, 0x42B17CCB), U64(0x88A66076, 0x400BB691), /* ~= 10^209 */
+    U64(0xC2ABF989, 0x935DDBFE), U64(0x6ACFF893, 0xD00EA435), /* ~= 10^210 */
+    U64(0xF356F7EB, 0xF83552FE), U64(0x0583F6B8, 0xC4124D43), /* ~= 10^211 */
+    U64(0x98165AF3, 0x7B2153DE), U64(0xC3727A33, 0x7A8B704A), /* ~= 10^212 */
+    U64(0xBE1BF1B0, 0x59E9A8D6), U64(0x744F18C0, 0x592E4C5C), /* ~= 10^213 */
+    U64(0xEDA2EE1C, 0x7064130C), U64(0x1162DEF0, 0x6F79DF73), /* ~= 10^214 */
+    U64(0x9485D4D1, 0xC63E8BE7), U64(0x8ADDCB56, 0x45AC2BA8), /* ~= 10^215 */
+    U64(0xB9A74A06, 0x37CE2EE1), U64(0x6D953E2B, 0xD7173692), /* ~= 10^216 */
+    U64(0xE8111C87, 0xC5C1BA99), U64(0xC8FA8DB6, 0xCCDD0437), /* ~= 10^217 */
+    U64(0x910AB1D4, 0xDB9914A0), U64(0x1D9C9892, 0x400A22A2), /* ~= 10^218 */
+    U64(0xB54D5E4A, 0x127F59C8), U64(0x2503BEB6, 0xD00CAB4B), /* ~= 10^219 */
+    U64(0xE2A0B5DC, 0x971F303A), U64(0x2E44AE64, 0x840FD61D), /* ~= 10^220 */
+    U64(0x8DA471A9, 0xDE737E24), U64(0x5CEAECFE, 0xD289E5D2), /* ~= 10^221 */
+    U64(0xB10D8E14, 0x56105DAD), U64(0x7425A83E, 0x872C5F47), /* ~= 10^222 */
+    U64(0xDD50F199, 0x6B947518), U64(0xD12F124E, 0x28F77719), /* ~= 10^223 */
+    U64(0x8A5296FF, 0xE33CC92F), U64(0x82BD6B70, 0xD99AAA6F), /* ~= 10^224 */
+    U64(0xACE73CBF, 0xDC0BFB7B), U64(0x636CC64D, 0x1001550B), /* ~= 10^225 */
+    U64(0xD8210BEF, 0xD30EFA5A), U64(0x3C47F7E0, 0x5401AA4E), /* ~= 10^226 */
+    U64(0x8714A775, 0xE3E95C78), U64(0x65ACFAEC, 0x34810A71), /* ~= 10^227 */
+    U64(0xA8D9D153, 0x5CE3B396), U64(0x7F1839A7, 0x41A14D0D), /* ~= 10^228 */
+    U64(0xD31045A8, 0x341CA07C), U64(0x1EDE4811, 0x1209A050), /* ~= 10^229 */
+    U64(0x83EA2B89, 0x2091E44D), U64(0x934AED0A, 0xAB460432), /* ~= 10^230 */
+    U64(0xA4E4B66B, 0x68B65D60), U64(0xF81DA84D, 0x5617853F), /* ~= 10^231 */
+    U64(0xCE1DE406, 0x42E3F4B9), U64(0x36251260, 0xAB9D668E), /* ~= 10^232 */
+    U64(0x80D2AE83, 0xE9CE78F3), U64(0xC1D72B7C, 0x6B426019), /* ~= 10^233 */
+    U64(0xA1075A24, 0xE4421730), U64(0xB24CF65B, 0x8612F81F), /* ~= 10^234 */
+    U64(0xC94930AE, 0x1D529CFC), U64(0xDEE033F2, 0x6797B627), /* ~= 10^235 */
+    U64(0xFB9B7CD9, 0xA4A7443C), U64(0x169840EF, 0x017DA3B1), /* ~= 10^236 */
+    U64(0x9D412E08, 0x06E88AA5), U64(0x8E1F2895, 0x60EE864E), /* ~= 10^237 */
+    U64(0xC491798A, 0x08A2AD4E), U64(0xF1A6F2BA, 0xB92A27E2), /* ~= 10^238 */
+    U64(0xF5B5D7EC, 0x8ACB58A2), U64(0xAE10AF69, 0x6774B1DB), /* ~= 10^239 */
+    U64(0x9991A6F3, 0xD6BF1765), U64(0xACCA6DA1, 0xE0A8EF29), /* ~= 10^240 */
+    U64(0xBFF610B0, 0xCC6EDD3F), U64(0x17FD090A, 0x58D32AF3), /* ~= 10^241 */
+    U64(0xEFF394DC, 0xFF8A948E), U64(0xDDFC4B4C, 0xEF07F5B0), /* ~= 10^242 */
+    U64(0x95F83D0A, 0x1FB69CD9), U64(0x4ABDAF10, 0x1564F98E), /* ~= 10^243 */
+    U64(0xBB764C4C, 0xA7A4440F), U64(0x9D6D1AD4, 0x1ABE37F1), /* ~= 10^244 */
+    U64(0xEA53DF5F, 0xD18D5513), U64(0x84C86189, 0x216DC5ED), /* ~= 10^245 */
+    U64(0x92746B9B, 0xE2F8552C), U64(0x32FD3CF5, 0xB4E49BB4), /* ~= 10^246 */
+    U64(0xB7118682, 0xDBB66A77), U64(0x3FBC8C33, 0x221DC2A1), /* ~= 10^247 */
+    U64(0xE4D5E823, 0x92A40515), U64(0x0FABAF3F, 0xEAA5334A), /* ~= 10^248 */
+    U64(0x8F05B116, 0x3BA6832D), U64(0x29CB4D87, 0xF2A7400E), /* ~= 10^249 */
+    U64(0xB2C71D5B, 0xCA9023F8), U64(0x743E20E9, 0xEF511012), /* ~= 10^250 */
+    U64(0xDF78E4B2, 0xBD342CF6), U64(0x914DA924, 0x6B255416), /* ~= 10^251 */
+    U64(0x8BAB8EEF, 0xB6409C1A), U64(0x1AD089B6, 0xC2F7548E), /* ~= 10^252 */
+    U64(0xAE9672AB, 0xA3D0C320), U64(0xA184AC24, 0x73B529B1), /* ~= 10^253 */
+    U64(0xDA3C0F56, 0x8CC4F3E8), U64(0xC9E5D72D, 0x90A2741E), /* ~= 10^254 */
+    U64(0x88658996, 0x17FB1871), U64(0x7E2FA67C, 0x7A658892), /* ~= 10^255 */
+    U64(0xAA7EEBFB, 0x9DF9DE8D), U64(0xDDBB901B, 0x98FEEAB7), /* ~= 10^256 */
+    U64(0xD51EA6FA, 0x85785631), U64(0x552A7422, 0x7F3EA565), /* ~= 10^257 */
+    U64(0x8533285C, 0x936B35DE), U64(0xD53A8895, 0x8F87275F), /* ~= 10^258 */
+    U64(0xA67FF273, 0xB8460356), U64(0x8A892ABA, 0xF368F137), /* ~= 10^259 */
+    U64(0xD01FEF10, 0xA657842C), U64(0x2D2B7569, 0xB0432D85), /* ~= 10^260 */
+    U64(0x8213F56A, 0x67F6B29B), U64(0x9C3B2962, 0x0E29FC73), /* ~= 10^261 */
+    U64(0xA298F2C5, 0x01F45F42), U64(0x8349F3BA, 0x91B47B8F), /* ~= 10^262 */
+    U64(0xCB3F2F76, 0x42717713), U64(0x241C70A9, 0x36219A73), /* ~= 10^263 */
+    U64(0xFE0EFB53, 0xD30DD4D7), U64(0xED238CD3, 0x83AA0110), /* ~= 10^264 */
+    U64(0x9EC95D14, 0x63E8A506), U64(0xF4363804, 0x324A40AA), /* ~= 10^265 */
+    U64(0xC67BB459, 0x7CE2CE48), U64(0xB143C605, 0x3EDCD0D5), /* ~= 10^266 */
+    U64(0xF81AA16F, 0xDC1B81DA), U64(0xDD94B786, 0x8E94050A), /* ~= 10^267 */
+    U64(0x9B10A4E5, 0xE9913128), U64(0xCA7CF2B4, 0x191C8326), /* ~= 10^268 */
+    U64(0xC1D4CE1F, 0x63F57D72), U64(0xFD1C2F61, 0x1F63A3F0), /* ~= 10^269 */
+    U64(0xF24A01A7, 0x3CF2DCCF), U64(0xBC633B39, 0x673C8CEC), /* ~= 10^270 */
+    U64(0x976E4108, 0x8617CA01), U64(0xD5BE0503, 0xE085D813), /* ~= 10^271 */
+    U64(0xBD49D14A, 0xA79DBC82), U64(0x4B2D8644, 0xD8A74E18), /* ~= 10^272 */
+    U64(0xEC9C459D, 0x51852BA2), U64(0xDDF8E7D6, 0x0ED1219E), /* ~= 10^273 */
+    U64(0x93E1AB82, 0x52F33B45), U64(0xCABB90E5, 0xC942B503), /* ~= 10^274 */
+    U64(0xB8DA1662, 0xE7B00A17), U64(0x3D6A751F, 0x3B936243), /* ~= 10^275 */
+    U64(0xE7109BFB, 0xA19C0C9D), U64(0x0CC51267, 0x0A783AD4), /* ~= 10^276 */
+    U64(0x906A617D, 0x450187E2), U64(0x27FB2B80, 0x668B24C5), /* ~= 10^277 */
+    U64(0xB484F9DC, 0x9641E9DA), U64(0xB1F9F660, 0x802DEDF6), /* ~= 10^278 */
+    U64(0xE1A63853, 0xBBD26451), U64(0x5E7873F8, 0xA0396973), /* ~= 10^279 */
+    U64(0x8D07E334, 0x55637EB2), U64(0xDB0B487B, 0x6423E1E8), /* ~= 10^280 */
+    U64(0xB049DC01, 0x6ABC5E5F), U64(0x91CE1A9A, 0x3D2CDA62), /* ~= 10^281 */
+    U64(0xDC5C5301, 0xC56B75F7), U64(0x7641A140, 0xCC7810FB), /* ~= 10^282 */
+    U64(0x89B9B3E1, 0x1B6329BA), U64(0xA9E904C8, 0x7FCB0A9D), /* ~= 10^283 */
+    U64(0xAC2820D9, 0x623BF429), U64(0x546345FA, 0x9FBDCD44), /* ~= 10^284 */
+    U64(0xD732290F, 0xBACAF133), U64(0xA97C1779, 0x47AD4095), /* ~= 10^285 */
+    U64(0x867F59A9, 0xD4BED6C0), U64(0x49ED8EAB, 0xCCCC485D), /* ~= 10^286 */
+    U64(0xA81F3014, 0x49EE8C70), U64(0x5C68F256, 0xBFFF5A74), /* ~= 10^287 */
+    U64(0xD226FC19, 0x5C6A2F8C), U64(0x73832EEC, 0x6FFF3111), /* ~= 10^288 */
+    U64(0x83585D8F, 0xD9C25DB7), U64(0xC831FD53, 0xC5FF7EAB), /* ~= 10^289 */
+    U64(0xA42E74F3, 0xD032F525), U64(0xBA3E7CA8, 0xB77F5E55), /* ~= 10^290 */
+    U64(0xCD3A1230, 0xC43FB26F), U64(0x28CE1BD2, 0xE55F35EB), /* ~= 10^291 */
+    U64(0x80444B5E, 0x7AA7CF85), U64(0x7980D163, 0xCF5B81B3), /* ~= 10^292 */
+    U64(0xA0555E36, 0x1951C366), U64(0xD7E105BC, 0xC332621F), /* ~= 10^293 */
+    U64(0xC86AB5C3, 0x9FA63440), U64(0x8DD9472B, 0xF3FEFAA7), /* ~= 10^294 */
+    U64(0xFA856334, 0x878FC150), U64(0xB14F98F6, 0xF0FEB951), /* ~= 10^295 */
+    U64(0x9C935E00, 0xD4B9D8D2), U64(0x6ED1BF9A, 0x569F33D3), /* ~= 10^296 */
+    U64(0xC3B83581, 0x09E84F07), U64(0x0A862F80, 0xEC4700C8), /* ~= 10^297 */
+    U64(0xF4A642E1, 0x4C6262C8), U64(0xCD27BB61, 0x2758C0FA), /* ~= 10^298 */
+    U64(0x98E7E9CC, 0xCFBD7DBD), U64(0x8038D51C, 0xB897789C), /* ~= 10^299 */
+    U64(0xBF21E440, 0x03ACDD2C), U64(0xE0470A63, 0xE6BD56C3), /* ~= 10^300 */
+    U64(0xEEEA5D50, 0x04981478), U64(0x1858CCFC, 0xE06CAC74), /* ~= 10^301 */
+    U64(0x95527A52, 0x02DF0CCB), U64(0x0F37801E, 0x0C43EBC8), /* ~= 10^302 */
+    U64(0xBAA718E6, 0x8396CFFD), U64(0xD3056025, 0x8F54E6BA), /* ~= 10^303 */
+    U64(0xE950DF20, 0x247C83FD), U64(0x47C6B82E, 0xF32A2069), /* ~= 10^304 */
+    U64(0x91D28B74, 0x16CDD27E), U64(0x4CDC331D, 0x57FA5441), /* ~= 10^305 */
+    U64(0xB6472E51, 0x1C81471D), U64(0xE0133FE4, 0xADF8E952), /* ~= 10^306 */
+    U64(0xE3D8F9E5, 0x63A198E5), U64(0x58180FDD, 0xD97723A6), /* ~= 10^307 */
+    U64(0x8E679C2F, 0x5E44FF8F), U64(0x570F09EA, 0xA7EA7648), /* ~= 10^308 */
+    U64(0xB201833B, 0x35D63F73), U64(0x2CD2CC65, 0x51E513DA), /* ~= 10^309 */
+    U64(0xDE81E40A, 0x034BCF4F), U64(0xF8077F7E, 0xA65E58D1), /* ~= 10^310 */
+    U64(0x8B112E86, 0x420F6191), U64(0xFB04AFAF, 0x27FAF782), /* ~= 10^311 */
+    U64(0xADD57A27, 0xD29339F6), U64(0x79C5DB9A, 0xF1F9B563), /* ~= 10^312 */
+    U64(0xD94AD8B1, 0xC7380874), U64(0x18375281, 0xAE7822BC), /* ~= 10^313 */
+    U64(0x87CEC76F, 0x1C830548), U64(0x8F229391, 0x0D0B15B5), /* ~= 10^314 */
+    U64(0xA9C2794A, 0xE3A3C69A), U64(0xB2EB3875, 0x504DDB22), /* ~= 10^315 */
+    U64(0xD433179D, 0x9C8CB841), U64(0x5FA60692, 0xA46151EB), /* ~= 10^316 */
+    U64(0x849FEEC2, 0x81D7F328), U64(0xDBC7C41B, 0xA6BCD333), /* ~= 10^317 */
+    U64(0xA5C7EA73, 0x224DEFF3), U64(0x12B9B522, 0x906C0800), /* ~= 10^318 */
+    U64(0xCF39E50F, 0xEAE16BEF), U64(0xD768226B, 0x34870A00), /* ~= 10^319 */
+    U64(0x81842F29, 0xF2CCE375), U64(0xE6A11583, 0x00D46640), /* ~= 10^320 */
+    U64(0xA1E53AF4, 0x6F801C53), U64(0x60495AE3, 0xC1097FD0), /* ~= 10^321 */
+    U64(0xCA5E89B1, 0x8B602368), U64(0x385BB19C, 0xB14BDFC4), /* ~= 10^322 */
+    U64(0xFCF62C1D, 0xEE382C42), U64(0x46729E03, 0xDD9ED7B5), /* ~= 10^323 */
+    U64(0x9E19DB92, 0xB4E31BA9), U64(0x6C07A2C2, 0x6A8346D1)  /* ~= 10^324 */
+};
+
+/**
+ Get the cached pow10 value from pow10_sig_table.
+ @param exp10 The exponent of pow(10, e). This value must in range
+              POW10_SIG_TABLE_MIN_EXP to POW10_SIG_TABLE_MAX_EXP.
+ @param hi    The highest 64 bits of pow(10, e).
+ @param lo    The lower 64 bits after `hi`.
+ */
+static_inline void pow10_table_get_sig(i32 exp10, u64 *hi, u64 *lo) {
+    i32 idx = exp10 - (POW10_SIG_TABLE_MIN_EXP);
+    *hi = pow10_sig_table[idx * 2];
+    *lo = pow10_sig_table[idx * 2 + 1];
+}
+
+/**
+ Get the exponent (base 2) for highest 64 bits significand in pow10_sig_table.
+ */
+static_inline void pow10_table_get_exp(i32 exp10, i32 *exp2) {
+    /* e2 = floor(log2(pow(10, e))) - 64 + 1 */
+    /*    = floor(e * log2(10) - 63)         */
+    *exp2 = (exp10 * 217706 - 4128768) >> 16;
+}
+
+#endif
+
+
+#if !YYJSON_DISABLE_READER
+
+/*==============================================================================
+ * JSON Character Matcher
+ *============================================================================*/
+
+/** Character type */
+typedef u8 char_type;
+
+/** Whitespace character: ' ', '\\t', '\\n', '\\r'. */
+static const char_type CHAR_TYPE_SPACE      = 1 << 0;
+
+/** Number character: '-', [0-9]. */
+static const char_type CHAR_TYPE_NUMBER     = 1 << 1;
+
+/** JSON Escaped character: '"', '\', [0x00-0x1F]. */
+static const char_type CHAR_TYPE_ESC_ASCII  = 1 << 2;
+
+/** Non-ASCII character: [0x80-0xFF]. */
+static const char_type CHAR_TYPE_NON_ASCII  = 1 << 3;
+
+/** JSON container character: '{', '['. */
+static const char_type CHAR_TYPE_CONTAINER  = 1 << 4;
+
+/** Comment character: '/'. */
+static const char_type CHAR_TYPE_COMMENT    = 1 << 5;
+
+/** Line end character '\\n', '\\r', '\0'. */
+static const char_type CHAR_TYPE_LINE_END   = 1 << 6;
+
+/** Character type table (generate with misc/make_tables.c) */
+static const char_type char_table[256] = {
+    0x44, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04,
+    0x04, 0x05, 0x45, 0x04, 0x04, 0x45, 0x04, 0x04,
+    0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04,
+    0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04,
+    0x01, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x20,
+    0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02,
+    0x02, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x10, 0x04, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08,
+    0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08, 0x08
+};
+
+/** Match a character with specified type. */
+static_inline bool char_is_type(u8 c, char_type type) {
+    return (char_table[c] & type) != 0;
+}
+
+/** Match a whitespace: ' ', '\\t', '\\n', '\\r'. */
+static_inline bool char_is_space(u8 c) {
+    return char_is_type(c, (char_type)CHAR_TYPE_SPACE);
+}
+
+/** Match a whitespace or comment: ' ', '\\t', '\\n', '\\r', '/'. */
+static_inline bool char_is_space_or_comment(u8 c) {
+    return char_is_type(c, (char_type)(CHAR_TYPE_SPACE | CHAR_TYPE_COMMENT));
+}
+
+/** Match a JSON number: '-', [0-9]. */
+static_inline bool char_is_number(u8 c) {
+    return char_is_type(c, (char_type)CHAR_TYPE_NUMBER);
+}
+
+/** Match a JSON container: '{', '['. */
+static_inline bool char_is_container(u8 c) {
+    return char_is_type(c, (char_type)CHAR_TYPE_CONTAINER);
+}
+
+/** Match a stop character in ASCII string: '"', '\', [0x00-0x1F], [0x80-0xFF]*/
+static_inline bool char_is_ascii_stop(u8 c) {
+    return char_is_type(c, (char_type)(CHAR_TYPE_ESC_ASCII |
+                                       CHAR_TYPE_NON_ASCII));
+}
+
+/** Match a line end character: '\\n', '\\r', '\0'*/
+static_inline bool char_is_line_end(u8 c) {
+    return char_is_type(c, (char_type)CHAR_TYPE_LINE_END);
+}
+
+
+
+/*==============================================================================
+ * Digit Character Matcher
+ *============================================================================*/
+
+/** Digit type */
+typedef u8 digi_type;
+
+/** Digit: '0'. */
+static const digi_type DIGI_TYPE_ZERO       = 1 << 0;
+
+/** Digit: [1-9]. */
+static const digi_type DIGI_TYPE_NONZERO    = 1 << 1;
+
+/** Plus sign (positive): '+'. */
+static const digi_type DIGI_TYPE_POS        = 1 << 2;
+
+/** Minus sign (negative): '-'. */
+static const digi_type DIGI_TYPE_NEG        = 1 << 3;
+
+/** Decimal point: '.' */
+static const digi_type DIGI_TYPE_DOT        = 1 << 4;
+
+/** Exponent sign: 'e, 'E'. */
+static const digi_type DIGI_TYPE_EXP        = 1 << 5;
+
+/** Digit type table (generate with misc/make_tables.c) */
+static const digi_type digi_table[256] = {
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x04, 0x00, 0x08, 0x10, 0x00,
+    0x01, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02,
+    0x02, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
+};
+
+/** Match a character with specified type. */
+static_inline bool digi_is_type(u8 d, digi_type type) {
+    return (digi_table[d] & type) != 0;
+}
+
+/** Match a sign: '+', '-' */
+static_inline bool digi_is_sign(u8 d) {
+    return digi_is_type(d, (digi_type)(DIGI_TYPE_POS | DIGI_TYPE_NEG));
+}
+
+/** Match a none zero digit: [1-9] */
+static_inline bool digi_is_nonzero(u8 d) {
+    return digi_is_type(d, (digi_type)DIGI_TYPE_NONZERO);
+}
+
+/** Match a digit: [0-9] */
+static_inline bool digi_is_digit(u8 d) {
+    return digi_is_type(d, (digi_type)(DIGI_TYPE_ZERO | DIGI_TYPE_NONZERO));
+}
+
+/** Match an exponent sign: 'e', 'E'. */
+static_inline bool digi_is_exp(u8 d) {
+    return digi_is_type(d, (digi_type)DIGI_TYPE_EXP);
+}
+
+/** Match a floating point indicator: '.', 'e', 'E'. */
+static_inline bool digi_is_fp(u8 d) {
+    return digi_is_type(d, (digi_type)(DIGI_TYPE_DOT | DIGI_TYPE_EXP));
+}
+
+/** Match a digit or floating point indicator: [0-9], '.', 'e', 'E'. */
+static_inline bool digi_is_digit_or_fp(u8 d) {
+    return digi_is_type(d, (digi_type)(DIGI_TYPE_ZERO | DIGI_TYPE_NONZERO |
+                                       DIGI_TYPE_DOT | DIGI_TYPE_EXP));
+}
+
+
+
+/*==============================================================================
+ * Hex Character Reader
+ * This function is used by JSON reader to read escaped characters.
+ *============================================================================*/
+
+/**
+ This table is used to convert 4 hex character sequence to a number,
+ A valid hex character [0-9A-Fa-f] will mapped to it's raw number [0x00, 0x0F],
+ an invalid hex character will mapped to [0xF0].
+ (generate with misc/make_tables.c)
+ */
+static const u8 hex_conv_table[256] = {
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,
+    0x08, 0x09, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0,
+    0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0, 0xF0
+};
+
+/**
+ Scans an escaped character sequence as a UTF-16 code unit (branchless).
+ e.g. "\\u005C" should pass "005C" as `cur`.
+ 
+ This requires the string has 4-byte zero padding.
+ */
+static_inline bool read_hex_u16(const u8 *cur, u16 *val) {
+    u16 c0, c1, c2, c3, t0, t1;
+    c0 = hex_conv_table[cur[0]];
+    c1 = hex_conv_table[cur[1]];
+    c2 = hex_conv_table[cur[2]];
+    c3 = hex_conv_table[cur[3]];
+    t0 = (u16)((c0 << 8) | c2);
+    t1 = (u16)((c1 << 8) | c3);
+    *val = (u16)((t0 << 4) | t1);
+    return ((t0 | t1) & (u16)0xF0F0) == 0;
+}
+
+
+
+/*==============================================================================
+ * JSON Reader Utils
+ * These functions are used by JSON reader to read literals and comments.
+ *============================================================================*/
+
+/** Read 'true' literal, '*cur' should be 't'. */
+static_inline bool read_true(u8 *cur, u8 **end, yyjson_val *val) {
+    if (likely(byte_match_4(cur, "true"))) {
+        val->tag = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE;
+        *end = cur + 4;
+        return true;
+    }
+    return false;
+}
+
+/** Read 'false' literal, '*cur' should be 'f'. */
+static_inline bool read_false(u8 *cur, u8 **end, yyjson_val *val) {
+    if (likely(byte_match_4(cur + 1, "alse"))) {
+        val->tag = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE;
+        *end = cur + 5;
+        return true;
+    }
+    return false;
+}
+
+/** Read 'null' literal, '*cur' should be 'n'. */
+static_inline bool read_null(u8 *cur, u8 **end, yyjson_val *val) {
+    if (likely(byte_match_4(cur, "null"))) {
+        val->tag = YYJSON_TYPE_NULL;
+        *end = cur + 4;
+        return true;
+    }
+    return false;
+}
+
+/** Read 'Inf' or 'Infinity' literal (ignoring case). */
+static_inline bool read_inf(bool sign, u8 *cur, u8 **end, yyjson_val *val) {
+#if !YYJSON_DISABLE_NON_STANDARD
+    if ((cur[0] == 'I' || cur[0] == 'i') &&
+        (cur[1] == 'N' || cur[1] == 'n') &&
+        (cur[2] == 'F' || cur[2] == 'f')) {
+        if ((cur[3] == 'I' || cur[3] == 'i') &&
+            (cur[4] == 'N' || cur[4] == 'n') &&
+            (cur[5] == 'I' || cur[5] == 'i') &&
+            (cur[6] == 'T' || cur[6] == 't') &&
+            (cur[7] == 'Y' || cur[7] == 'y')) {
+            *end = cur + 8;
+        } else {
+            *end = cur + 3;
+        }
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+        val->uni.u64 = f64_raw_get_inf(sign);
+        return true;
+    }
+#endif
+    return false;
+}
+
+/** Read 'NaN' literal (ignoring case). */
+static_inline bool read_nan(bool sign, u8 *cur, u8 **end, yyjson_val *val) {
+#if !YYJSON_DISABLE_NON_STANDARD
+    if ((cur[0] == 'N' || cur[0] == 'n') &&
+        (cur[1] == 'A' || cur[1] == 'a') &&
+        (cur[2] == 'N' || cur[2] == 'n')) {
+        *end = cur + 3;
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+        val->uni.u64 = f64_raw_get_nan(sign);
+        return true;
+    }
+#endif
+    return false;
+}
+
+/** Read 'Inf', 'Infinity' or 'NaN' literal (ignoring case). */
+static_inline bool read_inf_or_nan(bool sign, u8 *cur, u8 **end,
+                                   yyjson_val *val) {
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (read_inf(sign, cur, end, val)) return true;
+    if (read_nan(sign, cur, end, val)) return true;
+#endif
+    return false;
+}
+
+/**
+ Skips spaces and comments as many as possible.
+ 
+ It will return false in these cases:
+    1. No character is skipped. The 'end' pointer is set as input cursor.
+    2. A multiline comment is not closed. The 'end' pointer is set as the head
+       of this comment block.
+ */
+static_noinline bool skip_spaces_and_comments(u8 *cur, u8 **end) {
+    u8 *hdr = cur;
+    while (true) {
+        if (byte_match_2(cur, "/*")) {
+            hdr = cur;
+            cur += 2;
+            while (true) {
+                if (byte_match_2(cur, "*/")) {
+                    cur += 2;
+                    break;
+                }
+                if (byte_match_1(cur, "\0")) {
+                    *end = hdr;
+                    return false;
+                }
+                cur++;
+            }
+            continue;
+        }
+        if (byte_match_2(cur, "//")) {
+            cur += 2;
+            while (!char_is_line_end(*cur)) cur++;
+            continue;
+        }
+        if (char_is_space(*cur)) {
+            cur += 1;
+            while (char_is_space(*cur)) cur++;
+            continue;
+        }
+        break;
+    }
+    *end = cur;
+    return hdr != cur;
+}
+
+
+
+#if YYJSON_HAS_IEEE_754 && !YYJSON_DISABLE_FAST_FP_CONV
+
+/*==============================================================================
+ * BigInt For Floating Point Number Reader
+ *
+ * The bigint algorithm is used by floating-point number reader to get correctly
+ * rounded result for numbers with lots of digits. This part of code is rarely
+ * used for common numbers.
+ *============================================================================*/
+
+/** Maximum exponent of exact pow10 */
+#define U64_POW10_MAX_EXP 19
+
+/** Table: [ 10^0, ..., 10^19 ] (generate with misc/make_tables.c) */
+static const u64 u64_pow10_table[U64_POW10_MAX_EXP + 1] = {
+    U64(0x00000000, 0x00000001), U64(0x00000000, 0x0000000A),
+    U64(0x00000000, 0x00000064), U64(0x00000000, 0x000003E8),
+    U64(0x00000000, 0x00002710), U64(0x00000000, 0x000186A0),
+    U64(0x00000000, 0x000F4240), U64(0x00000000, 0x00989680),
+    U64(0x00000000, 0x05F5E100), U64(0x00000000, 0x3B9ACA00),
+    U64(0x00000002, 0x540BE400), U64(0x00000017, 0x4876E800),
+    U64(0x000000E8, 0xD4A51000), U64(0x00000918, 0x4E72A000),
+    U64(0x00005AF3, 0x107A4000), U64(0x00038D7E, 0xA4C68000),
+    U64(0x002386F2, 0x6FC10000), U64(0x01634578, 0x5D8A0000),
+    U64(0x0DE0B6B3, 0xA7640000), U64(0x8AC72304, 0x89E80000)
+};
+
+/** Maximum numbers of chunks used by a bigint (58 is enough here). */
+#define BIGINT_MAX_CHUNKS 64
+
+/** Unsigned arbitrarily large integer */
+typedef struct bigint {
+    u32 used; /* used chunks count, should not be 0 */
+    u64 bits[BIGINT_MAX_CHUNKS]; /* chunks */
+} bigint;
+
+/**
+ Evaluate 'big += val'.
+ @param big A big number (can be 0).
+ @param val An unsigned integer (can be 0).
+ */
+static_inline void bigint_add_u64(bigint *big, u64 val) {
+    u32 idx, max;
+    u64 num = big->bits[0];
+    u64 add = num + val;
+    big->bits[0] = add;
+    if (likely((add >= num) || (add >= val))) return;
+    for ((void)(idx = 1), max = big->used; idx < max; idx++) {
+        if (likely(big->bits[idx] != U64_MAX)) {
+            big->bits[idx] += 1;
+            return;
+        }
+        big->bits[idx] = 0;
+    }
+    big->bits[big->used++] = 1;
+}
+
+/**
+ Evaluate 'big *= val'.
+ @param big A big number (can be 0).
+ @param val An unsigned integer (cannot be 0).
+ */
+static_inline void bigint_mul_u64(bigint *big, u64 val) {
+    u32 idx = 0, max = big->used;
+    u64 hi, lo, carry = 0;
+    for (; idx < max; idx++) {
+        if (big->bits[idx]) break;
+    }
+    for (; idx < max; idx++) {
+        u128_mul_add(big->bits[idx], val, carry, &hi, &lo);
+        big->bits[idx] = lo;
+        carry = hi;
+    }
+    if (carry) big->bits[big->used++] = carry;
+}
+
+/**
+ Evaluate 'big *= 2^exp'.
+ @param big A big number (can be 0).
+ @param exp An exponent integer (can be 0).
+ */
+static_inline void bigint_mul_pow2(bigint *big, u32 exp) {
+    u32 shft = exp % 64;
+    u32 move = exp / 64;
+    u32 idx = big->used;
+    if (unlikely(shft == 0)) {
+        for (; idx > 0; idx--) {
+            big->bits[idx + move - 1] = big->bits[idx - 1];
+        }
+        big->used += move;
+        while (move) big->bits[--move] = 0;
+    } else {
+        big->bits[idx] = 0;
+        for (; idx > 0; idx--) {
+            u64 num = big->bits[idx] << shft;
+            num |= big->bits[idx - 1] >> (64 - shft);
+            big->bits[idx + move] = num;
+        }
+        big->bits[move] = big->bits[0] << shft;
+        big->used += move + (big->bits[big->used + move] > 0);
+        while (move) big->bits[--move] = 0;
+    }
+}
+
+/**
+ Evaluate 'big *= 10^exp'.
+ @param big A big number (can be 0).
+ @param exp An exponent integer (cannot be 0).
+ */
+static_inline void bigint_mul_pow10(bigint *big, i32 exp) {
+    for (; exp >= U64_POW10_MAX_EXP; exp -= U64_POW10_MAX_EXP) {
+        bigint_mul_u64(big, u64_pow10_table[U64_POW10_MAX_EXP]);
+    }
+    if (exp) {
+        bigint_mul_u64(big, u64_pow10_table[exp]);
+    }
+}
+
+/**
+ Compare two bigint.
+ @return -1 if 'a < b', +1 if 'a > b', 0 if 'a == b'.
+ */
+static_inline i32 bigint_cmp(bigint *a, bigint *b) {
+    u32 idx = a->used;
+    if (a->used < b->used) return -1;
+    if (a->used > b->used) return +1;
+    while (idx-- > 0) {
+        u64 av = a->bits[idx];
+        u64 bv = b->bits[idx];
+        if (av < bv) return -1;
+        if (av > bv) return +1;
+    }
+    return 0;
+}
+
+/**
+ Evaluate 'big = val'.
+ @param big A big number (can be 0).
+ @param val An unsigned integer (can be 0).
+ */
+static_inline void bigint_set_u64(bigint *big, u64 val) {
+    big->used = 1;
+    big->bits[0] = val;
+}
+
+/** Set a bigint with floating point number string. */
+static_noinline void bigint_set_buf(bigint *big, u64 sig, i32 *exp,
+                                    u8 *sig_cut, u8 *sig_end, u8 *dot_pos) {
+    
+    if (unlikely(!sig_cut)) {
+        /* no digit cut, set significant part only */
+        bigint_set_u64(big, sig);
+        return;
+        
+    } else {
+        /* some digits were cut, read them from 'sig_cut' to 'sig_end' */
+        u8 *hdr = sig_cut;
+        u8 *cur = hdr;
+        u32 len = 0;
+        u64 val = 0;
+        bool dig_big_cut = false;
+        bool has_dot = (hdr < dot_pos) & (dot_pos < sig_end);
+        u32 dig_len_total = U64_SAFE_DIG + (u32)(sig_end - hdr) - has_dot;
+        
+        sig -= (*sig_cut >= '5'); /* sig was rounded before */
+        if (dig_len_total > F64_MAX_DEC_DIG) {
+            dig_big_cut = true;
+            sig_end -= dig_len_total - (F64_MAX_DEC_DIG + 1);
+            sig_end -= (dot_pos + 1 == sig_end);
+            dig_len_total = (F64_MAX_DEC_DIG + 1);
+        }
+        *exp -= (i32)dig_len_total - U64_SAFE_DIG;
+        
+        big->used = 1;
+        big->bits[0] = sig;
+        while (cur < sig_end) {
+            if (likely(cur != dot_pos)) {
+                val = val * 10 + (u8)(*cur++ - '0');
+                len++;
+                if (unlikely(cur == sig_end && dig_big_cut)) {
+                    /* The last digit must be non-zero,    */
+                    /* set it to '1' for correct rounding. */
+                    val = val - (val % 10) + 1;
+                }
+                if (len == U64_SAFE_DIG || cur == sig_end) {
+                    bigint_mul_pow10(big, (i32)len);
+                    bigint_add_u64(big, val);
+                    val = 0;
+                    len = 0;
+                }
+            } else {
+                cur++;
+            }
+        }
+    }
+}
+
+
+
+/*==============================================================================
+ * Diy Floating Point
+ *============================================================================*/
+
+/** "Do It Yourself Floating Point" struct. */
+typedef struct diy_fp {
+    u64 sig; /* significand */
+    i32 exp; /* exponent, base 2 */
+    i32 pad; /* padding, useless */
+} diy_fp;
+
+/** Get cached rounded diy_fp with pow(10, e) The input value must in range
+    [POW10_SIG_TABLE_MIN_EXP, POW10_SIG_TABLE_MAX_EXP]. */
+static_inline diy_fp diy_fp_get_cached_pow10(i32 exp10) {
+    diy_fp fp;
+    u64 sig_ext;
+    pow10_table_get_sig(exp10, &fp.sig, &sig_ext);
+    pow10_table_get_exp(exp10, &fp.exp);
+    fp.sig += (sig_ext >> 63);
+    return fp;
+}
+
+/** Returns fp * fp2. */
+static_inline diy_fp diy_fp_mul(diy_fp fp, diy_fp fp2) {
+    u64 hi, lo;
+    u128_mul(fp.sig, fp2.sig, &hi, &lo);
+    fp.sig = hi + (lo >> 63);
+    fp.exp += fp2.exp + 64;
+    return fp;
+}
+
+/** Convert diy_fp to IEEE-754 raw value. */
+static_inline u64 diy_fp_to_ieee_raw(diy_fp fp) {
+    u64 sig = fp.sig;
+    i32 exp = fp.exp;
+    u32 lz_bits;
+    if (unlikely(fp.sig == 0)) return 0;
+    
+    lz_bits = u64_lz_bits(sig);
+    sig <<= lz_bits;
+    sig >>= F64_BITS - F64_SIG_FULL_BITS;
+    exp -= (i32)lz_bits;
+    exp += F64_BITS - F64_SIG_FULL_BITS;
+    exp += F64_SIG_BITS;
+    
+    if (unlikely(exp >= F64_MAX_BIN_EXP)) {
+        /* overflow */
+        return F64_RAW_INF;
+    } else if (likely(exp >= F64_MIN_BIN_EXP - 1)) {
+        /* normal */
+        exp += F64_EXP_BIAS;
+        return ((u64)exp << F64_SIG_BITS) | (sig & F64_SIG_MASK);
+    } else if (likely(exp >= F64_MIN_BIN_EXP - F64_SIG_FULL_BITS)) {
+        /* subnormal */
+        return sig >> (F64_MIN_BIN_EXP - exp - 1);
+    } else {
+        /* underflow */
+        return 0;
+    }
+}
+
+
+
+/*==============================================================================
+ * JSON Number Reader (IEEE-754)
+ *============================================================================*/
+
+/** Maximum exact pow10 exponent for double value. */
+#define F64_POW10_EXP_MAX_EXACT 22
+
+/** Cached pow10 table. */
+static const f64 f64_pow10_table[] = {
+    1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9, 1e10, 1e11, 1e12,
+    1e13, 1e14, 1e15, 1e16, 1e17, 1e18, 1e19, 1e20, 1e21, 1e22
+};
+
+/**
+ Read a JSON number.
+ 
+ 1. This function assume that the floating-point number is in IEEE-754 format.
+ 2. This function support uint64/int64/double number. If an integer number
+    cannot fit in uint64/int64, it will returns as a double number. If a double
+    number is infinite, the return value is based on flag.
+ 3. This function (with inline attribute) may generate a lot of instructions.
+ */
+static_inline bool read_number(u8 *cur,
+                               u8 **end,
+                               yyjson_read_flag flg,
+                               yyjson_val *val,
+                               const char **msg) {
+    
+#define has_flag(_flag) unlikely((flg & YYJSON_READ_##_flag) != 0)
+    
+#define return_err(_pos, _msg) do { \
+    *msg = _msg; \
+    *end = _pos; \
+    return false; \
+} while (false)
+    
+#define return_i64(_v) do { \
+    val->tag = YYJSON_TYPE_NUM | (u8)((u8)sign << 3); \
+    val->uni.u64 = (u64)(sign ? (u64)(~(_v) + 1) : (u64)(_v)); \
+    *end = cur; return true; \
+} while (false)
+    
+#define return_f64(_v) do { \
+    val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL; \
+    val->uni.f64 = sign ? -(f64)(_v) : (f64)(_v); \
+    *end = cur; return true; \
+} while (false)
+    
+#define return_f64_raw(_v) do { \
+    val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL; \
+    val->uni.u64 = ((u64)sign << 63) | (u64)(_v); \
+    *end = cur; return true; \
+} while (false)
+    
+#define return_inf() do { \
+    if (has_flag(ALLOW_INF_AND_NAN)) return_f64_raw(F64_RAW_INF); \
+    else return_err(hdr, "number is infinity when parsed as double"); \
+} while (false)
+    
+    u8 *sig_cut = NULL; /* significant part cutting position for long number */
+    u8 *sig_end = NULL; /* significant part ending position */
+    u8 *dot_pos = NULL; /* decimal point position */
+    
+    u64 sig = 0; /* significant part of the number */
+    i32 exp = 0; /* exponent part of the number */
+    
+    bool exp_sign; /* temporary exponent sign from literal part */
+    i64 exp_sig = 0; /* temporary exponent number from significant part */
+    i64 exp_lit = 0; /* temporary exponent number from exponent literal part */
+    u64 num; /* temporary number for reading */
+    u8 *tmp; /* temporary cursor for reading */
+    
+    u8 *hdr = cur;
+    bool sign = (*hdr == '-');
+    cur += sign;
+    
+    /* begin with a leading zero or non-digit */
+    if (unlikely(!digi_is_nonzero(*cur))) { /* 0 or non-digit char */
+        if (unlikely(*cur != '0')) { /* non-digit char */
+            if (has_flag(ALLOW_INF_AND_NAN)) {
+                if (read_inf_or_nan(sign, cur, end, val)) return true;
+            }
+            return_err(cur - 1, "no digit after minus sign");
+        }
+        /* begin with 0 */
+        if (likely(!digi_is_digit_or_fp(*++cur))) return_i64(0);
+        if (likely(*cur == '.')) {
+            dot_pos = cur++;
+            if (unlikely(!digi_is_digit(*cur))) {
+                return_err(cur - 1, "no digit after decimal point");
+            }
+            while (unlikely(*cur == '0')) cur++;
+            if (likely(digi_is_digit(*cur))) {
+                /* first non-zero digit after decimal point */
+                sig = (u64)(*cur - '0'); /* read first digit */
+                cur--;
+                goto digi_frac_1; /* continue read fraction part */
+            }
+        }
+        if (unlikely(digi_is_digit(*cur))) {
+            return_err(cur - 1, "number with leading zero is not allowed");
+        }
+        if (unlikely(digi_is_exp(*cur))) { /* 0 with any exponent is still 0 */
+            cur += (usize)1 + digi_is_sign(cur[1]);
+            if (unlikely(!digi_is_digit(*cur))) {
+                return_err(cur - 1, "no digit after exponent sign");
+            }
+            while (digi_is_digit(*++cur));
+        }
+        return_f64_raw(0);
+    }
+    
+    /* begin with non-zero digit */
+    sig = (u64)(*cur - '0');
+    
+    /*
+     Read integral part, same as the following code.
+     For more explanation, see the comments under label `skip_ascii_begin`.
+     
+         for (int i = 1; i <= 18; i++) {
+            num = cur[i] - '0';
+            if (num <= 9) sig = num + sig * 10;
+            else goto digi_sepr_i;
+         }
+     */
+#if YYJSON_IS_REAL_GCC
+#define expr_intg(i) \
+    if (likely((num = (u64)(cur[i] - (u8)'0')) <= 9)) sig = num + sig * 10; \
+    else { __asm volatile("":"=m"(cur[i])::); goto digi_sepr_##i; }
+#else
+#define expr_intg(i) \
+    if (likely((num = (u64)(cur[i] - (u8)'0')) <= 9)) sig = num + sig * 10; \
+    else { goto digi_sepr_##i; }
+#endif
+    repeat_in_1_18(expr_intg);
+#undef expr_intg
+    
+    
+    cur += 19; /* skip continuous 19 digits */
+    if (!digi_is_digit_or_fp(*cur)) {
+        /* this number is an integer consisting of 19 digits */
+        if (sign && (sig > ((u64)1 << 63))) { /* overflow */
+            return_f64(normalized_u64_to_f64(sig));
+        }
+        return_i64(sig);
+    }
+    goto digi_intg_more; /* read more digits in integral part */
+    
+    
+    /* process first non-digit character */
+#define expr_sepr(i) \
+    digi_sepr_##i: \
+    if (likely(!digi_is_fp(cur[i]))) { cur += i; return_i64(sig); } \
+    dot_pos = cur + i; \
+    if (likely(cur[i] == '.')) goto digi_frac_##i; \
+    cur += i; sig_end = cur; goto digi_exp_more;
+    repeat_in_1_18(expr_sepr)
+#undef expr_sepr
+    
+    
+    /* read fraction part */
+#if YYJSON_IS_REAL_GCC
+#define expr_frac(i) \
+    digi_frac_##i: \
+    if (likely((num = (u64)(cur[i + 1] - (u8)'0')) <= 9)) \
+        sig = num + sig * 10; \
+    else { __asm volatile("":"=m"(cur[i + 1])::); goto digi_stop_##i; }
+#else
+#define expr_frac(i) \
+    digi_frac_##i: \
+    if (likely((num = (u64)(cur[i + 1] - (u8)'0')) <= 9)) \
+        sig = num + sig * 10; \
+    else { goto digi_stop_##i; }
+#endif
+    repeat_in_1_18(expr_frac)
+#undef expr_frac
+    
+    cur += 20; /* skip 19 digits and 1 decimal point */
+    if (!digi_is_digit(*cur)) goto digi_frac_end; /* fraction part end */
+    goto digi_frac_more; /* read more digits in fraction part */
+    
+    
+    /* significant part end */
+#define expr_stop(i) \
+    digi_stop_##i: \
+    cur += i + 1; \
+    goto digi_frac_end;
+    repeat_in_1_18(expr_stop)
+#undef expr_stop
+    
+    
+    /* read more digits in integral part */
+digi_intg_more:
+    if (digi_is_digit(*cur)) {
+        if (!digi_is_digit_or_fp(cur[1])) {
+            /* this number is an integer consisting of 20 digits */
+            num = (u64)(*cur - '0');
+            if ((sig < (U64_MAX / 10)) ||
+                (sig == (U64_MAX / 10) && num <= (U64_MAX % 10))) {
+                sig = num + sig * 10;
+                cur++;
+                /* convert to double if overflow */
+                if (sign) return_f64(normalized_u64_to_f64(sig));
+                return_i64(sig);
+            }
+        }
+    }
+    
+    if (digi_is_exp(*cur)) {
+        dot_pos = cur;
+        goto digi_exp_more;
+    }
+    
+    if (*cur == '.') {
+        dot_pos = cur++;
+        if (!digi_is_digit(*cur)) {
+            return_err(cur, "no digit after decimal point");
+        }
+    }
+    
+    
+    /* read more digits in fraction part */
+digi_frac_more:
+    sig_cut = cur; /* too large to fit in u64, excess digits need to be cut */
+    sig += (*cur >= '5'); /* round */
+    while (digi_is_digit(*++cur));
+    if (!dot_pos) {
+        dot_pos = cur;
+        if (*cur == '.') {
+            if (!digi_is_digit(*++cur)) {
+                return_err(cur, "no digit after decimal point");
+            }
+            while (digi_is_digit(*cur)) cur++;
+        }
+    }
+    exp_sig = (i64)(dot_pos - sig_cut);
+    exp_sig += (dot_pos < sig_cut);
+    
+    /* ignore trailing zeros */
+    tmp = cur - 1;
+    while (*tmp == '0' || *tmp == '.') tmp--;
+    if (tmp < sig_cut) {
+        sig_cut = NULL;
+    } else {
+        sig_end = cur;
+    }
+    
+    if (digi_is_exp(*cur)) goto digi_exp_more;
+    goto digi_exp_finish;
+    
+    
+    /* fraction part end */
+digi_frac_end:
+    if (unlikely(dot_pos + 1 == cur)) {
+        return_err(cur - 1, "no digit after decimal point");
+    }
+    sig_end = cur;
+    exp_sig = -(i64)((u64)(cur - dot_pos) - 1);
+    if (likely(!digi_is_exp(*cur))) {
+        if (unlikely(exp_sig < F64_MIN_DEC_EXP - 19)) {
+            return_f64_raw(0); /* underflow */
+        }
+        exp = (i32)exp_sig;
+        goto digi_finish;
+    } else {
+        goto digi_exp_more;
+    }
+    
+    
+    /* read exponent part */
+digi_exp_more:
+    exp_sign = (*++cur == '-');
+    cur += digi_is_sign(*cur);
+    if (unlikely(!digi_is_digit(*cur))) {
+        return_err(cur - 1, "no digit after exponent sign");
+    }
+    while (*cur == '0') cur++;
+    
+    /* read exponent literal */
+    tmp = cur;
+    while (digi_is_digit(*cur)) {
+        exp_lit = (i64)((u8)(*cur++ - '0') + (u64)exp_lit * 10);
+    }
+    if (unlikely(cur - tmp >= U64_SAFE_DIG)) {
+        if (exp_sign) {
+            return_f64_raw(0); /* underflow */
+        } else {
+            return_inf(); /* overflow */
+        }
+    }
+    exp_sig += exp_sign ? -exp_lit : exp_lit;
+    
+    
+    /* validate exponent value */
+digi_exp_finish:
+    if (unlikely(exp_sig < F64_MIN_DEC_EXP - 19)) {
+        return_f64_raw(0); /* underflow */
+    }
+    if (unlikely(exp_sig > F64_MAX_DEC_EXP)) {
+        return_inf(); /* overflow */
+    }
+    exp = (i32)exp_sig;
+    
+    
+    /* all digit read finished */
+digi_finish:
+    
+    /*
+     Fast path 1:
+     
+     1. The floating-point number calculation should be accurate, see the
+        comments of macro `YYJSON_DOUBLE_MATH_CORRECT`.
+     2. Correct rounding should be performed (fegetround() == FE_TONEAREST).
+     3. The input of floating point number calculation does not lose precision,
+        which means: 64 - leading_zero(input) - trailing_zero(input) < 53.
+    
+     We don't check all available inputs here, because that would make the code
+     more complicated, and not friendly to branch predictor.
+     */
+#if YYJSON_DOUBLE_MATH_CORRECT
+    if (sig < ((u64)1 << 53) &&
+        exp >= -F64_POW10_EXP_MAX_EXACT &&
+        exp <= +F64_POW10_EXP_MAX_EXACT) {
+        f64 dbl = (f64)sig;
+        if (exp < 0) {
+            dbl /= f64_pow10_table[-exp];
+        } else {
+            dbl *= f64_pow10_table[+exp];
+        }
+        return_f64(dbl);
+    }
+#endif
+    
+    /*
+     Fast path 2:
+     
+     To keep it simple, we only accept normal number here,
+     let the slow path to handle subnormal and infinity number.
+     */
+    if (likely(!sig_cut &&
+               exp > -F64_MAX_DEC_EXP + 1 &&
+               exp < +F64_MAX_DEC_EXP - 20)) {
+        /*
+         The result value is exactly equal to (sig * 10^exp),
+         the exponent part (10^exp) can be converted to (sig2 * 2^exp2).
+         
+         The sig2 can be an infinite length number, only the highest 128 bits
+         is cached in the pow10_sig_table.
+         
+         Now we have these bits:
+         sig1 (normalized 64bit)        : aaaaaaaa
+         sig2 (higher 64bit)            : bbbbbbbb
+         sig2_ext (lower 64bit)         : cccccccc
+         sig2_cut (extra unknown bits)  : dddddddddddd....
+         
+         And the calculation process is:
+         ----------------------------------------
+                 aaaaaaaa *
+                 bbbbbbbbccccccccdddddddddddd....
+         ----------------------------------------
+         abababababababab +
+                 acacacacacacacac +
+                         adadadadadadadadadad....
+         ----------------------------------------
+         [hi____][lo____] +
+                 [hi2___][lo2___] +
+                         [unknown___________....]
+         ----------------------------------------
+         
+         The addition with carry may affect higher bits, but if there is a 0
+         in higher bits, the bits higher than 0 will not be affected.
+         
+         `lo2` + `unknown` may get a carry bit and may affect `hi2`, the max
+         value of `hi2` is 0xFFFFFFFFFFFFFFFE, so `hi2` will not overflow.
+         
+         `lo` + `hi2` may also get a carry bit and may affect `hi`, but only
+         the highest significant 53 bits of `hi` is needed. If there is a 0
+         in the lower bits of `hi`, then all the following bits can be dropped.
+         
+         To convert the result to IEEE-754 double number, we need to perform
+         correct rounding:
+         1. if bit 54 is 0, round down,
+         2. if bit 54 is 1 and any bit beyond bit 54 is 1, round up,
+         3. if bit 54 is 1 and all bits beyond bit 54 are 0, round to even,
+            as the extra bits is unknown, this case will not be handled here.
+         */
+        
+        u64 raw;
+        u64 sig1, sig2, sig2_ext, hi, lo, hi2, lo2, add, bits;
+        i32 exp2;
+        u32 lz;
+        bool exact = false, carry, round_up;
+        
+        /* convert (10^exp) to (sig2 * 2^exp2) */
+        pow10_table_get_sig(exp, &sig2, &sig2_ext);
+        pow10_table_get_exp(exp, &exp2);
+        
+        /* normalize and multiply */
+        lz = u64_lz_bits(sig);
+        sig1 = sig << lz;
+        exp2 -= (i32)lz;
+        u128_mul(sig1, sig2, &hi, &lo);
+        
+        /*
+         The `hi` is in range [0x4000000000000000, 0xFFFFFFFFFFFFFFFE],
+         To get normalized value, `hi` should be shifted to the left by 0 or 1.
+         
+         The highest significant 53 bits is used by IEEE-754 double number,
+         and the bit 54 is used to detect rounding direction.
+         
+         The lowest (64 - 54 - 1) bits is used to check whether it contains 0.
+         */
+        bits = hi & (((u64)1 << (64 - 54 - 1)) - 1);
+        if (bits - 1 < (((u64)1 << (64 - 54 - 1)) - 2)) {
+            /*
+             (bits != 0 && bits != 0x1FF) => (bits - 1 < 0x1FF - 1)
+             The `bits` is not zero, so we don't need to check `round to even`
+             case. The `bits` contains bit `0`, so we can drop the extra bits
+             after `0`.
+             */
+            exact = true;
+            
+        } else {
+            /*
+             (bits == 0 || bits == 0x1FF)
+             The `bits` is filled with all `0` or all `1`, so we need to check
+             lower bits with another 64-bit multiplication.
+             */
+            u128_mul(sig1, sig2_ext, &hi2, &lo2);
+            
+            add = lo + hi2;
+            if (add + 1 > (u64)1) {
+                /*
+                 (add != 0 && add != U64_MAX) => (add + 1 > 1)
+                 The `add` is not zero, so we don't need to check `round to
+                 even` case. The `add` contains bit `0`, so we can drop the
+                 extra bits after `0`. The `hi` cannot be U64_MAX, so it will
+                 not overflow.
+                 */
+                carry = add < lo || add < hi2;
+                hi += carry;
+                exact = true;
+            }
+        }
+        
+        if (exact) {
+            /* normalize */
+            lz = hi < ((u64)1 << 63);
+            hi <<= lz;
+            exp2 -= (i32)lz;
+            exp2 += 64;
+            
+            /* test the bit 54 and get rounding direction */
+            round_up = (hi & ((u64)1 << (64 - 54))) > (u64)0;
+            hi += (round_up ? ((u64)1 << (64 - 54)) : (u64)0);
+            
+            /* test overflow */
+            if (hi < ((u64)1 << (64 - 54))) {
+                hi = ((u64)1 << 63);
+                exp2 += 1;
+            }
+            
+            /* This is a normal number, convert it to IEEE-754 format. */
+            hi >>= F64_BITS - F64_SIG_FULL_BITS;
+            exp2 += F64_BITS - F64_SIG_FULL_BITS + F64_SIG_BITS;
+            exp2 += F64_EXP_BIAS;
+            raw = ((u64)exp2 << F64_SIG_BITS) | (hi & F64_SIG_MASK);
+            return_f64_raw(raw);
+        }
+    }
+    
+    /*
+     Slow path: read double number exactly with diyfp.
+     1. Use cached diyfp to get an approximation value.
+     2. Use bigcomp to check the approximation value if needed.
+     
+     This algorithm refers to google's double-conversion project:
+     https://github.com/google/double-conversion
+     */
+    {
+        const i32 ERR_ULP_LOG = 3;
+        const i32 ERR_ULP = 1 << ERR_ULP_LOG;
+        const i32 ERR_CACHED_POW = ERR_ULP / 2;
+        const i32 ERR_MUL_FIXED = ERR_ULP / 2;
+        const i32 DIY_SIG_BITS = 64;
+        const i32 EXP_BIAS = F64_EXP_BIAS + F64_SIG_BITS;
+        const i32 EXP_SUBNORMAL = -EXP_BIAS + 1;
+        
+        u64 fp_err;
+        u32 bits;
+        i32 order_of_magnitude;
+        i32 effective_significand_size;
+        i32 precision_digits_count;
+        u64 precision_bits;
+        u64 half_way;
+        
+        u64 raw;
+        diy_fp fp, fp_upper;
+        bigint big_full, big_comp;
+        i32 cmp;
+        
+        fp.sig = sig;
+        fp.exp = 0;
+        fp_err = sig_cut ? (u64)(ERR_ULP / 2) : (u64)0;
+        
+        /* normalize */
+        bits = u64_lz_bits(fp.sig);
+        fp.sig <<= bits;
+        fp.exp -= (i32)bits;
+        fp_err <<= bits;
+        
+        /* multiply and add error */
+        fp = diy_fp_mul(fp, diy_fp_get_cached_pow10(exp));
+        fp_err += (u64)ERR_CACHED_POW + (fp_err != 0) + (u64)ERR_MUL_FIXED;
+        
+        /* normalize */
+        bits = u64_lz_bits(fp.sig);
+        fp.sig <<= bits;
+        fp.exp -= (i32)bits;
+        fp_err <<= bits;
+        
+        /* effective significand */
+        order_of_magnitude = DIY_SIG_BITS + fp.exp;
+        if (likely(order_of_magnitude >= EXP_SUBNORMAL + F64_SIG_FULL_BITS)) {
+            effective_significand_size = F64_SIG_FULL_BITS;
+        } else if (order_of_magnitude <= EXP_SUBNORMAL) {
+            effective_significand_size = 0;
+        } else {
+            effective_significand_size = order_of_magnitude - EXP_SUBNORMAL;
+        }
+        
+        /* precision digits count */
+        precision_digits_count = DIY_SIG_BITS - effective_significand_size;
+        if (unlikely(precision_digits_count + ERR_ULP_LOG >= DIY_SIG_BITS)) {
+            i32 shr = (precision_digits_count + ERR_ULP_LOG) - DIY_SIG_BITS + 1;
+            fp.sig >>= shr;
+            fp.exp += shr;
+            fp_err = (fp_err >> shr) + 1 + (u32)ERR_ULP;
+            precision_digits_count -= shr;
+        }
+        
+        /* half way */
+        precision_bits = fp.sig & (((u64)1 << precision_digits_count) - 1);
+        precision_bits *= (u32)ERR_ULP;
+        half_way = (u64)1 << (precision_digits_count - 1);
+        half_way *= (u32)ERR_ULP;
+        
+        /* rounding */
+        fp.sig >>= precision_digits_count;
+        fp.sig += (precision_bits >= half_way + fp_err);
+        fp.exp += precision_digits_count;
+        
+        /* get IEEE double raw value */
+        raw = diy_fp_to_ieee_raw(fp);
+        if (unlikely(raw == F64_RAW_INF)) return_inf();
+        if (likely(precision_bits <= half_way - fp_err ||
+                   precision_bits >= half_way + fp_err)) {
+            return_f64_raw(raw); /* number is accurate */
+        }
+        /* now the number is the correct value, or the next lower value */
+        
+        /* upper boundary */
+        if (raw & F64_EXP_MASK) {
+            fp_upper.sig = (raw & F64_SIG_MASK) + ((u64)1 << F64_SIG_BITS);
+            fp_upper.exp = (i32)((raw & F64_EXP_MASK) >> F64_SIG_BITS);
+        } else {
+            fp_upper.sig = (raw & F64_SIG_MASK);
+            fp_upper.exp = 1;
+        }
+        fp_upper.exp -= F64_EXP_BIAS + F64_SIG_BITS;
+        fp_upper.sig <<= 1;
+        fp_upper.exp -= 1;
+        fp_upper.sig += 1; /* add half ulp */
+        
+        /* compare with bigint */
+        bigint_set_buf(&big_full, sig, &exp, sig_cut, sig_end, dot_pos);
+        bigint_set_u64(&big_comp, fp_upper.sig);
+        if (exp >= 0) {
+            bigint_mul_pow10(&big_full, +exp);
+        } else {
+            bigint_mul_pow10(&big_comp, -exp);
+        }
+        if (fp_upper.exp > 0) {
+            bigint_mul_pow2(&big_comp, (u32)+fp_upper.exp);
+        } else {
+            bigint_mul_pow2(&big_full, (u32)-fp_upper.exp);
+        }
+        cmp = bigint_cmp(&big_full, &big_comp);
+        if (likely(cmp != 0)) {
+            /* round down or round up */
+            raw += (cmp > 0);
+        } else {
+            /* falls midway, round to even */
+            raw += (raw & 1);
+        }
+        
+        if (unlikely(raw == F64_RAW_INF)) return_inf();
+        return_f64_raw(raw);
+    }
+    
+#undef has_flag
+#undef return_err
+#undef return_inf
+#undef return_i64
+#undef return_f64
+#undef return_f64_raw
+}
+
+#else /* FP_READER */
+
+/**
+ Read a JSON number.
+ This is a fallback function if the custom number reader is disabled.
+ This function use libc's strtod() to read floating-point number.
+ */
+static_noinline bool read_number(u8 *cur,
+                                 u8 **end,
+                                 yyjson_read_flag flg,
+                                 yyjson_val *val,
+                                 const char **msg) {
+    
+#define has_flag(_flag) unlikely((flg & YYJSON_READ_##_flag) != 0)
+    
+#define return_err(_pos, _msg) do { \
+    *msg = _msg; \
+    *end = _pos; \
+    return false; \
+} while (false)
+    
+#define return_i64(_v) do { \
+    val->tag = YYJSON_TYPE_NUM | (u64)((u8)sign << 3); \
+    val->uni.u64 = (u64)(sign ? (u64)(~(_v) + 1) : (u64)(_v)); \
+    *end = cur; return true; \
+} while (false)
+    
+#define return_f64(_v) do { \
+    val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL; \
+    val->uni.f64 = sign ? -(f64)(_v) : (f64)(_v); \
+    *end = cur; return true; \
+} while (false)
+    
+#define return_f64_raw(_v) do { \
+    val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL; \
+    val->uni.u64 = ((u64)sign << 63) | (u64)(_v); \
+    *end = cur; return true; \
+} while (false)
+    
+    u64 sig, num;
+    u8 *hdr = cur;
+    u8 *dot = NULL;
+    u8 *f64_end = NULL;
+    bool sign = (*hdr == '-');
+    
+    cur += sign;
+    sig = (u8)(*cur - '0');
+    
+    /* read first digit, check leading zero */
+    if (unlikely(!digi_is_digit(*cur))) {
+        if (has_flag(ALLOW_INF_AND_NAN)) {
+            if (read_inf_or_nan(sign, cur, end, val)) return true;
+        }
+        return_err(cur - 1, "no digit after minus sign");
+    }
+    if (*cur == '0') {
+        cur++;
+        if (unlikely(digi_is_digit(*cur))) {
+            return_err(cur - 1, "number with leading zero is not allowed");
+        }
+        if (!digi_is_fp(*cur)) return_i64(0);
+        goto read_double;
+    }
+    
+    /* read continuous digits, up to 19 characters */
+#define expr_intg(i) \
+    if (likely((num = (u64)(cur[i] - (u8)'0')) <= 9)) sig = num + sig * 10; \
+    else { cur += i; goto intg_end; }
+    repeat_in_1_18(expr_intg);
+#undef expr_intg
+    
+    /* here are 19 continuous digits, skip them */
+    cur += 19;
+    if (digi_is_digit(cur[0]) && !digi_is_digit_or_fp(cur[1])) {
+        /* this number is an integer consisting of 20 digits */
+        num = (u8)(*cur - '0');
+        if ((sig < (U64_MAX / 10)) ||
+            (sig == (U64_MAX / 10) && num <= (U64_MAX % 10))) {
+            sig = num + sig * 10;
+            cur++;
+            if (sign) return_f64(normalized_u64_to_f64(sig));
+            return_i64(sig);
+        }
+    }
+    
+intg_end:
+    /* continuous digits ended */
+    if (!digi_is_digit_or_fp(*cur)) {
+        /* this number is an integer consisting of 1 to 19 digits */
+        if (sign && (sig > ((u64)1 << 63))) {
+            return_f64(normalized_u64_to_f64(sig));
+        }
+        return_i64(sig);
+    }
+    
+read_double:
+    /* this number should be read as double */
+    while (digi_is_digit(*cur)) cur++;
+    if (*cur == '.') {
+        /* skip fraction part */
+        dot = cur;
+        cur++;
+        if (!digi_is_digit(*cur++)) {
+            return_err(cur - 1, "no digit after decimal point");
+        }
+        while (digi_is_digit(*cur)) cur++;
+    }
+    if (digi_is_exp(*cur)) {
+        /* skip exponent part */
+        cur += 1 + digi_is_sign(cur[1]);
+        if (!digi_is_digit(*cur++)) {
+            return_err(cur - 1, "no digit after exponent sign");
+        }
+        while (digi_is_digit(*cur)) cur++;
+    }
+    
+    /*
+     libc's strtod() is used to parse the floating-point number.
+     
+     Note that the decimal point character used by strtod() is locale-dependent,
+     and the rounding direction may affected by fesetround().
+     
+     For currently known locales, (en, zh, ja, ko, am, he, hi) use '.' as the
+     decimal point, while other locales use ',' as the decimal point.
+     
+     Here strtod() is called twice for different locales, but if another thread
+     happens calls setlocale() between two strtod(), parsing may still fail.
+     */
+    val->uni.f64 = strtod((const char *)hdr, (char **)&f64_end);
+    if (unlikely(f64_end != cur)) {
+        bool cut = (*cur == ',');
+        if (dot) *dot = ',';
+        if (cut) *cur = ' ';
+        val->uni.f64 = strtod((const char *)hdr, (char **)&f64_end);
+        if (cut) *cur = ',';
+        if (unlikely(f64_end != cur)) {
+            return_err(hdr, "strtod() failed to parse the number");
+        }
+    }
+    if (unlikely(val->uni.f64 == HUGE_VAL || val->uni.f64 == -HUGE_VAL)) {
+        if (!has_flag(ALLOW_INF_AND_NAN)) {
+            return_err(hdr, "number is infinity when parsed as double");
+        }
+    }
+    val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+    *end = cur;
+    return true;
+    
+#undef has_flag
+#undef return_err
+#undef return_i64
+#undef return_f64
+}
+
+#endif /* FP_READER */
+
+
+
+/*==============================================================================
+ * JSON String Reader
+ *============================================================================*/
+
+/**
+ Read a JSON string.
+ @param cur The head of string before '"' prefix.
+ @param end The end of string after '"' suffix.
+ @param val The string value to be written.
+ @param msg The error message pointer.
+ @return Whether success.
+ */
+static_inline bool read_string(u8 *cur,
+                               u8 **end,
+                               yyjson_val *val,
+                               const char **msg) {
+    
+    /*
+     Each unicode code point is encoded as 1 to 4 bytes in UTF-8 encoding,
+     we use 4-byte mask and pattern value to validate UTF-8 byte sequence,
+     this requires the input data to have 4-byte zero padding.
+     ---------------------------------------------------
+     1 byte
+     unicode range [U+0000, U+007F]
+     unicode min   [.......0]
+     unicode max   [.1111111]
+     bit pattern   [0.......]
+     ---------------------------------------------------
+     2 byte
+     unicode range [U+0080, U+07FF]
+     unicode min   [......10 ..000000]
+     unicode max   [...11111 ..111111]
+     bit require   [...xxxx. ........] (1E 00)
+     bit mask      [xxx..... xx......] (E0 C0)
+     bit pattern   [110..... 10......] (C0 80)
+     ---------------------------------------------------
+     3 byte
+     unicode range [U+0800, U+FFFF]
+     unicode min   [........ ..100000 ..000000]
+     unicode max   [....1111 ..111111 ..111111]
+     bit require   [....xxxx ..x..... ........] (0F 20 00)
+     bit mask      [xxxx.... xx...... xx......] (F0 C0 C0)
+     bit pattern   [1110.... 10...... 10......] (E0 80 80)
+     ---------------------------------------------------
+     3 byte invalid (reserved for surrogate halves)
+     unicode range [U+D800, U+DFFF]
+     unicode min   [....1101 ..100000 ..000000]
+     unicode max   [....1101 ..111111 ..111111]
+     bit mask      [....xxxx ..x..... ........] (0F 20 00)
+     bit pattern   [....1101 ..1..... ........] (0D 20 00)
+     ---------------------------------------------------
+     4 byte
+     unicode range [U+10000, U+10FFFF]
+     unicode min   [........ ...10000 ..000000 ..000000]
+     unicode max   [.....100 ..001111 ..111111 ..111111]
+     bit require   [.....xxx ..xx.... ........ ........] (07 30 00 00)
+     bit mask      [xxxxx... xx...... xx...... xx......] (F8 C0 C0 C0)
+     bit pattern   [11110... 10...... 10...... 10......] (F0 80 80 80)
+     ---------------------------------------------------
+     */
+#if YYJSON_ENDIAN == YYJSON_BIG_ENDIAN
+    const u32 b1_mask = 0x80000000UL;
+    const u32 b1_patt = 0x00000000UL;
+    const u32 b2_mask = 0xE0C00000UL;
+    const u32 b2_patt = 0xC0800000UL;
+    const u32 b2_requ = 0x1E000000UL;
+    const u32 b3_mask = 0xF0C0C000UL;
+    const u32 b3_patt = 0xE0808000UL;
+    const u32 b3_requ = 0x0F200000UL;
+    const u32 b3_erro = 0x0D200000UL;
+    const u32 b4_mask = 0xF8C0C0C0UL;
+    const u32 b4_patt = 0xF0808080UL;
+    const u32 b4_requ = 0x07300000UL;
+    const u32 b4_err0 = 0x04000000UL;
+    const u32 b4_err1 = 0x03300000UL;
+#elif YYJSON_ENDIAN == YYJSON_LITTLE_ENDIAN
+    const u32 b1_mask = 0x00000080UL;
+    const u32 b1_patt = 0x00000000UL;
+    const u32 b2_mask = 0x0000C0E0UL;
+    const u32 b2_patt = 0x000080C0UL;
+    const u32 b2_requ = 0x0000001EUL;
+    const u32 b3_mask = 0x00C0C0F0UL;
+    const u32 b3_patt = 0x008080E0UL;
+    const u32 b3_requ = 0x0000200FUL;
+    const u32 b3_erro = 0x0000200DUL;
+    const u32 b4_mask = 0xC0C0C0F8UL;
+    const u32 b4_patt = 0x808080F0UL;
+    const u32 b4_requ = 0x00003007UL;
+    const u32 b4_err0 = 0x00000004UL;
+    const u32 b4_err1 = 0x00003003UL;
+#else
+    v32_uni b1_mask_uni = {{ 0x80, 0x00, 0x00, 0x00 }};
+    v32_uni b1_patt_uni = {{ 0x00, 0x00, 0x00, 0x00 }};
+    v32_uni b2_mask_uni = {{ 0xE0, 0xC0, 0x00, 0x00 }};
+    v32_uni b2_patt_uni = {{ 0xC0, 0x80, 0x00, 0x00 }};
+    v32_uni b2_requ_uni = {{ 0x1E, 0x00, 0x00, 0x00 }};
+    v32_uni b3_mask_uni = {{ 0xF0, 0xC0, 0xC0, 0x00 }};
+    v32_uni b3_patt_uni = {{ 0xE0, 0x80, 0x80, 0x00 }};
+    v32_uni b3_requ_uni = {{ 0x0F, 0x20, 0x00, 0x00 }};
+    v32_uni b3_erro_uni = {{ 0x0D, 0x20, 0x00, 0x00 }};
+    v32_uni b4_mask_uni = {{ 0xF8, 0xC0, 0xC0, 0xC0 }};
+    v32_uni b4_patt_uni = {{ 0xF0, 0x80, 0x80, 0x80 }};
+    v32_uni b4_requ_uni = {{ 0x07, 0x30, 0x00, 0x00 }};
+    v32_uni b4_err0_uni = {{ 0x04, 0x00, 0x00, 0x00 }};
+    v32_uni b4_err1_uni = {{ 0x03, 0x30, 0x00, 0x00 }};
+    u32 b1_mask = b1_mask_uni.u;
+    u32 b1_patt = b1_patt_uni.u;
+    u32 b2_mask = b2_mask_uni.u;
+    u32 b2_patt = b2_patt_uni.u;
+    u32 b2_requ = b2_requ_uni.u;
+    u32 b3_mask = b3_mask_uni.u;
+    u32 b3_patt = b3_patt_uni.u;
+    u32 b3_requ = b3_requ_uni.u;
+    u32 b3_erro = b3_erro_uni.u;
+    u32 b4_mask = b4_mask_uni.u;
+    u32 b4_patt = b4_patt_uni.u;
+    u32 b4_requ = b4_requ_uni.u;
+    u32 b4_err0 = b4_err0_uni.u;
+    u32 b4_err1 = b4_err1_uni.u;
+#endif
+    
+#define is_valid_seq_1(uni) \
+    ((uni & b1_mask) == b1_patt)
+    
+#define is_valid_seq_2(uni) \
+    ((uni & b2_mask) == b2_patt) && \
+    ((uni & b2_requ))
+    
+#define is_valid_seq_3(uni) \
+    ((uni & b3_mask) == b3_patt) && \
+    ((tmp = (uni & b3_requ))) && \
+    ((tmp != b3_erro))
+    
+#define is_valid_seq_4(uni) \
+    ((uni & b4_mask) == b4_patt) && \
+    ((tmp = (uni & b4_requ))) && \
+    ((tmp & b4_err0) == 0 || (tmp & b4_err1) == 0)
+    
+#define return_err(_end, _msg) do { \
+    *msg = _msg; \
+    *end = _end; \
+    return false; \
+} while (false);
+    
+    u8 *src = ++cur, *dst, *pos;
+    u16 hi, lo;
+    u32 uni, tmp;
+    
+skip_ascii:
+    /* Most strings have no escaped characters, so we can jump them quickly. */
+    
+skip_ascii_begin:
+    /*
+     We want to make loop unrolling, as shown in the following code. Some
+     compiler may not generate instructions as expected, so we rewrite it with
+     explicit goto statements. We hope the compiler can generate instructions
+     like this: https://godbolt.org/z/8vjsYq
+     
+         while (true) repeat16({
+            if (likely(!(char_is_ascii_stop(*src)))) src++;
+            else break;
+         });
+     */
+#define expr_jump(i) \
+    if (likely(!char_is_ascii_stop(src[i]))) {} \
+    else goto skip_ascii_stop##i;
+    
+#define expr_stop(i) \
+    skip_ascii_stop##i: \
+    src += i; \
+    goto skip_ascii_end;
+    
+    repeat16_incr(expr_jump);
+    src += 16;
+    goto skip_ascii_begin;
+    repeat16_incr(expr_stop);
+    
+#undef expr_jump
+#undef expr_stop
+    
+skip_ascii_end:
+    
+    /*
+     GCC may store src[i] in a register at each line of expr_jump(i) above.
+     These instructions are useless and will degrade performance.
+     This inline asm is a hint for gcc: "the memory has been modified,
+     do not cache it".
+     
+     MSVC, Clang, ICC can generate expected instructions without this hint.
+     */
+#if YYJSON_IS_REAL_GCC
+    __asm volatile("":"=m"(*src)::);
+#endif
+    if (likely(*src == '"')) {
+        val->tag = ((u64)(src - cur) << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = (const char *)cur;
+        *src = '\0';
+        *end = src + 1;
+        return true;
+    }
+    
+skip_utf8:
+    if (*src & 0x80) { /* non-ASCII character */
+        /*
+         Non-ASCII character appears here, which means that the text is likely
+         to be written in non-English or emoticons. According to some common
+         data set statistics, byte sequences of the same length may appear
+         consecutively. We process the byte sequences of the same length in each
+         loop, which is more friendly to branch prediction.
+         */
+        pos = src;
+        uni = byte_load_4(src);
+        while (is_valid_seq_3(uni)) {
+            src += 3;
+            uni = byte_load_4(src);
+        }
+        if (is_valid_seq_1(uni)) goto skip_ascii;
+        while (is_valid_seq_2(uni)) {
+            src += 2;
+            uni = byte_load_4(src);
+        }
+        while (is_valid_seq_4(uni)) {
+            src += 4;
+            uni = byte_load_4(src);
+        }
+        if (unlikely(pos == src)) {
+            return_err(src, "invalid UTF-8 encoding in string");
+        }
+        goto skip_ascii;
+    }
+    
+    /* The escape character appears, we need to copy it. */
+    dst = src;
+copy_escape:
+    if (likely(*src == '\\')) {
+        switch (*++src) {
+            case '"':  *dst++ = '"';  src++; break;
+            case '\\': *dst++ = '\\'; src++; break;
+            case '/':  *dst++ = '/';  src++; break;
+            case 'b':  *dst++ = '\b'; src++; break;
+            case 'f':  *dst++ = '\f'; src++; break;
+            case 'n':  *dst++ = '\n'; src++; break;
+            case 'r':  *dst++ = '\r'; src++; break;
+            case 't':  *dst++ = '\t'; src++; break;
+            case 'u':
+                if (unlikely(!read_hex_u16(++src, &hi))) {
+                    return_err(src - 2, "invalid escaped unicode in string");
+                }
+                src += 4;
+                if (likely((hi & 0xF800) != 0xD800)) {
+                    /* a BMP character */
+                    if (hi >= 0x800) {
+                        *dst++ = (u8)(0xE0 | (hi >> 12));
+                        *dst++ = (u8)(0x80 | ((hi >> 6) & 0x3F));
+                        *dst++ = (u8)(0x80 | (hi & 0x3F));
+                    } else if (hi >= 0x80) {
+                        *dst++ = (u8)(0xC0 | (hi >> 6));
+                        *dst++ = (u8)(0x80 | (hi & 0x3F));
+                    } else {
+                        *dst++ = (u8)hi;
+                    }
+                } else {
+                    /* a non-BMP character, represented as a surrogate pair */
+                    if (unlikely((hi & 0xFC00) != 0xD800)) {
+                        return_err(src - 6, "invalid high surrogate in string");
+                    }
+                    if (unlikely(!byte_match_2(src, "\\u")) ||
+                        unlikely(!read_hex_u16(src + 2, &lo))) {
+                        return_err(src, "no matched low surrogate in string");
+                    }
+                    if (unlikely((lo & 0xFC00) != 0xDC00)) {
+                        return_err(src, "invalid low surrogate in string");
+                    }
+                    uni = ((((u32)hi - 0xD800) << 10) |
+                            ((u32)lo - 0xDC00)) + 0x10000;
+                    *dst++ = (u8)(0xF0 | (uni >> 18));
+                    *dst++ = (u8)(0x80 | ((uni >> 12) & 0x3F));
+                    *dst++ = (u8)(0x80 | ((uni >> 6) & 0x3F));
+                    *dst++ = (u8)(0x80 | (uni & 0x3F));
+                    src += 6;
+                }
+                break;
+            default: return_err(src, "invalid escaped character in string");
+        }
+    } else if (likely(*src == '"')) {
+        val->tag = ((u64)(dst - cur) << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = (const char *)cur;
+        *dst = '\0';
+        *end = src + 1;
+        return true;
+    } else {
+        return_err(src, "unexpected control character in string");
+    }
+    
+copy_ascii:
+    /*
+     Copy continuous ASCII, loop unrolling, same as the following code:
+     
+         while (true) repeat16({
+            if (unlikely(char_is_ascii_stop(*src))) break;
+            *dst++ = *src++;
+         });
+     */
+#if YYJSON_IS_REAL_GCC
+#   define expr_jump(i) \
+    if (likely(!(char_is_ascii_stop(src[i])))) {} \
+    else { __asm volatile("":"=m"(src[i])::); goto copy_ascii_stop_##i; }
+#else
+#   define expr_jump(i) \
+    if (likely(!(char_is_ascii_stop(src[i])))) {} \
+    else { goto copy_ascii_stop_##i; }
+#endif
+    repeat16_incr(expr_jump);
+#undef expr_jump
+    
+    byte_move_16(dst, src);
+    src += 16;
+    dst += 16;
+    goto copy_ascii;
+    
+copy_ascii_stop_0:
+    goto copy_utf8;
+copy_ascii_stop_1:
+    byte_move_2(dst, src);
+    src += 1;
+    dst += 1;
+    goto copy_utf8;
+copy_ascii_stop_2:
+    byte_move_2(dst, src);
+    src += 2;
+    dst += 2;
+    goto copy_utf8;
+copy_ascii_stop_3:
+    byte_move_4(dst, src);
+    src += 3;
+    dst += 3;
+    goto copy_utf8;
+copy_ascii_stop_4:
+    byte_move_4(dst, src);
+    src += 4;
+    dst += 4;
+    goto copy_utf8;
+copy_ascii_stop_5:
+    byte_move_4(dst, src);
+    byte_move_2(dst + 4, src + 4);
+    src += 5;
+    dst += 5;
+    goto copy_utf8;
+copy_ascii_stop_6:
+    byte_move_4(dst, src);
+    byte_move_2(dst + 4, src + 4);
+    src += 6;
+    dst += 6;
+    goto copy_utf8;
+copy_ascii_stop_7:
+    byte_move_8(dst, src);
+    src += 7;
+    dst += 7;
+    goto copy_utf8;
+copy_ascii_stop_8:
+    byte_move_8(dst, src);
+    src += 8;
+    dst += 8;
+    goto copy_utf8;
+copy_ascii_stop_9:
+    byte_move_8(dst, src);
+    byte_move_2(dst + 8, src + 8);
+    src += 9;
+    dst += 9;
+    goto copy_utf8;
+copy_ascii_stop_10:
+    byte_move_8(dst, src);
+    byte_move_2(dst + 8, src + 8);
+    src += 10;
+    dst += 10;
+    goto copy_utf8;
+copy_ascii_stop_11:
+    byte_move_8(dst, src);
+    byte_move_4(dst + 8, src + 8);
+    src += 11;
+    dst += 11;
+    goto copy_utf8;
+copy_ascii_stop_12:
+    byte_move_8(dst, src);
+    byte_move_4(dst + 8, src + 8);
+    src += 12;
+    dst += 12;
+    goto copy_utf8;
+copy_ascii_stop_13:
+    byte_move_8(dst, src);
+    byte_move_4(dst + 8, src + 8);
+    byte_move_2(dst + 12, src + 12);
+    src += 13;
+    dst += 13;
+    goto copy_utf8;
+copy_ascii_stop_14:
+    byte_move_8(dst, src);
+    byte_move_4(dst + 8, src + 8);
+    byte_move_2(dst + 12, src + 12);
+    src += 14;
+    dst += 14;
+    goto copy_utf8;
+copy_ascii_stop_15:
+    byte_move_16(dst, src);
+    src += 15;
+    dst += 15;
+    goto copy_utf8;
+    
+copy_utf8:
+    if (*src & 0x80) { /* non-ASCII character */
+        pos = src;
+        uni = byte_load_4(src);
+        while (is_valid_seq_3(uni)) {
+            byte_move_4(dst, &uni);
+            dst += 3;
+            src += 3;
+            uni = byte_load_4(src);
+        }
+        if (is_valid_seq_1(uni)) goto copy_ascii;
+        while (is_valid_seq_2(uni)) {
+            byte_move_2(dst, &uni);
+            dst += 2;
+            src += 2;
+            uni = byte_load_4(src);
+        }
+        while (is_valid_seq_4(uni)) {
+            byte_move_4(dst, &uni);
+            dst += 4;
+            src += 4;
+            uni = byte_load_4(src);
+        }
+        if (unlikely(pos == src)) {
+            return_err(src, "invalid UTF-8 encoding in string");
+        }
+        goto copy_ascii;
+    }
+    goto copy_escape;
+    
+#undef return_err
+#undef is_valid_seq_1
+#undef is_valid_seq_2
+#undef is_valid_seq_3
+#undef is_valid_seq_4
+}
+
+
+
+/*==============================================================================
+ * JSON Reader Implementation
+ *
+ * We use goto statements to build the finite state machine (FSM).
+ * The FSM's state was held by program counter (PC) and the 'goto' make the
+ * state transitions.
+ *============================================================================*/
+
+/** Read single value JSON document. */
+static_noinline yyjson_doc *read_root_single(u8 *hdr,
+                                             u8 *cur,
+                                             u8 *end,
+                                             yyjson_alc alc,
+                                             yyjson_read_flag flg,
+                                             yyjson_read_err *err) {
+    
+#define has_flag(_flag) unlikely((flg & YYJSON_READ_##_flag) != 0)
+    
+#define return_err(_pos, _code, _msg) do { \
+    if (_pos >= end) { \
+        err->pos = (usize)(end - hdr); \
+        err->code = YYJSON_READ_ERROR_UNEXPECTED_END; \
+        err->msg = "unexpected end of data"; \
+    } else { \
+        err->pos = (usize)(_pos - hdr); \
+        err->code = YYJSON_READ_ERROR_##_code; \
+        err->msg = _msg; \
+    } \
+    if (val_hdr) alc.free(alc.ctx, (void *)val_hdr); \
+    return NULL; \
+} while (false)
+    
+    usize hdr_len; /* value count used by doc */
+    usize alc_num; /* value count capacity */
+    yyjson_val *val_hdr; /* the head of allocated values */
+    yyjson_val *val; /* current value */
+    yyjson_doc *doc; /* the JSON document, equals to val_hdr */
+    const char *msg; /* error message */
+    
+    hdr_len = sizeof(yyjson_doc) / sizeof(yyjson_val);
+    hdr_len += (sizeof(yyjson_doc) % sizeof(yyjson_val)) > 0;
+    alc_num = hdr_len + 1; /* single value */
+    
+    val_hdr = (yyjson_val *)alc.malloc(alc.ctx, alc_num * sizeof(yyjson_val));
+    if (unlikely(!val_hdr)) goto fail_alloc;
+    val = val_hdr + hdr_len;
+    
+    if (char_is_number(*cur)) {
+        if (likely(read_number(cur, &cur, flg, val, &msg))) goto doc_end;
+        goto fail_number;
+    }
+    if (*cur == '"') {
+        if (likely(read_string(cur, &cur, val, &msg))) goto doc_end;
+        goto fail_string;
+    }
+    if (*cur == 't') {
+        if (likely(read_true(cur, &cur, val))) goto doc_end;
+        goto fail_literal;
+    }
+    if (*cur == 'f') {
+        if (likely(read_false(cur, &cur, val))) goto doc_end;
+        goto fail_literal;
+    }
+    if (*cur == 'n') {
+        if (likely(read_null(cur, &cur, val))) goto doc_end;
+        if (has_flag(ALLOW_INF_AND_NAN)) {
+            if (read_nan(false, cur, &cur, val)) goto doc_end;
+        }
+        goto fail_literal;
+    }
+    if (has_flag(ALLOW_INF_AND_NAN)) {
+        if (read_inf_or_nan(false, cur, &cur, val)) goto doc_end;
+    }
+    goto fail_character;
+    
+doc_end:
+    /* check invalid contents after json document */
+    if (unlikely(cur < end) && !has_flag(STOP_WHEN_DONE)) {
+#if !YYJSON_DISABLE_NON_STANDARD
+        if (has_flag(ALLOW_COMMENTS)) {
+            if (!skip_spaces_and_comments(cur, &cur)) {
+                if (byte_match_2(cur, "/*")) goto fail_comment;
+            }
+        } else while (char_is_space(*cur)) cur++;
+#else
+        while (char_is_space(*cur)) cur++;
+#endif
+        if (unlikely(cur < end)) goto fail_garbage;
+    }
+    
+    doc = (yyjson_doc *)val_hdr;
+    doc->root = val_hdr + hdr_len;
+    doc->alc = alc;
+    doc->dat_read = (usize)(cur - hdr);
+    doc->val_read = 1;
+    doc->str_pool = has_flag(INSITU) ? NULL : (char *)hdr;
+    return doc;
+    
+fail_string:
+    return_err(cur, INVALID_STRING, msg);
+fail_number:
+    return_err(cur, INVALID_NUMBER, msg);
+fail_alloc:
+    return_err(cur, MEMORY_ALLOCATION, "memory allocation failed");
+fail_literal:
+    return_err(cur, LITERAL, "invalid literal");
+fail_comment:
+    return_err(cur, INVALID_COMMENT, "unclosed multiline comment");
+fail_character:
+    return_err(cur, UNEXPECTED_CHARACTER, "unexpected character");
+fail_garbage:
+    return_err(cur, UNEXPECTED_CONTENT, "unexpected content after document");
+    
+#undef has_flag
+#undef return_err
+}
+
+/** Read JSON document (accept all style, but optimized for minify). */
+static_inline yyjson_doc *read_root_minify(u8 *hdr,
+                                           u8 *cur,
+                                           u8 *end,
+                                           yyjson_alc alc,
+                                           yyjson_read_flag flg,
+                                           yyjson_read_err *err) {
+    
+#define has_flag(_flag) unlikely((flg & YYJSON_READ_##_flag) != 0)
+    
+#define return_err(_pos, _code, _msg) do { \
+    if (_pos >= end) { \
+        err->pos = (usize)(end - hdr); \
+        err->code = YYJSON_READ_ERROR_UNEXPECTED_END; \
+        err->msg = "unexpected end of data"; \
+    } else { \
+        err->pos = (usize)(_pos - hdr); \
+        err->code = YYJSON_READ_ERROR_##_code; \
+        err->msg = _msg; \
+    } \
+    if (val_hdr) alc.free(alc.ctx, (void *)val_hdr); \
+    return NULL; \
+} while (false)
+    
+#define val_incr() do { \
+    val++; \
+    if (unlikely(val >= val_end)) { \
+        alc_len += alc_len / 2; \
+        if ((alc_len >= alc_max)) goto fail_alloc; \
+        val_tmp = (yyjson_val *)alc.realloc(alc.ctx, (void *)val_hdr, \
+            alc_len * sizeof(yyjson_val)); \
+        if ((!val_tmp)) goto fail_alloc; \
+        val = val_tmp + (size_t)(val - val_hdr); \
+        ctn = val_tmp + (size_t)(ctn - val_hdr); \
+        val_hdr = val_tmp; \
+        val_end = val_tmp + (alc_len - 2); \
+    } \
+} while (false)
+    
+    usize dat_len; /* data length in bytes, hint for allocator */
+    usize hdr_len; /* value count used by yyjson_doc */
+    usize alc_len; /* value count allocated */
+    usize alc_max; /* maximum value count for allocator */
+    usize ctn_len; /* the number of elements in current container */
+    yyjson_val *val_hdr; /* the head of allocated values */
+    yyjson_val *val_end; /* the end of allocated values */
+    yyjson_val *val_tmp; /* temporary pointer for realloc */
+    yyjson_val *val; /* current JSON value */
+    yyjson_val *ctn; /* current container */
+    yyjson_val *ctn_parent; /* parent of current container */
+    yyjson_doc *doc; /* the JSON document, equals to val_hdr */
+    const char *msg; /* error message */
+    
+    dat_len = has_flag(STOP_WHEN_DONE) ? 256 : (usize)(end - cur);
+    hdr_len = sizeof(yyjson_doc) / sizeof(yyjson_val);
+    hdr_len += (sizeof(yyjson_doc) % sizeof(yyjson_val)) > 0;
+    alc_max = USIZE_MAX / sizeof(yyjson_val);
+    alc_len = hdr_len + (dat_len / YYJSON_READER_ESTIMATED_MINIFY_RATIO) + 4;
+    alc_len = yyjson_min(alc_len, alc_max);
+    
+    val_hdr = (yyjson_val *)alc.malloc(alc.ctx, alc_len * sizeof(yyjson_val));
+    if (unlikely(!val_hdr)) goto fail_alloc;
+    val_end = val_hdr + (alc_len - 2); /* padding for key-value pair reading */
+    val = val_hdr + hdr_len;
+    ctn = val;
+    ctn_len = 0;
+    
+    if (*cur++ == '{') {
+        ctn->tag = YYJSON_TYPE_OBJ;
+        ctn->uni.ofs = 0;
+        goto obj_key_begin;
+    } else {
+        ctn->tag = YYJSON_TYPE_ARR;
+        ctn->uni.ofs = 0;
+        goto arr_val_begin;
+    }
+    
+arr_begin:
+    /* save current container */
+    ctn->tag = (((u64)ctn_len + 1) << YYJSON_TAG_BIT) |
+               (ctn->tag & YYJSON_TAG_MASK);
+    
+    /* create a new array value, save parent container offset */
+    val_incr();
+    val->tag = YYJSON_TYPE_ARR;
+    val->uni.ofs = (usize)((u8 *)val - (u8 *)ctn);
+    
+    /* push the new array value as current container */
+    ctn = val;
+    ctn_len = 0;
+    
+arr_val_begin:
+    if (*cur == '{') {
+        cur++;
+        goto obj_begin;
+    }
+    if (*cur == '[') {
+        cur++;
+        goto arr_begin;
+    }
+    if (char_is_number(*cur)) {
+        val_incr();
+        ctn_len++;
+        if (likely(read_number(cur, &cur, flg, val, &msg))) goto arr_val_end;
+        goto fail_number;
+    }
+    if (*cur == '"') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_string(cur, &cur, val, &msg))) goto arr_val_end;
+        goto fail_string;
+    }
+    if (*cur == 't') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_true(cur, &cur, val))) goto arr_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'f') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_false(cur, &cur, val))) goto arr_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'n') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_null(cur, &cur, val))) goto arr_val_end;
+        if (has_flag(ALLOW_INF_AND_NAN)) {
+            if (read_nan(false, cur, &cur, val)) goto arr_val_end;
+        }
+        goto fail_literal;
+    }
+    if (*cur == ']') {
+        cur++;
+        if (likely(ctn_len == 0)) goto arr_end;
+        if (has_flag(ALLOW_TRAILING_COMMAS)) goto arr_end;
+        goto fail_trailing_comma;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto arr_val_begin;
+    }
+    if (has_flag(ALLOW_INF_AND_NAN) && *cur == 'N') {
+        val_incr();
+        ctn_len++;
+        if (read_inf_or_nan(false, cur, &cur, val)) goto arr_val_end;
+        goto fail_character;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto arr_val_begin;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+arr_val_end:
+    if (*cur == ',') {
+        cur++;
+        goto arr_val_begin;
+    }
+    if (*cur == ']') {
+        cur++;
+        goto arr_end;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto arr_val_end;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto arr_val_end;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+arr_end:
+    /* get parent container */
+    ctn_parent = (yyjson_val *)(void *)((u8 *)ctn - ctn->uni.ofs);
+    
+    /* save the next sibling value offset */
+    ctn->uni.ofs = (usize)((u8 *)val - (u8 *)ctn) + sizeof(yyjson_val);
+    ctn->tag = ((ctn_len) << YYJSON_TAG_BIT) | YYJSON_TYPE_ARR;
+    if (unlikely(ctn == ctn_parent)) goto doc_end;
+    
+    /* pop parent as current container */
+    ctn = ctn_parent;
+    ctn_len = (usize)(ctn->tag >> YYJSON_TAG_BIT);
+    if ((ctn->tag & YYJSON_TYPE_MASK) == YYJSON_TYPE_OBJ) {
+        goto obj_val_end;
+    } else {
+        goto arr_val_end;
+    }
+    
+obj_begin:
+    /* push container */
+    ctn->tag = (((u64)ctn_len + 1) << YYJSON_TAG_BIT) |
+               (ctn->tag & YYJSON_TAG_MASK);
+    val_incr();
+    val->tag = YYJSON_TYPE_OBJ;
+    /* offset to the parent */
+    val->uni.ofs = (usize)((u8 *)val - (u8 *)ctn);
+    ctn = val;
+    ctn_len = 0;
+    
+obj_key_begin:
+    if (likely(*cur == '"')) {
+        val_incr();
+        ctn_len++;
+        if (likely(read_string(cur, &cur, val, &msg))) goto obj_key_end;
+        goto fail_string;
+    }
+    if (likely(*cur == '}')) {
+        cur++;
+        if (likely(ctn_len == 0)) goto obj_end;
+        if (has_flag(ALLOW_TRAILING_COMMAS)) goto obj_end;
+        goto fail_trailing_comma;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_key_begin;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_key_begin;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_key_end:
+    if (*cur == ':') {
+        cur++;
+        goto obj_val_begin;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_key_end;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_key_end;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_val_begin:
+    if (*cur == '"') {
+        val++;
+        ctn_len++;
+        if (likely(read_string(cur, &cur, val, &msg))) goto obj_val_end;
+        goto fail_string;
+    }
+    if (char_is_number(*cur)) {
+        val++;
+        ctn_len++;
+        if (likely(read_number(cur, &cur, flg, val, &msg))) goto obj_val_end;
+        goto fail_number;
+    }
+    if (*cur == '{') {
+        cur++;
+        goto obj_begin;
+    }
+    if (*cur == '[') {
+        cur++;
+        goto arr_begin;
+    }
+    if (*cur == 't') {
+        val++;
+        ctn_len++;
+        if (likely(read_true(cur, &cur, val))) goto obj_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'f') {
+        val++;
+        ctn_len++;
+        if (likely(read_false(cur, &cur, val))) goto obj_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'n') {
+        val++;
+        ctn_len++;
+        if (likely(read_null(cur, &cur, val))) goto obj_val_end;
+        if (has_flag(ALLOW_INF_AND_NAN)) {
+            if (read_nan(false, cur, &cur, val)) goto obj_val_end;
+        }
+        goto fail_literal;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_val_begin;
+    }
+    if (has_flag(ALLOW_INF_AND_NAN) && *cur == 'N') {
+        val++;
+        ctn_len++;
+        if (read_inf_or_nan(false, cur, &cur, val)) goto obj_val_end;
+        goto fail_character;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_val_begin;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_val_end:
+    if (likely(*cur == ',')) {
+        cur++;
+        goto obj_key_begin;
+    }
+    if (likely(*cur == '}')) {
+        cur++;
+        goto obj_end;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_val_end;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_val_end;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_end:
+    /* pop container */
+    ctn_parent = (yyjson_val *)(void *)((u8 *)ctn - ctn->uni.ofs);
+    /* point to the next value */
+    ctn->uni.ofs = (usize)((u8 *)val - (u8 *)ctn) + sizeof(yyjson_val);
+    ctn->tag = (ctn_len << (YYJSON_TAG_BIT - 1)) | YYJSON_TYPE_OBJ;
+    if (unlikely(ctn == ctn_parent)) goto doc_end;
+    ctn = ctn_parent;
+    ctn_len = (usize)(ctn->tag >> YYJSON_TAG_BIT);
+    if ((ctn->tag & YYJSON_TYPE_MASK) == YYJSON_TYPE_OBJ) {
+        goto obj_val_end;
+    } else {
+        goto arr_val_end;
+    }
+    
+doc_end:
+    
+    /* check invalid contents after json document */
+    if (unlikely(cur < end) && !has_flag(STOP_WHEN_DONE)) {
+#if !YYJSON_DISABLE_NON_STANDARD
+        if (has_flag(ALLOW_COMMENTS)) skip_spaces_and_comments(cur, &cur); else
+#endif
+        while (char_is_space(*cur)) cur++;
+        if (unlikely(cur < end)) goto fail_garbage;
+    }
+    
+    doc = (yyjson_doc *)val_hdr;
+    doc->root = val_hdr + hdr_len;
+    doc->alc = alc;
+    doc->dat_read = (usize)(cur - hdr);
+    doc->val_read = (usize)((val - doc->root) + 1);
+    doc->str_pool = has_flag(INSITU) ? NULL : (char *)hdr;
+    return doc;
+    
+fail_string:
+    return_err(cur, INVALID_STRING, msg);
+fail_number:
+    return_err(cur, INVALID_NUMBER, msg);
+fail_alloc:
+    return_err(cur, MEMORY_ALLOCATION, "memory allocation failed");
+fail_trailing_comma:
+    return_err(cur, JSON_STRUCTURE, "trailing comma is not allowed");
+fail_literal:
+    return_err(cur, LITERAL, "invalid literal");
+fail_comment:
+    return_err(cur, INVALID_COMMENT, "unclosed multiline comment");
+fail_character:
+    return_err(cur, UNEXPECTED_CHARACTER, "unexpected character");
+fail_garbage:
+    return_err(cur, UNEXPECTED_CONTENT, "unexpected content after document");
+    
+#undef has_flag
+#undef val_incr
+#undef return_err
+}
+
+/** Read JSON document (accept all style, but optimized for pretty). */
+static_inline yyjson_doc *read_root_pretty(u8 *hdr,
+                                           u8 *cur,
+                                           u8 *end,
+                                           yyjson_alc alc,
+                                           yyjson_read_flag flg,
+                                           yyjson_read_err *err) {
+    
+#define has_flag(_flag) unlikely((flg & YYJSON_READ_##_flag) != 0)
+    
+#define return_err(_pos, _code, _msg) do { \
+    if (_pos >= end) { \
+        err->pos = (usize)(end - hdr); \
+        err->code = YYJSON_READ_ERROR_UNEXPECTED_END; \
+        err->msg = "unexpected end of data"; \
+    } else { \
+        err->pos = (usize)(_pos - hdr); \
+        err->code = YYJSON_READ_ERROR_##_code; \
+        err->msg = _msg; \
+    } \
+    if (val_hdr) alc.free(alc.ctx, (void *)val_hdr); \
+    return NULL; \
+} while (false)
+    
+#define val_incr() do { \
+    val++; \
+    if (unlikely(val >= val_end)) { \
+        alc_len += alc_len / 2; \
+        if ((alc_len >= alc_max)) goto fail_alloc; \
+        val_tmp = (yyjson_val *)alc.realloc(alc.ctx, (void *)val_hdr, \
+            alc_len * sizeof(yyjson_val)); \
+        if ((!val_tmp)) goto fail_alloc; \
+        val = val_tmp + (size_t)(val - val_hdr); \
+        ctn = val_tmp + (size_t)(ctn - val_hdr); \
+        val_hdr = val_tmp; \
+        val_end = val_tmp + (alc_len - 2); \
+    } \
+} while (false)
+    
+    usize dat_len; /* data length in bytes, hint for allocator */
+    usize hdr_len; /* value count used by yyjson_doc */
+    usize alc_len; /* value count allocated */
+    usize alc_max; /* maximum value count for allocator */
+    usize ctn_len; /* the number of elements in current container */
+    yyjson_val *val_hdr; /* the head of allocated values */
+    yyjson_val *val_end; /* the end of allocated values */
+    yyjson_val *val_tmp; /* temporary pointer for realloc */
+    yyjson_val *val; /* current JSON value */
+    yyjson_val *ctn; /* current container */
+    yyjson_val *ctn_parent; /* parent of current container */
+    yyjson_doc *doc; /* the JSON document, equals to val_hdr */
+    const char *msg; /* error message */
+    
+    dat_len = has_flag(STOP_WHEN_DONE) ? 256 : (usize)(end - cur);
+    hdr_len = sizeof(yyjson_doc) / sizeof(yyjson_val);
+    hdr_len += (sizeof(yyjson_doc) % sizeof(yyjson_val)) > 0;
+    alc_max = USIZE_MAX / sizeof(yyjson_val);
+    alc_len = hdr_len + (dat_len / YYJSON_READER_ESTIMATED_PRETTY_RATIO) + 4;
+    alc_len = yyjson_min(alc_len, alc_max);
+    
+    val_hdr = (yyjson_val *)alc.malloc(alc.ctx, alc_len * sizeof(yyjson_val));
+    if (unlikely(!val_hdr)) goto fail_alloc;
+    val_end = val_hdr + (alc_len - 2); /* padding for key-value pair reading */
+    val = val_hdr + hdr_len;
+    ctn = val;
+    ctn_len = 0;
+    
+    if (*cur++ == '{') {
+        ctn->tag = YYJSON_TYPE_OBJ;
+        ctn->uni.ofs = 0;
+        if (*cur == '\n') cur++;
+        goto obj_key_begin;
+    } else {
+        ctn->tag = YYJSON_TYPE_ARR;
+        ctn->uni.ofs = 0;
+        if (*cur == '\n') cur++;
+        goto arr_val_begin;
+    }
+    
+arr_begin:
+    /* save current container */
+    ctn->tag = (((u64)ctn_len + 1) << YYJSON_TAG_BIT) |
+               (ctn->tag & YYJSON_TAG_MASK);
+    
+    /* create a new array value, save parent container offset */
+    val_incr();
+    val->tag = YYJSON_TYPE_ARR;
+    val->uni.ofs = (usize)((u8 *)val - (u8 *)ctn);
+    
+    /* push the new array value as current container */
+    ctn = val;
+    ctn_len = 0;
+    if (*cur == '\n') cur++;
+    
+arr_val_begin:
+#if YYJSON_IS_REAL_GCC
+    while (true) repeat16({
+        if (byte_match_2(cur, "  ")) cur += 2;
+        else break;
+    });
+#else
+    while (true) repeat16({
+        if (likely(byte_match_2(cur, "  "))) cur += 2;
+        else break;
+    });
+#endif
+    
+    if (*cur == '{') {
+        cur++;
+        goto obj_begin;
+    }
+    if (*cur == '[') {
+        cur++;
+        goto arr_begin;
+    }
+    if (char_is_number(*cur)) {
+        val_incr();
+        ctn_len++;
+        if (likely(read_number(cur, &cur, flg, val, &msg))) goto arr_val_end;
+        goto fail_number;
+    }
+    if (*cur == '"') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_string(cur, &cur, val, &msg))) goto arr_val_end;
+        goto fail_string;
+    }
+    if (*cur == 't') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_true(cur, &cur, val))) goto arr_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'f') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_false(cur, &cur, val))) goto arr_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'n') {
+        val_incr();
+        ctn_len++;
+        if (likely(read_null(cur, &cur, val))) goto arr_val_end;
+        if (has_flag(ALLOW_INF_AND_NAN)) {
+            if (read_nan(false, cur, &cur, val)) goto arr_val_end;
+        }
+        goto fail_literal;
+    }
+    if (*cur == ']') {
+        cur++;
+        if (likely(ctn_len == 0)) goto arr_end;
+        if (has_flag(ALLOW_TRAILING_COMMAS)) goto arr_end;
+        goto fail_trailing_comma;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto arr_val_begin;
+    }
+    if (has_flag(ALLOW_INF_AND_NAN) && *cur == 'N') {
+        val_incr();
+        ctn_len++;
+        if (read_inf_or_nan(false, cur, &cur, val)) goto arr_val_end;
+        goto fail_character;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto arr_val_begin;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+arr_val_end:
+    if (byte_match_2(cur, ",\n")) {
+        cur += 2;
+        goto arr_val_begin;
+    }
+    if (*cur == ',') {
+        cur++;
+        goto arr_val_begin;
+    }
+    if (*cur == ']') {
+        cur++;
+        goto arr_end;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto arr_val_end;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto arr_val_end;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+arr_end:
+    /* get parent container */
+    ctn_parent = (yyjson_val *)(void *)((u8 *)ctn - ctn->uni.ofs);
+    
+    /* save the next sibling value offset */
+    ctn->uni.ofs = (usize)((u8 *)val - (u8 *)ctn) + sizeof(yyjson_val);
+    ctn->tag = ((ctn_len) << YYJSON_TAG_BIT) | YYJSON_TYPE_ARR;
+    if (unlikely(ctn == ctn_parent)) goto doc_end;
+    
+    /* pop parent as current container */
+    ctn = ctn_parent;
+    ctn_len = (usize)(ctn->tag >> YYJSON_TAG_BIT);
+    if (*cur == '\n') cur++;
+    if ((ctn->tag & YYJSON_TYPE_MASK) == YYJSON_TYPE_OBJ) {
+        goto obj_val_end;
+    } else {
+        goto arr_val_end;
+    }
+    
+obj_begin:
+    /* push container */
+    ctn->tag = (((u64)ctn_len + 1) << YYJSON_TAG_BIT) |
+               (ctn->tag & YYJSON_TAG_MASK);
+    val_incr();
+    val->tag = YYJSON_TYPE_OBJ;
+    /* offset to the parent */
+    val->uni.ofs = (usize)((u8 *)val - (u8 *)ctn);
+    ctn = val;
+    ctn_len = 0;
+    if (*cur == '\n') cur++;
+    
+obj_key_begin:
+#if YYJSON_IS_REAL_GCC
+    while (true) repeat16({
+        if (byte_match_2(cur, "  ")) cur += 2;
+        else break;
+    });
+#else
+    while (true) repeat16({
+        if (likely(byte_match_2(cur, "  "))) cur += 2;
+        else break;
+    });
+#endif
+    if (likely(*cur == '"')) {
+        val_incr();
+        ctn_len++;
+        if (likely(read_string(cur, &cur, val, &msg))) goto obj_key_end;
+        goto fail_string;
+    }
+    if (likely(*cur == '}')) {
+        cur++;
+        if (likely(ctn_len == 0)) goto obj_end;
+        if (has_flag(ALLOW_TRAILING_COMMAS)) goto obj_end;
+        goto fail_trailing_comma;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_key_begin;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_key_begin;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_key_end:
+    if (byte_match_2(cur, ": ")) {
+        cur += 2;
+        goto obj_val_begin;
+    }
+    if (*cur == ':') {
+        cur++;
+        goto obj_val_begin;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_key_end;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_key_end;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_val_begin:
+    if (*cur == '"') {
+        val++;
+        ctn_len++;
+        if (likely(read_string(cur, &cur, val, &msg))) goto obj_val_end;
+        goto fail_string;
+    }
+    if (char_is_number(*cur)) {
+        val++;
+        ctn_len++;
+        if (likely(read_number(cur, &cur, flg, val, &msg))) goto obj_val_end;
+        goto fail_number;
+    }
+    if (*cur == '{') {
+        cur++;
+        goto obj_begin;
+    }
+    if (*cur == '[') {
+        cur++;
+        goto arr_begin;
+    }
+    if (*cur == 't') {
+        val++;
+        ctn_len++;
+        if (likely(read_true(cur, &cur, val))) goto obj_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'f') {
+        val++;
+        ctn_len++;
+        if (likely(read_false(cur, &cur, val))) goto obj_val_end;
+        goto fail_literal;
+    }
+    if (*cur == 'n') {
+        val++;
+        ctn_len++;
+        if (likely(read_null(cur, &cur, val))) goto obj_val_end;
+        if (has_flag(ALLOW_INF_AND_NAN)) {
+            if (read_nan(false, cur, &cur, val)) goto obj_val_end;
+        }
+        goto fail_literal;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_val_begin;
+    }
+    if (has_flag(ALLOW_INF_AND_NAN) && *cur == 'N') {
+        val++;
+        ctn_len++;
+        if (read_inf_or_nan(false, cur, &cur, val)) goto obj_val_end;
+        goto fail_character;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_val_begin;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_val_end:
+    if (byte_match_2(cur, ",\n")) {
+        cur += 2;
+        goto obj_key_begin;
+    }
+    if (likely(*cur == ',')) {
+        cur++;
+        goto obj_key_begin;
+    }
+    if (likely(*cur == '}')) {
+        cur++;
+        goto obj_end;
+    }
+    if (char_is_space(*cur)) {
+        while (char_is_space(*++cur));
+        goto obj_val_end;
+    }
+#if !YYJSON_DISABLE_NON_STANDARD
+    if (has_flag(ALLOW_COMMENTS)) {
+        if (skip_spaces_and_comments(cur, &cur)) goto obj_val_end;
+        if (byte_match_2(cur, "/*")) goto fail_comment;
+    }
+#endif
+    goto fail_character;
+    
+obj_end:
+    /* pop container */
+    ctn_parent = (yyjson_val *)(void *)((u8 *)ctn - ctn->uni.ofs);
+    /* point to the next value */
+    ctn->uni.ofs = (usize)((u8 *)val - (u8 *)ctn) + sizeof(yyjson_val);
+    ctn->tag = (ctn_len << (YYJSON_TAG_BIT - 1)) | YYJSON_TYPE_OBJ;
+    if (unlikely(ctn == ctn_parent)) goto doc_end;
+    ctn = ctn_parent;
+    ctn_len = (usize)(ctn->tag >> YYJSON_TAG_BIT);
+    if (*cur == '\n') cur++;
+    if ((ctn->tag & YYJSON_TYPE_MASK) == YYJSON_TYPE_OBJ) {
+        goto obj_val_end;
+    } else {
+        goto arr_val_end;
+    }
+    
+doc_end:
+    
+    /* check invalid contents after json document */
+    if (unlikely(cur < end) && !has_flag(STOP_WHEN_DONE)) {
+#if !YYJSON_DISABLE_NON_STANDARD
+        if (has_flag(ALLOW_COMMENTS)) skip_spaces_and_comments(cur, &cur); else
+#endif
+        while (char_is_space(*cur)) cur++;
+        if (unlikely(cur < end)) goto fail_garbage;
+    }
+    
+    doc = (yyjson_doc *)val_hdr;
+    doc->root = val_hdr + hdr_len;
+    doc->alc = alc;
+    doc->dat_read = (usize)(cur - hdr);
+    doc->val_read = (usize)((val - val_hdr)) - hdr_len + 1;
+    doc->str_pool = has_flag(INSITU) ? NULL : (char *)hdr;
+    return doc;
+    
+fail_string:
+    return_err(cur, INVALID_STRING, msg);
+fail_number:
+    return_err(cur, INVALID_NUMBER, msg);
+fail_alloc:
+    return_err(cur, MEMORY_ALLOCATION, "memory allocation failed");
+fail_trailing_comma:
+    return_err(cur, JSON_STRUCTURE, "trailing comma is not allowed");
+fail_literal:
+    return_err(cur, LITERAL, "invalid literal");
+fail_comment:
+    return_err(cur, INVALID_COMMENT, "unclosed multiline comment");
+fail_character:
+    return_err(cur, UNEXPECTED_CHARACTER, "unexpected character");
+fail_garbage:
+    return_err(cur, UNEXPECTED_CONTENT, "unexpected content after document");
+    
+#undef has_flag
+#undef val_incr
+#undef return_err
+}
+
+
+
+/*==============================================================================
+ * JSON Reader Entrance
+ *============================================================================*/
+
+yyjson_doc *yyjson_read_opts(char *dat,
+                             usize len,
+                             yyjson_read_flag flg,
+                             const yyjson_alc *alc_ptr,
+                             yyjson_read_err *err) {
+    
+#define has_flag(_flag) unlikely((flg & YYJSON_READ_##_flag) != 0)
+    
+#define return_err(_pos, _code, _msg) do { \
+    err->pos = (usize)(_pos); \
+    err->msg = _msg; \
+    err->code = YYJSON_READ_ERROR_##_code; \
+    if (!has_flag(INSITU) && hdr) alc.free(alc.ctx, (void *)hdr); \
+    return NULL; \
+} while (false)
+    
+    yyjson_read_err dummy_err;
+    yyjson_alc alc;
+    yyjson_doc *doc;
+    u8 *hdr = NULL, *end, *cur;
+    
+    /* validate input parameters */
+    if (!err) err = &dummy_err;
+    if (likely(!alc_ptr)) {
+        alc = YYJSON_DEFAULT_ALC;
+    } else {
+        alc = *alc_ptr;
+    }
+    if (unlikely(!dat)) {
+        return_err(0, INVALID_PARAMETER, "input data is NULL");
+    }
+    if (unlikely(!len)) {
+        return_err(0, INVALID_PARAMETER, "input length is 0");
+    }
+    
+    /* add 4-byte zero padding for input data if necessary */
+    if (!has_flag(INSITU)) {
+        if (unlikely(len >= USIZE_MAX - YYJSON_PADDING_SIZE)) {
+            return_err(0, MEMORY_ALLOCATION, "memory allocation failed");
+        }
+        hdr = (u8 *)alc.malloc(alc.ctx, len + YYJSON_PADDING_SIZE);
+        if (unlikely(!hdr)) {
+            return_err(0, MEMORY_ALLOCATION, "memory allocation failed");
+        }
+        end = hdr + len;
+        cur = hdr;
+        memcpy(hdr, dat, len);
+        memset(end, 0, YYJSON_PADDING_SIZE);
+    } else {
+        hdr = (u8 *)dat;
+        end = (u8 *)dat + len;
+        cur = (u8 *)dat;
+    }
+    
+    /* skip empty contents before json document */
+    if (unlikely(char_is_space_or_comment(*cur))) {
+#if !YYJSON_DISABLE_NON_STANDARD
+        if (has_flag(ALLOW_COMMENTS)) {
+            if (!skip_spaces_and_comments(cur, &cur)) {
+                return_err(cur - hdr, INVALID_COMMENT,
+                           "unclosed multiline comment");
+            }
+        } else {
+            if (likely(char_is_space(*cur))) {
+                while (char_is_space(*++cur));
+            }
+        }
+#else
+        if (likely(char_is_space(*cur))) {
+            while (char_is_space(*++cur));
+        }
+#endif
+        if (unlikely(cur >= end)) {
+            return_err(0, EMPTY_CONTENT, "input data is empty");
+        }
+    }
+    
+    /* read json document */
+    if (likely(char_is_container(*cur))) {
+        if (char_is_space(cur[1]) && char_is_space(cur[2])) {
+            doc = read_root_pretty(hdr, cur, end, alc, flg, err);
+        } else {
+            doc = read_root_minify(hdr, cur, end, alc, flg, err);
+        }
+    } else {
+        doc = read_root_single(hdr, cur, end, alc, flg, err);
+    }
+    
+    /* check result */
+    if (likely(doc)) {
+        memset(err, 0, sizeof(yyjson_read_err));
+    } else {
+        /* RFC 8259: JSON text MUST be encoded using UTF-8 */
+        if (err->pos == 0 && err->code != YYJSON_READ_ERROR_MEMORY_ALLOCATION) {
+            if ((hdr[0] == 0xEF && hdr[1] == 0xBB && hdr[2] == 0xBF)) {
+                err->msg = "byte order mark (BOM) is not supported";
+            } else if (len >= 4 &&
+                       ((hdr[0] == 0x00 && hdr[1] == 0x00 &&
+                         hdr[2] == 0xFE && hdr[3] == 0xFF) ||
+                        (hdr[0] == 0xFF && hdr[1] == 0xFE &&
+                         hdr[2] == 0x00 && hdr[3] == 0x00))) {
+                err->msg = "UTF-32 encoding is not supported";
+            } else if (len >= 2 &&
+                       ((hdr[0] == 0xFE && hdr[1] == 0xFF) ||
+                        (hdr[0] == 0xFF && hdr[1] == 0xFE))) {
+                err->msg = "UTF-16 encoding is not supported";
+            }
+        }
+        if (!has_flag(INSITU) && hdr) alc.free(alc.ctx, (void *)hdr);
+    }
+    return doc;
+    
+#undef has_flag
+#undef return_err
+}
+
+yyjson_doc *yyjson_read_file(const char *path,
+                             yyjson_read_flag flg,
+                             const yyjson_alc *alc_ptr,
+                             yyjson_read_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    err->pos = 0; \
+    err->msg = _msg; \
+    err->code = YYJSON_READ_ERROR_##_code; \
+    if (file) fclose(file); \
+    if (buf) alc.free(alc.ctx, buf); \
+    return NULL; \
+} while (false)
+    
+    yyjson_read_err dummy_err;
+    yyjson_alc alc = alc_ptr ? *alc_ptr : YYJSON_DEFAULT_ALC;
+    yyjson_doc *doc;
+    
+    FILE *file = NULL;
+    long file_size = 0;
+    void *buf = NULL;
+    usize buf_size = 0;
+    
+    /* validate input parameters */
+    if (!err) err = &dummy_err;
+    if (unlikely(!path)) return_err(INVALID_PARAMETER, "input path is NULL");
+    
+    /* open file */
+    file = fopen_readonly(path);
+    if (file == NULL) return_err(FILE_OPEN, "file opening failed");
+    
+    /* get file size */
+    if (fseek(file, 0, SEEK_END) == 0) {
+        file_size = ftell(file);
+        if (file_size < 0 || (file_size + 1) < 0) file_size = 0;
+    }
+    rewind(file);
+    
+    /* read file */
+    if (file_size > 0) {
+        /* read the entire file in one call */
+        buf_size = (size_t)file_size + YYJSON_PADDING_SIZE;
+        buf = alc.malloc(alc.ctx, buf_size);
+        if (buf == NULL) {
+            return_err(MEMORY_ALLOCATION, "fail to alloc memory");
+        }
+        if (fread_safe(buf, (usize)file_size, file) != (usize)file_size) {
+            return_err(FILE_READ, "file reading failed");
+        }
+    } else {
+        /* failed to get file size, read it as a stream */
+        usize chunk_min = (usize)64;
+        usize chunk_max = (usize)512 * 1024 * 1024;
+        usize chunk_now = chunk_min;
+        usize read_size;
+        void *tmp;
+        
+        buf_size = YYJSON_PADDING_SIZE;
+        while (true) {
+            if (buf_size + chunk_now < buf_size) { /* overflow */
+                return_err(MEMORY_ALLOCATION, "fail to alloc memory");
+            }
+            buf_size += chunk_now;
+            if (!buf) {
+                buf = alc.malloc(alc.ctx, buf_size);
+                if (!buf) return_err(MEMORY_ALLOCATION, "fail to alloc memory");
+            } else {
+                tmp = alc.realloc(alc.ctx, buf, buf_size);
+                if (!tmp) return_err(MEMORY_ALLOCATION, "fail to alloc memory");
+                buf = tmp;
+            }
+            tmp = ((u8 *)buf) + buf_size - YYJSON_PADDING_SIZE - chunk_now;
+            read_size = fread_safe(tmp, chunk_now, file);
+            file_size += (long)read_size;
+            if (read_size != chunk_now) break;
+            
+            chunk_now *= 2;
+            if (chunk_now > chunk_max) chunk_now = chunk_max;
+        }
+    }
+    fclose(file);
+    
+    /* read JSON */
+    memset((u8 *)buf + file_size, 0, YYJSON_PADDING_SIZE);
+    flg |= YYJSON_READ_INSITU;
+    doc = yyjson_read_opts((char *)buf, (usize)file_size, flg, &alc, err);
+    if (doc) {
+        doc->str_pool = (char *)buf;
+        return doc;
+    } else {
+        alc.free(alc.ctx, buf);
+        return NULL;
+    }
+    
+#undef return_err
+}
+
+#endif /* YYJSON_DISABLE_READER */
+
+
+
+#if !YYJSON_DISABLE_WRITER
+
+/*==============================================================================
+ * Integer Writer
+ *
+ * The maximum value of uint32_t is 4294967295 (10 digits),
+ * these digits are named as 'aabbccddee' here.
+ *
+ * Although most compilers may convert the "division by constant value" into
+ * "multiply and shift", manual conversion can still help some compilers
+ * generate fewer and better instructions.
+ *
+ * Reference:
+ * Division by Invariant Integers using Multiplication, 1994.
+ * https://gmplib.org/~tege/divcnst-pldi94.pdf
+ * Improved division by invariant integers, 2011.
+ * https://gmplib.org/~tege/division-paper.pdf
+ *============================================================================*/
+
+/** Digit table from 00 to 99. */
+yyjson_align(2)
+static const char digit_table[200] = {
+    '0', '0', '0', '1', '0', '2', '0', '3', '0', '4',
+    '0', '5', '0', '6', '0', '7', '0', '8', '0', '9',
+    '1', '0', '1', '1', '1', '2', '1', '3', '1', '4',
+    '1', '5', '1', '6', '1', '7', '1', '8', '1', '9',
+    '2', '0', '2', '1', '2', '2', '2', '3', '2', '4',
+    '2', '5', '2', '6', '2', '7', '2', '8', '2', '9',
+    '3', '0', '3', '1', '3', '2', '3', '3', '3', '4',
+    '3', '5', '3', '6', '3', '7', '3', '8', '3', '9',
+    '4', '0', '4', '1', '4', '2', '4', '3', '4', '4',
+    '4', '5', '4', '6', '4', '7', '4', '8', '4', '9',
+    '5', '0', '5', '1', '5', '2', '5', '3', '5', '4',
+    '5', '5', '5', '6', '5', '7', '5', '8', '5', '9',
+    '6', '0', '6', '1', '6', '2', '6', '3', '6', '4',
+    '6', '5', '6', '6', '6', '7', '6', '8', '6', '9',
+    '7', '0', '7', '1', '7', '2', '7', '3', '7', '4',
+    '7', '5', '7', '6', '7', '7', '7', '8', '7', '9',
+    '8', '0', '8', '1', '8', '2', '8', '3', '8', '4',
+    '8', '5', '8', '6', '8', '7', '8', '8', '8', '9',
+    '9', '0', '9', '1', '9', '2', '9', '3', '9', '4',
+    '9', '5', '9', '6', '9', '7', '9', '8', '9', '9'
+};
+
+static_inline u8 *write_u32_len_8(u32 val, u8 *buf) {
+    u32 aa, bb, cc, dd, aabb, ccdd;                 /* 8 digits: aabbccdd */
+    aabb = (u32)(((u64)val * 109951163) >> 40);     /* (val / 10000) */
+    ccdd = val - aabb * 10000;                      /* (val % 10000) */
+    aa = (aabb * 5243) >> 19;                       /* (aabb / 100) */
+    cc = (ccdd * 5243) >> 19;                       /* (ccdd / 100) */
+    bb = aabb - aa * 100;                           /* (aabb % 100) */
+    dd = ccdd - cc * 100;                           /* (ccdd % 100) */
+    ((v16 *)buf)[0] = ((const v16 *)digit_table)[aa];
+    ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+    ((v16 *)buf)[2] = ((const v16 *)digit_table)[cc];
+    ((v16 *)buf)[3] = ((const v16 *)digit_table)[dd];
+    return buf + 8;
+}
+
+static_inline u8 *write_u32_len_4(u32 val, u8 *buf) {
+    u32 aa, bb;                                     /* 4 digits: aabb */
+    aa = (val * 5243) >> 19;                        /* (val / 100) */
+    bb = val - aa * 100;                            /* (val % 100) */
+    ((v16 *)buf)[0] = ((const v16 *)digit_table)[aa];
+    ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+    return buf + 4;
+}
+
+static_inline u8 *write_u32_len_1_8(u32 val, u8 *buf) {
+    u32 aa, bb, cc, dd, aabb, bbcc, ccdd, lz;
+    
+    if (val < 100) {                                /* 1-2 digits: aa */
+        lz = val < 10;                              /* leading zero: 0 or 1 */
+        ((v16 *)buf)[0] = *(const v16 *)(digit_table + (val * 2 + lz));
+        buf -= lz;
+        return buf + 2;
+        
+    } else if (val < 10000) {                       /* 3-4 digits: aabb */
+        aa = (val * 5243) >> 19;                    /* (val / 100) */
+        bb = val - aa * 100;                        /* (val % 100) */
+        lz = aa < 10;                               /* leading zero: 0 or 1 */
+        ((v16 *)buf)[0] = *(const v16 *)(digit_table + (aa * 2 + lz));
+        buf -= lz;
+        ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+        return buf + 4;
+        
+    } else if (val < 1000000) {                     /* 5-6 digits: aabbcc */
+        aa = (u32)(((u64)val * 429497) >> 32);      /* (val / 10000) */
+        bbcc = val - aa * 10000;                    /* (val % 10000) */
+        bb = (bbcc * 5243) >> 19;                   /* (bbcc / 100) */
+        cc = bbcc - bb * 100;                       /* (bbcc % 100) */
+        lz = aa < 10;                               /* leading zero: 0 or 1 */
+        ((v16 *)buf)[0] = *(const v16 *)(digit_table + (aa * 2 + lz));
+        buf -= lz;
+        ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+        ((v16 *)buf)[2] = ((const v16 *)digit_table)[cc];
+        return buf + 6;
+        
+    } else {                                        /* 7-8 digits: aabbccdd */
+        aabb = (u32)(((u64)val * 109951163) >> 40); /* (val / 10000) */
+        ccdd = val - aabb * 10000;                  /* (val % 10000) */
+        aa = (aabb * 5243) >> 19;                   /* (aabb / 100) */
+        cc = (ccdd * 5243) >> 19;                   /* (ccdd / 100) */
+        bb = aabb - aa * 100;                       /* (aabb % 100) */
+        dd = ccdd - cc * 100;                       /* (ccdd % 100) */
+        lz = aa < 10;                               /* leading zero: 0 or 1 */
+        ((v16 *)buf)[0] = *(const v16 *)(digit_table + (aa * 2 + lz));
+        buf -= lz;
+        ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+        ((v16 *)buf)[2] = ((const v16 *)digit_table)[cc];
+        ((v16 *)buf)[3] = ((const v16 *)digit_table)[dd];
+        return buf + 8;
+    }
+}
+
+static_inline u8 *write_u64_len_5_8(u32 val, u8 *buf) {
+    u32 aa, bb, cc, dd, aabb, bbcc, ccdd, lz;
+    
+    if (val < 1000000) {                            /* 5-6 digits: aabbcc */
+        aa = (u32)(((u64)val * 429497) >> 32);      /* (val / 10000) */
+        bbcc = val - aa * 10000;                    /* (val % 10000) */
+        bb = (bbcc * 5243) >> 19;                   /* (bbcc / 100) */
+        cc = bbcc - bb * 100;                       /* (bbcc % 100) */
+        lz = aa < 10;                               /* leading zero: 0 or 1 */
+        ((v16 *)buf)[0] = *(const v16 *)(digit_table + (aa * 2 + lz));
+        buf -= lz;
+        ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+        ((v16 *)buf)[2] = ((const v16 *)digit_table)[cc];
+        return buf + 6;
+        
+    } else {                                        /* 7-8 digits: aabbccdd */
+        aabb = (u32)(((u64)val * 109951163) >> 40); /* (val / 10000) */
+        ccdd = val - aabb * 10000;                  /* (val % 10000) */
+        aa = (aabb * 5243) >> 19;                   /* (aabb / 100) */
+        cc = (ccdd * 5243) >> 19;                   /* (ccdd / 100) */
+        bb = aabb - aa * 100;                       /* (aabb % 100) */
+        dd = ccdd - cc * 100;                       /* (ccdd % 100) */
+        lz = aa < 10;                               /* leading zero: 0 or 1 */
+        ((v16 *)buf)[0] = *(const v16 *)(digit_table + (aa * 2 + lz));
+        buf -= lz;
+        ((v16 *)buf)[1] = ((const v16 *)digit_table)[bb];
+        ((v16 *)buf)[2] = ((const v16 *)digit_table)[cc];
+        ((v16 *)buf)[3] = ((const v16 *)digit_table)[dd];
+        return buf + 8;
+    }
+}
+
+static_inline u8 *write_u64(u64 val, u8 *buf) {
+    u64 tmp, hgh;
+    u32 mid, low;
+    
+    if (val < 100000000) {                          /* 1-8 digits */
+        buf = write_u32_len_1_8((u32)val, buf);
+        return buf;
+        
+    } else if (val < (u64)100000000 * 100000000) {  /* 9-16 digits */
+        hgh = val / 100000000;                      /* (val / 100000000) */
+        low = (u32)(val - hgh * 100000000);         /* (val % 100000000) */
+        buf = write_u32_len_1_8((u32)hgh, buf);
+        buf = write_u32_len_8(low, buf);
+        return buf;
+        
+    } else {                                        /* 17-20 digits */
+        tmp = val / 100000000;                      /* (val / 100000000) */
+        low = (u32)(val - tmp * 100000000);         /* (val % 100000000) */
+        hgh = (u32)(tmp / 10000);                   /* (tmp / 10000) */
+        mid = (u32)(tmp - hgh * 10000);             /* (tmp % 10000) */
+        buf = write_u64_len_5_8((u32)hgh, buf);
+        buf = write_u32_len_4(mid, buf);
+        buf = write_u32_len_8(low, buf);
+        return buf;
+    }
+}
+
+
+
+/*==============================================================================
+ * Number Writer
+ *============================================================================*/
+
+#if YYJSON_HAS_IEEE_754 && !YYJSON_DISABLE_FAST_FP_CONV
+
+/** Trailing zero count table for number 0 to 99.
+    (generate with misc/make_tables.c) */
+static const u8 dec_trailing_zero_table[] = {
+    2, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    1, 0, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/** Write an unsigned integer with a length of 1 to 16. */
+static_inline u8 *write_u64_len_1_to_16(u64 val, u8 *buf) {
+    u64 hgh;
+    u32 low;
+    if (val < 100000000) {                          /* 1-8 digits */
+        buf = write_u32_len_1_8((u32)val, buf);
+        return buf;
+    } else {                                        /* 9-16 digits */
+        hgh = val / 100000000;                      /* (val / 100000000) */
+        low = (u32)(val - hgh * 100000000);         /* (val % 100000000) */
+        buf = write_u32_len_1_8((u32)hgh, buf);
+        buf = write_u32_len_8(low, buf);
+        return buf;
+    }
+}
+
+/** Write an unsigned integer with a length of 1 to 17. */
+static_inline u8 *write_u64_len_1_to_17(u64 val, u8 *buf) {
+    u64 hgh;
+    u32 mid, low, one;
+    if (val >= (u64)100000000 * 10000000) {         /* len: 16 to 17 */
+        hgh = val / 100000000;                      /* (val / 100000000) */
+        low = (u32)(val - hgh * 100000000);         /* (val % 100000000) */
+        one = (u32)(hgh / 100000000);               /* (hgh / 100000000) */
+        mid = (u32)(hgh - (u64)one * 100000000);    /* (hgh % 100000000) */
+        *buf = (u8)((u8)one + (u8)'0');
+        buf += one > 0;
+        buf = write_u32_len_8(mid, buf);
+        buf = write_u32_len_8(low, buf);
+        return buf;
+    } else if (val >= (u64)100000000){              /* len: 9 to 15 */
+        hgh = val / 100000000;                      /* (val / 100000000) */
+        low = (u32)(val - hgh * 100000000);         /* (val % 100000000) */
+        buf = write_u32_len_1_8((u32)hgh, buf);
+        buf = write_u32_len_8(low, buf);
+        return buf;
+    } else { /* len: 1 to 8 */
+        buf = write_u32_len_1_8((u32)val, buf);
+        return buf;
+    }
+}
+
+/**
+ Write an unsigned integer with a length of 15 to 17 with trailing zero trimmed.
+ These digits are named as "aabbccddeeffgghhii" here.
+ For example, input 1234567890123000, output "1234567890123".
+ */
+static_inline u8 *write_u64_len_15_to_17_trim(u8 *buf, u64 sig) {
+    bool lz;                                        /* leading zero */
+    u32 tz1, tz2, tz;                               /* trailing zero */
+    
+    u32 abbccddee = (u32)(sig / 100000000);
+    u32 ffgghhii = (u32)(sig - (u64)abbccddee * 100000000);
+    u32 abbcc = abbccddee / 10000;                  /* (abbccddee / 10000) */
+    u32 ddee = abbccddee - abbcc * 10000;           /* (abbccddee % 10000) */
+    u32 abb = (u32)(((u64)abbcc * 167773) >> 24);   /* (abbcc / 100) */
+    u32 a = (abb * 41) >> 12;                       /* (abb / 100) */
+    u32 bb = abb - a * 100;                         /* (abb % 100) */
+    u32 cc = abbcc - abb * 100;                     /* (abbcc % 100) */
+    
+    /* write abbcc */
+    buf[0] = (u8)(a + '0');
+    buf += a > 0;
+    lz = bb < 10 && a == 0;
+    ((v16 *)buf)[0] = *(const v16 *)(digit_table + (bb * 2 + lz));
+    buf -= lz;
+    ((v16 *)buf)[1] = ((const v16 *)digit_table)[cc];
+    
+    if (ffgghhii) {
+        u32 dd = (ddee * 5243) >> 19;               /* (ddee / 100) */
+        u32 ee = ddee - dd * 100;                   /* (ddee % 100) */
+        u32 ffgg = (u32)(((u64)ffgghhii * 109951163) >> 40); /* (val / 10000) */
+        u32 hhii = ffgghhii - ffgg * 10000;         /* (val % 10000) */
+        u32 ff = (ffgg * 5243) >> 19;               /* (aabb / 100) */
+        u32 gg = ffgg - ff * 100;                   /* (aabb % 100) */
+        ((v16 *)buf)[2] = ((const v16 *)digit_table)[dd];
+        ((v16 *)buf)[3] = ((const v16 *)digit_table)[ee];
+        ((v16 *)buf)[4] = ((const v16 *)digit_table)[ff];
+        ((v16 *)buf)[5] = ((const v16 *)digit_table)[gg];
+        if (hhii) {
+            u32 hh = (hhii * 5243) >> 19;           /* (ccdd / 100) */
+            u32 ii = hhii - hh * 100;               /* (ccdd % 100) */
+            ((v16 *)buf)[6] = ((const v16 *)digit_table)[hh];
+            ((v16 *)buf)[7] = ((const v16 *)digit_table)[ii];
+            tz1 = dec_trailing_zero_table[hh];
+            tz2 = dec_trailing_zero_table[ii];
+            tz = ii ? tz2 : (tz1 + 2);
+            buf += 16 - tz;
+            return buf;
+        } else {
+            tz1 = dec_trailing_zero_table[ff];
+            tz2 = dec_trailing_zero_table[gg];
+            tz = gg ? tz2 : (tz1 + 2);
+            buf += 12 - tz;
+            return buf;
+        }
+    } else {
+        if (ddee) {
+            u32 dd = (ddee * 5243) >> 19;           /* (ddee / 100) */
+            u32 ee = ddee - dd * 100;               /* (ddee % 100) */
+            ((v16 *)buf)[2] = ((const v16 *)digit_table)[dd];
+            ((v16 *)buf)[3] = ((const v16 *)digit_table)[ee];
+            tz1 = dec_trailing_zero_table[dd];
+            tz2 = dec_trailing_zero_table[ee];
+            tz = ee ? tz2 : (tz1 + 2);
+            buf += 8 - tz;
+            return buf;
+        } else {
+            tz1 = dec_trailing_zero_table[bb];
+            tz2 = dec_trailing_zero_table[cc];
+            tz = cc ? tz2 : (tz1 + tz2);
+            buf += 4 - tz;
+            return buf;
+        }
+    }
+}
+
+/** Write a signed integer in the range -324 to 308. */
+static_inline u8 *write_f64_exp(i32 exp, u8 *buf) {
+    buf[0] = '-';
+    buf += exp < 0;
+    exp = exp < 0 ? -exp : exp;
+    if (exp < 100) {
+        u32 lz = exp < 10;
+        *(v16 *)&buf[0] = *(const v16 *)(digit_table + ((u32)exp * 2 + lz));
+        return buf + 2 - lz;
+    } else {
+        u32 hi = ((u32)exp * 656) >> 16;            /* exp / 100 */
+        u32 lo = (u32)exp - hi * 100;               /* exp % 100 */
+        buf[0] = (u8)((u8)hi + (u8)'0');
+        *(v16 *)&buf[1] = *(const v16 *)(digit_table + (lo * 2));
+        return buf + 3;
+    }
+}
+
+/** Multiplies 128-bit integer and returns highest 64-bit rounded value. */
+static_inline u64 round_to_odd(u64 hi, u64 lo, u64 cp) {
+    u64 x_hi, x_lo, y_hi, y_lo;
+    u128_mul(cp, lo, &x_hi, &x_lo);
+    u128_mul_add(cp, hi, x_hi, &y_hi, &y_lo);
+    return y_hi | (y_lo > 1);
+}
+
+/**
+ Convert double number from binary to decimal.
+ The output significand is shortest decimal but may have trailing zeros.
+ 
+ This function use the Schubfach algorithm:
+ Raffaello Giulietti, The Schubfach way to render doubles, 2020.
+ https://drive.google.com/open?id=1luHhyQF9zKlM8yJ1nebU0OgVYhfC6CBN
+ https://github.com/abolz/Drachennest
+ 
+ See also:
+ Dragonbox: A New Floating-Point Binary-to-Decimal Conversion Algorithm, 2020.
+ https://github.com/jk-jeon/dragonbox/blob/master/other_files/Dragonbox.pdf
+ https://github.com/jk-jeon/dragonbox
+ 
+ @param sig_raw The raw value of significand in IEEE 754 format.
+ @param exp_raw The raw value of exponent in IEEE 754 format.
+ @param sig_bin The decoded value of significand in binary.
+ @param exp_bin The decoded value of exponent in binary.
+ @param sig_dec The output value of significand in decimal.
+ @param exp_dec The output value of exponent in decimal.
+ @warning The input double number should not be 0, inf, nan.
+ */
+static_inline void f64_bin_to_dec(u64 sig_raw, u32 exp_raw,
+                                  u64 sig_bin, i32 exp_bin,
+                                  u64 *sig_dec, i32 *exp_dec) {
+    
+    bool is_even, lower_bound_closer, u_inside, w_inside, round_up;
+    u64 s, sp, cb, cbl, cbr, vb, vbl, vbr, pow10hi, pow10lo, upper, lower, mid;
+    i32 k, h, exp10;
+    
+    is_even = !(sig_bin & 1);
+    lower_bound_closer = (sig_raw == 0 && exp_raw > 1);
+    
+    cbl = 4 * sig_bin - 2 + lower_bound_closer;
+    cb  = 4 * sig_bin;
+    cbr = 4 * sig_bin + 2;
+    
+    /* exp_bin: [-1074, 971]                                                  */
+    /* k = lower_bound_closer ? floor(log10(pow(2, exp_bin)))                 */
+    /*                        : floor(log10(pow(2, exp_bin) * 3.0 / 4.0))     */
+    /*   = lower_bound_closer ? floor(exp_bin * log10(2))                     */
+    /*                        : floor(exp_bin * log10(2) + log10(3.0 / 4.0))  */
+    k = (i32)(exp_bin * 315653 - (lower_bound_closer ? 131237 : 0)) >> 20;
+    
+    /* k: [-324, 292]                                                         */
+    /* h = exp_bin + floor(log2(pow(10, e)))                                  */
+    /*   = exp_bin + floor(log2(10) * e)                                      */
+    exp10 = -k;
+    h = exp_bin + ((exp10 * 217707) >> 16) + 1;
+    
+    pow10_table_get_sig(exp10, &pow10hi, &pow10lo);
+    pow10lo += (exp10 < POW10_SIG_TABLE_MIN_EXACT_EXP ||
+                exp10 > POW10_SIG_TABLE_MAX_EXACT_EXP);
+    vbl = round_to_odd(pow10hi, pow10lo, cbl << h);
+    vb  = round_to_odd(pow10hi, pow10lo, cb  << h);
+    vbr = round_to_odd(pow10hi, pow10lo, cbr << h);
+    
+    lower = vbl + !is_even;
+    upper = vbr - !is_even;
+    
+    s = vb / 4;
+    if (s >= 10) {
+        sp = s / 10;
+        u_inside = (lower <= 40 * sp);
+        w_inside = (upper >= 40 * sp + 40);
+        if (u_inside != w_inside) {
+            *sig_dec = sp + w_inside;
+            *exp_dec = k + 1;
+            return;
+        }
+    }
+    
+    u_inside = (lower <= 4 * s);
+    w_inside = (upper >= 4 * s + 4);
+    
+    mid = 4 * s + 2;
+    round_up = (vb > mid) || (vb == mid && (s & 1) != 0);
+    
+    *sig_dec = s + ((u_inside != w_inside) ? w_inside : round_up);
+    *exp_dec = k;
+}
+
+/**
+ Write a double number (requires 32 bytes buffer).
+ 
+ We follows the ECMAScript specification to print floating point numbers,
+ but with the following changes:
+ 1. Keep the negative sign of 0.0 to preserve input information.
+ 2. Keep decimal point to indicate the number is floating point.
+ 3. Remove positive sign of exponent part.
+*/
+static_noinline u8 *write_f64_raw(u8 *buf, u64 raw, yyjson_write_flag flg) {
+    u64 sig_bin, sig_dec, sig_raw;
+    i32 exp_bin, exp_dec, sig_len, dot_pos, i, max;
+    u32 exp_raw, hi, lo;
+    u8 *hdr, *num_hdr, *num_end, *dot_end;
+    bool sign;
+    
+    /* decode from raw bytes from IEEE-754 double format. */
+    sign = (bool)(raw >> (F64_BITS - 1));
+    sig_raw = raw & F64_SIG_MASK;
+    exp_raw = (u32)((raw & F64_EXP_MASK) >> F64_SIG_BITS);
+    
+    /* return inf and nan */
+    if (unlikely(exp_raw == ((u32)1 << F64_EXP_BITS) - 1)) {
+        if (flg & YYJSON_WRITE_INF_AND_NAN_AS_NULL) {
+            *(v32 *)&buf[0] = v32_make('n', 'u', 'l', 'l');
+            return buf + 4;
+        } else if (flg & YYJSON_WRITE_ALLOW_INF_AND_NAN) {
+            if (sig_raw == 0) {
+                buf[0] = '-';
+                buf += sign;
+                *(v32 *)&buf[0] = v32_make('I', 'n', 'f', 'i');
+                *(v32 *)&buf[4] = v32_make('n', 'i', 't', 'y');
+                buf += 8;
+                return buf;
+            } else {
+                *(v32 *)&buf[0] = v32_make('N', 'a', 'N', '\0');
+                return buf + 3;
+            }
+        } else {
+            return NULL;
+        }
+    }
+    
+    /* add sign for all finite double value, include -0.0 */
+    buf[0] = '-';
+    buf += sign;
+    hdr = buf;
+    
+    /* return zero */
+    if ((raw << 1) == 0) {
+        *(v32 *)&buf[0] = v32_make('0', '.', '0', '\0');
+        buf += 3;
+        return buf;
+    }
+    
+    if (likely(exp_raw != 0)) {
+        /* normal number */
+        sig_bin = sig_raw | ((u64)1 << F64_SIG_BITS);
+        exp_bin = (i32)exp_raw - F64_EXP_BIAS - F64_SIG_BITS;
+        
+        /* fast path for small integer number without fraction */
+        if (-F64_SIG_BITS <= exp_bin && exp_bin <= 0) {
+            if (u64_tz_bits(sig_bin) >= (u32)-exp_bin) {
+                /* number is integer in range 1 to 0x1FFFFFFFFFFFFF */
+                sig_dec = sig_bin >> -exp_bin;
+                buf = write_u64_len_1_to_16(sig_dec, buf);
+                *(v16 *)buf = v16_make('.', '0');
+                buf += 2;
+                return buf;
+            }
+        }
+        
+        /* binary to decimal */
+        f64_bin_to_dec(sig_raw, exp_raw, sig_bin, exp_bin, &sig_dec, &exp_dec);
+        
+        /* the sig length is 15 to 17 */
+        sig_len = 17;
+        sig_len -= (sig_dec < (u64)100000000 * 100000000);
+        sig_len -= (sig_dec < (u64)100000000 * 10000000);
+        
+        /* the decimal point position relative to the first digit */
+        dot_pos = sig_len + exp_dec;
+        
+        if (-6 < dot_pos && dot_pos <= 21) {
+            /* no need to write exponent part */
+            if (dot_pos <= 0) {
+                /* dot before first digit */
+                /* such as 0.1234, 0.000001234 */
+                num_hdr = hdr + (2 - dot_pos);
+                num_end = write_u64_len_15_to_17_trim(num_hdr, sig_dec);
+                hdr[0] = '0';
+                hdr[1] = '.';
+                hdr += 2;
+                max = -dot_pos;
+                for (i = 0; i < max; i++) hdr[i] = '0';
+                return num_end;
+            } else {
+                /* dot after first digit */
+                /* such as 1.234, 1234.0, 123400000000000000000.0 */
+                memset(hdr +  0, '0', 8);
+                memset(hdr +  8, '0', 8);
+                memset(hdr + 16, '0', 8);
+                num_hdr = hdr + 1;
+                num_end = write_u64_len_15_to_17_trim(num_hdr, sig_dec);
+                for (i = 0; i < dot_pos; i++) hdr[i] = hdr[i + 1];
+                hdr[dot_pos] = '.';
+                dot_end = hdr + dot_pos + 2;
+                return dot_end < num_end ? num_end : dot_end;
+            }
+        } else {
+            /* write with scientific notation */
+            /* such as 1.234e56 */
+            u8 *end = write_u64_len_15_to_17_trim(buf + 1, sig_dec);
+            exp_dec += sig_len - 1;
+            hdr[0] = hdr[1];
+            hdr[1] = '.';
+            end[0] = 'e';
+            buf = write_f64_exp(exp_dec, end + 1);
+            return buf;
+        }
+        
+    } else {
+        /* subnormal number */
+        sig_bin = sig_raw;
+        exp_bin = 1 - F64_EXP_BIAS - F64_SIG_BITS;
+        
+        /* binary to decimal */
+        f64_bin_to_dec(sig_raw, exp_raw, sig_bin, exp_bin, &sig_dec, &exp_dec);
+        
+        /* write significand part */
+        buf = write_u64_len_1_to_17(sig_dec, buf + 1);
+        hdr[0] = hdr[1];
+        hdr[1] = '.';
+        do {
+            buf--;
+            exp_dec++;
+        } while (*buf == '0');
+        exp_dec += (i32)(buf - hdr - 2);
+        buf += (*buf != '.');
+        buf[0] = 'e';
+        buf++;
+        
+        /* write exponent part */
+        buf[0] = '-';
+        buf++;
+        exp_dec = -exp_dec;
+        hi = ((u32)exp_dec * 656) >> 16; /* exp / 100 */
+        lo = (u32)exp_dec - hi * 100; /* exp % 100 */
+        buf[0] = (u8)((u8)hi + (u8)'0');
+        *(v16 *)&buf[1] = *(const v16 *)(digit_table + (lo * 2));
+        buf += 3;
+        return buf;
+    }
+}
+
+#else /* FP_WRITER */
+
+/** Write a double number (requires 32 bytes buffer). */
+static_noinline u8 *write_f64_raw(u8 *buf, u64 raw, yyjson_write_flag flg) {
+    /*
+     For IEEE 754, `DBL_DECIMAL_DIG` is 17 for round-trip.
+     For non-IEEE formats, 17 is used to avoid buffer overflow,
+     round-trip is not guaranteed.
+     */
+#if defined(DBL_DECIMAL_DIG)
+    int dig = DBL_DECIMAL_DIG > 17 ? 17 : DBL_DECIMAL_DIG;
+#else
+    int dig = 17;
+#endif
+    
+    /*
+     The snprintf() function is locale-dependent. For currently known locales,
+     (en, zh, ja, ko, am, he, hi) use '.' as the decimal point, while other
+     locales use ',' as the decimal point. we need to replace ',' with '.'
+     to avoid the locale setting.
+     */
+    f64 val = f64_from_raw(raw);
+#if YYJSON_MSC_VER >= 1400
+    int len = sprintf_s((char *)buf, 32, "%.*g", dig, val);
+#elif defined(snprintf) || (YYJSON_STDC_VER >= 199901L)
+    int len = snprintf((char *)buf, 32, "%.*g", dig, val);
+#else
+    int len = sprintf((char *)buf, "%.*g", dig, val);
+#endif
+    
+    u8 *cur = buf;
+    if (unlikely(len < 1)) return NULL;
+    cur += (*cur == '-');
+    if (unlikely(!digi_is_digit(*cur))) {
+        /* nan, inf, or bad output */
+        if (flg & YYJSON_WRITE_INF_AND_NAN_AS_NULL) {
+            *(v32 *)&buf[0] = v32_make('n', 'u', 'l', 'l');
+            return buf + 4;
+        } else if (flg & YYJSON_WRITE_ALLOW_INF_AND_NAN) {
+            if (*cur == 'i') {
+                *(v32 *)&cur[0] = v32_make('I', 'n', 'f', 'i');
+                *(v32 *)&cur[4] = v32_make('n', 'i', 't', 'y');
+                cur += 8;
+                return cur;
+            } else if (*cur == 'n') {
+                *(v32 *)&buf[0] = v32_make('N', 'a', 'N', '\0');
+                return buf + 3;
+            }
+        }
+        return NULL;
+    } else {
+        /* finite number */
+        int i = 0;
+        for (; i < len; i++) {
+            if (buf[i] == ',') {
+                buf[i] = '.';
+                break;
+            }
+        }
+    }
+    return buf + len;
+}
+
+#endif /* FP_WRITER */
+
+/** Write a JSON number (requires 32 bytes buffer). */
+static_inline u8 *write_number(u8 *cur, yyjson_val *val,
+                               yyjson_write_flag flg) {
+    if (val->tag & YYJSON_SUBTYPE_REAL) {
+        u64 raw = val->uni.u64;
+        return write_f64_raw(cur, raw, flg);
+    } else {
+        u64 pos = val->uni.u64;
+        u64 neg = ~pos + 1;
+        usize sgn = ((val->tag & YYJSON_SUBTYPE_SINT) > 0) & ((i64)pos < 0);
+        *cur = '-';
+        return write_u64(sgn ? neg : pos, cur + sgn);
+    }
+}
+
+
+
+/*==============================================================================
+ * String Writer
+ *============================================================================*/
+
+/** Character escape type. */
+typedef u8 char_esc_type;
+#define CHAR_ESC_NONE   0 /* Character do not need to be escaped. */
+#define CHAR_ESC_ASCII  1 /* ASCII character, escaped as '\x'. */
+#define CHAR_ESC_UTF8_1 2 /* 1-byte UTF-8 character, escaped as '\uXXXX'. */
+#define CHAR_ESC_UTF8_2 3 /* 2-byte UTF-8 character, escaped as '\uXXXX'. */
+#define CHAR_ESC_UTF8_3 4 /* 3-byte UTF-8 character, escaped as '\uXXXX'. */
+#define CHAR_ESC_UTF8_4 5 /* 4-byte UTF-8 character, escaped as two '\uXXXX'. */
+
+/** Character escape type table: don't escape unicode, don't escape '/'.
+    (generate with misc/make_tables.c) */
+static const char_esc_type esc_table_default[256] = {
+    2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2,
+    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+    0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/** Character escape type table: don't escape unicode, escape '/'.
+    (generate with misc/make_tables.c) */
+static const char_esc_type esc_table_default_with_slashes[256] = {
+    2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2,
+    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+    0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/** Character escape type table: escape unicode, don't escape '/'.
+    (generate with misc/make_tables.c) */
+static const char_esc_type esc_table_unicode[256] = {
+    2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2,
+    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+    0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
+    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
+    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
+    5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/** Character escape type table: escape unicode, escape '/'.
+    (generate with misc/make_tables.c) */
+static const char_esc_type esc_table_unicode_with_slashes[256] = {
+    2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2,
+    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+    0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
+    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
+    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
+    5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/** Escaped hex character table: ["00" "01" "02" ... "FD" "FE" "FF"].
+    (generate with misc/make_tables.c) */
+yyjson_align(2)
+static const u8 esc_hex_char_table[512] = {
+    '0', '0', '0', '1', '0', '2', '0', '3',
+    '0', '4', '0', '5', '0', '6', '0', '7',
+    '0', '8', '0', '9', '0', 'A', '0', 'B',
+    '0', 'C', '0', 'D', '0', 'E', '0', 'F',
+    '1', '0', '1', '1', '1', '2', '1', '3',
+    '1', '4', '1', '5', '1', '6', '1', '7',
+    '1', '8', '1', '9', '1', 'A', '1', 'B',
+    '1', 'C', '1', 'D', '1', 'E', '1', 'F',
+    '2', '0', '2', '1', '2', '2', '2', '3',
+    '2', '4', '2', '5', '2', '6', '2', '7',
+    '2', '8', '2', '9', '2', 'A', '2', 'B',
+    '2', 'C', '2', 'D', '2', 'E', '2', 'F',
+    '3', '0', '3', '1', '3', '2', '3', '3',
+    '3', '4', '3', '5', '3', '6', '3', '7',
+    '3', '8', '3', '9', '3', 'A', '3', 'B',
+    '3', 'C', '3', 'D', '3', 'E', '3', 'F',
+    '4', '0', '4', '1', '4', '2', '4', '3',
+    '4', '4', '4', '5', '4', '6', '4', '7',
+    '4', '8', '4', '9', '4', 'A', '4', 'B',
+    '4', 'C', '4', 'D', '4', 'E', '4', 'F',
+    '5', '0', '5', '1', '5', '2', '5', '3',
+    '5', '4', '5', '5', '5', '6', '5', '7',
+    '5', '8', '5', '9', '5', 'A', '5', 'B',
+    '5', 'C', '5', 'D', '5', 'E', '5', 'F',
+    '6', '0', '6', '1', '6', '2', '6', '3',
+    '6', '4', '6', '5', '6', '6', '6', '7',
+    '6', '8', '6', '9', '6', 'A', '6', 'B',
+    '6', 'C', '6', 'D', '6', 'E', '6', 'F',
+    '7', '0', '7', '1', '7', '2', '7', '3',
+    '7', '4', '7', '5', '7', '6', '7', '7',
+    '7', '8', '7', '9', '7', 'A', '7', 'B',
+    '7', 'C', '7', 'D', '7', 'E', '7', 'F',
+    '8', '0', '8', '1', '8', '2', '8', '3',
+    '8', '4', '8', '5', '8', '6', '8', '7',
+    '8', '8', '8', '9', '8', 'A', '8', 'B',
+    '8', 'C', '8', 'D', '8', 'E', '8', 'F',
+    '9', '0', '9', '1', '9', '2', '9', '3',
+    '9', '4', '9', '5', '9', '6', '9', '7',
+    '9', '8', '9', '9', '9', 'A', '9', 'B',
+    '9', 'C', '9', 'D', '9', 'E', '9', 'F',
+    'A', '0', 'A', '1', 'A', '2', 'A', '3',
+    'A', '4', 'A', '5', 'A', '6', 'A', '7',
+    'A', '8', 'A', '9', 'A', 'A', 'A', 'B',
+    'A', 'C', 'A', 'D', 'A', 'E', 'A', 'F',
+    'B', '0', 'B', '1', 'B', '2', 'B', '3',
+    'B', '4', 'B', '5', 'B', '6', 'B', '7',
+    'B', '8', 'B', '9', 'B', 'A', 'B', 'B',
+    'B', 'C', 'B', 'D', 'B', 'E', 'B', 'F',
+    'C', '0', 'C', '1', 'C', '2', 'C', '3',
+    'C', '4', 'C', '5', 'C', '6', 'C', '7',
+    'C', '8', 'C', '9', 'C', 'A', 'C', 'B',
+    'C', 'C', 'C', 'D', 'C', 'E', 'C', 'F',
+    'D', '0', 'D', '1', 'D', '2', 'D', '3',
+    'D', '4', 'D', '5', 'D', '6', 'D', '7',
+    'D', '8', 'D', '9', 'D', 'A', 'D', 'B',
+    'D', 'C', 'D', 'D', 'D', 'E', 'D', 'F',
+    'E', '0', 'E', '1', 'E', '2', 'E', '3',
+    'E', '4', 'E', '5', 'E', '6', 'E', '7',
+    'E', '8', 'E', '9', 'E', 'A', 'E', 'B',
+    'E', 'C', 'E', 'D', 'E', 'E', 'E', 'F',
+    'F', '0', 'F', '1', 'F', '2', 'F', '3',
+    'F', '4', 'F', '5', 'F', '6', 'F', '7',
+    'F', '8', 'F', '9', 'F', 'A', 'F', 'B',
+    'F', 'C', 'F', 'D', 'F', 'E', 'F', 'F'
+};
+
+/** Escaped single character table. (generate with misc/make_tables.c) */
+yyjson_align(2)
+static const u8 esc_single_char_table[512] = {
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    '\\', 'b', '\\', 't', '\\', 'n', ' ', ' ',
+    '\\', 'f', '\\', 'r', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', '\\', '"', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', '\\', '/',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    '\\', '\\', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',
+    ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '
+};
+
+/** Returns the escape table with options. */
+static_inline const char_esc_type *get_esc_table_with_flag(
+    yyjson_read_flag flg) {
+    if (unlikely(flg & YYJSON_WRITE_ESCAPE_UNICODE)) {
+        if (unlikely(flg & YYJSON_WRITE_ESCAPE_SLASHES)) {
+            return esc_table_unicode_with_slashes;
+        } else {
+            return esc_table_unicode;
+        }
+    } else {
+        if (unlikely(flg & YYJSON_WRITE_ESCAPE_SLASHES)) {
+            return esc_table_default_with_slashes;
+        } else {
+            return esc_table_default;
+        }
+    }
+}
+
+/**
+ Write UTF-8 string (requires len * 6 + 2 bytes buffer).
+ If the input string is not valid UTF-8 encoding, undefined behavior may occur.
+ @param cur Buffer cursor.
+ @param str A valid UTF-8 string with null-terminator.
+ @param str_len Length of string in bytes.
+ @param esc_table Escape type table for character escaping.
+ @return The buffer cursor after string.
+ */
+static_inline u8 *write_string(u8 *cur,
+                               const u8 *str,
+                               usize str_len,
+                               const char_esc_type *esc_table) {
+    const u8 *end = str + str_len;
+    *cur++ = '"';
+    
+copy_char:
+    /*
+     Copy continuous unescaped char, loop unrolling, same as the following code:
+     
+         while (true) repeat16({
+            if (unlikely(esc_table[*str] != CHAR_ESC_NONE)) break;
+            *cur++ = *str++;
+         });
+     */
+#define expr_jump(i) \
+    if (unlikely(esc_table[str[i]] != CHAR_ESC_NONE)) goto stop_char_##i;
+    
+#define expr_stop(i) \
+    stop_char_##i: \
+    memcpy(cur, str, i); \
+    cur += i; str += i; goto copy_next;
+    
+    repeat16_incr(expr_jump);
+    memcpy(cur, str, 16);
+    cur += 16; str += 16;
+    goto copy_char;
+    repeat16_incr(expr_stop);
+    
+#undef expr_jump
+#undef expr_stop
+    
+copy_next:
+    while (str < end) {
+        switch (esc_table[*str]) {
+            case CHAR_ESC_NONE: {
+                *cur++ = *str++;
+                goto copy_char;
+            }
+            case CHAR_ESC_ASCII: {
+                *(v16 *)cur = ((const v16 *)esc_single_char_table)[*str];
+                cur += 2;
+                str += 1;
+                continue;
+            }
+            case CHAR_ESC_UTF8_1: {
+                ((v32 *)cur)[0] = v32_make('\\', 'u', '0', '0');
+                ((v16 *)cur)[2] = ((const v16 *)esc_hex_char_table)[*str];
+                cur += 6;
+                str += 1;
+                continue;
+            }
+            case CHAR_ESC_UTF8_2: {
+                u16 u = (u16)(((u16)(str[0] & 0x1F) << 6) |
+                              ((u16)(str[1] & 0x3F) << 0));
+                ((v16 *)cur)[0] = v16_make('\\', 'u');
+                ((v16 *)cur)[1] = ((const v16 *)esc_hex_char_table)[u >> 8];
+                ((v16 *)cur)[2] = ((const v16 *)esc_hex_char_table)[u & 0xFF];
+                cur += 6;
+                str += 2;
+                continue;
+            }
+            case CHAR_ESC_UTF8_3: {
+                u16 u = (u16)(((u16)(str[0] & 0x0F) << 12) |
+                              ((u16)(str[1] & 0x3F) << 6) |
+                              ((u16)(str[2] & 0x3F) << 0));
+                ((v16 *)cur)[0] = v16_make('\\', 'u');
+                ((v16 *)cur)[1] = ((const v16 *)esc_hex_char_table)[u >> 8];
+                ((v16 *)cur)[2] = ((const v16 *)esc_hex_char_table)[u & 0xFF];
+                cur += 6;
+                str += 3;
+                continue;
+            }
+            case CHAR_ESC_UTF8_4: {
+                u32 hi, lo;
+                u32 u = ((u32)(str[0] & 0x07) << 18) |
+                        ((u32)(str[1] & 0x3F) << 12) |
+                        ((u32)(str[2] & 0x3F) << 6) |
+                        ((u32)(str[3] & 0x3F) << 0);
+                u -= 0x10000;
+                hi = (u >> 10) + 0xD800;
+                lo = (u & 0x3FF) + 0xDC00;
+                ((v16 *)cur)[0] = v16_make('\\', 'u');
+                ((v16 *)cur)[1] = ((const v16 *)esc_hex_char_table)[hi >> 8];
+                ((v16 *)cur)[2] = ((const v16 *)esc_hex_char_table)[hi & 0xFF];
+                ((v16 *)cur)[3] = v16_make('\\', 'u');
+                ((v16 *)cur)[4] = ((const v16 *)esc_hex_char_table)[lo >> 8];
+                ((v16 *)cur)[5] = ((const v16 *)esc_hex_char_table)[lo & 0xFF];
+                cur += 12;
+                str += 4;
+                continue;
+            }
+            default:
+                break;
+        }
+    }
+    
+copy_end:
+    *cur++ = '"';
+    return cur;
+}
+
+/*==============================================================================
+ * Writer Utilities
+ *============================================================================*/
+
+/** Write null (requires 8 bytes buffer). */
+static_inline u8 *write_null(u8 *cur) {
+    *(v64 *)cur = v64_make('n', 'u', 'l', 'l', ',', '\n', 0, 0);
+    return cur + 4;
+}
+
+/** Write bool (requires 8 bytes buffer). */
+static_inline u8 *write_bool(u8 *cur, bool val) {
+    *(v64 *)cur = val ? v64_make('t', 'r', 'u', 'e', ',', '\n', 0, 0) :
+                        v64_make('f', 'a', 'l', 's', 'e', ',', '\n', 0);
+    return cur + 5 - val;
+}
+
+/** Write indent (requires level * 4 bytes buffer). */
+static_inline u8 *write_indent(u8 *cur, usize level) {
+    while (level-- > 0) {
+        *(v32 *)cur = v32_make(' ', ' ', ' ', ' ');
+        cur += 4;
+    }
+    return cur;
+}
+
+/** Write data to file. */
+static bool write_dat_to_file(const char *path, u8 *dat, usize len,
+                              yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    err->msg = _msg; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    if (file) fclose(file); \
+    return false; \
+} while (false)
+    
+    FILE *file = fopen_writeonly(path);
+    if (file == NULL) {
+        return_err(FILE_OPEN, "file opening failed");
+    }
+    if (fwrite(dat, len, 1, file) != 1) {
+        return_err(FILE_WRITE, "file writing failed");
+    }
+    if (fclose(file) != 0) {
+        file = NULL;
+        return_err(FILE_WRITE, "file closing failed");
+    }
+    return true;
+    
+#undef return_err
+}
+
+
+
+/*==============================================================================
+ * JSON Writer Implementation
+ *============================================================================*/
+
+typedef struct yyjson_write_ctx {
+    usize tag;
+} yyjson_write_ctx;
+
+static_inline void yyjson_write_ctx_set(yyjson_write_ctx *ctx,
+                                        usize size, bool is_obj) {
+    ctx->tag = (size << 1) | (usize)is_obj;
+}
+
+static_inline void yyjson_write_ctx_get(yyjson_write_ctx *ctx,
+                                        usize *size, bool *is_obj) {
+    usize tag = ctx->tag;
+    *size = tag >> 1;
+    *is_obj = (bool)(tag & 1);
+}
+
+/** Write single JSON value. */
+static_inline u8 *yyjson_write_single(yyjson_val *val,
+                                      yyjson_write_flag flg,
+                                      yyjson_alc alc,
+                                      usize *dat_len,
+                                      yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    if (hdr) alc.free(alc.ctx, (void *)hdr); \
+    *dat_len = 0; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    err->msg = _msg; \
+    return NULL; \
+} while (false)
+    
+#define incr_len(_len) do { \
+    hdr = (u8 *)alc.malloc(alc.ctx, _len); \
+    if (!hdr) goto fail_alloc; \
+    cur = hdr; \
+} while (false)
+    
+#define check_str_len(_len) do { \
+    if ((USIZE_MAX < U64_MAX) && (_len >= (USIZE_MAX - 16) / 6)) \
+        goto fail_alloc; \
+} while (false)
+    
+    u8 *hdr = NULL, *cur;
+    usize str_len;
+    const u8 *str_ptr;
+    const char_esc_type *esc_table = get_esc_table_with_flag(flg);
+    
+    switch (unsafe_yyjson_get_type(val)) {
+        case YYJSON_TYPE_STR:
+            str_len = unsafe_yyjson_get_len(val);
+            str_ptr = (const u8 *)unsafe_yyjson_get_str(val);
+            check_str_len(str_len);
+            incr_len(str_len * 6 + 4);
+            cur = write_string(cur, str_ptr, str_len, esc_table);
+            break;
+            
+        case YYJSON_TYPE_NUM:
+            incr_len(32);
+            cur = write_number(cur, val, flg);
+            if (unlikely(!cur)) goto fail_num;
+            break;
+            
+        case YYJSON_TYPE_BOOL:
+            incr_len(8);
+            cur = write_bool(cur, unsafe_yyjson_get_bool(val));
+            break;
+            
+        case YYJSON_TYPE_NULL:
+            incr_len(8);
+            cur = write_null(cur);
+            break;
+            
+        case YYJSON_TYPE_ARR:
+            incr_len(4);
+            *(v16 *)cur = v16_make('[', ']');
+            cur += 2;
+            break;
+            
+        case YYJSON_TYPE_OBJ:
+            incr_len(4);
+            *(v16 *)cur = v16_make('{', '}');
+            cur += 2;
+            break;
+            
+        default:
+            goto fail_type;
+    }
+    
+    *cur = '\0';
+    *dat_len = (usize)(cur - hdr);
+    memset(err, 0, sizeof(yyjson_write_err));
+    return hdr;
+    
+fail_alloc:
+    return_err(MEMORY_ALLOCATION, "memory allocation failed");
+fail_type:
+    return_err(INVALID_VALUE_TYPE, "invalid JSON value type");
+fail_num:
+    return_err(NAN_OR_INF, "nan or inf number is not allowed");
+    
+#undef return_err
+#undef check_str_len
+#undef incr_len
+}
+
+/** Write JSON document minify.
+    The root of this document should be a non-empty container. */
+static_inline u8 *yyjson_write_minify(const yyjson_doc *doc,
+                                      const yyjson_write_flag flg,
+                                      const yyjson_alc alc,
+                                      usize *dat_len,
+                                      yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    *dat_len = 0; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    err->msg = _msg; \
+    if (hdr) alc.free(alc.ctx, hdr); \
+    return NULL; \
+} while (false)
+    
+#define incr_len(_len) do { \
+    ext_len = (usize)(_len); \
+    if (unlikely((u8 *)(cur + ext_len) >= (u8 *)ctx)) { \
+        alc_inc = yyjson_max(alc_len / 2, ext_len); \
+        alc_inc = size_align_up(alc_inc, sizeof(yyjson_write_ctx)); \
+        if (size_add_is_overflow(alc_len, alc_inc)) goto fail_alloc; \
+        alc_len += alc_inc; \
+        tmp = (u8 *)alc.realloc(alc.ctx, hdr, alc_len); \
+        if (unlikely(!tmp)) goto fail_alloc; \
+        ctx_len = (usize)(end - (u8 *)ctx); \
+        ctx_tmp = (yyjson_write_ctx *)(void *)(tmp + (alc_len - ctx_len)); \
+        memmove((void *)ctx_tmp, (void *)(tmp + ((u8 *)ctx - hdr)), ctx_len); \
+        ctx = ctx_tmp; \
+        cur = tmp + (cur - hdr); \
+        end = tmp + alc_len; \
+        hdr = tmp; \
+    } \
+} while (false)
+    
+#define check_str_len(_len) do { \
+    if ((USIZE_MAX < U64_MAX) && (_len >= (USIZE_MAX - 16) / 6)) \
+        goto fail_alloc; \
+} while (false)
+    
+    yyjson_val *val;
+    yyjson_type val_type;
+    usize ctn_len, ctn_len_tmp;
+    bool ctn_obj, ctn_obj_tmp, is_key;
+    u8 *hdr, *cur, *end, *tmp;
+    yyjson_write_ctx *ctx, *ctx_tmp;
+    usize alc_len, alc_inc, ctx_len, ext_len, str_len;
+    const u8 *str_ptr;
+    const char_esc_type *esc_table = get_esc_table_with_flag(flg);
+    
+    alc_len = doc->val_read * YYJSON_WRITER_ESTIMATED_MINIFY_RATIO + 64;
+    alc_len = size_align_up(alc_len, sizeof(yyjson_write_ctx));
+    hdr = (u8 *)alc.malloc(alc.ctx, alc_len);
+    if (!hdr) goto fail_alloc;
+    cur = hdr;
+    end = hdr + alc_len;
+    ctx = (yyjson_write_ctx *)(void *)end;
+    
+doc_begin:
+    val = doc->root;
+    val_type = unsafe_yyjson_get_type(val);
+    ctn_obj = (val_type == YYJSON_TYPE_OBJ);
+    ctn_len = unsafe_yyjson_get_len(val) << (u8)ctn_obj;
+    *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+    val++;
+    
+val_begin:
+    val_type = unsafe_yyjson_get_type(val);
+    switch (val_type) {
+        case YYJSON_TYPE_STR:
+            is_key = ((u8)ctn_obj & (u8)~ctn_len);
+            str_len = unsafe_yyjson_get_len(val);
+            str_ptr = (const u8 *)unsafe_yyjson_get_str(val);
+            check_str_len(str_len);
+            incr_len(str_len * 6 + 16);
+            cur = write_string(cur, str_ptr, str_len, esc_table);
+            *cur++ = is_key ? ':' : ',';
+            break;
+            
+        case YYJSON_TYPE_NUM:
+            incr_len(32);
+            cur = write_number(cur, val, flg);
+            if (unlikely(!cur)) goto fail_num;
+            *cur++ = ',';
+            break;
+            
+        case YYJSON_TYPE_BOOL:
+            incr_len(16);
+            cur = write_bool(cur, unsafe_yyjson_get_bool(val));
+            cur++;
+            break;
+            
+        case YYJSON_TYPE_NULL:
+            incr_len(16);
+            cur = write_null(cur);
+            cur++;
+            break;
+            
+        case YYJSON_TYPE_ARR:
+        case YYJSON_TYPE_OBJ:
+            ctn_len_tmp = unsafe_yyjson_get_len(val);
+            ctn_obj_tmp = (val_type == YYJSON_TYPE_OBJ);
+            incr_len(16);
+            if (unlikely(ctn_len_tmp == 0)) {
+                /* write empty container */
+                *cur++ = (u8)('[' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = (u8)(']' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = ',';
+                break;
+            } else {
+                /* push context, setup new container */
+                yyjson_write_ctx_set(--ctx, ctn_len, ctn_obj);
+                ctn_len = ctn_len_tmp << (u8)ctn_obj_tmp;
+                ctn_obj = ctn_obj_tmp;
+                *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+                val++;
+                goto val_begin;
+            }
+            
+        default:
+            goto fail_type;
+    }
+    
+    val++;
+    ctn_len--;
+    if (unlikely(ctn_len == 0)) goto ctn_end;
+    goto val_begin;
+    
+ctn_end:
+    cur--;
+    *cur++ = (u8)(']' | ((u8)ctn_obj << 5));
+    *cur++ = ',';
+    if (unlikely((u8 *)ctx >= end)) goto doc_end;
+    yyjson_write_ctx_get(ctx++, &ctn_len, &ctn_obj);
+    ctn_len--;
+    if (likely(ctn_len > 0)) {
+        goto val_begin;
+    } else {
+        goto ctn_end;
+    }
+    
+doc_end:
+    *--cur = '\0';
+    *dat_len = (usize)(cur - hdr);
+    memset(err, 0, sizeof(yyjson_write_err));
+    return hdr;
+    
+fail_alloc:
+    return_err(MEMORY_ALLOCATION, "memory allocation failed");
+fail_type:
+    return_err(INVALID_VALUE_TYPE, "invalid JSON value type");
+fail_num:
+    return_err(NAN_OR_INF, "nan or inf number is not allowed");
+    
+#undef return_err
+#undef incr_len
+#undef check_str_len
+}
+
+/** Write JSON document pretty.
+    The root of this document should be a non-empty container. */
+static_inline u8 *yyjson_write_pretty(const yyjson_doc *doc,
+                                      const yyjson_write_flag flg,
+                                      const yyjson_alc alc,
+                                      usize *dat_len,
+                                      yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    *dat_len = 0; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    err->msg = _msg; \
+    if (hdr) alc.free(alc.ctx, hdr); \
+    return NULL; \
+} while (false)
+    
+#define incr_len(_len) do { \
+    ext_len = (usize)(_len); \
+    if (unlikely((u8 *)(cur + ext_len) >= (u8 *)ctx)) { \
+        alc_inc = yyjson_max(alc_len / 2, ext_len); \
+        alc_inc = size_align_up(alc_inc, sizeof(yyjson_write_ctx)); \
+        if (size_add_is_overflow(alc_len, alc_inc)) goto fail_alloc; \
+        alc_len += alc_inc; \
+        tmp = (u8 *)alc.realloc(alc.ctx, hdr, alc_len); \
+        if (unlikely(!tmp)) goto fail_alloc; \
+        ctx_len = (usize)(end - (u8 *)ctx); \
+        ctx_tmp = (yyjson_write_ctx *)(void *)(tmp + (alc_len - ctx_len)); \
+        memmove((void *)ctx_tmp, (void *)(tmp + ((u8 *)ctx - hdr)), ctx_len); \
+        ctx = ctx_tmp; \
+        cur = tmp + (cur - hdr); \
+        end = tmp + alc_len; \
+        hdr = tmp; \
+    } \
+} while (false)
+    
+#define check_str_len(_len) do { \
+    if ((USIZE_MAX < U64_MAX) && (_len >= (USIZE_MAX - 16) / 6)) \
+        goto fail_alloc; \
+} while (false)
+    
+    yyjson_val *val;
+    yyjson_type val_type;
+    usize ctn_len, ctn_len_tmp;
+    bool ctn_obj, ctn_obj_tmp, is_key, no_indent;
+    u8 *hdr, *cur, *end, *tmp;
+    yyjson_write_ctx *ctx, *ctx_tmp;
+    usize alc_len, alc_inc, ctx_len, ext_len, str_len, level;
+    const u8 *str_ptr;
+    const char_esc_type *esc_table = get_esc_table_with_flag(flg);
+    
+    alc_len = doc->val_read * YYJSON_WRITER_ESTIMATED_PRETTY_RATIO + 64;
+    alc_len = size_align_up(alc_len, sizeof(yyjson_write_ctx));
+    hdr = (u8 *)alc.malloc(alc.ctx, alc_len);
+    if (!hdr) goto fail_alloc;
+    cur = hdr;
+    end = hdr + alc_len;
+    ctx = (yyjson_write_ctx *)(void *)end;
+    
+doc_begin:
+    val = doc->root;
+    val_type = unsafe_yyjson_get_type(val);
+    ctn_obj = (val_type == YYJSON_TYPE_OBJ);
+    ctn_len = unsafe_yyjson_get_len(val) << (u8)ctn_obj;
+    *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+    *cur++ = '\n';
+    val++;
+    level = 1;
+    
+val_begin:
+    val_type = unsafe_yyjson_get_type(val);
+    switch (val_type) {
+        case YYJSON_TYPE_STR:
+            is_key = ((u8)ctn_obj & (u8)~ctn_len);
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            str_len = unsafe_yyjson_get_len(val);
+            str_ptr = (const u8 *)unsafe_yyjson_get_str(val);
+            check_str_len(str_len);
+            incr_len(str_len * 6 + 16 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_string(cur, str_ptr, str_len, esc_table);
+            *cur++ = is_key ? ':' : ',';
+            *cur++ = is_key ? ' ' : '\n';
+            break;
+            
+        case YYJSON_TYPE_NUM:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            incr_len(32 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_number(cur, val, flg);
+            if (unlikely(!cur)) goto fail_num;
+            *cur++ = ',';
+            *cur++ = '\n';
+            break;
+            
+        case YYJSON_TYPE_BOOL:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            incr_len(16 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_bool(cur, unsafe_yyjson_get_bool(val));
+            cur += 2;
+            break;
+            
+        case YYJSON_TYPE_NULL:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            incr_len(16 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_null(cur);
+            cur += 2;
+            break;
+            
+        case YYJSON_TYPE_ARR:
+        case YYJSON_TYPE_OBJ:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            ctn_len_tmp = unsafe_yyjson_get_len(val);
+            ctn_obj_tmp = (val_type == YYJSON_TYPE_OBJ);
+            if (unlikely(ctn_len_tmp == 0)) {
+                /* write empty container */
+                incr_len(16 + (no_indent ? 0 : level * 4));
+                cur = write_indent(cur, no_indent ? 0 : level);
+                *cur++ = (u8)('[' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = (u8)(']' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = ',';
+                *cur++ = '\n';
+                break;
+            } else {
+                /* push context, setup new container */
+                incr_len(32 + (no_indent ? 0 : level * 4));
+                yyjson_write_ctx_set(--ctx, ctn_len, ctn_obj);
+                ctn_len = ctn_len_tmp << (u8)ctn_obj_tmp;
+                ctn_obj = ctn_obj_tmp;
+                cur = write_indent(cur, no_indent ? 0 : level);
+                level++;
+                *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+                *cur++ = '\n';
+                val++;
+                goto val_begin;
+            }
+            
+        default:
+            goto fail_type;
+    }
+    
+    val++;
+    ctn_len--;
+    if (unlikely(ctn_len == 0)) goto ctn_end;
+    goto val_begin;
+    
+ctn_end:
+    cur -= 2;
+    *cur++ = '\n';
+    incr_len(level * 4);
+    cur = write_indent(cur, --level);
+    *cur++ = (u8)(']' | ((u8)ctn_obj << 5));
+    if (unlikely((u8 *)ctx >= end)) goto doc_end;
+    yyjson_write_ctx_get(ctx++, &ctn_len, &ctn_obj);
+    ctn_len--;
+    *cur++ = ',';
+    *cur++ = '\n';
+    if (likely(ctn_len > 0)) {
+        goto val_begin;
+    } else {
+        goto ctn_end;
+    }
+    
+doc_end:
+    *cur = '\0';
+    *dat_len = (usize)(cur - hdr);
+    memset(err, 0, sizeof(yyjson_write_err));
+    return hdr;
+    
+fail_alloc:
+    return_err(MEMORY_ALLOCATION, "memory allocation failed");
+fail_type:
+    return_err(INVALID_VALUE_TYPE, "invalid JSON value type");
+fail_num:
+    return_err(NAN_OR_INF, "nan or inf number is not allowed");
+    
+#undef return_err
+#undef incr_len
+#undef check_str_len
+}
+
+char *yyjson_write_opts(const yyjson_doc *doc,
+                        yyjson_write_flag flg,
+                        const yyjson_alc *alc_ptr,
+                        usize *dat_len,
+                        yyjson_write_err *err) {
+    
+    yyjson_write_err dummy_err;
+    usize dummy_dat_len;
+    yyjson_alc alc;
+    
+    err = err ? err : &dummy_err;
+    dat_len = dat_len ? dat_len : &dummy_dat_len;
+    alc = alc_ptr ? *alc_ptr : YYJSON_DEFAULT_ALC;
+    
+    if (unlikely(!doc)) {
+        *dat_len = 0;
+        err->msg = "input JSON document is NULL";
+        err->code = YYJSON_READ_ERROR_INVALID_PARAMETER;
+        return NULL;
+    }
+    
+    if (doc->val_read == 1) {
+        return (char *)yyjson_write_single(doc->root, flg, alc, dat_len, err);
+    }
+    if (flg & YYJSON_WRITE_PRETTY) {
+        return (char *)yyjson_write_pretty(doc, flg, alc, dat_len, err);
+    } else {
+        return (char *)yyjson_write_minify(doc, flg, alc, dat_len, err);
+    }
+}
+
+bool yyjson_write_file(const char *path,
+                       const yyjson_doc *doc,
+                       yyjson_write_flag flg,
+                       const yyjson_alc *alc_ptr,
+                       yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    err->msg = _msg; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    return false; \
+} while (false)
+    
+    yyjson_write_err dummy_err;
+    u8 *dat;
+    usize dat_len = 0;
+    bool suc;
+    
+    /* validate input parameters */
+    if (!err) err = &dummy_err;
+    if (unlikely(!path)) {
+        return_err(INVALID_PARAMETER, "input path is NULL");
+    }
+    if (unlikely(*path == 0)) {
+        return_err(INVALID_PARAMETER, "input path is empty");
+    }
+    
+    dat = (u8 *)yyjson_write_opts(doc, flg, alc_ptr, &dat_len, err);
+    if (unlikely(!dat)) return false;
+    suc = write_dat_to_file(path, dat, dat_len, err);
+    if (alc_ptr) alc_ptr->free(alc_ptr->ctx, dat);
+    else free(dat);
+    return suc;
+    
+#undef return_err
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Writer Implementation
+ *============================================================================*/
+
+typedef struct yyjson_mut_write_ctx {
+    usize tag;
+    yyjson_mut_val *ctn;
+} yyjson_mut_write_ctx;
+
+static_inline void yyjson_mut_write_ctx_set(yyjson_mut_write_ctx *ctx,
+                                            yyjson_mut_val *ctn,
+                                            usize size, bool is_obj) {
+    ctx->tag = (size << 1) | (usize)is_obj;
+    ctx->ctn = ctn;
+}
+
+static_inline void yyjson_mut_write_ctx_get(yyjson_mut_write_ctx *ctx,
+                                            yyjson_mut_val **ctn,
+                                            usize *size, bool *is_obj) {
+    usize tag = ctx->tag;
+    *size = tag >> 1;
+    *is_obj = (bool)(tag & 1);
+    *ctn = ctx->ctn;
+}
+
+/** Write single JSON value. */
+static_inline u8 *yyjson_mut_write_single(yyjson_mut_val *val,
+                                          yyjson_write_flag flg,
+                                          yyjson_alc alc,
+                                          usize *dat_len,
+                                          yyjson_write_err *err) {
+    return yyjson_write_single((yyjson_val *)val, flg, alc, dat_len, err);
+}
+
+/** Write JSON document minify.
+    The root of this document should be a non-empty container. */
+static_inline u8 *yyjson_mut_write_minify(const yyjson_mut_doc *doc,
+                                          yyjson_write_flag flg,
+                                          yyjson_alc alc,
+                                          usize *dat_len,
+                                          yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    *dat_len = 0; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    err->msg = _msg; \
+    if (hdr) alc.free(alc.ctx, hdr); \
+    return NULL; \
+} while (false)
+    
+#define incr_len(_len) do { \
+    ext_len = (usize)(_len); \
+    if (unlikely((u8 *)(cur + ext_len) >= (u8 *)ctx)) { \
+        alc_inc = yyjson_max(alc_len / 2, ext_len); \
+        alc_inc = size_align_up(alc_inc, sizeof(yyjson_mut_write_ctx)); \
+        if (size_add_is_overflow(alc_len, alc_inc)) goto fail_alloc; \
+        alc_len += alc_inc; \
+        tmp = (u8 *)alc.realloc(alc.ctx, hdr, alc_len); \
+        if (unlikely(!tmp)) goto fail_alloc; \
+        ctx_len = (usize)(end - (u8 *)ctx); \
+        ctx_tmp = (yyjson_mut_write_ctx *)(void *)(tmp + (alc_len - ctx_len)); \
+        memmove((void *)ctx_tmp, (void *)(tmp + ((u8 *)ctx - hdr)), ctx_len); \
+        ctx = ctx_tmp; \
+        cur = tmp + (cur - hdr); \
+        end = tmp + alc_len; \
+        hdr = tmp; \
+    } \
+} while (false)
+    
+#define check_str_len(_len) do { \
+    if ((USIZE_MAX < U64_MAX) && (_len >= (USIZE_MAX - 16) / 6)) \
+        goto fail_alloc; \
+} while (false)
+    
+    yyjson_mut_val *val, *ctn;
+    yyjson_type val_type;
+    usize ctn_len, ctn_len_tmp;
+    bool ctn_obj, ctn_obj_tmp, is_key;
+    u8 *hdr, *cur, *end, *tmp;
+    yyjson_mut_write_ctx *ctx, *ctx_tmp;
+    usize alc_len, alc_inc, ctx_len, ext_len, str_len;
+    const u8 *str_ptr;
+    const char_esc_type *esc_table = get_esc_table_with_flag(flg);
+    
+    alc_len = 0 * YYJSON_WRITER_ESTIMATED_MINIFY_RATIO + 64;
+    alc_len = size_align_up(alc_len, sizeof(yyjson_mut_write_ctx));
+    hdr = (u8 *)alc.malloc(alc.ctx, alc_len);
+    if (!hdr) goto fail_alloc;
+    cur = hdr;
+    end = hdr + alc_len;
+    ctx = (yyjson_mut_write_ctx *)(void *)end;
+    
+doc_begin:
+    val = doc->root;
+    val_type = unsafe_yyjson_get_type(val);
+    ctn_obj = (val_type == YYJSON_TYPE_OBJ);
+    ctn_len = unsafe_yyjson_get_len(val) << (u8)ctn_obj;
+    *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+    ctn = val;
+    val = (yyjson_mut_val *)val->uni.ptr; /* tail */
+    val = ctn_obj ? val->next->next : val->next;
+    
+val_begin:
+    val_type = unsafe_yyjson_get_type(val);
+    switch (val_type) {
+        case YYJSON_TYPE_STR:
+            is_key = ((u8)ctn_obj & (u8)~ctn_len);
+            str_len = unsafe_yyjson_get_len(val);
+            str_ptr = (const u8 *)unsafe_yyjson_get_str(val);
+            check_str_len(str_len);
+            incr_len(str_len * 6 + 16);
+            cur = write_string(cur, str_ptr, str_len, esc_table);
+            *cur++ = is_key ? ':' : ',';
+            break;
+            
+        case YYJSON_TYPE_NUM:
+            incr_len(32);
+            cur = write_number(cur, (yyjson_val *)val, flg);
+            if (unlikely(!cur)) goto fail_num;
+            *cur++ = ',';
+            break;
+            
+        case YYJSON_TYPE_BOOL:
+            incr_len(16);
+            cur = write_bool(cur, unsafe_yyjson_get_bool(val));
+            cur++;
+            break;
+            
+        case YYJSON_TYPE_NULL:
+            incr_len(16);
+            cur = write_null(cur);
+            cur++;
+            break;
+            
+        case YYJSON_TYPE_ARR:
+        case YYJSON_TYPE_OBJ:
+            ctn_len_tmp = unsafe_yyjson_get_len(val);
+            ctn_obj_tmp = (val_type == YYJSON_TYPE_OBJ);
+            incr_len(16);
+            if (unlikely(ctn_len_tmp == 0)) {
+                /* write empty container */
+                *cur++ = (u8)('[' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = (u8)(']' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = ',';
+                break;
+            } else {
+                /* push context, setup new container */
+                yyjson_mut_write_ctx_set(--ctx, ctn, ctn_len, ctn_obj);
+                ctn_len = ctn_len_tmp << (u8)ctn_obj_tmp;
+                ctn_obj = ctn_obj_tmp;
+                *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+                ctn = val;
+                val = (yyjson_mut_val *)ctn->uni.ptr; /* tail */
+                val = ctn_obj ? val->next->next : val->next;
+                goto val_begin;
+            }
+            
+        default:
+            goto fail_type;
+    }
+    
+    ctn_len--;
+    if (unlikely(ctn_len == 0)) goto ctn_end;
+    val = val->next;
+    goto val_begin;
+    
+ctn_end:
+    cur--;
+    *cur++ = (u8)(']' | ((u8)ctn_obj << 5));
+    *cur++ = ',';
+    if (unlikely((u8 *)ctx >= end)) goto doc_end;
+    val = ctn->next;
+    yyjson_mut_write_ctx_get(ctx++, &ctn, &ctn_len, &ctn_obj);
+    ctn_len--;
+    if (likely(ctn_len > 0)) {
+        goto val_begin;
+    } else {
+        goto ctn_end;
+    }
+    
+doc_end:
+    *--cur = '\0';
+    *dat_len = (usize)(cur - hdr);
+    err->code = YYJSON_WRITE_SUCCESS;
+    err->msg = "success";
+    return hdr;
+    
+fail_alloc:
+    return_err(MEMORY_ALLOCATION, "memory allocation failed");
+fail_type:
+    return_err(INVALID_VALUE_TYPE, "invalid JSON value type");
+fail_num:
+    return_err(NAN_OR_INF, "nan or inf number is not allowed");
+    
+#undef return_err
+#undef incr_len
+#undef check_str_len
+}
+
+/** Write JSON document pretty.
+    The root of this document should be a non-empty container. */
+static_inline u8 *yyjson_mut_write_pretty(const yyjson_mut_doc *doc,
+                                          yyjson_write_flag flg,
+                                          yyjson_alc alc,
+                                          usize *dat_len,
+                                          yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    *dat_len = 0; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    err->msg = _msg; \
+    if (hdr) alc.free(alc.ctx, hdr); \
+    return NULL; \
+} while (false)
+    
+#define incr_len(_len) do { \
+    ext_len = (usize)(_len); \
+    if (unlikely((u8 *)(cur + ext_len) >= (u8 *)ctx)) { \
+        alc_inc = yyjson_max(alc_len / 2, ext_len); \
+        alc_inc = size_align_up(alc_inc, sizeof(yyjson_mut_write_ctx)); \
+        if (size_add_is_overflow(alc_len, alc_inc)) goto fail_alloc; \
+        alc_len += alc_inc; \
+        tmp = (u8 *)alc.realloc(alc.ctx, hdr, alc_len); \
+        if (unlikely(!tmp)) goto fail_alloc; \
+        ctx_len = (usize)(end - (u8 *)ctx); \
+        ctx_tmp = (yyjson_mut_write_ctx *)(void *)(tmp + (alc_len - ctx_len)); \
+        memmove((void *)ctx_tmp, (void *)(tmp + ((u8 *)ctx - hdr)), ctx_len); \
+        ctx = ctx_tmp; \
+        cur = tmp + (cur - hdr); \
+        end = tmp + alc_len; \
+        hdr = tmp; \
+    } \
+} while (false)
+    
+#define check_str_len(_len) do { \
+    if ((USIZE_MAX < U64_MAX) && (_len >= (USIZE_MAX - 16) / 6)) \
+        goto fail_alloc; \
+} while (false)
+    
+    yyjson_mut_val *val, *ctn;
+    yyjson_type val_type;
+    usize ctn_len, ctn_len_tmp;
+    bool ctn_obj, ctn_obj_tmp, is_key, no_indent;
+    u8 *hdr, *cur, *end, *tmp;
+    yyjson_mut_write_ctx *ctx, *ctx_tmp;
+    usize alc_len, alc_inc, ctx_len, ext_len, str_len, level;
+    const u8 *str_ptr;
+    const char_esc_type *esc_table = get_esc_table_with_flag(flg);
+    
+    alc_len = 0 * YYJSON_WRITER_ESTIMATED_PRETTY_RATIO + 64;
+    alc_len = size_align_up(alc_len, sizeof(yyjson_mut_write_ctx));
+    hdr = (u8 *)alc.malloc(alc.ctx, alc_len);
+    if (!hdr) goto fail_alloc;
+    cur = hdr;
+    end = hdr + alc_len;
+    ctx = (yyjson_mut_write_ctx *)(void *)end;
+    
+doc_begin:
+    val = doc->root;
+    val_type = unsafe_yyjson_get_type(val);
+    ctn_obj = (val_type == YYJSON_TYPE_OBJ);
+    ctn_len = unsafe_yyjson_get_len(val) << (u8)ctn_obj;
+    *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+    *cur++ = '\n';
+    ctn = val;
+    val = (yyjson_mut_val *)val->uni.ptr; /* tail */
+    val = ctn_obj ? val->next->next : val->next;
+    level = 1;
+    
+val_begin:
+    val_type = unsafe_yyjson_get_type(val);
+    switch (val_type) {
+        case YYJSON_TYPE_STR:
+            is_key = ((u8)ctn_obj & (u8)~ctn_len);
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            str_len = unsafe_yyjson_get_len(val);
+            str_ptr = (const u8 *)unsafe_yyjson_get_str(val);
+            check_str_len(str_len);
+            incr_len(str_len * 6 + 16 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_string(cur, str_ptr, str_len, esc_table);
+            *cur++ = is_key ? ':' : ',';
+            *cur++ = is_key ? ' ' : '\n';
+            break;
+            
+        case YYJSON_TYPE_NUM:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            incr_len(32 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_number(cur, (yyjson_val *)val, flg);
+            if (unlikely(!cur)) goto fail_num;
+            *cur++ = ',';
+            *cur++ = '\n';
+            break;
+            
+        case YYJSON_TYPE_BOOL:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            incr_len(16 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_bool(cur, unsafe_yyjson_get_bool(val));
+            cur += 2;
+            break;
+            
+        case YYJSON_TYPE_NULL:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            incr_len(16 + (no_indent ? 0 : level * 4));
+            cur = write_indent(cur, no_indent ? 0 : level);
+            cur = write_null(cur);
+            cur += 2;
+            break;
+            
+        case YYJSON_TYPE_ARR:
+        case YYJSON_TYPE_OBJ:
+            no_indent = ((u8)ctn_obj & (u8)ctn_len);
+            ctn_len_tmp = unsafe_yyjson_get_len(val);
+            ctn_obj_tmp = (val_type == YYJSON_TYPE_OBJ);
+            if (unlikely(ctn_len_tmp == 0)) {
+                /* write empty container */
+                incr_len(16 + (no_indent ? 0 : level * 4));
+                cur = write_indent(cur, no_indent ? 0 : level);
+                *cur++ = (u8)('[' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = (u8)(']' | ((u8)ctn_obj_tmp << 5));
+                *cur++ = ',';
+                *cur++ = '\n';
+                break;
+            } else {
+                /* push context, setup new container */
+                incr_len(32 + (no_indent ? 0 : level * 4));
+                yyjson_mut_write_ctx_set(--ctx, ctn, ctn_len, ctn_obj);
+                ctn_len = ctn_len_tmp << (u8)ctn_obj_tmp;
+                ctn_obj = ctn_obj_tmp;
+                cur = write_indent(cur, no_indent ? 0 : level);
+                level++;
+                *cur++ = (u8)('[' | ((u8)ctn_obj << 5));
+                *cur++ = '\n';
+                ctn = val;
+                val = (yyjson_mut_val *)ctn->uni.ptr; /* tail */
+                val = ctn_obj ? val->next->next : val->next;
+                goto val_begin;
+            }
+            
+        default:
+            goto fail_type;
+    }
+    
+    ctn_len--;
+    if (unlikely(ctn_len == 0)) goto ctn_end;
+    val = val->next;
+    goto val_begin;
+    
+ctn_end:
+    cur -= 2;
+    *cur++ = '\n';
+    incr_len(level * 4);
+    cur = write_indent(cur, --level);
+    *cur++ = (u8)(']' | ((u8)ctn_obj << 5));
+    if (unlikely((u8 *)ctx >= end)) goto doc_end;
+    val = ctn->next;
+    yyjson_mut_write_ctx_get(ctx++, &ctn, &ctn_len, &ctn_obj);
+    ctn_len--;
+    *cur++ = ',';
+    *cur++ = '\n';
+    if (likely(ctn_len > 0)) {
+        goto val_begin;
+    } else {
+        goto ctn_end;
+    }
+    
+doc_end:
+    *cur = '\0';
+    *dat_len = (usize)(cur - hdr);
+    err->code = YYJSON_WRITE_SUCCESS;
+    err->msg = "success";
+    return hdr;
+    
+fail_alloc:
+    return_err(MEMORY_ALLOCATION, "memory allocation failed");
+fail_type:
+    return_err(INVALID_VALUE_TYPE, "invalid JSON value type");
+fail_num:
+    return_err(NAN_OR_INF, "nan or inf number is not allowed");
+    
+#undef return_err
+#undef incr_len
+#undef check_str_len
+}
+
+char *yyjson_mut_write_opts(const yyjson_mut_doc *doc,
+                            yyjson_write_flag flg,
+                            const yyjson_alc *alc_ptr,
+                            usize *dat_len,
+                            yyjson_write_err *err) {
+#define return_err(_code, _msg) do { \
+    err->msg = _msg; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    return NULL; \
+} while (false)
+    
+    yyjson_write_err dummy_err;
+    usize dummy_dat_len;
+    yyjson_alc alc;
+    yyjson_mut_val *root;
+    
+    err = err ? err : &dummy_err;
+    dat_len = dat_len ? dat_len : &dummy_dat_len;
+    alc = alc_ptr ? *alc_ptr : YYJSON_DEFAULT_ALC;
+    
+    if (unlikely(!doc)) {
+        *dat_len = 0;
+        return_err(INVALID_PARAMETER, "input JSON document is NULL");
+    }
+    root = doc->root;
+    if (!root) {
+        *dat_len = 0;
+        return_err(INVALID_PARAMETER, "input JSON document has no root value");
+    }
+    
+    if (!unsafe_yyjson_is_ctn(root) || unsafe_yyjson_get_len(root) == 0) {
+        return (char *)yyjson_mut_write_single(root, flg, alc, dat_len, err);
+    }
+    if (flg & YYJSON_WRITE_PRETTY) {
+        return (char *)yyjson_mut_write_pretty(doc, flg, alc, dat_len, err);
+    } else {
+        return (char *)yyjson_mut_write_minify(doc, flg, alc, dat_len, err);
+    }
+#undef return_err
+}
+
+bool yyjson_mut_write_file(const char *path,
+                           const yyjson_mut_doc *doc,
+                           yyjson_write_flag flg,
+                           const yyjson_alc *alc_ptr,
+                           yyjson_write_err *err) {
+    
+#define return_err(_code, _msg) do { \
+    err->msg = _msg; \
+    err->code = YYJSON_WRITE_ERROR_##_code; \
+    return false; \
+} while (false)
+    
+    yyjson_write_err dummy_err;
+    u8 *dat;
+    usize dat_len = 0;
+    bool suc;
+    
+    /* validate input parameters */
+    if (!err) err = &dummy_err;
+    if (unlikely(!path)) {
+        return_err(INVALID_PARAMETER, "input path is NULL");
+    }
+    if (unlikely(*path == 0)) {
+        return_err(INVALID_PARAMETER, "input path is empty");
+    }
+    
+    dat = (u8 *)yyjson_mut_write_opts(doc, flg, alc_ptr, &dat_len, err);
+    if (unlikely(!dat)) return false;
+    suc = write_dat_to_file(path, dat, dat_len, err);
+    if (alc_ptr) alc_ptr->free(alc_ptr->ctx, dat);
+    else free(dat);
+    return suc;
+    
+#undef return_err
+}
+
+#endif /* YYJSON_DISABLE_WRITER */
+
+
+
+/*==============================================================================
+ * Compiler Hint End
+ *============================================================================*/
+
+#if defined(__clang__)
+#   pragma clang diagnostic pop
+#elif defined(__GNUC__)
+#   if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6)
+#   pragma GCC diagnostic pop
+#   endif
+#elif defined(_MSC_VER)
+#   pragma warning(pop)
+#endif /* warning suppress end */
diff --git a/src/backend/optimizer/path/costsize.c b/src/backend/optimizer/path/costsize.c
index a4522a0..ec4f074 100644
--- a/src/backend/optimizer/path/costsize.c
+++ b/src/backend/optimizer/path/costsize.c
@@ -95,6 +95,7 @@
 #include "utils/selfuncs.h"
 #include "utils/spccache.h"
 #include "utils/tuplesort.h"
+#include "lero/lero_extension.h"
 
 
 #define LOG2(x)  (log(x) / 0.693147180559945)
@@ -106,6 +107,13 @@
  */
 #define APPEND_CPU_COST_MULTIPLIER 0.5
 
+/** Lero Extension */
+#define CARD_EST_QUERY_NUM 30000
+double 		join_card_ests[CARD_EST_QUERY_NUM] = {0.0};
+int         join_est_no = 0;
+int			max_join_est_no = CARD_EST_QUERY_NUM - 1;
+char        *lero_joinest_fname = "";
+/** ==== Lero Extension ==== */
 
 double		seq_page_cost = DEFAULT_SEQ_PAGE_COST;
 double		random_page_cost = DEFAULT_RANDOM_PAGE_COST;
@@ -178,6 +186,23 @@ static double relation_byte_size(double tuples, int width);
 static double page_size(double tuples, int width);
 static double get_parallel_divisor(Path *path);
 
+/** Lero Extension */
+void
+read_from_fspn_join_estimate(const char* filename){
+    FILE* fp = fopen(filename, "r");
+
+    double card_est;
+    int cnt = -1;
+
+    while (fscanf(fp, "%lf", &card_est) == 1){
+		cnt += 1;
+        join_card_ests[cnt] = card_est;
+    }
+
+	max_join_est_no = cnt;
+    fclose(fp);
+}
+/** ==== Lero Extension ==== */
 
 /*
  * clamp_row_est
@@ -4503,6 +4528,10 @@ set_joinrel_size_estimates(PlannerInfo *root, RelOptInfo *rel,
 										   inner_rel->rows,
 										   sjinfo,
 										   restrictlist);
+	if (enable_lero) {
+        lero_pgsysml_set_joinrel_size_estimates(root, rel, outer_rel,
+                                               inner_rel, sjinfo, restrictlist);
+    }
 }
 
 /*
@@ -4695,6 +4724,23 @@ calc_joinrel_size_estimate(PlannerInfo *root,
 			break;
 	}
 
+	/** Lero extension */
+    if (lero_joinest_fname != NULL && strlen(lero_joinest_fname) > 0) {
+        if (join_est_no == 0) {
+            read_from_fspn_join_estimate(lero_joinest_fname);
+        }
+
+		if (join_est_no <= max_join_est_no) {
+			double join_est = join_card_ests[join_est_no];
+			Assert(join_est >= 0);
+			elog(WARNING, "set card %f", join_est);
+
+			join_est_no++;
+			return clamp_row_est(join_est);
+		}
+    }
+	/** ==== Lero Extension ==== */
+
 	return clamp_row_est(nrows);
 }
 
diff --git a/src/backend/optimizer/path/costsize.c.orig b/src/backend/optimizer/path/costsize.c.orig
new file mode 100644
index 0000000..a4522a0
--- /dev/null
+++ b/src/backend/optimizer/path/costsize.c.orig
@@ -0,0 +1,5620 @@
+/*-------------------------------------------------------------------------
+ *
+ * costsize.c
+ *	  Routines to compute (and set) relation sizes and path costs
+ *
+ * Path costs are measured in arbitrary units established by these basic
+ * parameters:
+ *
+ *	seq_page_cost		Cost of a sequential page fetch
+ *	random_page_cost	Cost of a non-sequential page fetch
+ *	cpu_tuple_cost		Cost of typical CPU time to process a tuple
+ *	cpu_index_tuple_cost  Cost of typical CPU time to process an index tuple
+ *	cpu_operator_cost	Cost of CPU time to execute an operator or function
+ *	parallel_tuple_cost Cost of CPU time to pass a tuple from worker to master backend
+ *	parallel_setup_cost Cost of setting up shared memory for parallelism
+ *
+ * We expect that the kernel will typically do some amount of read-ahead
+ * optimization; this in conjunction with seek costs means that seq_page_cost
+ * is normally considerably less than random_page_cost.  (However, if the
+ * database is fully cached in RAM, it is reasonable to set them equal.)
+ *
+ * We also use a rough estimate "effective_cache_size" of the number of
+ * disk pages in Postgres + OS-level disk cache.  (We can't simply use
+ * NBuffers for this purpose because that would ignore the effects of
+ * the kernel's disk cache.)
+ *
+ * Obviously, taking constants for these values is an oversimplification,
+ * but it's tough enough to get any useful estimates even at this level of
+ * detail.  Note that all of these parameters are user-settable, in case
+ * the default values are drastically off for a particular platform.
+ *
+ * seq_page_cost and random_page_cost can also be overridden for an individual
+ * tablespace, in case some data is on a fast disk and other data is on a slow
+ * disk.  Per-tablespace overrides never apply to temporary work files such as
+ * an external sort or a materialize node that overflows work_mem.
+ *
+ * We compute two separate costs for each path:
+ *		total_cost: total estimated cost to fetch all tuples
+ *		startup_cost: cost that is expended before first tuple is fetched
+ * In some scenarios, such as when there is a LIMIT or we are implementing
+ * an EXISTS(...) sub-select, it is not necessary to fetch all tuples of the
+ * path's result.  A caller can estimate the cost of fetching a partial
+ * result by interpolating between startup_cost and total_cost.  In detail:
+ *		actual_cost = startup_cost +
+ *			(total_cost - startup_cost) * tuples_to_fetch / path->rows;
+ * Note that a base relation's rows count (and, by extension, plan_rows for
+ * plan nodes below the LIMIT node) are set without regard to any LIMIT, so
+ * that this equation works properly.  (Note: while path->rows is never zero
+ * for ordinary relations, it is zero for paths for provably-empty relations,
+ * so beware of division-by-zero.)	The LIMIT is applied as a top-level
+ * plan node.
+ *
+ * For largely historical reasons, most of the routines in this module use
+ * the passed result Path only to store their results (rows, startup_cost and
+ * total_cost) into.  All the input data they need is passed as separate
+ * parameters, even though much of it could be extracted from the Path.
+ * An exception is made for the cost_XXXjoin() routines, which expect all
+ * the other fields of the passed XXXPath to be filled in, and similarly
+ * cost_index() assumes the passed IndexPath is valid except for its output
+ * values.
+ *
+ *
+ * Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group
+ * Portions Copyright (c) 1994, Regents of the University of California
+ *
+ * IDENTIFICATION
+ *	  src/backend/optimizer/path/costsize.c
+ *
+ *-------------------------------------------------------------------------
+ */
+
+#include "postgres.h"
+
+#include <math.h>
+
+#include "access/amapi.h"
+#include "access/htup_details.h"
+#include "access/tsmapi.h"
+#include "executor/executor.h"
+#include "executor/nodeHash.h"
+#include "miscadmin.h"
+#include "nodes/makefuncs.h"
+#include "nodes/nodeFuncs.h"
+#include "optimizer/clauses.h"
+#include "optimizer/cost.h"
+#include "optimizer/optimizer.h"
+#include "optimizer/pathnode.h"
+#include "optimizer/paths.h"
+#include "optimizer/placeholder.h"
+#include "optimizer/plancat.h"
+#include "optimizer/planmain.h"
+#include "optimizer/restrictinfo.h"
+#include "parser/parsetree.h"
+#include "utils/lsyscache.h"
+#include "utils/selfuncs.h"
+#include "utils/spccache.h"
+#include "utils/tuplesort.h"
+
+
+#define LOG2(x)  (log(x) / 0.693147180559945)
+
+/*
+ * Append and MergeAppend nodes are less expensive than some other operations
+ * which use cpu_tuple_cost; instead of adding a separate GUC, estimate the
+ * per-tuple cost as cpu_tuple_cost multiplied by this value.
+ */
+#define APPEND_CPU_COST_MULTIPLIER 0.5
+
+
+double		seq_page_cost = DEFAULT_SEQ_PAGE_COST;
+double		random_page_cost = DEFAULT_RANDOM_PAGE_COST;
+double		cpu_tuple_cost = DEFAULT_CPU_TUPLE_COST;
+double		cpu_index_tuple_cost = DEFAULT_CPU_INDEX_TUPLE_COST;
+double		cpu_operator_cost = DEFAULT_CPU_OPERATOR_COST;
+double		parallel_tuple_cost = DEFAULT_PARALLEL_TUPLE_COST;
+double		parallel_setup_cost = DEFAULT_PARALLEL_SETUP_COST;
+
+int			effective_cache_size = DEFAULT_EFFECTIVE_CACHE_SIZE;
+
+Cost		disable_cost = 1.0e10;
+
+int			max_parallel_workers_per_gather = 2;
+
+bool		enable_seqscan = true;
+bool		enable_indexscan = true;
+bool		enable_indexonlyscan = true;
+bool		enable_bitmapscan = true;
+bool		enable_tidscan = true;
+bool		enable_sort = true;
+bool		enable_hashagg = true;
+bool		enable_nestloop = true;
+bool		enable_material = true;
+bool		enable_mergejoin = true;
+bool		enable_hashjoin = true;
+bool		enable_gathermerge = true;
+bool		enable_partitionwise_join = false;
+bool		enable_partitionwise_aggregate = false;
+bool		enable_parallel_append = true;
+bool		enable_parallel_hash = true;
+bool		enable_partition_pruning = true;
+
+typedef struct
+{
+	PlannerInfo *root;
+	QualCost	total;
+} cost_qual_eval_context;
+
+static List *extract_nonindex_conditions(List *qual_clauses, List *indexclauses);
+static MergeScanSelCache *cached_scansel(PlannerInfo *root,
+										 RestrictInfo *rinfo,
+										 PathKey *pathkey);
+static void cost_rescan(PlannerInfo *root, Path *path,
+						Cost *rescan_startup_cost, Cost *rescan_total_cost);
+static bool cost_qual_eval_walker(Node *node, cost_qual_eval_context *context);
+static void get_restriction_qual_cost(PlannerInfo *root, RelOptInfo *baserel,
+									  ParamPathInfo *param_info,
+									  QualCost *qpqual_cost);
+static bool has_indexed_join_quals(NestPath *joinpath);
+static double approx_tuple_count(PlannerInfo *root, JoinPath *path,
+								 List *quals);
+static double calc_joinrel_size_estimate(PlannerInfo *root,
+										 RelOptInfo *joinrel,
+										 RelOptInfo *outer_rel,
+										 RelOptInfo *inner_rel,
+										 double outer_rows,
+										 double inner_rows,
+										 SpecialJoinInfo *sjinfo,
+										 List *restrictlist);
+static Selectivity get_foreign_key_join_selectivity(PlannerInfo *root,
+													Relids outer_relids,
+													Relids inner_relids,
+													SpecialJoinInfo *sjinfo,
+													List **restrictlist);
+static Cost append_nonpartial_cost(List *subpaths, int numpaths,
+								   int parallel_workers);
+static void set_rel_width(PlannerInfo *root, RelOptInfo *rel);
+static double relation_byte_size(double tuples, int width);
+static double page_size(double tuples, int width);
+static double get_parallel_divisor(Path *path);
+
+
+/*
+ * clamp_row_est
+ *		Force a row-count estimate to a sane value.
+ */
+double
+clamp_row_est(double nrows)
+{
+	/*
+	 * Force estimate to be at least one row, to make explain output look
+	 * better and to avoid possible divide-by-zero when interpolating costs.
+	 * Make it an integer, too.
+	 */
+	if (nrows <= 1.0)
+		nrows = 1.0;
+	else
+		nrows = rint(nrows);
+
+	return nrows;
+}
+
+
+/*
+ * cost_seqscan
+ *	  Determines and returns the cost of scanning a relation sequentially.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_seqscan(Path *path, PlannerInfo *root,
+			 RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		cpu_run_cost;
+	Cost		disk_run_cost;
+	double		spc_seq_page_cost;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to base relations */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_RELATION);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	if (!enable_seqscan)
+		startup_cost += disable_cost;
+
+	/* fetch estimated page cost for tablespace containing table */
+	get_tablespace_page_costs(baserel->reltablespace,
+							  NULL,
+							  &spc_seq_page_cost);
+
+	/*
+	 * disk costs
+	 */
+	disk_run_cost = spc_seq_page_cost * baserel->pages;
+
+	/* CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	cpu_run_cost = cpu_per_tuple * baserel->tuples;
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	cpu_run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	/* Adjust costing for parallelism, if used. */
+	if (path->parallel_workers > 0)
+	{
+		double		parallel_divisor = get_parallel_divisor(path);
+
+		/* The CPU cost is divided among all the workers. */
+		cpu_run_cost /= parallel_divisor;
+
+		/*
+		 * It may be possible to amortize some of the I/O cost, but probably
+		 * not very much, because most operating systems already do aggressive
+		 * prefetching.  For now, we assume that the disk run cost can't be
+		 * amortized at all.
+		 */
+
+		/*
+		 * In the case of a parallel plan, the row count needs to represent
+		 * the number of tuples processed per worker.
+		 */
+		path->rows = clamp_row_est(path->rows / parallel_divisor);
+	}
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + cpu_run_cost + disk_run_cost;
+}
+
+/*
+ * cost_samplescan
+ *	  Determines and returns the cost of scanning a relation using sampling.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_samplescan(Path *path, PlannerInfo *root,
+				RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	RangeTblEntry *rte;
+	TableSampleClause *tsc;
+	TsmRoutine *tsm;
+	double		spc_seq_page_cost,
+				spc_random_page_cost,
+				spc_page_cost;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to base relations with tablesample clauses */
+	Assert(baserel->relid > 0);
+	rte = planner_rt_fetch(baserel->relid, root);
+	Assert(rte->rtekind == RTE_RELATION);
+	tsc = rte->tablesample;
+	Assert(tsc != NULL);
+	tsm = GetTsmRoutine(tsc->tsmhandler);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/* fetch estimated page cost for tablespace containing table */
+	get_tablespace_page_costs(baserel->reltablespace,
+							  &spc_random_page_cost,
+							  &spc_seq_page_cost);
+
+	/* if NextSampleBlock is used, assume random access, else sequential */
+	spc_page_cost = (tsm->NextSampleBlock != NULL) ?
+		spc_random_page_cost : spc_seq_page_cost;
+
+	/*
+	 * disk costs (recall that baserel->pages has already been set to the
+	 * number of pages the sampling method will visit)
+	 */
+	run_cost += spc_page_cost * baserel->pages;
+
+	/*
+	 * CPU costs (recall that baserel->tuples has already been set to the
+	 * number of tuples the sampling method will select).  Note that we ignore
+	 * execution cost of the TABLESAMPLE parameter expressions; they will be
+	 * evaluated only once per scan, and in most usages they'll likely be
+	 * simple constants anyway.  We also don't charge anything for the
+	 * calculations the sampling method might do internally.
+	 */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_gather
+ *	  Determines and returns the cost of gather path.
+ *
+ * 'rel' is the relation to be operated upon
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ * 'rows' may be used to point to a row estimate; if non-NULL, it overrides
+ * both 'rel' and 'param_info'.  This is useful when the path doesn't exactly
+ * correspond to any particular RelOptInfo.
+ */
+void
+cost_gather(GatherPath *path, PlannerInfo *root,
+			RelOptInfo *rel, ParamPathInfo *param_info,
+			double *rows)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+
+	/* Mark the path with the correct row estimate */
+	if (rows)
+		path->path.rows = *rows;
+	else if (param_info)
+		path->path.rows = param_info->ppi_rows;
+	else
+		path->path.rows = rel->rows;
+
+	startup_cost = path->subpath->startup_cost;
+
+	run_cost = path->subpath->total_cost - path->subpath->startup_cost;
+
+	/* Parallel setup and communication cost. */
+	startup_cost += parallel_setup_cost;
+	run_cost += parallel_tuple_cost * path->path.rows;
+
+	path->path.startup_cost = startup_cost;
+	path->path.total_cost = (startup_cost + run_cost);
+}
+
+/*
+ * cost_gather_merge
+ *	  Determines and returns the cost of gather merge path.
+ *
+ * GatherMerge merges several pre-sorted input streams, using a heap that at
+ * any given instant holds the next tuple from each stream. If there are N
+ * streams, we need about N*log2(N) tuple comparisons to construct the heap at
+ * startup, and then for each output tuple, about log2(N) comparisons to
+ * replace the top heap entry with the next tuple from the same stream.
+ */
+void
+cost_gather_merge(GatherMergePath *path, PlannerInfo *root,
+				  RelOptInfo *rel, ParamPathInfo *param_info,
+				  Cost input_startup_cost, Cost input_total_cost,
+				  double *rows)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	Cost		comparison_cost;
+	double		N;
+	double		logN;
+
+	/* Mark the path with the correct row estimate */
+	if (rows)
+		path->path.rows = *rows;
+	else if (param_info)
+		path->path.rows = param_info->ppi_rows;
+	else
+		path->path.rows = rel->rows;
+
+	if (!enable_gathermerge)
+		startup_cost += disable_cost;
+
+	/*
+	 * Add one to the number of workers to account for the leader.  This might
+	 * be overgenerous since the leader will do less work than other workers
+	 * in typical cases, but we'll go with it for now.
+	 */
+	Assert(path->num_workers > 0);
+	N = (double) path->num_workers + 1;
+	logN = LOG2(N);
+
+	/* Assumed cost per tuple comparison */
+	comparison_cost = 2.0 * cpu_operator_cost;
+
+	/* Heap creation cost */
+	startup_cost += comparison_cost * N * logN;
+
+	/* Per-tuple heap maintenance cost */
+	run_cost += path->path.rows * comparison_cost * logN;
+
+	/* small cost for heap management, like cost_merge_append */
+	run_cost += cpu_operator_cost * path->path.rows;
+
+	/*
+	 * Parallel setup and communication cost.  Since Gather Merge, unlike
+	 * Gather, requires us to block until a tuple is available from every
+	 * worker, we bump the IPC cost up a little bit as compared with Gather.
+	 * For lack of a better idea, charge an extra 5%.
+	 */
+	startup_cost += parallel_setup_cost;
+	run_cost += parallel_tuple_cost * path->path.rows * 1.05;
+
+	path->path.startup_cost = startup_cost + input_startup_cost;
+	path->path.total_cost = (startup_cost + run_cost + input_total_cost);
+}
+
+/*
+ * cost_index
+ *	  Determines and returns the cost of scanning a relation using an index.
+ *
+ * 'path' describes the indexscan under consideration, and is complete
+ *		except for the fields to be set by this routine
+ * 'loop_count' is the number of repetitions of the indexscan to factor into
+ *		estimates of caching behavior
+ *
+ * In addition to rows, startup_cost and total_cost, cost_index() sets the
+ * path's indextotalcost and indexselectivity fields.  These values will be
+ * needed if the IndexPath is used in a BitmapIndexScan.
+ *
+ * NOTE: path->indexquals must contain only clauses usable as index
+ * restrictions.  Any additional quals evaluated as qpquals may reduce the
+ * number of returned tuples, but they won't reduce the number of tuples
+ * we have to fetch from the table, so they don't reduce the scan cost.
+ */
+void
+cost_index(IndexPath *path, PlannerInfo *root, double loop_count,
+		   bool partial_path)
+{
+	IndexOptInfo *index = path->indexinfo;
+	RelOptInfo *baserel = index->rel;
+	bool		indexonly = (path->path.pathtype == T_IndexOnlyScan);
+	amcostestimate_function amcostestimate;
+	List	   *qpquals;
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	Cost		cpu_run_cost = 0;
+	Cost		indexStartupCost;
+	Cost		indexTotalCost;
+	Selectivity indexSelectivity;
+	double		indexCorrelation,
+				csquared;
+	double		spc_seq_page_cost,
+				spc_random_page_cost;
+	Cost		min_IO_cost,
+				max_IO_cost;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+	double		tuples_fetched;
+	double		pages_fetched;
+	double		rand_heap_pages;
+	double		index_pages;
+
+	/* Should only be applied to base relations */
+	Assert(IsA(baserel, RelOptInfo) &&
+		   IsA(index, IndexOptInfo));
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_RELATION);
+
+	/*
+	 * Mark the path with the correct row estimate, and identify which quals
+	 * will need to be enforced as qpquals.  We need not check any quals that
+	 * are implied by the index's predicate, so we can use indrestrictinfo not
+	 * baserestrictinfo as the list of relevant restriction clauses for the
+	 * rel.
+	 */
+	if (path->path.param_info)
+	{
+		path->path.rows = path->path.param_info->ppi_rows;
+		/* qpquals come from the rel's restriction clauses and ppi_clauses */
+		qpquals = list_concat(extract_nonindex_conditions(path->indexinfo->indrestrictinfo,
+														  path->indexclauses),
+							  extract_nonindex_conditions(path->path.param_info->ppi_clauses,
+														  path->indexclauses));
+	}
+	else
+	{
+		path->path.rows = baserel->rows;
+		/* qpquals come from just the rel's restriction clauses */
+		qpquals = extract_nonindex_conditions(path->indexinfo->indrestrictinfo,
+											  path->indexclauses);
+	}
+
+	if (!enable_indexscan)
+		startup_cost += disable_cost;
+	/* we don't need to check enable_indexonlyscan; indxpath.c does that */
+
+	/*
+	 * Call index-access-method-specific code to estimate the processing cost
+	 * for scanning the index, as well as the selectivity of the index (ie,
+	 * the fraction of main-table tuples we will have to retrieve) and its
+	 * correlation to the main-table tuple order.  We need a cast here because
+	 * pathnodes.h uses a weak function type to avoid including amapi.h.
+	 */
+	amcostestimate = (amcostestimate_function) index->amcostestimate;
+	amcostestimate(root, path, loop_count,
+				   &indexStartupCost, &indexTotalCost,
+				   &indexSelectivity, &indexCorrelation,
+				   &index_pages);
+
+	/*
+	 * Save amcostestimate's results for possible use in bitmap scan planning.
+	 * We don't bother to save indexStartupCost or indexCorrelation, because a
+	 * bitmap scan doesn't care about either.
+	 */
+	path->indextotalcost = indexTotalCost;
+	path->indexselectivity = indexSelectivity;
+
+	/* all costs for touching index itself included here */
+	startup_cost += indexStartupCost;
+	run_cost += indexTotalCost - indexStartupCost;
+
+	/* estimate number of main-table tuples fetched */
+	tuples_fetched = clamp_row_est(indexSelectivity * baserel->tuples);
+
+	/* fetch estimated page costs for tablespace containing table */
+	get_tablespace_page_costs(baserel->reltablespace,
+							  &spc_random_page_cost,
+							  &spc_seq_page_cost);
+
+	/*----------
+	 * Estimate number of main-table pages fetched, and compute I/O cost.
+	 *
+	 * When the index ordering is uncorrelated with the table ordering,
+	 * we use an approximation proposed by Mackert and Lohman (see
+	 * index_pages_fetched() for details) to compute the number of pages
+	 * fetched, and then charge spc_random_page_cost per page fetched.
+	 *
+	 * When the index ordering is exactly correlated with the table ordering
+	 * (just after a CLUSTER, for example), the number of pages fetched should
+	 * be exactly selectivity * table_size.  What's more, all but the first
+	 * will be sequential fetches, not the random fetches that occur in the
+	 * uncorrelated case.  So if the number of pages is more than 1, we
+	 * ought to charge
+	 *		spc_random_page_cost + (pages_fetched - 1) * spc_seq_page_cost
+	 * For partially-correlated indexes, we ought to charge somewhere between
+	 * these two estimates.  We currently interpolate linearly between the
+	 * estimates based on the correlation squared (XXX is that appropriate?).
+	 *
+	 * If it's an index-only scan, then we will not need to fetch any heap
+	 * pages for which the visibility map shows all tuples are visible.
+	 * Hence, reduce the estimated number of heap fetches accordingly.
+	 * We use the measured fraction of the entire heap that is all-visible,
+	 * which might not be particularly relevant to the subset of the heap
+	 * that this query will fetch; but it's not clear how to do better.
+	 *----------
+	 */
+	if (loop_count > 1)
+	{
+		/*
+		 * For repeated indexscans, the appropriate estimate for the
+		 * uncorrelated case is to scale up the number of tuples fetched in
+		 * the Mackert and Lohman formula by the number of scans, so that we
+		 * estimate the number of pages fetched by all the scans; then
+		 * pro-rate the costs for one scan.  In this case we assume all the
+		 * fetches are random accesses.
+		 */
+		pages_fetched = index_pages_fetched(tuples_fetched * loop_count,
+											baserel->pages,
+											(double) index->pages,
+											root);
+
+		if (indexonly)
+			pages_fetched = ceil(pages_fetched * (1.0 - baserel->allvisfrac));
+
+		rand_heap_pages = pages_fetched;
+
+		max_IO_cost = (pages_fetched * spc_random_page_cost) / loop_count;
+
+		/*
+		 * In the perfectly correlated case, the number of pages touched by
+		 * each scan is selectivity * table_size, and we can use the Mackert
+		 * and Lohman formula at the page level to estimate how much work is
+		 * saved by caching across scans.  We still assume all the fetches are
+		 * random, though, which is an overestimate that's hard to correct for
+		 * without double-counting the cache effects.  (But in most cases
+		 * where such a plan is actually interesting, only one page would get
+		 * fetched per scan anyway, so it shouldn't matter much.)
+		 */
+		pages_fetched = ceil(indexSelectivity * (double) baserel->pages);
+
+		pages_fetched = index_pages_fetched(pages_fetched * loop_count,
+											baserel->pages,
+											(double) index->pages,
+											root);
+
+		if (indexonly)
+			pages_fetched = ceil(pages_fetched * (1.0 - baserel->allvisfrac));
+
+		min_IO_cost = (pages_fetched * spc_random_page_cost) / loop_count;
+	}
+	else
+	{
+		/*
+		 * Normal case: apply the Mackert and Lohman formula, and then
+		 * interpolate between that and the correlation-derived result.
+		 */
+		pages_fetched = index_pages_fetched(tuples_fetched,
+											baserel->pages,
+											(double) index->pages,
+											root);
+
+		if (indexonly)
+			pages_fetched = ceil(pages_fetched * (1.0 - baserel->allvisfrac));
+
+		rand_heap_pages = pages_fetched;
+
+		/* max_IO_cost is for the perfectly uncorrelated case (csquared=0) */
+		max_IO_cost = pages_fetched * spc_random_page_cost;
+
+		/* min_IO_cost is for the perfectly correlated case (csquared=1) */
+		pages_fetched = ceil(indexSelectivity * (double) baserel->pages);
+
+		if (indexonly)
+			pages_fetched = ceil(pages_fetched * (1.0 - baserel->allvisfrac));
+
+		if (pages_fetched > 0)
+		{
+			min_IO_cost = spc_random_page_cost;
+			if (pages_fetched > 1)
+				min_IO_cost += (pages_fetched - 1) * spc_seq_page_cost;
+		}
+		else
+			min_IO_cost = 0;
+	}
+
+	if (partial_path)
+	{
+		/*
+		 * For index only scans compute workers based on number of index pages
+		 * fetched; the number of heap pages we fetch might be so small as to
+		 * effectively rule out parallelism, which we don't want to do.
+		 */
+		if (indexonly)
+			rand_heap_pages = -1;
+
+		/*
+		 * Estimate the number of parallel workers required to scan index. Use
+		 * the number of heap pages computed considering heap fetches won't be
+		 * sequential as for parallel scans the pages are accessed in random
+		 * order.
+		 */
+		path->path.parallel_workers = compute_parallel_worker(baserel,
+															  rand_heap_pages,
+															  index_pages,
+															  max_parallel_workers_per_gather);
+
+		/*
+		 * Fall out if workers can't be assigned for parallel scan, because in
+		 * such a case this path will be rejected.  So there is no benefit in
+		 * doing extra computation.
+		 */
+		if (path->path.parallel_workers <= 0)
+			return;
+
+		path->path.parallel_aware = true;
+	}
+
+	/*
+	 * Now interpolate based on estimated index order correlation to get total
+	 * disk I/O cost for main table accesses.
+	 */
+	csquared = indexCorrelation * indexCorrelation;
+
+	run_cost += max_IO_cost + csquared * (min_IO_cost - max_IO_cost);
+
+	/*
+	 * Estimate CPU costs per tuple.
+	 *
+	 * What we want here is cpu_tuple_cost plus the evaluation costs of any
+	 * qual clauses that we have to evaluate as qpquals.
+	 */
+	cost_qual_eval(&qpqual_cost, qpquals, root);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+
+	cpu_run_cost += cpu_per_tuple * tuples_fetched;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->path.pathtarget->cost.startup;
+	cpu_run_cost += path->path.pathtarget->cost.per_tuple * path->path.rows;
+
+	/* Adjust costing for parallelism, if used. */
+	if (path->path.parallel_workers > 0)
+	{
+		double		parallel_divisor = get_parallel_divisor(&path->path);
+
+		path->path.rows = clamp_row_est(path->path.rows / parallel_divisor);
+
+		/* The CPU cost is divided among all the workers. */
+		cpu_run_cost /= parallel_divisor;
+	}
+
+	run_cost += cpu_run_cost;
+
+	path->path.startup_cost = startup_cost;
+	path->path.total_cost = startup_cost + run_cost;
+}
+
+/*
+ * extract_nonindex_conditions
+ *
+ * Given a list of quals to be enforced in an indexscan, extract the ones that
+ * will have to be applied as qpquals (ie, the index machinery won't handle
+ * them).  Here we detect only whether a qual clause is directly redundant
+ * with some indexclause.  If the index path is chosen for use, createplan.c
+ * will try a bit harder to get rid of redundant qual conditions; specifically
+ * it will see if quals can be proven to be implied by the indexquals.  But
+ * it does not seem worth the cycles to try to factor that in at this stage,
+ * since we're only trying to estimate qual eval costs.  Otherwise this must
+ * match the logic in create_indexscan_plan().
+ *
+ * qual_clauses, and the result, are lists of RestrictInfos.
+ * indexclauses is a list of IndexClauses.
+ */
+static List *
+extract_nonindex_conditions(List *qual_clauses, List *indexclauses)
+{
+	List	   *result = NIL;
+	ListCell   *lc;
+
+	foreach(lc, qual_clauses)
+	{
+		RestrictInfo *rinfo = lfirst_node(RestrictInfo, lc);
+
+		if (rinfo->pseudoconstant)
+			continue;			/* we may drop pseudoconstants here */
+		if (is_redundant_with_indexclauses(rinfo, indexclauses))
+			continue;			/* dup or derived from same EquivalenceClass */
+		/* ... skip the predicate proof attempt createplan.c will try ... */
+		result = lappend(result, rinfo);
+	}
+	return result;
+}
+
+/*
+ * index_pages_fetched
+ *	  Estimate the number of pages actually fetched after accounting for
+ *	  cache effects.
+ *
+ * We use an approximation proposed by Mackert and Lohman, "Index Scans
+ * Using a Finite LRU Buffer: A Validated I/O Model", ACM Transactions
+ * on Database Systems, Vol. 14, No. 3, September 1989, Pages 401-424.
+ * The Mackert and Lohman approximation is that the number of pages
+ * fetched is
+ *	PF =
+ *		min(2TNs/(2T+Ns), T)			when T <= b
+ *		2TNs/(2T+Ns)					when T > b and Ns <= 2Tb/(2T-b)
+ *		b + (Ns - 2Tb/(2T-b))*(T-b)/T	when T > b and Ns > 2Tb/(2T-b)
+ * where
+ *		T = # pages in table
+ *		N = # tuples in table
+ *		s = selectivity = fraction of table to be scanned
+ *		b = # buffer pages available (we include kernel space here)
+ *
+ * We assume that effective_cache_size is the total number of buffer pages
+ * available for the whole query, and pro-rate that space across all the
+ * tables in the query and the index currently under consideration.  (This
+ * ignores space needed for other indexes used by the query, but since we
+ * don't know which indexes will get used, we can't estimate that very well;
+ * and in any case counting all the tables may well be an overestimate, since
+ * depending on the join plan not all the tables may be scanned concurrently.)
+ *
+ * The product Ns is the number of tuples fetched; we pass in that
+ * product rather than calculating it here.  "pages" is the number of pages
+ * in the object under consideration (either an index or a table).
+ * "index_pages" is the amount to add to the total table space, which was
+ * computed for us by make_one_rel.
+ *
+ * Caller is expected to have ensured that tuples_fetched is greater than zero
+ * and rounded to integer (see clamp_row_est).  The result will likewise be
+ * greater than zero and integral.
+ */
+double
+index_pages_fetched(double tuples_fetched, BlockNumber pages,
+					double index_pages, PlannerInfo *root)
+{
+	double		pages_fetched;
+	double		total_pages;
+	double		T,
+				b;
+
+	/* T is # pages in table, but don't allow it to be zero */
+	T = (pages > 1) ? (double) pages : 1.0;
+
+	/* Compute number of pages assumed to be competing for cache space */
+	total_pages = root->total_table_pages + index_pages;
+	total_pages = Max(total_pages, 1.0);
+	Assert(T <= total_pages);
+
+	/* b is pro-rated share of effective_cache_size */
+	b = (double) effective_cache_size * T / total_pages;
+
+	/* force it positive and integral */
+	if (b <= 1.0)
+		b = 1.0;
+	else
+		b = ceil(b);
+
+	/* This part is the Mackert and Lohman formula */
+	if (T <= b)
+	{
+		pages_fetched =
+			(2.0 * T * tuples_fetched) / (2.0 * T + tuples_fetched);
+		if (pages_fetched >= T)
+			pages_fetched = T;
+		else
+			pages_fetched = ceil(pages_fetched);
+	}
+	else
+	{
+		double		lim;
+
+		lim = (2.0 * T * b) / (2.0 * T - b);
+		if (tuples_fetched <= lim)
+		{
+			pages_fetched =
+				(2.0 * T * tuples_fetched) / (2.0 * T + tuples_fetched);
+		}
+		else
+		{
+			pages_fetched =
+				b + (tuples_fetched - lim) * (T - b) / T;
+		}
+		pages_fetched = ceil(pages_fetched);
+	}
+	return pages_fetched;
+}
+
+/*
+ * get_indexpath_pages
+ *		Determine the total size of the indexes used in a bitmap index path.
+ *
+ * Note: if the same index is used more than once in a bitmap tree, we will
+ * count it multiple times, which perhaps is the wrong thing ... but it's
+ * not completely clear, and detecting duplicates is difficult, so ignore it
+ * for now.
+ */
+static double
+get_indexpath_pages(Path *bitmapqual)
+{
+	double		result = 0;
+	ListCell   *l;
+
+	if (IsA(bitmapqual, BitmapAndPath))
+	{
+		BitmapAndPath *apath = (BitmapAndPath *) bitmapqual;
+
+		foreach(l, apath->bitmapquals)
+		{
+			result += get_indexpath_pages((Path *) lfirst(l));
+		}
+	}
+	else if (IsA(bitmapqual, BitmapOrPath))
+	{
+		BitmapOrPath *opath = (BitmapOrPath *) bitmapqual;
+
+		foreach(l, opath->bitmapquals)
+		{
+			result += get_indexpath_pages((Path *) lfirst(l));
+		}
+	}
+	else if (IsA(bitmapqual, IndexPath))
+	{
+		IndexPath  *ipath = (IndexPath *) bitmapqual;
+
+		result = (double) ipath->indexinfo->pages;
+	}
+	else
+		elog(ERROR, "unrecognized node type: %d", nodeTag(bitmapqual));
+
+	return result;
+}
+
+/*
+ * cost_bitmap_heap_scan
+ *	  Determines and returns the cost of scanning a relation using a bitmap
+ *	  index-then-heap plan.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ * 'bitmapqual' is a tree of IndexPaths, BitmapAndPaths, and BitmapOrPaths
+ * 'loop_count' is the number of repetitions of the indexscan to factor into
+ *		estimates of caching behavior
+ *
+ * Note: the component IndexPaths in bitmapqual should have been costed
+ * using the same loop_count.
+ */
+void
+cost_bitmap_heap_scan(Path *path, PlannerInfo *root, RelOptInfo *baserel,
+					  ParamPathInfo *param_info,
+					  Path *bitmapqual, double loop_count)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	Cost		indexTotalCost;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+	Cost		cost_per_page;
+	Cost		cpu_run_cost;
+	double		tuples_fetched;
+	double		pages_fetched;
+	double		spc_seq_page_cost,
+				spc_random_page_cost;
+	double		T;
+
+	/* Should only be applied to base relations */
+	Assert(IsA(baserel, RelOptInfo));
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_RELATION);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	if (!enable_bitmapscan)
+		startup_cost += disable_cost;
+
+	pages_fetched = compute_bitmap_pages(root, baserel, bitmapqual,
+										 loop_count, &indexTotalCost,
+										 &tuples_fetched);
+
+	startup_cost += indexTotalCost;
+	T = (baserel->pages > 1) ? (double) baserel->pages : 1.0;
+
+	/* Fetch estimated page costs for tablespace containing table. */
+	get_tablespace_page_costs(baserel->reltablespace,
+							  &spc_random_page_cost,
+							  &spc_seq_page_cost);
+
+	/*
+	 * For small numbers of pages we should charge spc_random_page_cost
+	 * apiece, while if nearly all the table's pages are being read, it's more
+	 * appropriate to charge spc_seq_page_cost apiece.  The effect is
+	 * nonlinear, too. For lack of a better idea, interpolate like this to
+	 * determine the cost per page.
+	 */
+	if (pages_fetched >= 2.0)
+		cost_per_page = spc_random_page_cost -
+			(spc_random_page_cost - spc_seq_page_cost)
+			* sqrt(pages_fetched / T);
+	else
+		cost_per_page = spc_random_page_cost;
+
+	run_cost += pages_fetched * cost_per_page;
+
+	/*
+	 * Estimate CPU costs per tuple.
+	 *
+	 * Often the indexquals don't need to be rechecked at each tuple ... but
+	 * not always, especially not if there are enough tuples involved that the
+	 * bitmaps become lossy.  For the moment, just assume they will be
+	 * rechecked always.  This means we charge the full freight for all the
+	 * scan clauses.
+	 */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	cpu_run_cost = cpu_per_tuple * tuples_fetched;
+
+	/* Adjust costing for parallelism, if used. */
+	if (path->parallel_workers > 0)
+	{
+		double		parallel_divisor = get_parallel_divisor(path);
+
+		/* The CPU cost is divided among all the workers. */
+		cpu_run_cost /= parallel_divisor;
+
+		path->rows = clamp_row_est(path->rows / parallel_divisor);
+	}
+
+
+	run_cost += cpu_run_cost;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_bitmap_tree_node
+ *		Extract cost and selectivity from a bitmap tree node (index/and/or)
+ */
+void
+cost_bitmap_tree_node(Path *path, Cost *cost, Selectivity *selec)
+{
+	if (IsA(path, IndexPath))
+	{
+		*cost = ((IndexPath *) path)->indextotalcost;
+		*selec = ((IndexPath *) path)->indexselectivity;
+
+		/*
+		 * Charge a small amount per retrieved tuple to reflect the costs of
+		 * manipulating the bitmap.  This is mostly to make sure that a bitmap
+		 * scan doesn't look to be the same cost as an indexscan to retrieve a
+		 * single tuple.
+		 */
+		*cost += 0.1 * cpu_operator_cost * path->rows;
+	}
+	else if (IsA(path, BitmapAndPath))
+	{
+		*cost = path->total_cost;
+		*selec = ((BitmapAndPath *) path)->bitmapselectivity;
+	}
+	else if (IsA(path, BitmapOrPath))
+	{
+		*cost = path->total_cost;
+		*selec = ((BitmapOrPath *) path)->bitmapselectivity;
+	}
+	else
+	{
+		elog(ERROR, "unrecognized node type: %d", nodeTag(path));
+		*cost = *selec = 0;		/* keep compiler quiet */
+	}
+}
+
+/*
+ * cost_bitmap_and_node
+ *		Estimate the cost of a BitmapAnd node
+ *
+ * Note that this considers only the costs of index scanning and bitmap
+ * creation, not the eventual heap access.  In that sense the object isn't
+ * truly a Path, but it has enough path-like properties (costs in particular)
+ * to warrant treating it as one.  We don't bother to set the path rows field,
+ * however.
+ */
+void
+cost_bitmap_and_node(BitmapAndPath *path, PlannerInfo *root)
+{
+	Cost		totalCost;
+	Selectivity selec;
+	ListCell   *l;
+
+	/*
+	 * We estimate AND selectivity on the assumption that the inputs are
+	 * independent.  This is probably often wrong, but we don't have the info
+	 * to do better.
+	 *
+	 * The runtime cost of the BitmapAnd itself is estimated at 100x
+	 * cpu_operator_cost for each tbm_intersect needed.  Probably too small,
+	 * definitely too simplistic?
+	 */
+	totalCost = 0.0;
+	selec = 1.0;
+	foreach(l, path->bitmapquals)
+	{
+		Path	   *subpath = (Path *) lfirst(l);
+		Cost		subCost;
+		Selectivity subselec;
+
+		cost_bitmap_tree_node(subpath, &subCost, &subselec);
+
+		selec *= subselec;
+
+		totalCost += subCost;
+		if (l != list_head(path->bitmapquals))
+			totalCost += 100.0 * cpu_operator_cost;
+	}
+	path->bitmapselectivity = selec;
+	path->path.rows = 0;		/* per above, not used */
+	path->path.startup_cost = totalCost;
+	path->path.total_cost = totalCost;
+}
+
+/*
+ * cost_bitmap_or_node
+ *		Estimate the cost of a BitmapOr node
+ *
+ * See comments for cost_bitmap_and_node.
+ */
+void
+cost_bitmap_or_node(BitmapOrPath *path, PlannerInfo *root)
+{
+	Cost		totalCost;
+	Selectivity selec;
+	ListCell   *l;
+
+	/*
+	 * We estimate OR selectivity on the assumption that the inputs are
+	 * non-overlapping, since that's often the case in "x IN (list)" type
+	 * situations.  Of course, we clamp to 1.0 at the end.
+	 *
+	 * The runtime cost of the BitmapOr itself is estimated at 100x
+	 * cpu_operator_cost for each tbm_union needed.  Probably too small,
+	 * definitely too simplistic?  We are aware that the tbm_unions are
+	 * optimized out when the inputs are BitmapIndexScans.
+	 */
+	totalCost = 0.0;
+	selec = 0.0;
+	foreach(l, path->bitmapquals)
+	{
+		Path	   *subpath = (Path *) lfirst(l);
+		Cost		subCost;
+		Selectivity subselec;
+
+		cost_bitmap_tree_node(subpath, &subCost, &subselec);
+
+		selec += subselec;
+
+		totalCost += subCost;
+		if (l != list_head(path->bitmapquals) &&
+			!IsA(subpath, IndexPath))
+			totalCost += 100.0 * cpu_operator_cost;
+	}
+	path->bitmapselectivity = Min(selec, 1.0);
+	path->path.rows = 0;		/* per above, not used */
+	path->path.startup_cost = totalCost;
+	path->path.total_cost = totalCost;
+}
+
+/*
+ * cost_tidscan
+ *	  Determines and returns the cost of scanning a relation using TIDs.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'tidquals' is the list of TID-checkable quals
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_tidscan(Path *path, PlannerInfo *root,
+			 RelOptInfo *baserel, List *tidquals, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	bool		isCurrentOf = false;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+	QualCost	tid_qual_cost;
+	int			ntuples;
+	ListCell   *l;
+	double		spc_random_page_cost;
+
+	/* Should only be applied to base relations */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_RELATION);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/* Count how many tuples we expect to retrieve */
+	ntuples = 0;
+	foreach(l, tidquals)
+	{
+		RestrictInfo *rinfo = lfirst_node(RestrictInfo, l);
+		Expr	   *qual = rinfo->clause;
+
+		if (IsA(qual, ScalarArrayOpExpr))
+		{
+			/* Each element of the array yields 1 tuple */
+			ScalarArrayOpExpr *saop = (ScalarArrayOpExpr *) qual;
+			Node	   *arraynode = (Node *) lsecond(saop->args);
+
+			ntuples += estimate_array_length(arraynode);
+		}
+		else if (IsA(qual, CurrentOfExpr))
+		{
+			/* CURRENT OF yields 1 tuple */
+			isCurrentOf = true;
+			ntuples++;
+		}
+		else
+		{
+			/* It's just CTID = something, count 1 tuple */
+			ntuples++;
+		}
+	}
+
+	/*
+	 * We must force TID scan for WHERE CURRENT OF, because only nodeTidscan.c
+	 * understands how to do it correctly.  Therefore, honor enable_tidscan
+	 * only when CURRENT OF isn't present.  Also note that cost_qual_eval
+	 * counts a CurrentOfExpr as having startup cost disable_cost, which we
+	 * subtract off here; that's to prevent other plan types such as seqscan
+	 * from winning.
+	 */
+	if (isCurrentOf)
+	{
+		Assert(baserel->baserestrictcost.startup >= disable_cost);
+		startup_cost -= disable_cost;
+	}
+	else if (!enable_tidscan)
+		startup_cost += disable_cost;
+
+	/*
+	 * The TID qual expressions will be computed once, any other baserestrict
+	 * quals once per retrieved tuple.
+	 */
+	cost_qual_eval(&tid_qual_cost, tidquals, root);
+
+	/* fetch estimated page cost for tablespace containing table */
+	get_tablespace_page_costs(baserel->reltablespace,
+							  &spc_random_page_cost,
+							  NULL);
+
+	/* disk costs --- assume each tuple on a different page */
+	run_cost += spc_random_page_cost * ntuples;
+
+	/* Add scanning CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	/* XXX currently we assume TID quals are a subset of qpquals */
+	startup_cost += qpqual_cost.startup + tid_qual_cost.per_tuple;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple -
+		tid_qual_cost.per_tuple;
+	run_cost += cpu_per_tuple * ntuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_subqueryscan
+ *	  Determines and returns the cost of scanning a subquery RTE.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_subqueryscan(SubqueryScanPath *path, PlannerInfo *root,
+				  RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost;
+	Cost		run_cost;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to base relations that are subqueries */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_SUBQUERY);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->path.rows = param_info->ppi_rows;
+	else
+		path->path.rows = baserel->rows;
+
+	/*
+	 * Cost of path is cost of evaluating the subplan, plus cost of evaluating
+	 * any restriction clauses and tlist that will be attached to the
+	 * SubqueryScan node, plus cpu_tuple_cost to account for selection and
+	 * projection overhead.
+	 */
+	path->path.startup_cost = path->subpath->startup_cost;
+	path->path.total_cost = path->subpath->total_cost;
+
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost = qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost = cpu_per_tuple * baserel->tuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->path.pathtarget->cost.startup;
+	run_cost += path->path.pathtarget->cost.per_tuple * path->path.rows;
+
+	path->path.startup_cost += startup_cost;
+	path->path.total_cost += startup_cost + run_cost;
+}
+
+/*
+ * cost_functionscan
+ *	  Determines and returns the cost of scanning a function RTE.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_functionscan(Path *path, PlannerInfo *root,
+				  RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+	RangeTblEntry *rte;
+	QualCost	exprcost;
+
+	/* Should only be applied to base relations that are functions */
+	Assert(baserel->relid > 0);
+	rte = planner_rt_fetch(baserel->relid, root);
+	Assert(rte->rtekind == RTE_FUNCTION);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/*
+	 * Estimate costs of executing the function expression(s).
+	 *
+	 * Currently, nodeFunctionscan.c always executes the functions to
+	 * completion before returning any rows, and caches the results in a
+	 * tuplestore.  So the function eval cost is all startup cost, and per-row
+	 * costs are minimal.
+	 *
+	 * XXX in principle we ought to charge tuplestore spill costs if the
+	 * number of rows is large.  However, given how phony our rowcount
+	 * estimates for functions tend to be, there's not a lot of point in that
+	 * refinement right now.
+	 */
+	cost_qual_eval_node(&exprcost, (Node *) rte->functions, root);
+
+	startup_cost += exprcost.startup + exprcost.per_tuple;
+
+	/* Add scanning CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_tablefuncscan
+ *	  Determines and returns the cost of scanning a table function.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_tablefuncscan(Path *path, PlannerInfo *root,
+				   RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+	RangeTblEntry *rte;
+	QualCost	exprcost;
+
+	/* Should only be applied to base relations that are functions */
+	Assert(baserel->relid > 0);
+	rte = planner_rt_fetch(baserel->relid, root);
+	Assert(rte->rtekind == RTE_TABLEFUNC);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/*
+	 * Estimate costs of executing the table func expression(s).
+	 *
+	 * XXX in principle we ought to charge tuplestore spill costs if the
+	 * number of rows is large.  However, given how phony our rowcount
+	 * estimates for tablefuncs tend to be, there's not a lot of point in that
+	 * refinement right now.
+	 */
+	cost_qual_eval_node(&exprcost, (Node *) rte->tablefunc, root);
+
+	startup_cost += exprcost.startup + exprcost.per_tuple;
+
+	/* Add scanning CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_valuesscan
+ *	  Determines and returns the cost of scanning a VALUES RTE.
+ *
+ * 'baserel' is the relation to be scanned
+ * 'param_info' is the ParamPathInfo if this is a parameterized path, else NULL
+ */
+void
+cost_valuesscan(Path *path, PlannerInfo *root,
+				RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to base relations that are values lists */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_VALUES);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/*
+	 * For now, estimate list evaluation cost at one operator eval per list
+	 * (probably pretty bogus, but is it worth being smarter?)
+	 */
+	cpu_per_tuple = cpu_operator_cost;
+
+	/* Add scanning CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple += cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_ctescan
+ *	  Determines and returns the cost of scanning a CTE RTE.
+ *
+ * Note: this is used for both self-reference and regular CTEs; the
+ * possible cost differences are below the threshold of what we could
+ * estimate accurately anyway.  Note that the costs of evaluating the
+ * referenced CTE query are added into the final plan as initplan costs,
+ * and should NOT be counted here.
+ */
+void
+cost_ctescan(Path *path, PlannerInfo *root,
+			 RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to base relations that are CTEs */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_CTE);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/* Charge one CPU tuple cost per row for tuplestore manipulation */
+	cpu_per_tuple = cpu_tuple_cost;
+
+	/* Add scanning CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple += cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->pathtarget->cost.startup;
+	run_cost += path->pathtarget->cost.per_tuple * path->rows;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_namedtuplestorescan
+ *	  Determines and returns the cost of scanning a named tuplestore.
+ */
+void
+cost_namedtuplestorescan(Path *path, PlannerInfo *root,
+						 RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to base relations that are Tuplestores */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_NAMEDTUPLESTORE);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/* Charge one CPU tuple cost per row for tuplestore manipulation */
+	cpu_per_tuple = cpu_tuple_cost;
+
+	/* Add scanning CPU costs */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple += cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_resultscan
+ *	  Determines and returns the cost of scanning an RTE_RESULT relation.
+ */
+void
+cost_resultscan(Path *path, PlannerInfo *root,
+				RelOptInfo *baserel, ParamPathInfo *param_info)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	QualCost	qpqual_cost;
+	Cost		cpu_per_tuple;
+
+	/* Should only be applied to RTE_RESULT base relations */
+	Assert(baserel->relid > 0);
+	Assert(baserel->rtekind == RTE_RESULT);
+
+	/* Mark the path with the correct row estimate */
+	if (param_info)
+		path->rows = param_info->ppi_rows;
+	else
+		path->rows = baserel->rows;
+
+	/* We charge qual cost plus cpu_tuple_cost */
+	get_restriction_qual_cost(root, baserel, param_info, &qpqual_cost);
+
+	startup_cost += qpqual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qpqual_cost.per_tuple;
+	run_cost += cpu_per_tuple * baserel->tuples;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_recursive_union
+ *	  Determines and returns the cost of performing a recursive union,
+ *	  and also the estimated output size.
+ *
+ * We are given Paths for the nonrecursive and recursive terms.
+ */
+void
+cost_recursive_union(Path *runion, Path *nrterm, Path *rterm)
+{
+	Cost		startup_cost;
+	Cost		total_cost;
+	double		total_rows;
+
+	/* We probably have decent estimates for the non-recursive term */
+	startup_cost = nrterm->startup_cost;
+	total_cost = nrterm->total_cost;
+	total_rows = nrterm->rows;
+
+	/*
+	 * We arbitrarily assume that about 10 recursive iterations will be
+	 * needed, and that we've managed to get a good fix on the cost and output
+	 * size of each one of them.  These are mighty shaky assumptions but it's
+	 * hard to see how to do better.
+	 */
+	total_cost += 10 * rterm->total_cost;
+	total_rows += 10 * rterm->rows;
+
+	/*
+	 * Also charge cpu_tuple_cost per row to account for the costs of
+	 * manipulating the tuplestores.  (We don't worry about possible
+	 * spill-to-disk costs.)
+	 */
+	total_cost += cpu_tuple_cost * total_rows;
+
+	runion->startup_cost = startup_cost;
+	runion->total_cost = total_cost;
+	runion->rows = total_rows;
+	runion->pathtarget->width = Max(nrterm->pathtarget->width,
+									rterm->pathtarget->width);
+}
+
+/*
+ * cost_sort
+ *	  Determines and returns the cost of sorting a relation, including
+ *	  the cost of reading the input data.
+ *
+ * If the total volume of data to sort is less than sort_mem, we will do
+ * an in-memory sort, which requires no I/O and about t*log2(t) tuple
+ * comparisons for t tuples.
+ *
+ * If the total volume exceeds sort_mem, we switch to a tape-style merge
+ * algorithm.  There will still be about t*log2(t) tuple comparisons in
+ * total, but we will also need to write and read each tuple once per
+ * merge pass.  We expect about ceil(logM(r)) merge passes where r is the
+ * number of initial runs formed and M is the merge order used by tuplesort.c.
+ * Since the average initial run should be about sort_mem, we have
+ *		disk traffic = 2 * relsize * ceil(logM(p / sort_mem))
+ *		cpu = comparison_cost * t * log2(t)
+ *
+ * If the sort is bounded (i.e., only the first k result tuples are needed)
+ * and k tuples can fit into sort_mem, we use a heap method that keeps only
+ * k tuples in the heap; this will require about t*log2(k) tuple comparisons.
+ *
+ * The disk traffic is assumed to be 3/4ths sequential and 1/4th random
+ * accesses (XXX can't we refine that guess?)
+ *
+ * By default, we charge two operator evals per tuple comparison, which should
+ * be in the right ballpark in most cases.  The caller can tweak this by
+ * specifying nonzero comparison_cost; typically that's used for any extra
+ * work that has to be done to prepare the inputs to the comparison operators.
+ *
+ * 'pathkeys' is a list of sort keys
+ * 'input_cost' is the total cost for reading the input data
+ * 'tuples' is the number of tuples in the relation
+ * 'width' is the average tuple width in bytes
+ * 'comparison_cost' is the extra cost per comparison, if any
+ * 'sort_mem' is the number of kilobytes of work memory allowed for the sort
+ * 'limit_tuples' is the bound on the number of output tuples; -1 if no bound
+ *
+ * NOTE: some callers currently pass NIL for pathkeys because they
+ * can't conveniently supply the sort keys.  Since this routine doesn't
+ * currently do anything with pathkeys anyway, that doesn't matter...
+ * but if it ever does, it should react gracefully to lack of key data.
+ * (Actually, the thing we'd most likely be interested in is just the number
+ * of sort keys, which all callers *could* supply.)
+ */
+void
+cost_sort(Path *path, PlannerInfo *root,
+		  List *pathkeys, Cost input_cost, double tuples, int width,
+		  Cost comparison_cost, int sort_mem,
+		  double limit_tuples)
+{
+	Cost		startup_cost = input_cost;
+	Cost		run_cost = 0;
+	double		input_bytes = relation_byte_size(tuples, width);
+	double		output_bytes;
+	double		output_tuples;
+	long		sort_mem_bytes = sort_mem * 1024L;
+
+	if (!enable_sort)
+		startup_cost += disable_cost;
+
+	path->rows = tuples;
+
+	/*
+	 * We want to be sure the cost of a sort is never estimated as zero, even
+	 * if passed-in tuple count is zero.  Besides, mustn't do log(0)...
+	 */
+	if (tuples < 2.0)
+		tuples = 2.0;
+
+	/* Include the default cost-per-comparison */
+	comparison_cost += 2.0 * cpu_operator_cost;
+
+	/* Do we have a useful LIMIT? */
+	if (limit_tuples > 0 && limit_tuples < tuples)
+	{
+		output_tuples = limit_tuples;
+		output_bytes = relation_byte_size(output_tuples, width);
+	}
+	else
+	{
+		output_tuples = tuples;
+		output_bytes = input_bytes;
+	}
+
+	if (output_bytes > sort_mem_bytes)
+	{
+		/*
+		 * We'll have to use a disk-based sort of all the tuples
+		 */
+		double		npages = ceil(input_bytes / BLCKSZ);
+		double		nruns = input_bytes / sort_mem_bytes;
+		double		mergeorder = tuplesort_merge_order(sort_mem_bytes);
+		double		log_runs;
+		double		npageaccesses;
+
+		/*
+		 * CPU costs
+		 *
+		 * Assume about N log2 N comparisons
+		 */
+		startup_cost += comparison_cost * tuples * LOG2(tuples);
+
+		/* Disk costs */
+
+		/* Compute logM(r) as log(r) / log(M) */
+		if (nruns > mergeorder)
+			log_runs = ceil(log(nruns) / log(mergeorder));
+		else
+			log_runs = 1.0;
+		npageaccesses = 2.0 * npages * log_runs;
+		/* Assume 3/4ths of accesses are sequential, 1/4th are not */
+		startup_cost += npageaccesses *
+			(seq_page_cost * 0.75 + random_page_cost * 0.25);
+	}
+	else if (tuples > 2 * output_tuples || input_bytes > sort_mem_bytes)
+	{
+		/*
+		 * We'll use a bounded heap-sort keeping just K tuples in memory, for
+		 * a total number of tuple comparisons of N log2 K; but the constant
+		 * factor is a bit higher than for quicksort.  Tweak it so that the
+		 * cost curve is continuous at the crossover point.
+		 */
+		startup_cost += comparison_cost * tuples * LOG2(2.0 * output_tuples);
+	}
+	else
+	{
+		/* We'll use plain quicksort on all the input tuples */
+		startup_cost += comparison_cost * tuples * LOG2(tuples);
+	}
+
+	/*
+	 * Also charge a small amount (arbitrarily set equal to operator cost) per
+	 * extracted tuple.  We don't charge cpu_tuple_cost because a Sort node
+	 * doesn't do qual-checking or projection, so it has less overhead than
+	 * most plan nodes.  Note it's correct to use tuples not output_tuples
+	 * here --- the upper LIMIT will pro-rate the run cost so we'd be double
+	 * counting the LIMIT otherwise.
+	 */
+	run_cost += cpu_operator_cost * tuples;
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * append_nonpartial_cost
+ *	  Estimate the cost of the non-partial paths in a Parallel Append.
+ *	  The non-partial paths are assumed to be the first "numpaths" paths
+ *	  from the subpaths list, and to be in order of decreasing cost.
+ */
+static Cost
+append_nonpartial_cost(List *subpaths, int numpaths, int parallel_workers)
+{
+	Cost	   *costarr;
+	int			arrlen;
+	ListCell   *l;
+	ListCell   *cell;
+	int			i;
+	int			path_index;
+	int			min_index;
+	int			max_index;
+
+	if (numpaths == 0)
+		return 0;
+
+	/*
+	 * Array length is number of workers or number of relevants paths,
+	 * whichever is less.
+	 */
+	arrlen = Min(parallel_workers, numpaths);
+	costarr = (Cost *) palloc(sizeof(Cost) * arrlen);
+
+	/* The first few paths will each be claimed by a different worker. */
+	path_index = 0;
+	foreach(cell, subpaths)
+	{
+		Path	   *subpath = (Path *) lfirst(cell);
+
+		if (path_index == arrlen)
+			break;
+		costarr[path_index++] = subpath->total_cost;
+	}
+
+	/*
+	 * Since subpaths are sorted by decreasing cost, the last one will have
+	 * the minimum cost.
+	 */
+	min_index = arrlen - 1;
+
+	/*
+	 * For each of the remaining subpaths, add its cost to the array element
+	 * with minimum cost.
+	 */
+	for_each_cell(l, cell)
+	{
+		Path	   *subpath = (Path *) lfirst(l);
+		int			i;
+
+		/* Consider only the non-partial paths */
+		if (path_index++ == numpaths)
+			break;
+
+		costarr[min_index] += subpath->total_cost;
+
+		/* Update the new min cost array index */
+		for (min_index = i = 0; i < arrlen; i++)
+		{
+			if (costarr[i] < costarr[min_index])
+				min_index = i;
+		}
+	}
+
+	/* Return the highest cost from the array */
+	for (max_index = i = 0; i < arrlen; i++)
+	{
+		if (costarr[i] > costarr[max_index])
+			max_index = i;
+	}
+
+	return costarr[max_index];
+}
+
+/*
+ * cost_append
+ *	  Determines and returns the cost of an Append node.
+ */
+void
+cost_append(AppendPath *apath)
+{
+	ListCell   *l;
+
+	apath->path.startup_cost = 0;
+	apath->path.total_cost = 0;
+	apath->path.rows = 0;
+
+	if (apath->subpaths == NIL)
+		return;
+
+	if (!apath->path.parallel_aware)
+	{
+		List	   *pathkeys = apath->path.pathkeys;
+
+		if (pathkeys == NIL)
+		{
+			Path	   *subpath = (Path *) linitial(apath->subpaths);
+
+			/*
+			 * For an unordered, non-parallel-aware Append we take the startup
+			 * cost as the startup cost of the first subpath.
+			 */
+			apath->path.startup_cost = subpath->startup_cost;
+
+			/* Compute rows and costs as sums of subplan rows and costs. */
+			foreach(l, apath->subpaths)
+			{
+				Path	   *subpath = (Path *) lfirst(l);
+
+				apath->path.rows += subpath->rows;
+				apath->path.total_cost += subpath->total_cost;
+			}
+		}
+		else
+		{
+			/*
+			 * For an ordered, non-parallel-aware Append we take the startup
+			 * cost as the sum of the subpath startup costs.  This ensures
+			 * that we don't underestimate the startup cost when a query's
+			 * LIMIT is such that several of the children have to be run to
+			 * satisfy it.  This might be overkill --- another plausible hack
+			 * would be to take the Append's startup cost as the maximum of
+			 * the child startup costs.  But we don't want to risk believing
+			 * that an ORDER BY LIMIT query can be satisfied at small cost
+			 * when the first child has small startup cost but later ones
+			 * don't.  (If we had the ability to deal with nonlinear cost
+			 * interpolation for partial retrievals, we would not need to be
+			 * so conservative about this.)
+			 *
+			 * This case is also different from the above in that we have to
+			 * account for possibly injecting sorts into subpaths that aren't
+			 * natively ordered.
+			 */
+			foreach(l, apath->subpaths)
+			{
+				Path	   *subpath = (Path *) lfirst(l);
+				Path		sort_path;	/* dummy for result of cost_sort */
+
+				if (!pathkeys_contained_in(pathkeys, subpath->pathkeys))
+				{
+					/*
+					 * We'll need to insert a Sort node, so include costs for
+					 * that.  We can use the parent's LIMIT if any, since we
+					 * certainly won't pull more than that many tuples from
+					 * any child.
+					 */
+					cost_sort(&sort_path,
+							  NULL, /* doesn't currently need root */
+							  pathkeys,
+							  subpath->total_cost,
+							  subpath->rows,
+							  subpath->pathtarget->width,
+							  0.0,
+							  work_mem,
+							  apath->limit_tuples);
+					subpath = &sort_path;
+				}
+
+				apath->path.rows += subpath->rows;
+				apath->path.startup_cost += subpath->startup_cost;
+				apath->path.total_cost += subpath->total_cost;
+			}
+		}
+	}
+	else						/* parallel-aware */
+	{
+		int			i = 0;
+		double		parallel_divisor = get_parallel_divisor(&apath->path);
+
+		/* Parallel-aware Append never produces ordered output. */
+		Assert(apath->path.pathkeys == NIL);
+
+		/* Calculate startup cost. */
+		foreach(l, apath->subpaths)
+		{
+			Path	   *subpath = (Path *) lfirst(l);
+
+			/*
+			 * Append will start returning tuples when the child node having
+			 * lowest startup cost is done setting up. We consider only the
+			 * first few subplans that immediately get a worker assigned.
+			 */
+			if (i == 0)
+				apath->path.startup_cost = subpath->startup_cost;
+			else if (i < apath->path.parallel_workers)
+				apath->path.startup_cost = Min(apath->path.startup_cost,
+											   subpath->startup_cost);
+
+			/*
+			 * Apply parallel divisor to subpaths.  Scale the number of rows
+			 * for each partial subpath based on the ratio of the parallel
+			 * divisor originally used for the subpath to the one we adopted.
+			 * Also add the cost of partial paths to the total cost, but
+			 * ignore non-partial paths for now.
+			 */
+			if (i < apath->first_partial_path)
+				apath->path.rows += subpath->rows / parallel_divisor;
+			else
+			{
+				double		subpath_parallel_divisor;
+
+				subpath_parallel_divisor = get_parallel_divisor(subpath);
+				apath->path.rows += subpath->rows * (subpath_parallel_divisor /
+													 parallel_divisor);
+				apath->path.total_cost += subpath->total_cost;
+			}
+
+			apath->path.rows = clamp_row_est(apath->path.rows);
+
+			i++;
+		}
+
+		/* Add cost for non-partial subpaths. */
+		apath->path.total_cost +=
+			append_nonpartial_cost(apath->subpaths,
+								   apath->first_partial_path,
+								   apath->path.parallel_workers);
+	}
+
+	/*
+	 * Although Append does not do any selection or projection, it's not free;
+	 * add a small per-tuple overhead.
+	 */
+	apath->path.total_cost +=
+		cpu_tuple_cost * APPEND_CPU_COST_MULTIPLIER * apath->path.rows;
+}
+
+/*
+ * cost_merge_append
+ *	  Determines and returns the cost of a MergeAppend node.
+ *
+ * MergeAppend merges several pre-sorted input streams, using a heap that
+ * at any given instant holds the next tuple from each stream.  If there
+ * are N streams, we need about N*log2(N) tuple comparisons to construct
+ * the heap at startup, and then for each output tuple, about log2(N)
+ * comparisons to replace the top entry.
+ *
+ * (The effective value of N will drop once some of the input streams are
+ * exhausted, but it seems unlikely to be worth trying to account for that.)
+ *
+ * The heap is never spilled to disk, since we assume N is not very large.
+ * So this is much simpler than cost_sort.
+ *
+ * As in cost_sort, we charge two operator evals per tuple comparison.
+ *
+ * 'pathkeys' is a list of sort keys
+ * 'n_streams' is the number of input streams
+ * 'input_startup_cost' is the sum of the input streams' startup costs
+ * 'input_total_cost' is the sum of the input streams' total costs
+ * 'tuples' is the number of tuples in all the streams
+ */
+void
+cost_merge_append(Path *path, PlannerInfo *root,
+				  List *pathkeys, int n_streams,
+				  Cost input_startup_cost, Cost input_total_cost,
+				  double tuples)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	Cost		comparison_cost;
+	double		N;
+	double		logN;
+
+	/*
+	 * Avoid log(0)...
+	 */
+	N = (n_streams < 2) ? 2.0 : (double) n_streams;
+	logN = LOG2(N);
+
+	/* Assumed cost per tuple comparison */
+	comparison_cost = 2.0 * cpu_operator_cost;
+
+	/* Heap creation cost */
+	startup_cost += comparison_cost * N * logN;
+
+	/* Per-tuple heap maintenance cost */
+	run_cost += tuples * comparison_cost * logN;
+
+	/*
+	 * Although MergeAppend does not do any selection or projection, it's not
+	 * free; add a small per-tuple overhead.
+	 */
+	run_cost += cpu_tuple_cost * APPEND_CPU_COST_MULTIPLIER * tuples;
+
+	path->startup_cost = startup_cost + input_startup_cost;
+	path->total_cost = startup_cost + run_cost + input_total_cost;
+}
+
+/*
+ * cost_material
+ *	  Determines and returns the cost of materializing a relation, including
+ *	  the cost of reading the input data.
+ *
+ * If the total volume of data to materialize exceeds work_mem, we will need
+ * to write it to disk, so the cost is much higher in that case.
+ *
+ * Note that here we are estimating the costs for the first scan of the
+ * relation, so the materialization is all overhead --- any savings will
+ * occur only on rescan, which is estimated in cost_rescan.
+ */
+void
+cost_material(Path *path,
+			  Cost input_startup_cost, Cost input_total_cost,
+			  double tuples, int width)
+{
+	Cost		startup_cost = input_startup_cost;
+	Cost		run_cost = input_total_cost - input_startup_cost;
+	double		nbytes = relation_byte_size(tuples, width);
+	long		work_mem_bytes = work_mem * 1024L;
+
+	path->rows = tuples;
+
+	/*
+	 * Whether spilling or not, charge 2x cpu_operator_cost per tuple to
+	 * reflect bookkeeping overhead.  (This rate must be more than what
+	 * cost_rescan charges for materialize, ie, cpu_operator_cost per tuple;
+	 * if it is exactly the same then there will be a cost tie between
+	 * nestloop with A outer, materialized B inner and nestloop with B outer,
+	 * materialized A inner.  The extra cost ensures we'll prefer
+	 * materializing the smaller rel.)	Note that this is normally a good deal
+	 * less than cpu_tuple_cost; which is OK because a Material plan node
+	 * doesn't do qual-checking or projection, so it's got less overhead than
+	 * most plan nodes.
+	 */
+	run_cost += 2 * cpu_operator_cost * tuples;
+
+	/*
+	 * If we will spill to disk, charge at the rate of seq_page_cost per page.
+	 * This cost is assumed to be evenly spread through the plan run phase,
+	 * which isn't exactly accurate but our cost model doesn't allow for
+	 * nonuniform costs within the run phase.
+	 */
+	if (nbytes > work_mem_bytes)
+	{
+		double		npages = ceil(nbytes / BLCKSZ);
+
+		run_cost += seq_page_cost * npages;
+	}
+
+	path->startup_cost = startup_cost;
+	path->total_cost = startup_cost + run_cost;
+}
+
+/*
+ * cost_agg
+ *		Determines and returns the cost of performing an Agg plan node,
+ *		including the cost of its input.
+ *
+ * aggcosts can be NULL when there are no actual aggregate functions (i.e.,
+ * we are using a hashed Agg node just to do grouping).
+ *
+ * Note: when aggstrategy == AGG_SORTED, caller must ensure that input costs
+ * are for appropriately-sorted input.
+ */
+void
+cost_agg(Path *path, PlannerInfo *root,
+		 AggStrategy aggstrategy, const AggClauseCosts *aggcosts,
+		 int numGroupCols, double numGroups,
+		 List *quals,
+		 Cost input_startup_cost, Cost input_total_cost,
+		 double input_tuples)
+{
+	double		output_tuples;
+	Cost		startup_cost;
+	Cost		total_cost;
+	AggClauseCosts dummy_aggcosts;
+
+	/* Use all-zero per-aggregate costs if NULL is passed */
+	if (aggcosts == NULL)
+	{
+		Assert(aggstrategy == AGG_HASHED);
+		MemSet(&dummy_aggcosts, 0, sizeof(AggClauseCosts));
+		aggcosts = &dummy_aggcosts;
+	}
+
+	/*
+	 * The transCost.per_tuple component of aggcosts should be charged once
+	 * per input tuple, corresponding to the costs of evaluating the aggregate
+	 * transfns and their input expressions. The finalCost.per_tuple component
+	 * is charged once per output tuple, corresponding to the costs of
+	 * evaluating the finalfns.  Startup costs are of course charged but once.
+	 *
+	 * If we are grouping, we charge an additional cpu_operator_cost per
+	 * grouping column per input tuple for grouping comparisons.
+	 *
+	 * We will produce a single output tuple if not grouping, and a tuple per
+	 * group otherwise.  We charge cpu_tuple_cost for each output tuple.
+	 *
+	 * Note: in this cost model, AGG_SORTED and AGG_HASHED have exactly the
+	 * same total CPU cost, but AGG_SORTED has lower startup cost.  If the
+	 * input path is already sorted appropriately, AGG_SORTED should be
+	 * preferred (since it has no risk of memory overflow).  This will happen
+	 * as long as the computed total costs are indeed exactly equal --- but if
+	 * there's roundoff error we might do the wrong thing.  So be sure that
+	 * the computations below form the same intermediate values in the same
+	 * order.
+	 */
+	if (aggstrategy == AGG_PLAIN)
+	{
+		startup_cost = input_total_cost;
+		startup_cost += aggcosts->transCost.startup;
+		startup_cost += aggcosts->transCost.per_tuple * input_tuples;
+		startup_cost += aggcosts->finalCost.startup;
+		startup_cost += aggcosts->finalCost.per_tuple;
+		/* we aren't grouping */
+		total_cost = startup_cost + cpu_tuple_cost;
+		output_tuples = 1;
+	}
+	else if (aggstrategy == AGG_SORTED || aggstrategy == AGG_MIXED)
+	{
+		/* Here we are able to deliver output on-the-fly */
+		startup_cost = input_startup_cost;
+		total_cost = input_total_cost;
+		if (aggstrategy == AGG_MIXED && !enable_hashagg)
+		{
+			startup_cost += disable_cost;
+			total_cost += disable_cost;
+		}
+		/* calcs phrased this way to match HASHED case, see note above */
+		total_cost += aggcosts->transCost.startup;
+		total_cost += aggcosts->transCost.per_tuple * input_tuples;
+		total_cost += (cpu_operator_cost * numGroupCols) * input_tuples;
+		total_cost += aggcosts->finalCost.startup;
+		total_cost += aggcosts->finalCost.per_tuple * numGroups;
+		total_cost += cpu_tuple_cost * numGroups;
+		output_tuples = numGroups;
+	}
+	else
+	{
+		/* must be AGG_HASHED */
+		startup_cost = input_total_cost;
+		if (!enable_hashagg)
+			startup_cost += disable_cost;
+		startup_cost += aggcosts->transCost.startup;
+		startup_cost += aggcosts->transCost.per_tuple * input_tuples;
+		startup_cost += (cpu_operator_cost * numGroupCols) * input_tuples;
+		startup_cost += aggcosts->finalCost.startup;
+		total_cost = startup_cost;
+		total_cost += aggcosts->finalCost.per_tuple * numGroups;
+		total_cost += cpu_tuple_cost * numGroups;
+		output_tuples = numGroups;
+	}
+
+	/*
+	 * If there are quals (HAVING quals), account for their cost and
+	 * selectivity.
+	 */
+	if (quals)
+	{
+		QualCost	qual_cost;
+
+		cost_qual_eval(&qual_cost, quals, root);
+		startup_cost += qual_cost.startup;
+		total_cost += qual_cost.startup + output_tuples * qual_cost.per_tuple;
+
+		output_tuples = clamp_row_est(output_tuples *
+									  clauselist_selectivity(root,
+															 quals,
+															 0,
+															 JOIN_INNER,
+															 NULL));
+	}
+
+	path->rows = output_tuples;
+	path->startup_cost = startup_cost;
+	path->total_cost = total_cost;
+}
+
+/*
+ * cost_windowagg
+ *		Determines and returns the cost of performing a WindowAgg plan node,
+ *		including the cost of its input.
+ *
+ * Input is assumed already properly sorted.
+ */
+void
+cost_windowagg(Path *path, PlannerInfo *root,
+			   List *windowFuncs, int numPartCols, int numOrderCols,
+			   Cost input_startup_cost, Cost input_total_cost,
+			   double input_tuples)
+{
+	Cost		startup_cost;
+	Cost		total_cost;
+	ListCell   *lc;
+
+	startup_cost = input_startup_cost;
+	total_cost = input_total_cost;
+
+	/*
+	 * Window functions are assumed to cost their stated execution cost, plus
+	 * the cost of evaluating their input expressions, per tuple.  Since they
+	 * may in fact evaluate their inputs at multiple rows during each cycle,
+	 * this could be a drastic underestimate; but without a way to know how
+	 * many rows the window function will fetch, it's hard to do better.  In
+	 * any case, it's a good estimate for all the built-in window functions,
+	 * so we'll just do this for now.
+	 */
+	foreach(lc, windowFuncs)
+	{
+		WindowFunc *wfunc = lfirst_node(WindowFunc, lc);
+		Cost		wfunccost;
+		QualCost	argcosts;
+
+		argcosts.startup = argcosts.per_tuple = 0;
+		add_function_cost(root, wfunc->winfnoid, (Node *) wfunc,
+						  &argcosts);
+		startup_cost += argcosts.startup;
+		wfunccost = argcosts.per_tuple;
+
+		/* also add the input expressions' cost to per-input-row costs */
+		cost_qual_eval_node(&argcosts, (Node *) wfunc->args, root);
+		startup_cost += argcosts.startup;
+		wfunccost += argcosts.per_tuple;
+
+		/*
+		 * Add the filter's cost to per-input-row costs.  XXX We should reduce
+		 * input expression costs according to filter selectivity.
+		 */
+		cost_qual_eval_node(&argcosts, (Node *) wfunc->aggfilter, root);
+		startup_cost += argcosts.startup;
+		wfunccost += argcosts.per_tuple;
+
+		total_cost += wfunccost * input_tuples;
+	}
+
+	/*
+	 * We also charge cpu_operator_cost per grouping column per tuple for
+	 * grouping comparisons, plus cpu_tuple_cost per tuple for general
+	 * overhead.
+	 *
+	 * XXX this neglects costs of spooling the data to disk when it overflows
+	 * work_mem.  Sooner or later that should get accounted for.
+	 */
+	total_cost += cpu_operator_cost * (numPartCols + numOrderCols) * input_tuples;
+	total_cost += cpu_tuple_cost * input_tuples;
+
+	path->rows = input_tuples;
+	path->startup_cost = startup_cost;
+	path->total_cost = total_cost;
+}
+
+/*
+ * cost_group
+ *		Determines and returns the cost of performing a Group plan node,
+ *		including the cost of its input.
+ *
+ * Note: caller must ensure that input costs are for appropriately-sorted
+ * input.
+ */
+void
+cost_group(Path *path, PlannerInfo *root,
+		   int numGroupCols, double numGroups,
+		   List *quals,
+		   Cost input_startup_cost, Cost input_total_cost,
+		   double input_tuples)
+{
+	double		output_tuples;
+	Cost		startup_cost;
+	Cost		total_cost;
+
+	output_tuples = numGroups;
+	startup_cost = input_startup_cost;
+	total_cost = input_total_cost;
+
+	/*
+	 * Charge one cpu_operator_cost per comparison per input tuple. We assume
+	 * all columns get compared at most of the tuples.
+	 */
+	total_cost += cpu_operator_cost * input_tuples * numGroupCols;
+
+	/*
+	 * If there are quals (HAVING quals), account for their cost and
+	 * selectivity.
+	 */
+	if (quals)
+	{
+		QualCost	qual_cost;
+
+		cost_qual_eval(&qual_cost, quals, root);
+		startup_cost += qual_cost.startup;
+		total_cost += qual_cost.startup + output_tuples * qual_cost.per_tuple;
+
+		output_tuples = clamp_row_est(output_tuples *
+									  clauselist_selectivity(root,
+															 quals,
+															 0,
+															 JOIN_INNER,
+															 NULL));
+	}
+
+	path->rows = output_tuples;
+	path->startup_cost = startup_cost;
+	path->total_cost = total_cost;
+}
+
+/*
+ * initial_cost_nestloop
+ *	  Preliminary estimate of the cost of a nestloop join path.
+ *
+ * This must quickly produce lower-bound estimates of the path's startup and
+ * total costs.  If we are unable to eliminate the proposed path from
+ * consideration using the lower bounds, final_cost_nestloop will be called
+ * to obtain the final estimates.
+ *
+ * The exact division of labor between this function and final_cost_nestloop
+ * is private to them, and represents a tradeoff between speed of the initial
+ * estimate and getting a tight lower bound.  We choose to not examine the
+ * join quals here, since that's by far the most expensive part of the
+ * calculations.  The end result is that CPU-cost considerations must be
+ * left for the second phase; and for SEMI/ANTI joins, we must also postpone
+ * incorporation of the inner path's run cost.
+ *
+ * 'workspace' is to be filled with startup_cost, total_cost, and perhaps
+ *		other data to be used by final_cost_nestloop
+ * 'jointype' is the type of join to be performed
+ * 'outer_path' is the outer input to the join
+ * 'inner_path' is the inner input to the join
+ * 'extra' contains miscellaneous information about the join
+ */
+void
+initial_cost_nestloop(PlannerInfo *root, JoinCostWorkspace *workspace,
+					  JoinType jointype,
+					  Path *outer_path, Path *inner_path,
+					  JoinPathExtraData *extra)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	double		outer_path_rows = outer_path->rows;
+	Cost		inner_rescan_start_cost;
+	Cost		inner_rescan_total_cost;
+	Cost		inner_run_cost;
+	Cost		inner_rescan_run_cost;
+
+	/* estimate costs to rescan the inner relation */
+	cost_rescan(root, inner_path,
+				&inner_rescan_start_cost,
+				&inner_rescan_total_cost);
+
+	/* cost of source data */
+
+	/*
+	 * NOTE: clearly, we must pay both outer and inner paths' startup_cost
+	 * before we can start returning tuples, so the join's startup cost is
+	 * their sum.  We'll also pay the inner path's rescan startup cost
+	 * multiple times.
+	 */
+	startup_cost += outer_path->startup_cost + inner_path->startup_cost;
+	run_cost += outer_path->total_cost - outer_path->startup_cost;
+	if (outer_path_rows > 1)
+		run_cost += (outer_path_rows - 1) * inner_rescan_start_cost;
+
+	inner_run_cost = inner_path->total_cost - inner_path->startup_cost;
+	inner_rescan_run_cost = inner_rescan_total_cost - inner_rescan_start_cost;
+
+	if (jointype == JOIN_SEMI || jointype == JOIN_ANTI ||
+		extra->inner_unique)
+	{
+		/*
+		 * With a SEMI or ANTI join, or if the innerrel is known unique, the
+		 * executor will stop after the first match.
+		 *
+		 * Getting decent estimates requires inspection of the join quals,
+		 * which we choose to postpone to final_cost_nestloop.
+		 */
+
+		/* Save private data for final_cost_nestloop */
+		workspace->inner_run_cost = inner_run_cost;
+		workspace->inner_rescan_run_cost = inner_rescan_run_cost;
+	}
+	else
+	{
+		/* Normal case; we'll scan whole input rel for each outer row */
+		run_cost += inner_run_cost;
+		if (outer_path_rows > 1)
+			run_cost += (outer_path_rows - 1) * inner_rescan_run_cost;
+	}
+
+	/* CPU costs left for later */
+
+	/* Public result fields */
+	workspace->startup_cost = startup_cost;
+	workspace->total_cost = startup_cost + run_cost;
+	/* Save private data for final_cost_nestloop */
+	workspace->run_cost = run_cost;
+}
+
+/*
+ * final_cost_nestloop
+ *	  Final estimate of the cost and result size of a nestloop join path.
+ *
+ * 'path' is already filled in except for the rows and cost fields
+ * 'workspace' is the result from initial_cost_nestloop
+ * 'extra' contains miscellaneous information about the join
+ */
+void
+final_cost_nestloop(PlannerInfo *root, NestPath *path,
+					JoinCostWorkspace *workspace,
+					JoinPathExtraData *extra)
+{
+	Path	   *outer_path = path->outerjoinpath;
+	Path	   *inner_path = path->innerjoinpath;
+	double		outer_path_rows = outer_path->rows;
+	double		inner_path_rows = inner_path->rows;
+	Cost		startup_cost = workspace->startup_cost;
+	Cost		run_cost = workspace->run_cost;
+	Cost		cpu_per_tuple;
+	QualCost	restrict_qual_cost;
+	double		ntuples;
+
+	/* Protect some assumptions below that rowcounts aren't zero or NaN */
+	if (outer_path_rows <= 0 || isnan(outer_path_rows))
+		outer_path_rows = 1;
+	if (inner_path_rows <= 0 || isnan(inner_path_rows))
+		inner_path_rows = 1;
+
+	/* Mark the path with the correct row estimate */
+	if (path->path.param_info)
+		path->path.rows = path->path.param_info->ppi_rows;
+	else
+		path->path.rows = path->path.parent->rows;
+
+	/* For partial paths, scale row estimate. */
+	if (path->path.parallel_workers > 0)
+	{
+		double		parallel_divisor = get_parallel_divisor(&path->path);
+
+		path->path.rows =
+			clamp_row_est(path->path.rows / parallel_divisor);
+	}
+
+	/*
+	 * We could include disable_cost in the preliminary estimate, but that
+	 * would amount to optimizing for the case where the join method is
+	 * disabled, which doesn't seem like the way to bet.
+	 */
+	if (!enable_nestloop)
+		startup_cost += disable_cost;
+
+	/* cost of inner-relation source data (we already dealt with outer rel) */
+
+	if (path->jointype == JOIN_SEMI || path->jointype == JOIN_ANTI ||
+		extra->inner_unique)
+	{
+		/*
+		 * With a SEMI or ANTI join, or if the innerrel is known unique, the
+		 * executor will stop after the first match.
+		 */
+		Cost		inner_run_cost = workspace->inner_run_cost;
+		Cost		inner_rescan_run_cost = workspace->inner_rescan_run_cost;
+		double		outer_matched_rows;
+		double		outer_unmatched_rows;
+		Selectivity inner_scan_frac;
+
+		/*
+		 * For an outer-rel row that has at least one match, we can expect the
+		 * inner scan to stop after a fraction 1/(match_count+1) of the inner
+		 * rows, if the matches are evenly distributed.  Since they probably
+		 * aren't quite evenly distributed, we apply a fuzz factor of 2.0 to
+		 * that fraction.  (If we used a larger fuzz factor, we'd have to
+		 * clamp inner_scan_frac to at most 1.0; but since match_count is at
+		 * least 1, no such clamp is needed now.)
+		 */
+		outer_matched_rows = rint(outer_path_rows * extra->semifactors.outer_match_frac);
+		outer_unmatched_rows = outer_path_rows - outer_matched_rows;
+		inner_scan_frac = 2.0 / (extra->semifactors.match_count + 1.0);
+
+		/*
+		 * Compute number of tuples processed (not number emitted!).  First,
+		 * account for successfully-matched outer rows.
+		 */
+		ntuples = outer_matched_rows * inner_path_rows * inner_scan_frac;
+
+		/*
+		 * Now we need to estimate the actual costs of scanning the inner
+		 * relation, which may be quite a bit less than N times inner_run_cost
+		 * due to early scan stops.  We consider two cases.  If the inner path
+		 * is an indexscan using all the joinquals as indexquals, then an
+		 * unmatched outer row results in an indexscan returning no rows,
+		 * which is probably quite cheap.  Otherwise, the executor will have
+		 * to scan the whole inner rel for an unmatched row; not so cheap.
+		 */
+		if (has_indexed_join_quals(path))
+		{
+			/*
+			 * Successfully-matched outer rows will only require scanning
+			 * inner_scan_frac of the inner relation.  In this case, we don't
+			 * need to charge the full inner_run_cost even when that's more
+			 * than inner_rescan_run_cost, because we can assume that none of
+			 * the inner scans ever scan the whole inner relation.  So it's
+			 * okay to assume that all the inner scan executions can be
+			 * fractions of the full cost, even if materialization is reducing
+			 * the rescan cost.  At this writing, it's impossible to get here
+			 * for a materialized inner scan, so inner_run_cost and
+			 * inner_rescan_run_cost will be the same anyway; but just in
+			 * case, use inner_run_cost for the first matched tuple and
+			 * inner_rescan_run_cost for additional ones.
+			 */
+			run_cost += inner_run_cost * inner_scan_frac;
+			if (outer_matched_rows > 1)
+				run_cost += (outer_matched_rows - 1) * inner_rescan_run_cost * inner_scan_frac;
+
+			/*
+			 * Add the cost of inner-scan executions for unmatched outer rows.
+			 * We estimate this as the same cost as returning the first tuple
+			 * of a nonempty scan.  We consider that these are all rescans,
+			 * since we used inner_run_cost once already.
+			 */
+			run_cost += outer_unmatched_rows *
+				inner_rescan_run_cost / inner_path_rows;
+
+			/*
+			 * We won't be evaluating any quals at all for unmatched rows, so
+			 * don't add them to ntuples.
+			 */
+		}
+		else
+		{
+			/*
+			 * Here, a complicating factor is that rescans may be cheaper than
+			 * first scans.  If we never scan all the way to the end of the
+			 * inner rel, it might be (depending on the plan type) that we'd
+			 * never pay the whole inner first-scan run cost.  However it is
+			 * difficult to estimate whether that will happen (and it could
+			 * not happen if there are any unmatched outer rows!), so be
+			 * conservative and always charge the whole first-scan cost once.
+			 * We consider this charge to correspond to the first unmatched
+			 * outer row, unless there isn't one in our estimate, in which
+			 * case blame it on the first matched row.
+			 */
+
+			/* First, count all unmatched join tuples as being processed */
+			ntuples += outer_unmatched_rows * inner_path_rows;
+
+			/* Now add the forced full scan, and decrement appropriate count */
+			run_cost += inner_run_cost;
+			if (outer_unmatched_rows >= 1)
+				outer_unmatched_rows -= 1;
+			else
+				outer_matched_rows -= 1;
+
+			/* Add inner run cost for additional outer tuples having matches */
+			if (outer_matched_rows > 0)
+				run_cost += outer_matched_rows * inner_rescan_run_cost * inner_scan_frac;
+
+			/* Add inner run cost for additional unmatched outer tuples */
+			if (outer_unmatched_rows > 0)
+				run_cost += outer_unmatched_rows * inner_rescan_run_cost;
+		}
+	}
+	else
+	{
+		/* Normal-case source costs were included in preliminary estimate */
+
+		/* Compute number of tuples processed (not number emitted!) */
+		ntuples = outer_path_rows * inner_path_rows;
+	}
+
+	/* CPU costs */
+	cost_qual_eval(&restrict_qual_cost, path->joinrestrictinfo, root);
+	startup_cost += restrict_qual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + restrict_qual_cost.per_tuple;
+	run_cost += cpu_per_tuple * ntuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->path.pathtarget->cost.startup;
+	run_cost += path->path.pathtarget->cost.per_tuple * path->path.rows;
+
+	path->path.startup_cost = startup_cost;
+	path->path.total_cost = startup_cost + run_cost;
+}
+
+/*
+ * initial_cost_mergejoin
+ *	  Preliminary estimate of the cost of a mergejoin path.
+ *
+ * This must quickly produce lower-bound estimates of the path's startup and
+ * total costs.  If we are unable to eliminate the proposed path from
+ * consideration using the lower bounds, final_cost_mergejoin will be called
+ * to obtain the final estimates.
+ *
+ * The exact division of labor between this function and final_cost_mergejoin
+ * is private to them, and represents a tradeoff between speed of the initial
+ * estimate and getting a tight lower bound.  We choose to not examine the
+ * join quals here, except for obtaining the scan selectivity estimate which
+ * is really essential (but fortunately, use of caching keeps the cost of
+ * getting that down to something reasonable).
+ * We also assume that cost_sort is cheap enough to use here.
+ *
+ * 'workspace' is to be filled with startup_cost, total_cost, and perhaps
+ *		other data to be used by final_cost_mergejoin
+ * 'jointype' is the type of join to be performed
+ * 'mergeclauses' is the list of joinclauses to be used as merge clauses
+ * 'outer_path' is the outer input to the join
+ * 'inner_path' is the inner input to the join
+ * 'outersortkeys' is the list of sort keys for the outer path
+ * 'innersortkeys' is the list of sort keys for the inner path
+ * 'extra' contains miscellaneous information about the join
+ *
+ * Note: outersortkeys and innersortkeys should be NIL if no explicit
+ * sort is needed because the respective source path is already ordered.
+ */
+void
+initial_cost_mergejoin(PlannerInfo *root, JoinCostWorkspace *workspace,
+					   JoinType jointype,
+					   List *mergeclauses,
+					   Path *outer_path, Path *inner_path,
+					   List *outersortkeys, List *innersortkeys,
+					   JoinPathExtraData *extra)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	double		outer_path_rows = outer_path->rows;
+	double		inner_path_rows = inner_path->rows;
+	Cost		inner_run_cost;
+	double		outer_rows,
+				inner_rows,
+				outer_skip_rows,
+				inner_skip_rows;
+	Selectivity outerstartsel,
+				outerendsel,
+				innerstartsel,
+				innerendsel;
+	Path		sort_path;		/* dummy for result of cost_sort */
+
+	/* Protect some assumptions below that rowcounts aren't zero or NaN */
+	if (outer_path_rows <= 0 || isnan(outer_path_rows))
+		outer_path_rows = 1;
+	if (inner_path_rows <= 0 || isnan(inner_path_rows))
+		inner_path_rows = 1;
+
+	/*
+	 * A merge join will stop as soon as it exhausts either input stream
+	 * (unless it's an outer join, in which case the outer side has to be
+	 * scanned all the way anyway).  Estimate fraction of the left and right
+	 * inputs that will actually need to be scanned.  Likewise, we can
+	 * estimate the number of rows that will be skipped before the first join
+	 * pair is found, which should be factored into startup cost. We use only
+	 * the first (most significant) merge clause for this purpose. Since
+	 * mergejoinscansel() is a fairly expensive computation, we cache the
+	 * results in the merge clause RestrictInfo.
+	 */
+	if (mergeclauses && jointype != JOIN_FULL)
+	{
+		RestrictInfo *firstclause = (RestrictInfo *) linitial(mergeclauses);
+		List	   *opathkeys;
+		List	   *ipathkeys;
+		PathKey    *opathkey;
+		PathKey    *ipathkey;
+		MergeScanSelCache *cache;
+
+		/* Get the input pathkeys to determine the sort-order details */
+		opathkeys = outersortkeys ? outersortkeys : outer_path->pathkeys;
+		ipathkeys = innersortkeys ? innersortkeys : inner_path->pathkeys;
+		Assert(opathkeys);
+		Assert(ipathkeys);
+		opathkey = (PathKey *) linitial(opathkeys);
+		ipathkey = (PathKey *) linitial(ipathkeys);
+		/* debugging check */
+		if (opathkey->pk_opfamily != ipathkey->pk_opfamily ||
+			opathkey->pk_eclass->ec_collation != ipathkey->pk_eclass->ec_collation ||
+			opathkey->pk_strategy != ipathkey->pk_strategy ||
+			opathkey->pk_nulls_first != ipathkey->pk_nulls_first)
+			elog(ERROR, "left and right pathkeys do not match in mergejoin");
+
+		/* Get the selectivity with caching */
+		cache = cached_scansel(root, firstclause, opathkey);
+
+		if (bms_is_subset(firstclause->left_relids,
+						  outer_path->parent->relids))
+		{
+			/* left side of clause is outer */
+			outerstartsel = cache->leftstartsel;
+			outerendsel = cache->leftendsel;
+			innerstartsel = cache->rightstartsel;
+			innerendsel = cache->rightendsel;
+		}
+		else
+		{
+			/* left side of clause is inner */
+			outerstartsel = cache->rightstartsel;
+			outerendsel = cache->rightendsel;
+			innerstartsel = cache->leftstartsel;
+			innerendsel = cache->leftendsel;
+		}
+		if (jointype == JOIN_LEFT ||
+			jointype == JOIN_ANTI)
+		{
+			outerstartsel = 0.0;
+			outerendsel = 1.0;
+		}
+		else if (jointype == JOIN_RIGHT)
+		{
+			innerstartsel = 0.0;
+			innerendsel = 1.0;
+		}
+	}
+	else
+	{
+		/* cope with clauseless or full mergejoin */
+		outerstartsel = innerstartsel = 0.0;
+		outerendsel = innerendsel = 1.0;
+	}
+
+	/*
+	 * Convert selectivities to row counts.  We force outer_rows and
+	 * inner_rows to be at least 1, but the skip_rows estimates can be zero.
+	 */
+	outer_skip_rows = rint(outer_path_rows * outerstartsel);
+	inner_skip_rows = rint(inner_path_rows * innerstartsel);
+	outer_rows = clamp_row_est(outer_path_rows * outerendsel);
+	inner_rows = clamp_row_est(inner_path_rows * innerendsel);
+
+	/* skip rows can become NaN when path rows has become infinite */
+	Assert(outer_skip_rows <= outer_rows || isnan(outer_skip_rows));
+	Assert(inner_skip_rows <= inner_rows || isnan(inner_skip_rows));
+
+	/*
+	 * Readjust scan selectivities to account for above rounding.  This is
+	 * normally an insignificant effect, but when there are only a few rows in
+	 * the inputs, failing to do this makes for a large percentage error.
+	 */
+	outerstartsel = outer_skip_rows / outer_path_rows;
+	innerstartsel = inner_skip_rows / inner_path_rows;
+	outerendsel = outer_rows / outer_path_rows;
+	innerendsel = inner_rows / inner_path_rows;
+
+	/* start sel can become NaN when path rows has become infinite */
+	Assert(outerstartsel <= outerendsel || isnan(outerstartsel));
+	Assert(innerstartsel <= innerendsel || isnan(innerstartsel));
+
+	/* cost of source data */
+
+	if (outersortkeys)			/* do we need to sort outer? */
+	{
+		cost_sort(&sort_path,
+				  root,
+				  outersortkeys,
+				  outer_path->total_cost,
+				  outer_path_rows,
+				  outer_path->pathtarget->width,
+				  0.0,
+				  work_mem,
+				  -1.0);
+		startup_cost += sort_path.startup_cost;
+		startup_cost += (sort_path.total_cost - sort_path.startup_cost)
+			* outerstartsel;
+		run_cost += (sort_path.total_cost - sort_path.startup_cost)
+			* (outerendsel - outerstartsel);
+	}
+	else
+	{
+		startup_cost += outer_path->startup_cost;
+		startup_cost += (outer_path->total_cost - outer_path->startup_cost)
+			* outerstartsel;
+		run_cost += (outer_path->total_cost - outer_path->startup_cost)
+			* (outerendsel - outerstartsel);
+	}
+
+	if (innersortkeys)			/* do we need to sort inner? */
+	{
+		cost_sort(&sort_path,
+				  root,
+				  innersortkeys,
+				  inner_path->total_cost,
+				  inner_path_rows,
+				  inner_path->pathtarget->width,
+				  0.0,
+				  work_mem,
+				  -1.0);
+		startup_cost += sort_path.startup_cost;
+		startup_cost += (sort_path.total_cost - sort_path.startup_cost)
+			* innerstartsel;
+		inner_run_cost = (sort_path.total_cost - sort_path.startup_cost)
+			* (innerendsel - innerstartsel);
+	}
+	else
+	{
+		startup_cost += inner_path->startup_cost;
+		startup_cost += (inner_path->total_cost - inner_path->startup_cost)
+			* innerstartsel;
+		inner_run_cost = (inner_path->total_cost - inner_path->startup_cost)
+			* (innerendsel - innerstartsel);
+	}
+
+	/*
+	 * We can't yet determine whether rescanning occurs, or whether
+	 * materialization of the inner input should be done.  The minimum
+	 * possible inner input cost, regardless of rescan and materialization
+	 * considerations, is inner_run_cost.  We include that in
+	 * workspace->total_cost, but not yet in run_cost.
+	 */
+
+	/* CPU costs left for later */
+
+	/* Public result fields */
+	workspace->startup_cost = startup_cost;
+	workspace->total_cost = startup_cost + run_cost + inner_run_cost;
+	/* Save private data for final_cost_mergejoin */
+	workspace->run_cost = run_cost;
+	workspace->inner_run_cost = inner_run_cost;
+	workspace->outer_rows = outer_rows;
+	workspace->inner_rows = inner_rows;
+	workspace->outer_skip_rows = outer_skip_rows;
+	workspace->inner_skip_rows = inner_skip_rows;
+}
+
+/*
+ * final_cost_mergejoin
+ *	  Final estimate of the cost and result size of a mergejoin path.
+ *
+ * Unlike other costsize functions, this routine makes two actual decisions:
+ * whether the executor will need to do mark/restore, and whether we should
+ * materialize the inner path.  It would be logically cleaner to build
+ * separate paths testing these alternatives, but that would require repeating
+ * most of the cost calculations, which are not all that cheap.  Since the
+ * choice will not affect output pathkeys or startup cost, only total cost,
+ * there is no possibility of wanting to keep more than one path.  So it seems
+ * best to make the decisions here and record them in the path's
+ * skip_mark_restore and materialize_inner fields.
+ *
+ * Mark/restore overhead is usually required, but can be skipped if we know
+ * that the executor need find only one match per outer tuple, and that the
+ * mergeclauses are sufficient to identify a match.
+ *
+ * We materialize the inner path if we need mark/restore and either the inner
+ * path can't support mark/restore, or it's cheaper to use an interposed
+ * Material node to handle mark/restore.
+ *
+ * 'path' is already filled in except for the rows and cost fields and
+ *		skip_mark_restore and materialize_inner
+ * 'workspace' is the result from initial_cost_mergejoin
+ * 'extra' contains miscellaneous information about the join
+ */
+void
+final_cost_mergejoin(PlannerInfo *root, MergePath *path,
+					 JoinCostWorkspace *workspace,
+					 JoinPathExtraData *extra)
+{
+	Path	   *outer_path = path->jpath.outerjoinpath;
+	Path	   *inner_path = path->jpath.innerjoinpath;
+	double		inner_path_rows = inner_path->rows;
+	List	   *mergeclauses = path->path_mergeclauses;
+	List	   *innersortkeys = path->innersortkeys;
+	Cost		startup_cost = workspace->startup_cost;
+	Cost		run_cost = workspace->run_cost;
+	Cost		inner_run_cost = workspace->inner_run_cost;
+	double		outer_rows = workspace->outer_rows;
+	double		inner_rows = workspace->inner_rows;
+	double		outer_skip_rows = workspace->outer_skip_rows;
+	double		inner_skip_rows = workspace->inner_skip_rows;
+	Cost		cpu_per_tuple,
+				bare_inner_cost,
+				mat_inner_cost;
+	QualCost	merge_qual_cost;
+	QualCost	qp_qual_cost;
+	double		mergejointuples,
+				rescannedtuples;
+	double		rescanratio;
+
+	/* Protect some assumptions below that rowcounts aren't zero or NaN */
+	if (inner_path_rows <= 0 || isnan(inner_path_rows))
+		inner_path_rows = 1;
+
+	/* Mark the path with the correct row estimate */
+	if (path->jpath.path.param_info)
+		path->jpath.path.rows = path->jpath.path.param_info->ppi_rows;
+	else
+		path->jpath.path.rows = path->jpath.path.parent->rows;
+
+	/* For partial paths, scale row estimate. */
+	if (path->jpath.path.parallel_workers > 0)
+	{
+		double		parallel_divisor = get_parallel_divisor(&path->jpath.path);
+
+		path->jpath.path.rows =
+			clamp_row_est(path->jpath.path.rows / parallel_divisor);
+	}
+
+	/*
+	 * We could include disable_cost in the preliminary estimate, but that
+	 * would amount to optimizing for the case where the join method is
+	 * disabled, which doesn't seem like the way to bet.
+	 */
+	if (!enable_mergejoin)
+		startup_cost += disable_cost;
+
+	/*
+	 * Compute cost of the mergequals and qpquals (other restriction clauses)
+	 * separately.
+	 */
+	cost_qual_eval(&merge_qual_cost, mergeclauses, root);
+	cost_qual_eval(&qp_qual_cost, path->jpath.joinrestrictinfo, root);
+	qp_qual_cost.startup -= merge_qual_cost.startup;
+	qp_qual_cost.per_tuple -= merge_qual_cost.per_tuple;
+
+	/*
+	 * With a SEMI or ANTI join, or if the innerrel is known unique, the
+	 * executor will stop scanning for matches after the first match.  When
+	 * all the joinclauses are merge clauses, this means we don't ever need to
+	 * back up the merge, and so we can skip mark/restore overhead.
+	 */
+	if ((path->jpath.jointype == JOIN_SEMI ||
+		 path->jpath.jointype == JOIN_ANTI ||
+		 extra->inner_unique) &&
+		(list_length(path->jpath.joinrestrictinfo) ==
+		 list_length(path->path_mergeclauses)))
+		path->skip_mark_restore = true;
+	else
+		path->skip_mark_restore = false;
+
+	/*
+	 * Get approx # tuples passing the mergequals.  We use approx_tuple_count
+	 * here because we need an estimate done with JOIN_INNER semantics.
+	 */
+	mergejointuples = approx_tuple_count(root, &path->jpath, mergeclauses);
+
+	/*
+	 * When there are equal merge keys in the outer relation, the mergejoin
+	 * must rescan any matching tuples in the inner relation. This means
+	 * re-fetching inner tuples; we have to estimate how often that happens.
+	 *
+	 * For regular inner and outer joins, the number of re-fetches can be
+	 * estimated approximately as size of merge join output minus size of
+	 * inner relation. Assume that the distinct key values are 1, 2, ..., and
+	 * denote the number of values of each key in the outer relation as m1,
+	 * m2, ...; in the inner relation, n1, n2, ...  Then we have
+	 *
+	 * size of join = m1 * n1 + m2 * n2 + ...
+	 *
+	 * number of rescanned tuples = (m1 - 1) * n1 + (m2 - 1) * n2 + ... = m1 *
+	 * n1 + m2 * n2 + ... - (n1 + n2 + ...) = size of join - size of inner
+	 * relation
+	 *
+	 * This equation works correctly for outer tuples having no inner match
+	 * (nk = 0), but not for inner tuples having no outer match (mk = 0); we
+	 * are effectively subtracting those from the number of rescanned tuples,
+	 * when we should not.  Can we do better without expensive selectivity
+	 * computations?
+	 *
+	 * The whole issue is moot if we are working from a unique-ified outer
+	 * input, or if we know we don't need to mark/restore at all.
+	 */
+	if (IsA(outer_path, UniquePath) ||path->skip_mark_restore)
+		rescannedtuples = 0;
+	else
+	{
+		rescannedtuples = mergejointuples - inner_path_rows;
+		/* Must clamp because of possible underestimate */
+		if (rescannedtuples < 0)
+			rescannedtuples = 0;
+	}
+
+	/*
+	 * We'll inflate various costs this much to account for rescanning.  Note
+	 * that this is to be multiplied by something involving inner_rows, or
+	 * another number related to the portion of the inner rel we'll scan.
+	 */
+	rescanratio = 1.0 + (rescannedtuples / inner_rows);
+
+	/*
+	 * Decide whether we want to materialize the inner input to shield it from
+	 * mark/restore and performing re-fetches.  Our cost model for regular
+	 * re-fetches is that a re-fetch costs the same as an original fetch,
+	 * which is probably an overestimate; but on the other hand we ignore the
+	 * bookkeeping costs of mark/restore.  Not clear if it's worth developing
+	 * a more refined model.  So we just need to inflate the inner run cost by
+	 * rescanratio.
+	 */
+	bare_inner_cost = inner_run_cost * rescanratio;
+
+	/*
+	 * When we interpose a Material node the re-fetch cost is assumed to be
+	 * just cpu_operator_cost per tuple, independently of the underlying
+	 * plan's cost; and we charge an extra cpu_operator_cost per original
+	 * fetch as well.  Note that we're assuming the materialize node will
+	 * never spill to disk, since it only has to remember tuples back to the
+	 * last mark.  (If there are a huge number of duplicates, our other cost
+	 * factors will make the path so expensive that it probably won't get
+	 * chosen anyway.)	So we don't use cost_rescan here.
+	 *
+	 * Note: keep this estimate in sync with create_mergejoin_plan's labeling
+	 * of the generated Material node.
+	 */
+	mat_inner_cost = inner_run_cost +
+		cpu_operator_cost * inner_rows * rescanratio;
+
+	/*
+	 * If we don't need mark/restore at all, we don't need materialization.
+	 */
+	if (path->skip_mark_restore)
+		path->materialize_inner = false;
+
+	/*
+	 * Prefer materializing if it looks cheaper, unless the user has asked to
+	 * suppress materialization.
+	 */
+	else if (enable_material && mat_inner_cost < bare_inner_cost)
+		path->materialize_inner = true;
+
+	/*
+	 * Even if materializing doesn't look cheaper, we *must* do it if the
+	 * inner path is to be used directly (without sorting) and it doesn't
+	 * support mark/restore.
+	 *
+	 * Since the inner side must be ordered, and only Sorts and IndexScans can
+	 * create order to begin with, and they both support mark/restore, you
+	 * might think there's no problem --- but you'd be wrong.  Nestloop and
+	 * merge joins can *preserve* the order of their inputs, so they can be
+	 * selected as the input of a mergejoin, and they don't support
+	 * mark/restore at present.
+	 *
+	 * We don't test the value of enable_material here, because
+	 * materialization is required for correctness in this case, and turning
+	 * it off does not entitle us to deliver an invalid plan.
+	 */
+	else if (innersortkeys == NIL &&
+			 !ExecSupportsMarkRestore(inner_path))
+		path->materialize_inner = true;
+
+	/*
+	 * Also, force materializing if the inner path is to be sorted and the
+	 * sort is expected to spill to disk.  This is because the final merge
+	 * pass can be done on-the-fly if it doesn't have to support mark/restore.
+	 * We don't try to adjust the cost estimates for this consideration,
+	 * though.
+	 *
+	 * Since materialization is a performance optimization in this case,
+	 * rather than necessary for correctness, we skip it if enable_material is
+	 * off.
+	 */
+	else if (enable_material && innersortkeys != NIL &&
+			 relation_byte_size(inner_path_rows,
+								inner_path->pathtarget->width) >
+			 (work_mem * 1024L))
+		path->materialize_inner = true;
+	else
+		path->materialize_inner = false;
+
+	/* Charge the right incremental cost for the chosen case */
+	if (path->materialize_inner)
+		run_cost += mat_inner_cost;
+	else
+		run_cost += bare_inner_cost;
+
+	/* CPU costs */
+
+	/*
+	 * The number of tuple comparisons needed is approximately number of outer
+	 * rows plus number of inner rows plus number of rescanned tuples (can we
+	 * refine this?).  At each one, we need to evaluate the mergejoin quals.
+	 */
+	startup_cost += merge_qual_cost.startup;
+	startup_cost += merge_qual_cost.per_tuple *
+		(outer_skip_rows + inner_skip_rows * rescanratio);
+	run_cost += merge_qual_cost.per_tuple *
+		((outer_rows - outer_skip_rows) +
+		 (inner_rows - inner_skip_rows) * rescanratio);
+
+	/*
+	 * For each tuple that gets through the mergejoin proper, we charge
+	 * cpu_tuple_cost plus the cost of evaluating additional restriction
+	 * clauses that are to be applied at the join.  (This is pessimistic since
+	 * not all of the quals may get evaluated at each tuple.)
+	 *
+	 * Note: we could adjust for SEMI/ANTI joins skipping some qual
+	 * evaluations here, but it's probably not worth the trouble.
+	 */
+	startup_cost += qp_qual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qp_qual_cost.per_tuple;
+	run_cost += cpu_per_tuple * mergejointuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->jpath.path.pathtarget->cost.startup;
+	run_cost += path->jpath.path.pathtarget->cost.per_tuple * path->jpath.path.rows;
+
+	path->jpath.path.startup_cost = startup_cost;
+	path->jpath.path.total_cost = startup_cost + run_cost;
+}
+
+/*
+ * run mergejoinscansel() with caching
+ */
+static MergeScanSelCache *
+cached_scansel(PlannerInfo *root, RestrictInfo *rinfo, PathKey *pathkey)
+{
+	MergeScanSelCache *cache;
+	ListCell   *lc;
+	Selectivity leftstartsel,
+				leftendsel,
+				rightstartsel,
+				rightendsel;
+	MemoryContext oldcontext;
+
+	/* Do we have this result already? */
+	foreach(lc, rinfo->scansel_cache)
+	{
+		cache = (MergeScanSelCache *) lfirst(lc);
+		if (cache->opfamily == pathkey->pk_opfamily &&
+			cache->collation == pathkey->pk_eclass->ec_collation &&
+			cache->strategy == pathkey->pk_strategy &&
+			cache->nulls_first == pathkey->pk_nulls_first)
+			return cache;
+	}
+
+	/* Nope, do the computation */
+	mergejoinscansel(root,
+					 (Node *) rinfo->clause,
+					 pathkey->pk_opfamily,
+					 pathkey->pk_strategy,
+					 pathkey->pk_nulls_first,
+					 &leftstartsel,
+					 &leftendsel,
+					 &rightstartsel,
+					 &rightendsel);
+
+	/* Cache the result in suitably long-lived workspace */
+	oldcontext = MemoryContextSwitchTo(root->planner_cxt);
+
+	cache = (MergeScanSelCache *) palloc(sizeof(MergeScanSelCache));
+	cache->opfamily = pathkey->pk_opfamily;
+	cache->collation = pathkey->pk_eclass->ec_collation;
+	cache->strategy = pathkey->pk_strategy;
+	cache->nulls_first = pathkey->pk_nulls_first;
+	cache->leftstartsel = leftstartsel;
+	cache->leftendsel = leftendsel;
+	cache->rightstartsel = rightstartsel;
+	cache->rightendsel = rightendsel;
+
+	rinfo->scansel_cache = lappend(rinfo->scansel_cache, cache);
+
+	MemoryContextSwitchTo(oldcontext);
+
+	return cache;
+}
+
+/*
+ * initial_cost_hashjoin
+ *	  Preliminary estimate of the cost of a hashjoin path.
+ *
+ * This must quickly produce lower-bound estimates of the path's startup and
+ * total costs.  If we are unable to eliminate the proposed path from
+ * consideration using the lower bounds, final_cost_hashjoin will be called
+ * to obtain the final estimates.
+ *
+ * The exact division of labor between this function and final_cost_hashjoin
+ * is private to them, and represents a tradeoff between speed of the initial
+ * estimate and getting a tight lower bound.  We choose to not examine the
+ * join quals here (other than by counting the number of hash clauses),
+ * so we can't do much with CPU costs.  We do assume that
+ * ExecChooseHashTableSize is cheap enough to use here.
+ *
+ * 'workspace' is to be filled with startup_cost, total_cost, and perhaps
+ *		other data to be used by final_cost_hashjoin
+ * 'jointype' is the type of join to be performed
+ * 'hashclauses' is the list of joinclauses to be used as hash clauses
+ * 'outer_path' is the outer input to the join
+ * 'inner_path' is the inner input to the join
+ * 'extra' contains miscellaneous information about the join
+ * 'parallel_hash' indicates that inner_path is partial and that a shared
+ *		hash table will be built in parallel
+ */
+void
+initial_cost_hashjoin(PlannerInfo *root, JoinCostWorkspace *workspace,
+					  JoinType jointype,
+					  List *hashclauses,
+					  Path *outer_path, Path *inner_path,
+					  JoinPathExtraData *extra,
+					  bool parallel_hash)
+{
+	Cost		startup_cost = 0;
+	Cost		run_cost = 0;
+	double		outer_path_rows = outer_path->rows;
+	double		inner_path_rows = inner_path->rows;
+	double		inner_path_rows_total = inner_path_rows;
+	int			num_hashclauses = list_length(hashclauses);
+	int			numbuckets;
+	int			numbatches;
+	int			num_skew_mcvs;
+	size_t		space_allowed;	/* unused */
+
+	/* cost of source data */
+	startup_cost += outer_path->startup_cost;
+	run_cost += outer_path->total_cost - outer_path->startup_cost;
+	startup_cost += inner_path->total_cost;
+
+	/*
+	 * Cost of computing hash function: must do it once per input tuple. We
+	 * charge one cpu_operator_cost for each column's hash function.  Also,
+	 * tack on one cpu_tuple_cost per inner row, to model the costs of
+	 * inserting the row into the hashtable.
+	 *
+	 * XXX when a hashclause is more complex than a single operator, we really
+	 * should charge the extra eval costs of the left or right side, as
+	 * appropriate, here.  This seems more work than it's worth at the moment.
+	 */
+	startup_cost += (cpu_operator_cost * num_hashclauses + cpu_tuple_cost)
+		* inner_path_rows;
+	run_cost += cpu_operator_cost * num_hashclauses * outer_path_rows;
+
+	/*
+	 * If this is a parallel hash build, then the value we have for
+	 * inner_rows_total currently refers only to the rows returned by each
+	 * participant.  For shared hash table size estimation, we need the total
+	 * number, so we need to undo the division.
+	 */
+	if (parallel_hash)
+		inner_path_rows_total *= get_parallel_divisor(inner_path);
+
+	/*
+	 * Get hash table size that executor would use for inner relation.
+	 *
+	 * XXX for the moment, always assume that skew optimization will be
+	 * performed.  As long as SKEW_WORK_MEM_PERCENT is small, it's not worth
+	 * trying to determine that for sure.
+	 *
+	 * XXX at some point it might be interesting to try to account for skew
+	 * optimization in the cost estimate, but for now, we don't.
+	 */
+	ExecChooseHashTableSize(inner_path_rows_total,
+							inner_path->pathtarget->width,
+							true,	/* useskew */
+							parallel_hash,	/* try_combined_work_mem */
+							outer_path->parallel_workers,
+							&space_allowed,
+							&numbuckets,
+							&numbatches,
+							&num_skew_mcvs);
+
+	/*
+	 * If inner relation is too big then we will need to "batch" the join,
+	 * which implies writing and reading most of the tuples to disk an extra
+	 * time.  Charge seq_page_cost per page, since the I/O should be nice and
+	 * sequential.  Writing the inner rel counts as startup cost, all the rest
+	 * as run cost.
+	 */
+	if (numbatches > 1)
+	{
+		double		outerpages = page_size(outer_path_rows,
+										   outer_path->pathtarget->width);
+		double		innerpages = page_size(inner_path_rows,
+										   inner_path->pathtarget->width);
+
+		startup_cost += seq_page_cost * innerpages;
+		run_cost += seq_page_cost * (innerpages + 2 * outerpages);
+	}
+
+	/* CPU costs left for later */
+
+	/* Public result fields */
+	workspace->startup_cost = startup_cost;
+	workspace->total_cost = startup_cost + run_cost;
+	/* Save private data for final_cost_hashjoin */
+	workspace->run_cost = run_cost;
+	workspace->numbuckets = numbuckets;
+	workspace->numbatches = numbatches;
+	workspace->inner_rows_total = inner_path_rows_total;
+}
+
+/*
+ * final_cost_hashjoin
+ *	  Final estimate of the cost and result size of a hashjoin path.
+ *
+ * Note: the numbatches estimate is also saved into 'path' for use later
+ *
+ * 'path' is already filled in except for the rows and cost fields and
+ *		num_batches
+ * 'workspace' is the result from initial_cost_hashjoin
+ * 'extra' contains miscellaneous information about the join
+ */
+void
+final_cost_hashjoin(PlannerInfo *root, HashPath *path,
+					JoinCostWorkspace *workspace,
+					JoinPathExtraData *extra)
+{
+	Path	   *outer_path = path->jpath.outerjoinpath;
+	Path	   *inner_path = path->jpath.innerjoinpath;
+	double		outer_path_rows = outer_path->rows;
+	double		inner_path_rows = inner_path->rows;
+	double		inner_path_rows_total = workspace->inner_rows_total;
+	List	   *hashclauses = path->path_hashclauses;
+	Cost		startup_cost = workspace->startup_cost;
+	Cost		run_cost = workspace->run_cost;
+	int			numbuckets = workspace->numbuckets;
+	int			numbatches = workspace->numbatches;
+	Cost		cpu_per_tuple;
+	QualCost	hash_qual_cost;
+	QualCost	qp_qual_cost;
+	double		hashjointuples;
+	double		virtualbuckets;
+	Selectivity innerbucketsize;
+	Selectivity innermcvfreq;
+	ListCell   *hcl;
+
+	/* Mark the path with the correct row estimate */
+	if (path->jpath.path.param_info)
+		path->jpath.path.rows = path->jpath.path.param_info->ppi_rows;
+	else
+		path->jpath.path.rows = path->jpath.path.parent->rows;
+
+	/* For partial paths, scale row estimate. */
+	if (path->jpath.path.parallel_workers > 0)
+	{
+		double		parallel_divisor = get_parallel_divisor(&path->jpath.path);
+
+		path->jpath.path.rows =
+			clamp_row_est(path->jpath.path.rows / parallel_divisor);
+	}
+
+	/*
+	 * We could include disable_cost in the preliminary estimate, but that
+	 * would amount to optimizing for the case where the join method is
+	 * disabled, which doesn't seem like the way to bet.
+	 */
+	if (!enable_hashjoin)
+		startup_cost += disable_cost;
+
+	/* mark the path with estimated # of batches */
+	path->num_batches = numbatches;
+
+	/* store the total number of tuples (sum of partial row estimates) */
+	path->inner_rows_total = inner_path_rows_total;
+
+	/* and compute the number of "virtual" buckets in the whole join */
+	virtualbuckets = (double) numbuckets * (double) numbatches;
+
+	/*
+	 * Determine bucketsize fraction and MCV frequency for the inner relation.
+	 * We use the smallest bucketsize or MCV frequency estimated for any
+	 * individual hashclause; this is undoubtedly conservative.
+	 *
+	 * BUT: if inner relation has been unique-ified, we can assume it's good
+	 * for hashing.  This is important both because it's the right answer, and
+	 * because we avoid contaminating the cache with a value that's wrong for
+	 * non-unique-ified paths.
+	 */
+	if (IsA(inner_path, UniquePath))
+	{
+		innerbucketsize = 1.0 / virtualbuckets;
+		innermcvfreq = 0.0;
+	}
+	else
+	{
+		innerbucketsize = 1.0;
+		innermcvfreq = 1.0;
+		foreach(hcl, hashclauses)
+		{
+			RestrictInfo *restrictinfo = lfirst_node(RestrictInfo, hcl);
+			Selectivity thisbucketsize;
+			Selectivity thismcvfreq;
+
+			/*
+			 * First we have to figure out which side of the hashjoin clause
+			 * is the inner side.
+			 *
+			 * Since we tend to visit the same clauses over and over when
+			 * planning a large query, we cache the bucket stats estimates in
+			 * the RestrictInfo node to avoid repeated lookups of statistics.
+			 */
+			if (bms_is_subset(restrictinfo->right_relids,
+							  inner_path->parent->relids))
+			{
+				/* righthand side is inner */
+				thisbucketsize = restrictinfo->right_bucketsize;
+				if (thisbucketsize < 0)
+				{
+					/* not cached yet */
+					estimate_hash_bucket_stats(root,
+											   get_rightop(restrictinfo->clause),
+											   virtualbuckets,
+											   &restrictinfo->right_mcvfreq,
+											   &restrictinfo->right_bucketsize);
+					thisbucketsize = restrictinfo->right_bucketsize;
+				}
+				thismcvfreq = restrictinfo->right_mcvfreq;
+			}
+			else
+			{
+				Assert(bms_is_subset(restrictinfo->left_relids,
+									 inner_path->parent->relids));
+				/* lefthand side is inner */
+				thisbucketsize = restrictinfo->left_bucketsize;
+				if (thisbucketsize < 0)
+				{
+					/* not cached yet */
+					estimate_hash_bucket_stats(root,
+											   get_leftop(restrictinfo->clause),
+											   virtualbuckets,
+											   &restrictinfo->left_mcvfreq,
+											   &restrictinfo->left_bucketsize);
+					thisbucketsize = restrictinfo->left_bucketsize;
+				}
+				thismcvfreq = restrictinfo->left_mcvfreq;
+			}
+
+			if (innerbucketsize > thisbucketsize)
+				innerbucketsize = thisbucketsize;
+			if (innermcvfreq > thismcvfreq)
+				innermcvfreq = thismcvfreq;
+		}
+	}
+
+	/*
+	 * If the bucket holding the inner MCV would exceed work_mem, we don't
+	 * want to hash unless there is really no other alternative, so apply
+	 * disable_cost.  (The executor normally copes with excessive memory usage
+	 * by splitting batches, but obviously it cannot separate equal values
+	 * that way, so it will be unable to drive the batch size below work_mem
+	 * when this is true.)
+	 */
+	if (relation_byte_size(clamp_row_est(inner_path_rows * innermcvfreq),
+						   inner_path->pathtarget->width) >
+		(work_mem * 1024L))
+		startup_cost += disable_cost;
+
+	/*
+	 * Compute cost of the hashquals and qpquals (other restriction clauses)
+	 * separately.
+	 */
+	cost_qual_eval(&hash_qual_cost, hashclauses, root);
+	cost_qual_eval(&qp_qual_cost, path->jpath.joinrestrictinfo, root);
+	qp_qual_cost.startup -= hash_qual_cost.startup;
+	qp_qual_cost.per_tuple -= hash_qual_cost.per_tuple;
+
+	/* CPU costs */
+
+	if (path->jpath.jointype == JOIN_SEMI ||
+		path->jpath.jointype == JOIN_ANTI ||
+		extra->inner_unique)
+	{
+		double		outer_matched_rows;
+		Selectivity inner_scan_frac;
+
+		/*
+		 * With a SEMI or ANTI join, or if the innerrel is known unique, the
+		 * executor will stop after the first match.
+		 *
+		 * For an outer-rel row that has at least one match, we can expect the
+		 * bucket scan to stop after a fraction 1/(match_count+1) of the
+		 * bucket's rows, if the matches are evenly distributed.  Since they
+		 * probably aren't quite evenly distributed, we apply a fuzz factor of
+		 * 2.0 to that fraction.  (If we used a larger fuzz factor, we'd have
+		 * to clamp inner_scan_frac to at most 1.0; but since match_count is
+		 * at least 1, no such clamp is needed now.)
+		 */
+		outer_matched_rows = rint(outer_path_rows * extra->semifactors.outer_match_frac);
+		inner_scan_frac = 2.0 / (extra->semifactors.match_count + 1.0);
+
+		startup_cost += hash_qual_cost.startup;
+		run_cost += hash_qual_cost.per_tuple * outer_matched_rows *
+			clamp_row_est(inner_path_rows * innerbucketsize * inner_scan_frac) * 0.5;
+
+		/*
+		 * For unmatched outer-rel rows, the picture is quite a lot different.
+		 * In the first place, there is no reason to assume that these rows
+		 * preferentially hit heavily-populated buckets; instead assume they
+		 * are uncorrelated with the inner distribution and so they see an
+		 * average bucket size of inner_path_rows / virtualbuckets.  In the
+		 * second place, it seems likely that they will have few if any exact
+		 * hash-code matches and so very few of the tuples in the bucket will
+		 * actually require eval of the hash quals.  We don't have any good
+		 * way to estimate how many will, but for the moment assume that the
+		 * effective cost per bucket entry is one-tenth what it is for
+		 * matchable tuples.
+		 */
+		run_cost += hash_qual_cost.per_tuple *
+			(outer_path_rows - outer_matched_rows) *
+			clamp_row_est(inner_path_rows / virtualbuckets) * 0.05;
+
+		/* Get # of tuples that will pass the basic join */
+		if (path->jpath.jointype == JOIN_ANTI)
+			hashjointuples = outer_path_rows - outer_matched_rows;
+		else
+			hashjointuples = outer_matched_rows;
+	}
+	else
+	{
+		/*
+		 * The number of tuple comparisons needed is the number of outer
+		 * tuples times the typical number of tuples in a hash bucket, which
+		 * is the inner relation size times its bucketsize fraction.  At each
+		 * one, we need to evaluate the hashjoin quals.  But actually,
+		 * charging the full qual eval cost at each tuple is pessimistic,
+		 * since we don't evaluate the quals unless the hash values match
+		 * exactly.  For lack of a better idea, halve the cost estimate to
+		 * allow for that.
+		 */
+		startup_cost += hash_qual_cost.startup;
+		run_cost += hash_qual_cost.per_tuple * outer_path_rows *
+			clamp_row_est(inner_path_rows * innerbucketsize) * 0.5;
+
+		/*
+		 * Get approx # tuples passing the hashquals.  We use
+		 * approx_tuple_count here because we need an estimate done with
+		 * JOIN_INNER semantics.
+		 */
+		hashjointuples = approx_tuple_count(root, &path->jpath, hashclauses);
+	}
+
+	/*
+	 * For each tuple that gets through the hashjoin proper, we charge
+	 * cpu_tuple_cost plus the cost of evaluating additional restriction
+	 * clauses that are to be applied at the join.  (This is pessimistic since
+	 * not all of the quals may get evaluated at each tuple.)
+	 */
+	startup_cost += qp_qual_cost.startup;
+	cpu_per_tuple = cpu_tuple_cost + qp_qual_cost.per_tuple;
+	run_cost += cpu_per_tuple * hashjointuples;
+
+	/* tlist eval costs are paid per output row, not per tuple scanned */
+	startup_cost += path->jpath.path.pathtarget->cost.startup;
+	run_cost += path->jpath.path.pathtarget->cost.per_tuple * path->jpath.path.rows;
+
+	path->jpath.path.startup_cost = startup_cost;
+	path->jpath.path.total_cost = startup_cost + run_cost;
+}
+
+
+/*
+ * cost_subplan
+ *		Figure the costs for a SubPlan (or initplan).
+ *
+ * Note: we could dig the subplan's Plan out of the root list, but in practice
+ * all callers have it handy already, so we make them pass it.
+ */
+void
+cost_subplan(PlannerInfo *root, SubPlan *subplan, Plan *plan)
+{
+	QualCost	sp_cost;
+
+	/* Figure any cost for evaluating the testexpr */
+	cost_qual_eval(&sp_cost,
+				   make_ands_implicit((Expr *) subplan->testexpr),
+				   root);
+
+	if (subplan->useHashTable)
+	{
+		/*
+		 * If we are using a hash table for the subquery outputs, then the
+		 * cost of evaluating the query is a one-time cost.  We charge one
+		 * cpu_operator_cost per tuple for the work of loading the hashtable,
+		 * too.
+		 */
+		sp_cost.startup += plan->total_cost +
+			cpu_operator_cost * plan->plan_rows;
+
+		/*
+		 * The per-tuple costs include the cost of evaluating the lefthand
+		 * expressions, plus the cost of probing the hashtable.  We already
+		 * accounted for the lefthand expressions as part of the testexpr, and
+		 * will also have counted one cpu_operator_cost for each comparison
+		 * operator.  That is probably too low for the probing cost, but it's
+		 * hard to make a better estimate, so live with it for now.
+		 */
+	}
+	else
+	{
+		/*
+		 * Otherwise we will be rescanning the subplan output on each
+		 * evaluation.  We need to estimate how much of the output we will
+		 * actually need to scan.  NOTE: this logic should agree with the
+		 * tuple_fraction estimates used by make_subplan() in
+		 * plan/subselect.c.
+		 */
+		Cost		plan_run_cost = plan->total_cost - plan->startup_cost;
+
+		if (subplan->subLinkType == EXISTS_SUBLINK)
+		{
+			/* we only need to fetch 1 tuple; clamp to avoid zero divide */
+			sp_cost.per_tuple += plan_run_cost / clamp_row_est(plan->plan_rows);
+		}
+		else if (subplan->subLinkType == ALL_SUBLINK ||
+				 subplan->subLinkType == ANY_SUBLINK)
+		{
+			/* assume we need 50% of the tuples */
+			sp_cost.per_tuple += 0.50 * plan_run_cost;
+			/* also charge a cpu_operator_cost per row examined */
+			sp_cost.per_tuple += 0.50 * plan->plan_rows * cpu_operator_cost;
+		}
+		else
+		{
+			/* assume we need all tuples */
+			sp_cost.per_tuple += plan_run_cost;
+		}
+
+		/*
+		 * Also account for subplan's startup cost. If the subplan is
+		 * uncorrelated or undirect correlated, AND its topmost node is one
+		 * that materializes its output, assume that we'll only need to pay
+		 * its startup cost once; otherwise assume we pay the startup cost
+		 * every time.
+		 */
+		if (subplan->parParam == NIL &&
+			ExecMaterializesOutput(nodeTag(plan)))
+			sp_cost.startup += plan->startup_cost;
+		else
+			sp_cost.per_tuple += plan->startup_cost;
+	}
+
+	subplan->startup_cost = sp_cost.startup;
+	subplan->per_call_cost = sp_cost.per_tuple;
+}
+
+
+/*
+ * cost_rescan
+ *		Given a finished Path, estimate the costs of rescanning it after
+ *		having done so the first time.  For some Path types a rescan is
+ *		cheaper than an original scan (if no parameters change), and this
+ *		function embodies knowledge about that.  The default is to return
+ *		the same costs stored in the Path.  (Note that the cost estimates
+ *		actually stored in Paths are always for first scans.)
+ *
+ * This function is not currently intended to model effects such as rescans
+ * being cheaper due to disk block caching; what we are concerned with is
+ * plan types wherein the executor caches results explicitly, or doesn't
+ * redo startup calculations, etc.
+ */
+static void
+cost_rescan(PlannerInfo *root, Path *path,
+			Cost *rescan_startup_cost,	/* output parameters */
+			Cost *rescan_total_cost)
+{
+	switch (path->pathtype)
+	{
+		case T_FunctionScan:
+
+			/*
+			 * Currently, nodeFunctionscan.c always executes the function to
+			 * completion before returning any rows, and caches the results in
+			 * a tuplestore.  So the function eval cost is all startup cost
+			 * and isn't paid over again on rescans. However, all run costs
+			 * will be paid over again.
+			 */
+			*rescan_startup_cost = 0;
+			*rescan_total_cost = path->total_cost - path->startup_cost;
+			break;
+		case T_HashJoin:
+
+			/*
+			 * If it's a single-batch join, we don't need to rebuild the hash
+			 * table during a rescan.
+			 */
+			if (((HashPath *) path)->num_batches == 1)
+			{
+				/* Startup cost is exactly the cost of hash table building */
+				*rescan_startup_cost = 0;
+				*rescan_total_cost = path->total_cost - path->startup_cost;
+			}
+			else
+			{
+				/* Otherwise, no special treatment */
+				*rescan_startup_cost = path->startup_cost;
+				*rescan_total_cost = path->total_cost;
+			}
+			break;
+		case T_CteScan:
+		case T_WorkTableScan:
+			{
+				/*
+				 * These plan types materialize their final result in a
+				 * tuplestore or tuplesort object.  So the rescan cost is only
+				 * cpu_tuple_cost per tuple, unless the result is large enough
+				 * to spill to disk.
+				 */
+				Cost		run_cost = cpu_tuple_cost * path->rows;
+				double		nbytes = relation_byte_size(path->rows,
+														path->pathtarget->width);
+				long		work_mem_bytes = work_mem * 1024L;
+
+				if (nbytes > work_mem_bytes)
+				{
+					/* It will spill, so account for re-read cost */
+					double		npages = ceil(nbytes / BLCKSZ);
+
+					run_cost += seq_page_cost * npages;
+				}
+				*rescan_startup_cost = 0;
+				*rescan_total_cost = run_cost;
+			}
+			break;
+		case T_Material:
+		case T_Sort:
+			{
+				/*
+				 * These plan types not only materialize their results, but do
+				 * not implement qual filtering or projection.  So they are
+				 * even cheaper to rescan than the ones above.  We charge only
+				 * cpu_operator_cost per tuple.  (Note: keep that in sync with
+				 * the run_cost charge in cost_sort, and also see comments in
+				 * cost_material before you change it.)
+				 */
+				Cost		run_cost = cpu_operator_cost * path->rows;
+				double		nbytes = relation_byte_size(path->rows,
+														path->pathtarget->width);
+				long		work_mem_bytes = work_mem * 1024L;
+
+				if (nbytes > work_mem_bytes)
+				{
+					/* It will spill, so account for re-read cost */
+					double		npages = ceil(nbytes / BLCKSZ);
+
+					run_cost += seq_page_cost * npages;
+				}
+				*rescan_startup_cost = 0;
+				*rescan_total_cost = run_cost;
+			}
+			break;
+		default:
+			*rescan_startup_cost = path->startup_cost;
+			*rescan_total_cost = path->total_cost;
+			break;
+	}
+}
+
+
+/*
+ * cost_qual_eval
+ *		Estimate the CPU costs of evaluating a WHERE clause.
+ *		The input can be either an implicitly-ANDed list of boolean
+ *		expressions, or a list of RestrictInfo nodes.  (The latter is
+ *		preferred since it allows caching of the results.)
+ *		The result includes both a one-time (startup) component,
+ *		and a per-evaluation component.
+ */
+void
+cost_qual_eval(QualCost *cost, List *quals, PlannerInfo *root)
+{
+	cost_qual_eval_context context;
+	ListCell   *l;
+
+	context.root = root;
+	context.total.startup = 0;
+	context.total.per_tuple = 0;
+
+	/* We don't charge any cost for the implicit ANDing at top level ... */
+
+	foreach(l, quals)
+	{
+		Node	   *qual = (Node *) lfirst(l);
+
+		cost_qual_eval_walker(qual, &context);
+	}
+
+	*cost = context.total;
+}
+
+/*
+ * cost_qual_eval_node
+ *		As above, for a single RestrictInfo or expression.
+ */
+void
+cost_qual_eval_node(QualCost *cost, Node *qual, PlannerInfo *root)
+{
+	cost_qual_eval_context context;
+
+	context.root = root;
+	context.total.startup = 0;
+	context.total.per_tuple = 0;
+
+	cost_qual_eval_walker(qual, &context);
+
+	*cost = context.total;
+}
+
+static bool
+cost_qual_eval_walker(Node *node, cost_qual_eval_context *context)
+{
+	if (node == NULL)
+		return false;
+
+	/*
+	 * RestrictInfo nodes contain an eval_cost field reserved for this
+	 * routine's use, so that it's not necessary to evaluate the qual clause's
+	 * cost more than once.  If the clause's cost hasn't been computed yet,
+	 * the field's startup value will contain -1.
+	 */
+	if (IsA(node, RestrictInfo))
+	{
+		RestrictInfo *rinfo = (RestrictInfo *) node;
+
+		if (rinfo->eval_cost.startup < 0)
+		{
+			cost_qual_eval_context locContext;
+
+			locContext.root = context->root;
+			locContext.total.startup = 0;
+			locContext.total.per_tuple = 0;
+
+			/*
+			 * For an OR clause, recurse into the marked-up tree so that we
+			 * set the eval_cost for contained RestrictInfos too.
+			 */
+			if (rinfo->orclause)
+				cost_qual_eval_walker((Node *) rinfo->orclause, &locContext);
+			else
+				cost_qual_eval_walker((Node *) rinfo->clause, &locContext);
+
+			/*
+			 * If the RestrictInfo is marked pseudoconstant, it will be tested
+			 * only once, so treat its cost as all startup cost.
+			 */
+			if (rinfo->pseudoconstant)
+			{
+				/* count one execution during startup */
+				locContext.total.startup += locContext.total.per_tuple;
+				locContext.total.per_tuple = 0;
+			}
+			rinfo->eval_cost = locContext.total;
+		}
+		context->total.startup += rinfo->eval_cost.startup;
+		context->total.per_tuple += rinfo->eval_cost.per_tuple;
+		/* do NOT recurse into children */
+		return false;
+	}
+
+	/*
+	 * For each operator or function node in the given tree, we charge the
+	 * estimated execution cost given by pg_proc.procost (remember to multiply
+	 * this by cpu_operator_cost).
+	 *
+	 * Vars and Consts are charged zero, and so are boolean operators (AND,
+	 * OR, NOT). Simplistic, but a lot better than no model at all.
+	 *
+	 * Should we try to account for the possibility of short-circuit
+	 * evaluation of AND/OR?  Probably *not*, because that would make the
+	 * results depend on the clause ordering, and we are not in any position
+	 * to expect that the current ordering of the clauses is the one that's
+	 * going to end up being used.  The above per-RestrictInfo caching would
+	 * not mix well with trying to re-order clauses anyway.
+	 *
+	 * Another issue that is entirely ignored here is that if a set-returning
+	 * function is below top level in the tree, the functions/operators above
+	 * it will need to be evaluated multiple times.  In practical use, such
+	 * cases arise so seldom as to not be worth the added complexity needed;
+	 * moreover, since our rowcount estimates for functions tend to be pretty
+	 * phony, the results would also be pretty phony.
+	 */
+	if (IsA(node, FuncExpr))
+	{
+		add_function_cost(context->root, ((FuncExpr *) node)->funcid, node,
+						  &context->total);
+	}
+	else if (IsA(node, OpExpr) ||
+			 IsA(node, DistinctExpr) ||
+			 IsA(node, NullIfExpr))
+	{
+		/* rely on struct equivalence to treat these all alike */
+		set_opfuncid((OpExpr *) node);
+		add_function_cost(context->root, ((OpExpr *) node)->opfuncid, node,
+						  &context->total);
+	}
+	else if (IsA(node, ScalarArrayOpExpr))
+	{
+		/*
+		 * Estimate that the operator will be applied to about half of the
+		 * array elements before the answer is determined.
+		 */
+		ScalarArrayOpExpr *saop = (ScalarArrayOpExpr *) node;
+		Node	   *arraynode = (Node *) lsecond(saop->args);
+		QualCost	sacosts;
+
+		set_sa_opfuncid(saop);
+		sacosts.startup = sacosts.per_tuple = 0;
+		add_function_cost(context->root, saop->opfuncid, NULL,
+						  &sacosts);
+		context->total.startup += sacosts.startup;
+		context->total.per_tuple += sacosts.per_tuple *
+			estimate_array_length(arraynode) * 0.5;
+	}
+	else if (IsA(node, Aggref) ||
+			 IsA(node, WindowFunc))
+	{
+		/*
+		 * Aggref and WindowFunc nodes are (and should be) treated like Vars,
+		 * ie, zero execution cost in the current model, because they behave
+		 * essentially like Vars at execution.  We disregard the costs of
+		 * their input expressions for the same reason.  The actual execution
+		 * costs of the aggregate/window functions and their arguments have to
+		 * be factored into plan-node-specific costing of the Agg or WindowAgg
+		 * plan node.
+		 */
+		return false;			/* don't recurse into children */
+	}
+	else if (IsA(node, GroupingFunc))
+	{
+		/* Treat this as having cost 1 */
+		context->total.per_tuple += cpu_operator_cost;
+		return false;			/* don't recurse into children */
+	}
+	else if (IsA(node, CoerceViaIO))
+	{
+		CoerceViaIO *iocoerce = (CoerceViaIO *) node;
+		Oid			iofunc;
+		Oid			typioparam;
+		bool		typisvarlena;
+
+		/* check the result type's input function */
+		getTypeInputInfo(iocoerce->resulttype,
+						 &iofunc, &typioparam);
+		add_function_cost(context->root, iofunc, NULL,
+						  &context->total);
+		/* check the input type's output function */
+		getTypeOutputInfo(exprType((Node *) iocoerce->arg),
+						  &iofunc, &typisvarlena);
+		add_function_cost(context->root, iofunc, NULL,
+						  &context->total);
+	}
+	else if (IsA(node, ArrayCoerceExpr))
+	{
+		ArrayCoerceExpr *acoerce = (ArrayCoerceExpr *) node;
+		QualCost	perelemcost;
+
+		cost_qual_eval_node(&perelemcost, (Node *) acoerce->elemexpr,
+							context->root);
+		context->total.startup += perelemcost.startup;
+		if (perelemcost.per_tuple > 0)
+			context->total.per_tuple += perelemcost.per_tuple *
+				estimate_array_length((Node *) acoerce->arg);
+	}
+	else if (IsA(node, RowCompareExpr))
+	{
+		/* Conservatively assume we will check all the columns */
+		RowCompareExpr *rcexpr = (RowCompareExpr *) node;
+		ListCell   *lc;
+
+		foreach(lc, rcexpr->opnos)
+		{
+			Oid			opid = lfirst_oid(lc);
+
+			add_function_cost(context->root, get_opcode(opid), NULL,
+							  &context->total);
+		}
+	}
+	else if (IsA(node, MinMaxExpr) ||
+			 IsA(node, SQLValueFunction) ||
+			 IsA(node, XmlExpr) ||
+			 IsA(node, CoerceToDomain) ||
+			 IsA(node, NextValueExpr))
+	{
+		/* Treat all these as having cost 1 */
+		context->total.per_tuple += cpu_operator_cost;
+	}
+	else if (IsA(node, CurrentOfExpr))
+	{
+		/* Report high cost to prevent selection of anything but TID scan */
+		context->total.startup += disable_cost;
+	}
+	else if (IsA(node, SubLink))
+	{
+		/* This routine should not be applied to un-planned expressions */
+		elog(ERROR, "cannot handle unplanned sub-select");
+	}
+	else if (IsA(node, SubPlan))
+	{
+		/*
+		 * A subplan node in an expression typically indicates that the
+		 * subplan will be executed on each evaluation, so charge accordingly.
+		 * (Sub-selects that can be executed as InitPlans have already been
+		 * removed from the expression.)
+		 */
+		SubPlan    *subplan = (SubPlan *) node;
+
+		context->total.startup += subplan->startup_cost;
+		context->total.per_tuple += subplan->per_call_cost;
+
+		/*
+		 * We don't want to recurse into the testexpr, because it was already
+		 * counted in the SubPlan node's costs.  So we're done.
+		 */
+		return false;
+	}
+	else if (IsA(node, AlternativeSubPlan))
+	{
+		/*
+		 * Arbitrarily use the first alternative plan for costing.  (We should
+		 * certainly only include one alternative, and we don't yet have
+		 * enough information to know which one the executor is most likely to
+		 * use.)
+		 */
+		AlternativeSubPlan *asplan = (AlternativeSubPlan *) node;
+
+		return cost_qual_eval_walker((Node *) linitial(asplan->subplans),
+									 context);
+	}
+	else if (IsA(node, PlaceHolderVar))
+	{
+		/*
+		 * A PlaceHolderVar should be given cost zero when considering general
+		 * expression evaluation costs.  The expense of doing the contained
+		 * expression is charged as part of the tlist eval costs of the scan
+		 * or join where the PHV is first computed (see set_rel_width and
+		 * add_placeholders_to_joinrel).  If we charged it again here, we'd be
+		 * double-counting the cost for each level of plan that the PHV
+		 * bubbles up through.  Hence, return without recursing into the
+		 * phexpr.
+		 */
+		return false;
+	}
+
+	/* recurse into children */
+	return expression_tree_walker(node, cost_qual_eval_walker,
+								  (void *) context);
+}
+
+/*
+ * get_restriction_qual_cost
+ *	  Compute evaluation costs of a baserel's restriction quals, plus any
+ *	  movable join quals that have been pushed down to the scan.
+ *	  Results are returned into *qpqual_cost.
+ *
+ * This is a convenience subroutine that works for seqscans and other cases
+ * where all the given quals will be evaluated the hard way.  It's not useful
+ * for cost_index(), for example, where the index machinery takes care of
+ * some of the quals.  We assume baserestrictcost was previously set by
+ * set_baserel_size_estimates().
+ */
+static void
+get_restriction_qual_cost(PlannerInfo *root, RelOptInfo *baserel,
+						  ParamPathInfo *param_info,
+						  QualCost *qpqual_cost)
+{
+	if (param_info)
+	{
+		/* Include costs of pushed-down clauses */
+		cost_qual_eval(qpqual_cost, param_info->ppi_clauses, root);
+
+		qpqual_cost->startup += baserel->baserestrictcost.startup;
+		qpqual_cost->per_tuple += baserel->baserestrictcost.per_tuple;
+	}
+	else
+		*qpqual_cost = baserel->baserestrictcost;
+}
+
+
+/*
+ * compute_semi_anti_join_factors
+ *	  Estimate how much of the inner input a SEMI, ANTI, or inner_unique join
+ *	  can be expected to scan.
+ *
+ * In a hash or nestloop SEMI/ANTI join, the executor will stop scanning
+ * inner rows as soon as it finds a match to the current outer row.
+ * The same happens if we have detected the inner rel is unique.
+ * We should therefore adjust some of the cost components for this effect.
+ * This function computes some estimates needed for these adjustments.
+ * These estimates will be the same regardless of the particular paths used
+ * for the outer and inner relation, so we compute these once and then pass
+ * them to all the join cost estimation functions.
+ *
+ * Input parameters:
+ *	joinrel: join relation under consideration
+ *	outerrel: outer relation under consideration
+ *	innerrel: inner relation under consideration
+ *	jointype: if not JOIN_SEMI or JOIN_ANTI, we assume it's inner_unique
+ *	sjinfo: SpecialJoinInfo relevant to this join
+ *	restrictlist: join quals
+ * Output parameters:
+ *	*semifactors is filled in (see pathnodes.h for field definitions)
+ */
+void
+compute_semi_anti_join_factors(PlannerInfo *root,
+							   RelOptInfo *joinrel,
+							   RelOptInfo *outerrel,
+							   RelOptInfo *innerrel,
+							   JoinType jointype,
+							   SpecialJoinInfo *sjinfo,
+							   List *restrictlist,
+							   SemiAntiJoinFactors *semifactors)
+{
+	Selectivity jselec;
+	Selectivity nselec;
+	Selectivity avgmatch;
+	SpecialJoinInfo norm_sjinfo;
+	List	   *joinquals;
+	ListCell   *l;
+
+	/*
+	 * In an ANTI join, we must ignore clauses that are "pushed down", since
+	 * those won't affect the match logic.  In a SEMI join, we do not
+	 * distinguish joinquals from "pushed down" quals, so just use the whole
+	 * restrictinfo list.  For other outer join types, we should consider only
+	 * non-pushed-down quals, so that this devolves to an IS_OUTER_JOIN check.
+	 */
+	if (IS_OUTER_JOIN(jointype))
+	{
+		joinquals = NIL;
+		foreach(l, restrictlist)
+		{
+			RestrictInfo *rinfo = lfirst_node(RestrictInfo, l);
+
+			if (!RINFO_IS_PUSHED_DOWN(rinfo, joinrel->relids))
+				joinquals = lappend(joinquals, rinfo);
+		}
+	}
+	else
+		joinquals = restrictlist;
+
+	/*
+	 * Get the JOIN_SEMI or JOIN_ANTI selectivity of the join clauses.
+	 */
+	jselec = clauselist_selectivity(root,
+									joinquals,
+									0,
+									(jointype == JOIN_ANTI) ? JOIN_ANTI : JOIN_SEMI,
+									sjinfo);
+
+	/*
+	 * Also get the normal inner-join selectivity of the join clauses.
+	 */
+	norm_sjinfo.type = T_SpecialJoinInfo;
+	norm_sjinfo.min_lefthand = outerrel->relids;
+	norm_sjinfo.min_righthand = innerrel->relids;
+	norm_sjinfo.syn_lefthand = outerrel->relids;
+	norm_sjinfo.syn_righthand = innerrel->relids;
+	norm_sjinfo.jointype = JOIN_INNER;
+	/* we don't bother trying to make the remaining fields valid */
+	norm_sjinfo.lhs_strict = false;
+	norm_sjinfo.delay_upper_joins = false;
+	norm_sjinfo.semi_can_btree = false;
+	norm_sjinfo.semi_can_hash = false;
+	norm_sjinfo.semi_operators = NIL;
+	norm_sjinfo.semi_rhs_exprs = NIL;
+
+	nselec = clauselist_selectivity(root,
+									joinquals,
+									0,
+									JOIN_INNER,
+									&norm_sjinfo);
+
+	/* Avoid leaking a lot of ListCells */
+	if (IS_OUTER_JOIN(jointype))
+		list_free(joinquals);
+
+	/*
+	 * jselec can be interpreted as the fraction of outer-rel rows that have
+	 * any matches (this is true for both SEMI and ANTI cases).  And nselec is
+	 * the fraction of the Cartesian product that matches.  So, the average
+	 * number of matches for each outer-rel row that has at least one match is
+	 * nselec * inner_rows / jselec.
+	 *
+	 * Note: it is correct to use the inner rel's "rows" count here, even
+	 * though we might later be considering a parameterized inner path with
+	 * fewer rows.  This is because we have included all the join clauses in
+	 * the selectivity estimate.
+	 */
+	if (jselec > 0)				/* protect against zero divide */
+	{
+		avgmatch = nselec * innerrel->rows / jselec;
+		/* Clamp to sane range */
+		avgmatch = Max(1.0, avgmatch);
+	}
+	else
+		avgmatch = 1.0;
+
+	semifactors->outer_match_frac = jselec;
+	semifactors->match_count = avgmatch;
+}
+
+/*
+ * has_indexed_join_quals
+ *	  Check whether all the joinquals of a nestloop join are used as
+ *	  inner index quals.
+ *
+ * If the inner path of a SEMI/ANTI join is an indexscan (including bitmap
+ * indexscan) that uses all the joinquals as indexquals, we can assume that an
+ * unmatched outer tuple is cheap to process, whereas otherwise it's probably
+ * expensive.
+ */
+static bool
+has_indexed_join_quals(NestPath *joinpath)
+{
+	Relids		joinrelids = joinpath->path.parent->relids;
+	Path	   *innerpath = joinpath->innerjoinpath;
+	List	   *indexclauses;
+	bool		found_one;
+	ListCell   *lc;
+
+	/* If join still has quals to evaluate, it's not fast */
+	if (joinpath->joinrestrictinfo != NIL)
+		return false;
+	/* Nor if the inner path isn't parameterized at all */
+	if (innerpath->param_info == NULL)
+		return false;
+
+	/* Find the indexclauses list for the inner scan */
+	switch (innerpath->pathtype)
+	{
+		case T_IndexScan:
+		case T_IndexOnlyScan:
+			indexclauses = ((IndexPath *) innerpath)->indexclauses;
+			break;
+		case T_BitmapHeapScan:
+			{
+				/* Accept only a simple bitmap scan, not AND/OR cases */
+				Path	   *bmqual = ((BitmapHeapPath *) innerpath)->bitmapqual;
+
+				if (IsA(bmqual, IndexPath))
+					indexclauses = ((IndexPath *) bmqual)->indexclauses;
+				else
+					return false;
+				break;
+			}
+		default:
+
+			/*
+			 * If it's not a simple indexscan, it probably doesn't run quickly
+			 * for zero rows out, even if it's a parameterized path using all
+			 * the joinquals.
+			 */
+			return false;
+	}
+
+	/*
+	 * Examine the inner path's param clauses.  Any that are from the outer
+	 * path must be found in the indexclauses list, either exactly or in an
+	 * equivalent form generated by equivclass.c.  Also, we must find at least
+	 * one such clause, else it's a clauseless join which isn't fast.
+	 */
+	found_one = false;
+	foreach(lc, innerpath->param_info->ppi_clauses)
+	{
+		RestrictInfo *rinfo = (RestrictInfo *) lfirst(lc);
+
+		if (join_clause_is_movable_into(rinfo,
+										innerpath->parent->relids,
+										joinrelids))
+		{
+			if (!is_redundant_with_indexclauses(rinfo, indexclauses))
+				return false;
+			found_one = true;
+		}
+	}
+	return found_one;
+}
+
+
+/*
+ * approx_tuple_count
+ *		Quick-and-dirty estimation of the number of join rows passing
+ *		a set of qual conditions.
+ *
+ * The quals can be either an implicitly-ANDed list of boolean expressions,
+ * or a list of RestrictInfo nodes (typically the latter).
+ *
+ * We intentionally compute the selectivity under JOIN_INNER rules, even
+ * if it's some type of outer join.  This is appropriate because we are
+ * trying to figure out how many tuples pass the initial merge or hash
+ * join step.
+ *
+ * This is quick-and-dirty because we bypass clauselist_selectivity, and
+ * simply multiply the independent clause selectivities together.  Now
+ * clauselist_selectivity often can't do any better than that anyhow, but
+ * for some situations (such as range constraints) it is smarter.  However,
+ * we can't effectively cache the results of clauselist_selectivity, whereas
+ * the individual clause selectivities can be and are cached.
+ *
+ * Since we are only using the results to estimate how many potential
+ * output tuples are generated and passed through qpqual checking, it
+ * seems OK to live with the approximation.
+ */
+static double
+approx_tuple_count(PlannerInfo *root, JoinPath *path, List *quals)
+{
+	double		tuples;
+	double		outer_tuples = path->outerjoinpath->rows;
+	double		inner_tuples = path->innerjoinpath->rows;
+	SpecialJoinInfo sjinfo;
+	Selectivity selec = 1.0;
+	ListCell   *l;
+
+	/*
+	 * Make up a SpecialJoinInfo for JOIN_INNER semantics.
+	 */
+	sjinfo.type = T_SpecialJoinInfo;
+	sjinfo.min_lefthand = path->outerjoinpath->parent->relids;
+	sjinfo.min_righthand = path->innerjoinpath->parent->relids;
+	sjinfo.syn_lefthand = path->outerjoinpath->parent->relids;
+	sjinfo.syn_righthand = path->innerjoinpath->parent->relids;
+	sjinfo.jointype = JOIN_INNER;
+	/* we don't bother trying to make the remaining fields valid */
+	sjinfo.lhs_strict = false;
+	sjinfo.delay_upper_joins = false;
+	sjinfo.semi_can_btree = false;
+	sjinfo.semi_can_hash = false;
+	sjinfo.semi_operators = NIL;
+	sjinfo.semi_rhs_exprs = NIL;
+
+	/* Get the approximate selectivity */
+	foreach(l, quals)
+	{
+		Node	   *qual = (Node *) lfirst(l);
+
+		/* Note that clause_selectivity will be able to cache its result */
+		selec *= clause_selectivity(root, qual, 0, JOIN_INNER, &sjinfo);
+	}
+
+	/* Apply it to the input relation sizes */
+	tuples = selec * outer_tuples * inner_tuples;
+
+	return clamp_row_est(tuples);
+}
+
+
+/*
+ * set_baserel_size_estimates
+ *		Set the size estimates for the given base relation.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already, and rel->tuples must be set.
+ *
+ * We set the following fields of the rel node:
+ *	rows: the estimated number of output tuples (after applying
+ *		  restriction clauses).
+ *	width: the estimated average output tuple width in bytes.
+ *	baserestrictcost: estimated cost of evaluating baserestrictinfo clauses.
+ */
+void
+set_baserel_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	double		nrows;
+
+	/* Should only be applied to base relations */
+	Assert(rel->relid > 0);
+
+	nrows = rel->tuples *
+		clauselist_selectivity(root,
+							   rel->baserestrictinfo,
+							   0,
+							   JOIN_INNER,
+							   NULL);
+
+	rel->rows = clamp_row_est(nrows);
+
+	cost_qual_eval(&rel->baserestrictcost, rel->baserestrictinfo, root);
+
+	set_rel_width(root, rel);
+}
+
+/*
+ * get_parameterized_baserel_size
+ *		Make a size estimate for a parameterized scan of a base relation.
+ *
+ * 'param_clauses' lists the additional join clauses to be used.
+ *
+ * set_baserel_size_estimates must have been applied already.
+ */
+double
+get_parameterized_baserel_size(PlannerInfo *root, RelOptInfo *rel,
+							   List *param_clauses)
+{
+	List	   *allclauses;
+	double		nrows;
+
+	/*
+	 * Estimate the number of rows returned by the parameterized scan, knowing
+	 * that it will apply all the extra join clauses as well as the rel's own
+	 * restriction clauses.  Note that we force the clauses to be treated as
+	 * non-join clauses during selectivity estimation.
+	 */
+	allclauses = list_concat(list_copy(param_clauses),
+							 rel->baserestrictinfo);
+	nrows = rel->tuples *
+		clauselist_selectivity(root,
+							   allclauses,
+							   rel->relid,	/* do not use 0! */
+							   JOIN_INNER,
+							   NULL);
+	nrows = clamp_row_est(nrows);
+	/* For safety, make sure result is not more than the base estimate */
+	if (nrows > rel->rows)
+		nrows = rel->rows;
+	return nrows;
+}
+
+/*
+ * set_joinrel_size_estimates
+ *		Set the size estimates for the given join relation.
+ *
+ * The rel's targetlist must have been constructed already, and a
+ * restriction clause list that matches the given component rels must
+ * be provided.
+ *
+ * Since there is more than one way to make a joinrel for more than two
+ * base relations, the results we get here could depend on which component
+ * rel pair is provided.  In theory we should get the same answers no matter
+ * which pair is provided; in practice, since the selectivity estimation
+ * routines don't handle all cases equally well, we might not.  But there's
+ * not much to be done about it.  (Would it make sense to repeat the
+ * calculations for each pair of input rels that's encountered, and somehow
+ * average the results?  Probably way more trouble than it's worth, and
+ * anyway we must keep the rowcount estimate the same for all paths for the
+ * joinrel.)
+ *
+ * We set only the rows field here.  The reltarget field was already set by
+ * build_joinrel_tlist, and baserestrictcost is not used for join rels.
+ */
+void
+set_joinrel_size_estimates(PlannerInfo *root, RelOptInfo *rel,
+						   RelOptInfo *outer_rel,
+						   RelOptInfo *inner_rel,
+						   SpecialJoinInfo *sjinfo,
+						   List *restrictlist)
+{
+	rel->rows = calc_joinrel_size_estimate(root,
+										   rel,
+										   outer_rel,
+										   inner_rel,
+										   outer_rel->rows,
+										   inner_rel->rows,
+										   sjinfo,
+										   restrictlist);
+}
+
+/*
+ * get_parameterized_joinrel_size
+ *		Make a size estimate for a parameterized scan of a join relation.
+ *
+ * 'rel' is the joinrel under consideration.
+ * 'outer_path', 'inner_path' are (probably also parameterized) Paths that
+ *		produce the relations being joined.
+ * 'sjinfo' is any SpecialJoinInfo relevant to this join.
+ * 'restrict_clauses' lists the join clauses that need to be applied at the
+ * join node (including any movable clauses that were moved down to this join,
+ * and not including any movable clauses that were pushed down into the
+ * child paths).
+ *
+ * set_joinrel_size_estimates must have been applied already.
+ */
+double
+get_parameterized_joinrel_size(PlannerInfo *root, RelOptInfo *rel,
+							   Path *outer_path,
+							   Path *inner_path,
+							   SpecialJoinInfo *sjinfo,
+							   List *restrict_clauses)
+{
+	double		nrows;
+
+	/*
+	 * Estimate the number of rows returned by the parameterized join as the
+	 * sizes of the input paths times the selectivity of the clauses that have
+	 * ended up at this join node.
+	 *
+	 * As with set_joinrel_size_estimates, the rowcount estimate could depend
+	 * on the pair of input paths provided, though ideally we'd get the same
+	 * estimate for any pair with the same parameterization.
+	 */
+	nrows = calc_joinrel_size_estimate(root,
+									   rel,
+									   outer_path->parent,
+									   inner_path->parent,
+									   outer_path->rows,
+									   inner_path->rows,
+									   sjinfo,
+									   restrict_clauses);
+	/* For safety, make sure result is not more than the base estimate */
+	if (nrows > rel->rows)
+		nrows = rel->rows;
+	return nrows;
+}
+
+/*
+ * calc_joinrel_size_estimate
+ *		Workhorse for set_joinrel_size_estimates and
+ *		get_parameterized_joinrel_size.
+ *
+ * outer_rel/inner_rel are the relations being joined, but they should be
+ * assumed to have sizes outer_rows/inner_rows; those numbers might be less
+ * than what rel->rows says, when we are considering parameterized paths.
+ */
+static double
+calc_joinrel_size_estimate(PlannerInfo *root,
+						   RelOptInfo *joinrel,
+						   RelOptInfo *outer_rel,
+						   RelOptInfo *inner_rel,
+						   double outer_rows,
+						   double inner_rows,
+						   SpecialJoinInfo *sjinfo,
+						   List *restrictlist_in)
+{
+	/* This apparently-useless variable dodges a compiler bug in VS2013: */
+	List	   *restrictlist = restrictlist_in;
+	JoinType	jointype = sjinfo->jointype;
+	Selectivity fkselec;
+	Selectivity jselec;
+	Selectivity pselec;
+	double		nrows;
+
+	/*
+	 * Compute joinclause selectivity.  Note that we are only considering
+	 * clauses that become restriction clauses at this join level; we are not
+	 * double-counting them because they were not considered in estimating the
+	 * sizes of the component rels.
+	 *
+	 * First, see whether any of the joinclauses can be matched to known FK
+	 * constraints.  If so, drop those clauses from the restrictlist, and
+	 * instead estimate their selectivity using FK semantics.  (We do this
+	 * without regard to whether said clauses are local or "pushed down".
+	 * Probably, an FK-matching clause could never be seen as pushed down at
+	 * an outer join, since it would be strict and hence would be grounds for
+	 * join strength reduction.)  fkselec gets the net selectivity for
+	 * FK-matching clauses, or 1.0 if there are none.
+	 */
+	fkselec = get_foreign_key_join_selectivity(root,
+											   outer_rel->relids,
+											   inner_rel->relids,
+											   sjinfo,
+											   &restrictlist);
+
+	/*
+	 * For an outer join, we have to distinguish the selectivity of the join's
+	 * own clauses (JOIN/ON conditions) from any clauses that were "pushed
+	 * down".  For inner joins we just count them all as joinclauses.
+	 */
+	if (IS_OUTER_JOIN(jointype))
+	{
+		List	   *joinquals = NIL;
+		List	   *pushedquals = NIL;
+		ListCell   *l;
+
+		/* Grovel through the clauses to separate into two lists */
+		foreach(l, restrictlist)
+		{
+			RestrictInfo *rinfo = lfirst_node(RestrictInfo, l);
+
+			if (RINFO_IS_PUSHED_DOWN(rinfo, joinrel->relids))
+				pushedquals = lappend(pushedquals, rinfo);
+			else
+				joinquals = lappend(joinquals, rinfo);
+		}
+
+		/* Get the separate selectivities */
+		jselec = clauselist_selectivity(root,
+										joinquals,
+										0,
+										jointype,
+										sjinfo);
+		pselec = clauselist_selectivity(root,
+										pushedquals,
+										0,
+										jointype,
+										sjinfo);
+
+		/* Avoid leaking a lot of ListCells */
+		list_free(joinquals);
+		list_free(pushedquals);
+	}
+	else
+	{
+		jselec = clauselist_selectivity(root,
+										restrictlist,
+										0,
+										jointype,
+										sjinfo);
+		pselec = 0.0;			/* not used, keep compiler quiet */
+	}
+
+	/*
+	 * Basically, we multiply size of Cartesian product by selectivity.
+	 *
+	 * If we are doing an outer join, take that into account: the joinqual
+	 * selectivity has to be clamped using the knowledge that the output must
+	 * be at least as large as the non-nullable input.  However, any
+	 * pushed-down quals are applied after the outer join, so their
+	 * selectivity applies fully.
+	 *
+	 * For JOIN_SEMI and JOIN_ANTI, the selectivity is defined as the fraction
+	 * of LHS rows that have matches, and we apply that straightforwardly.
+	 */
+	switch (jointype)
+	{
+		case JOIN_INNER:
+			nrows = outer_rows * inner_rows * fkselec * jselec;
+			/* pselec not used */
+			break;
+		case JOIN_LEFT:
+			nrows = outer_rows * inner_rows * fkselec * jselec;
+			if (nrows < outer_rows)
+				nrows = outer_rows;
+			nrows *= pselec;
+			break;
+		case JOIN_FULL:
+			nrows = outer_rows * inner_rows * fkselec * jselec;
+			if (nrows < outer_rows)
+				nrows = outer_rows;
+			if (nrows < inner_rows)
+				nrows = inner_rows;
+			nrows *= pselec;
+			break;
+		case JOIN_SEMI:
+			nrows = outer_rows * fkselec * jselec;
+			/* pselec not used */
+			break;
+		case JOIN_ANTI:
+			nrows = outer_rows * (1.0 - fkselec * jselec);
+			nrows *= pselec;
+			break;
+		default:
+			/* other values not expected here */
+			elog(ERROR, "unrecognized join type: %d", (int) jointype);
+			nrows = 0;			/* keep compiler quiet */
+			break;
+	}
+
+	return clamp_row_est(nrows);
+}
+
+/*
+ * get_foreign_key_join_selectivity
+ *		Estimate join selectivity for foreign-key-related clauses.
+ *
+ * Remove any clauses that can be matched to FK constraints from *restrictlist,
+ * and return a substitute estimate of their selectivity.  1.0 is returned
+ * when there are no such clauses.
+ *
+ * The reason for treating such clauses specially is that we can get better
+ * estimates this way than by relying on clauselist_selectivity(), especially
+ * for multi-column FKs where that function's assumption that the clauses are
+ * independent falls down badly.  But even with single-column FKs, we may be
+ * able to get a better answer when the pg_statistic stats are missing or out
+ * of date.
+ */
+static Selectivity
+get_foreign_key_join_selectivity(PlannerInfo *root,
+								 Relids outer_relids,
+								 Relids inner_relids,
+								 SpecialJoinInfo *sjinfo,
+								 List **restrictlist)
+{
+	Selectivity fkselec = 1.0;
+	JoinType	jointype = sjinfo->jointype;
+	List	   *worklist = *restrictlist;
+	ListCell   *lc;
+
+	/* Consider each FK constraint that is known to match the query */
+	foreach(lc, root->fkey_list)
+	{
+		ForeignKeyOptInfo *fkinfo = (ForeignKeyOptInfo *) lfirst(lc);
+		bool		ref_is_outer;
+		List	   *removedlist;
+		ListCell   *cell;
+		ListCell   *prev;
+		ListCell   *next;
+
+		/*
+		 * This FK is not relevant unless it connects a baserel on one side of
+		 * this join to a baserel on the other side.
+		 */
+		if (bms_is_member(fkinfo->con_relid, outer_relids) &&
+			bms_is_member(fkinfo->ref_relid, inner_relids))
+			ref_is_outer = false;
+		else if (bms_is_member(fkinfo->ref_relid, outer_relids) &&
+				 bms_is_member(fkinfo->con_relid, inner_relids))
+			ref_is_outer = true;
+		else
+			continue;
+
+		/*
+		 * If we're dealing with a semi/anti join, and the FK's referenced
+		 * relation is on the outside, then knowledge of the FK doesn't help
+		 * us figure out what we need to know (which is the fraction of outer
+		 * rows that have matches).  On the other hand, if the referenced rel
+		 * is on the inside, then all outer rows must have matches in the
+		 * referenced table (ignoring nulls).  But any restriction or join
+		 * clauses that filter that table will reduce the fraction of matches.
+		 * We can account for restriction clauses, but it's too hard to guess
+		 * how many table rows would get through a join that's inside the RHS.
+		 * Hence, if either case applies, punt and ignore the FK.
+		 */
+		if ((jointype == JOIN_SEMI || jointype == JOIN_ANTI) &&
+			(ref_is_outer || bms_membership(inner_relids) != BMS_SINGLETON))
+			continue;
+
+		/*
+		 * Modify the restrictlist by removing clauses that match the FK (and
+		 * putting them into removedlist instead).  It seems unsafe to modify
+		 * the originally-passed List structure, so we make a shallow copy the
+		 * first time through.
+		 */
+		if (worklist == *restrictlist)
+			worklist = list_copy(worklist);
+
+		removedlist = NIL;
+		prev = NULL;
+		for (cell = list_head(worklist); cell; cell = next)
+		{
+			RestrictInfo *rinfo = (RestrictInfo *) lfirst(cell);
+			bool		remove_it = false;
+			int			i;
+
+			next = lnext(cell);
+			/* Drop this clause if it matches any column of the FK */
+			for (i = 0; i < fkinfo->nkeys; i++)
+			{
+				if (rinfo->parent_ec)
+				{
+					/*
+					 * EC-derived clauses can only match by EC.  It is okay to
+					 * consider any clause derived from the same EC as
+					 * matching the FK: even if equivclass.c chose to generate
+					 * a clause equating some other pair of Vars, it could
+					 * have generated one equating the FK's Vars.  So for
+					 * purposes of estimation, we can act as though it did so.
+					 *
+					 * Note: checking parent_ec is a bit of a cheat because
+					 * there are EC-derived clauses that don't have parent_ec
+					 * set; but such clauses must compare expressions that
+					 * aren't just Vars, so they cannot match the FK anyway.
+					 */
+					if (fkinfo->eclass[i] == rinfo->parent_ec)
+					{
+						remove_it = true;
+						break;
+					}
+				}
+				else
+				{
+					/*
+					 * Otherwise, see if rinfo was previously matched to FK as
+					 * a "loose" clause.
+					 */
+					if (list_member_ptr(fkinfo->rinfos[i], rinfo))
+					{
+						remove_it = true;
+						break;
+					}
+				}
+			}
+			if (remove_it)
+			{
+				worklist = list_delete_cell(worklist, cell, prev);
+				removedlist = lappend(removedlist, rinfo);
+			}
+			else
+				prev = cell;
+		}
+
+		/*
+		 * If we failed to remove all the matching clauses we expected to
+		 * find, chicken out and ignore this FK; applying its selectivity
+		 * might result in double-counting.  Put any clauses we did manage to
+		 * remove back into the worklist.
+		 *
+		 * Since the matching clauses are known not outerjoin-delayed, they
+		 * should certainly have appeared in the initial joinclause list.  If
+		 * we didn't find them, they must have been matched to, and removed
+		 * by, some other FK in a previous iteration of this loop.  (A likely
+		 * case is that two FKs are matched to the same EC; there will be only
+		 * one EC-derived clause in the initial list, so the first FK will
+		 * consume it.)  Applying both FKs' selectivity independently risks
+		 * underestimating the join size; in particular, this would undo one
+		 * of the main things that ECs were invented for, namely to avoid
+		 * double-counting the selectivity of redundant equality conditions.
+		 * Later we might think of a reasonable way to combine the estimates,
+		 * but for now, just punt, since this is a fairly uncommon situation.
+		 */
+		if (list_length(removedlist) !=
+			(fkinfo->nmatched_ec + fkinfo->nmatched_ri))
+		{
+			worklist = list_concat(worklist, removedlist);
+			continue;
+		}
+
+		/*
+		 * Finally we get to the payoff: estimate selectivity using the
+		 * knowledge that each referencing row will match exactly one row in
+		 * the referenced table.
+		 *
+		 * XXX that's not true in the presence of nulls in the referencing
+		 * column(s), so in principle we should derate the estimate for those.
+		 * However (1) if there are any strict restriction clauses for the
+		 * referencing column(s) elsewhere in the query, derating here would
+		 * be double-counting the null fraction, and (2) it's not very clear
+		 * how to combine null fractions for multiple referencing columns. So
+		 * we do nothing for now about correcting for nulls.
+		 *
+		 * XXX another point here is that if either side of an FK constraint
+		 * is an inheritance parent, we estimate as though the constraint
+		 * covers all its children as well.  This is not an unreasonable
+		 * assumption for a referencing table, ie the user probably applied
+		 * identical constraints to all child tables (though perhaps we ought
+		 * to check that).  But it's not possible to have done that for a
+		 * referenced table.  Fortunately, precisely because that doesn't
+		 * work, it is uncommon in practice to have an FK referencing a parent
+		 * table.  So, at least for now, disregard inheritance here.
+		 */
+		if (jointype == JOIN_SEMI || jointype == JOIN_ANTI)
+		{
+			/*
+			 * For JOIN_SEMI and JOIN_ANTI, we only get here when the FK's
+			 * referenced table is exactly the inside of the join.  The join
+			 * selectivity is defined as the fraction of LHS rows that have
+			 * matches.  The FK implies that every LHS row has a match *in the
+			 * referenced table*; but any restriction clauses on it will
+			 * reduce the number of matches.  Hence we take the join
+			 * selectivity as equal to the selectivity of the table's
+			 * restriction clauses, which is rows / tuples; but we must guard
+			 * against tuples == 0.
+			 */
+			RelOptInfo *ref_rel = find_base_rel(root, fkinfo->ref_relid);
+			double		ref_tuples = Max(ref_rel->tuples, 1.0);
+
+			fkselec *= ref_rel->rows / ref_tuples;
+		}
+		else
+		{
+			/*
+			 * Otherwise, selectivity is exactly 1/referenced-table-size; but
+			 * guard against tuples == 0.  Note we should use the raw table
+			 * tuple count, not any estimate of its filtered or joined size.
+			 */
+			RelOptInfo *ref_rel = find_base_rel(root, fkinfo->ref_relid);
+			double		ref_tuples = Max(ref_rel->tuples, 1.0);
+
+			fkselec *= 1.0 / ref_tuples;
+		}
+	}
+
+	*restrictlist = worklist;
+	return fkselec;
+}
+
+/*
+ * set_subquery_size_estimates
+ *		Set the size estimates for a base relation that is a subquery.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already, and the Paths for the subquery must have been completed.
+ * We look at the subquery's PlannerInfo to extract data.
+ *
+ * We set the same fields as set_baserel_size_estimates.
+ */
+void
+set_subquery_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	PlannerInfo *subroot = rel->subroot;
+	RelOptInfo *sub_final_rel;
+	ListCell   *lc;
+
+	/* Should only be applied to base relations that are subqueries */
+	Assert(rel->relid > 0);
+	Assert(planner_rt_fetch(rel->relid, root)->rtekind == RTE_SUBQUERY);
+
+	/*
+	 * Copy raw number of output rows from subquery.  All of its paths should
+	 * have the same output rowcount, so just look at cheapest-total.
+	 */
+	sub_final_rel = fetch_upper_rel(subroot, UPPERREL_FINAL, NULL);
+	rel->tuples = sub_final_rel->cheapest_total_path->rows;
+
+	/*
+	 * Compute per-output-column width estimates by examining the subquery's
+	 * targetlist.  For any output that is a plain Var, get the width estimate
+	 * that was made while planning the subquery.  Otherwise, we leave it to
+	 * set_rel_width to fill in a datatype-based default estimate.
+	 */
+	foreach(lc, subroot->parse->targetList)
+	{
+		TargetEntry *te = lfirst_node(TargetEntry, lc);
+		Node	   *texpr = (Node *) te->expr;
+		int32		item_width = 0;
+
+		/* junk columns aren't visible to upper query */
+		if (te->resjunk)
+			continue;
+
+		/*
+		 * The subquery could be an expansion of a view that's had columns
+		 * added to it since the current query was parsed, so that there are
+		 * non-junk tlist columns in it that don't correspond to any column
+		 * visible at our query level.  Ignore such columns.
+		 */
+		if (te->resno < rel->min_attr || te->resno > rel->max_attr)
+			continue;
+
+		/*
+		 * XXX This currently doesn't work for subqueries containing set
+		 * operations, because the Vars in their tlists are bogus references
+		 * to the first leaf subquery, which wouldn't give the right answer
+		 * even if we could still get to its PlannerInfo.
+		 *
+		 * Also, the subquery could be an appendrel for which all branches are
+		 * known empty due to constraint exclusion, in which case
+		 * set_append_rel_pathlist will have left the attr_widths set to zero.
+		 *
+		 * In either case, we just leave the width estimate zero until
+		 * set_rel_width fixes it.
+		 */
+		if (IsA(texpr, Var) &&
+			subroot->parse->setOperations == NULL)
+		{
+			Var		   *var = (Var *) texpr;
+			RelOptInfo *subrel = find_base_rel(subroot, var->varno);
+
+			item_width = subrel->attr_widths[var->varattno - subrel->min_attr];
+		}
+		rel->attr_widths[te->resno - rel->min_attr] = item_width;
+	}
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_function_size_estimates
+ *		Set the size estimates for a base relation that is a function call.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already.
+ *
+ * We set the same fields as set_baserel_size_estimates.
+ */
+void
+set_function_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	RangeTblEntry *rte;
+	ListCell   *lc;
+
+	/* Should only be applied to base relations that are functions */
+	Assert(rel->relid > 0);
+	rte = planner_rt_fetch(rel->relid, root);
+	Assert(rte->rtekind == RTE_FUNCTION);
+
+	/*
+	 * Estimate number of rows the functions will return. The rowcount of the
+	 * node is that of the largest function result.
+	 */
+	rel->tuples = 0;
+	foreach(lc, rte->functions)
+	{
+		RangeTblFunction *rtfunc = (RangeTblFunction *) lfirst(lc);
+		double		ntup = expression_returns_set_rows(root, rtfunc->funcexpr);
+
+		if (ntup > rel->tuples)
+			rel->tuples = ntup;
+	}
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_function_size_estimates
+ *		Set the size estimates for a base relation that is a function call.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already.
+ *
+ * We set the same fields as set_tablefunc_size_estimates.
+ */
+void
+set_tablefunc_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	/* Should only be applied to base relations that are functions */
+	Assert(rel->relid > 0);
+	Assert(planner_rt_fetch(rel->relid, root)->rtekind == RTE_TABLEFUNC);
+
+	rel->tuples = 100;
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_values_size_estimates
+ *		Set the size estimates for a base relation that is a values list.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already.
+ *
+ * We set the same fields as set_baserel_size_estimates.
+ */
+void
+set_values_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	RangeTblEntry *rte;
+
+	/* Should only be applied to base relations that are values lists */
+	Assert(rel->relid > 0);
+	rte = planner_rt_fetch(rel->relid, root);
+	Assert(rte->rtekind == RTE_VALUES);
+
+	/*
+	 * Estimate number of rows the values list will return. We know this
+	 * precisely based on the list length (well, barring set-returning
+	 * functions in list items, but that's a refinement not catered for
+	 * anywhere else either).
+	 */
+	rel->tuples = list_length(rte->values_lists);
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_cte_size_estimates
+ *		Set the size estimates for a base relation that is a CTE reference.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already, and we need an estimate of the number of rows returned by the CTE
+ * (if a regular CTE) or the non-recursive term (if a self-reference).
+ *
+ * We set the same fields as set_baserel_size_estimates.
+ */
+void
+set_cte_size_estimates(PlannerInfo *root, RelOptInfo *rel, double cte_rows)
+{
+	RangeTblEntry *rte;
+
+	/* Should only be applied to base relations that are CTE references */
+	Assert(rel->relid > 0);
+	rte = planner_rt_fetch(rel->relid, root);
+	Assert(rte->rtekind == RTE_CTE);
+
+	if (rte->self_reference)
+	{
+		/*
+		 * In a self-reference, arbitrarily assume the average worktable size
+		 * is about 10 times the nonrecursive term's size.
+		 */
+		rel->tuples = 10 * cte_rows;
+	}
+	else
+	{
+		/* Otherwise just believe the CTE's rowcount estimate */
+		rel->tuples = cte_rows;
+	}
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_namedtuplestore_size_estimates
+ *		Set the size estimates for a base relation that is a tuplestore reference.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already.
+ *
+ * We set the same fields as set_baserel_size_estimates.
+ */
+void
+set_namedtuplestore_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	RangeTblEntry *rte;
+
+	/* Should only be applied to base relations that are tuplestore references */
+	Assert(rel->relid > 0);
+	rte = planner_rt_fetch(rel->relid, root);
+	Assert(rte->rtekind == RTE_NAMEDTUPLESTORE);
+
+	/*
+	 * Use the estimate provided by the code which is generating the named
+	 * tuplestore.  In some cases, the actual number might be available; in
+	 * others the same plan will be re-used, so a "typical" value might be
+	 * estimated and used.
+	 */
+	rel->tuples = rte->enrtuples;
+	if (rel->tuples < 0)
+		rel->tuples = 1000;
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_result_size_estimates
+ *		Set the size estimates for an RTE_RESULT base relation
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already.
+ *
+ * We set the same fields as set_baserel_size_estimates.
+ */
+void
+set_result_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	/* Should only be applied to RTE_RESULT base relations */
+	Assert(rel->relid > 0);
+	Assert(planner_rt_fetch(rel->relid, root)->rtekind == RTE_RESULT);
+
+	/* RTE_RESULT always generates a single row, natively */
+	rel->tuples = 1;
+
+	/* Now estimate number of output rows, etc */
+	set_baserel_size_estimates(root, rel);
+}
+
+/*
+ * set_foreign_size_estimates
+ *		Set the size estimates for a base relation that is a foreign table.
+ *
+ * There is not a whole lot that we can do here; the foreign-data wrapper
+ * is responsible for producing useful estimates.  We can do a decent job
+ * of estimating baserestrictcost, so we set that, and we also set up width
+ * using what will be purely datatype-driven estimates from the targetlist.
+ * There is no way to do anything sane with the rows value, so we just put
+ * a default estimate and hope that the wrapper can improve on it.  The
+ * wrapper's GetForeignRelSize function will be called momentarily.
+ *
+ * The rel's targetlist and restrictinfo list must have been constructed
+ * already.
+ */
+void
+set_foreign_size_estimates(PlannerInfo *root, RelOptInfo *rel)
+{
+	/* Should only be applied to base relations */
+	Assert(rel->relid > 0);
+
+	rel->rows = 1000;			/* entirely bogus default estimate */
+
+	cost_qual_eval(&rel->baserestrictcost, rel->baserestrictinfo, root);
+
+	set_rel_width(root, rel);
+}
+
+
+/*
+ * set_rel_width
+ *		Set the estimated output width of a base relation.
+ *
+ * The estimated output width is the sum of the per-attribute width estimates
+ * for the actually-referenced columns, plus any PHVs or other expressions
+ * that have to be calculated at this relation.  This is the amount of data
+ * we'd need to pass upwards in case of a sort, hash, etc.
+ *
+ * This function also sets reltarget->cost, so it's a bit misnamed now.
+ *
+ * NB: this works best on plain relations because it prefers to look at
+ * real Vars.  For subqueries, set_subquery_size_estimates will already have
+ * copied up whatever per-column estimates were made within the subquery,
+ * and for other types of rels there isn't much we can do anyway.  We fall
+ * back on (fairly stupid) datatype-based width estimates if we can't get
+ * any better number.
+ *
+ * The per-attribute width estimates are cached for possible re-use while
+ * building join relations or post-scan/join pathtargets.
+ */
+static void
+set_rel_width(PlannerInfo *root, RelOptInfo *rel)
+{
+	Oid			reloid = planner_rt_fetch(rel->relid, root)->relid;
+	int32		tuple_width = 0;
+	bool		have_wholerow_var = false;
+	ListCell   *lc;
+
+	/* Vars are assumed to have cost zero, but other exprs do not */
+	rel->reltarget->cost.startup = 0;
+	rel->reltarget->cost.per_tuple = 0;
+
+	foreach(lc, rel->reltarget->exprs)
+	{
+		Node	   *node = (Node *) lfirst(lc);
+
+		/*
+		 * Ordinarily, a Var in a rel's targetlist must belong to that rel;
+		 * but there are corner cases involving LATERAL references where that
+		 * isn't so.  If the Var has the wrong varno, fall through to the
+		 * generic case (it doesn't seem worth the trouble to be any smarter).
+		 */
+		if (IsA(node, Var) &&
+			((Var *) node)->varno == rel->relid)
+		{
+			Var		   *var = (Var *) node;
+			int			ndx;
+			int32		item_width;
+
+			Assert(var->varattno >= rel->min_attr);
+			Assert(var->varattno <= rel->max_attr);
+
+			ndx = var->varattno - rel->min_attr;
+
+			/*
+			 * If it's a whole-row Var, we'll deal with it below after we have
+			 * already cached as many attr widths as possible.
+			 */
+			if (var->varattno == 0)
+			{
+				have_wholerow_var = true;
+				continue;
+			}
+
+			/*
+			 * The width may have been cached already (especially if it's a
+			 * subquery), so don't duplicate effort.
+			 */
+			if (rel->attr_widths[ndx] > 0)
+			{
+				tuple_width += rel->attr_widths[ndx];
+				continue;
+			}
+
+			/* Try to get column width from statistics */
+			if (reloid != InvalidOid && var->varattno > 0)
+			{
+				item_width = get_attavgwidth(reloid, var->varattno);
+				if (item_width > 0)
+				{
+					rel->attr_widths[ndx] = item_width;
+					tuple_width += item_width;
+					continue;
+				}
+			}
+
+			/*
+			 * Not a plain relation, or can't find statistics for it. Estimate
+			 * using just the type info.
+			 */
+			item_width = get_typavgwidth(var->vartype, var->vartypmod);
+			Assert(item_width > 0);
+			rel->attr_widths[ndx] = item_width;
+			tuple_width += item_width;
+		}
+		else if (IsA(node, PlaceHolderVar))
+		{
+			/*
+			 * We will need to evaluate the PHV's contained expression while
+			 * scanning this rel, so be sure to include it in reltarget->cost.
+			 */
+			PlaceHolderVar *phv = (PlaceHolderVar *) node;
+			PlaceHolderInfo *phinfo = find_placeholder_info(root, phv, false);
+			QualCost	cost;
+
+			tuple_width += phinfo->ph_width;
+			cost_qual_eval_node(&cost, (Node *) phv->phexpr, root);
+			rel->reltarget->cost.startup += cost.startup;
+			rel->reltarget->cost.per_tuple += cost.per_tuple;
+		}
+		else
+		{
+			/*
+			 * We could be looking at an expression pulled up from a subquery,
+			 * or a ROW() representing a whole-row child Var, etc.  Do what we
+			 * can using the expression type information.
+			 */
+			int32		item_width;
+			QualCost	cost;
+
+			item_width = get_typavgwidth(exprType(node), exprTypmod(node));
+			Assert(item_width > 0);
+			tuple_width += item_width;
+			/* Not entirely clear if we need to account for cost, but do so */
+			cost_qual_eval_node(&cost, node, root);
+			rel->reltarget->cost.startup += cost.startup;
+			rel->reltarget->cost.per_tuple += cost.per_tuple;
+		}
+	}
+
+	/*
+	 * If we have a whole-row reference, estimate its width as the sum of
+	 * per-column widths plus heap tuple header overhead.
+	 */
+	if (have_wholerow_var)
+	{
+		int32		wholerow_width = MAXALIGN(SizeofHeapTupleHeader);
+
+		if (reloid != InvalidOid)
+		{
+			/* Real relation, so estimate true tuple width */
+			wholerow_width += get_relation_data_width(reloid,
+													  rel->attr_widths - rel->min_attr);
+		}
+		else
+		{
+			/* Do what we can with info for a phony rel */
+			AttrNumber	i;
+
+			for (i = 1; i <= rel->max_attr; i++)
+				wholerow_width += rel->attr_widths[i - rel->min_attr];
+		}
+
+		rel->attr_widths[0 - rel->min_attr] = wholerow_width;
+
+		/*
+		 * Include the whole-row Var as part of the output tuple.  Yes, that
+		 * really is what happens at runtime.
+		 */
+		tuple_width += wholerow_width;
+	}
+
+	Assert(tuple_width >= 0);
+	rel->reltarget->width = tuple_width;
+}
+
+/*
+ * set_pathtarget_cost_width
+ *		Set the estimated eval cost and output width of a PathTarget tlist.
+ *
+ * As a notational convenience, returns the same PathTarget pointer passed in.
+ *
+ * Most, though not quite all, uses of this function occur after we've run
+ * set_rel_width() for base relations; so we can usually obtain cached width
+ * estimates for Vars.  If we can't, fall back on datatype-based width
+ * estimates.  Present early-planning uses of PathTargets don't need accurate
+ * widths badly enough to justify going to the catalogs for better data.
+ */
+PathTarget *
+set_pathtarget_cost_width(PlannerInfo *root, PathTarget *target)
+{
+	int32		tuple_width = 0;
+	ListCell   *lc;
+
+	/* Vars are assumed to have cost zero, but other exprs do not */
+	target->cost.startup = 0;
+	target->cost.per_tuple = 0;
+
+	foreach(lc, target->exprs)
+	{
+		Node	   *node = (Node *) lfirst(lc);
+
+		if (IsA(node, Var))
+		{
+			Var		   *var = (Var *) node;
+			int32		item_width;
+
+			/* We should not see any upper-level Vars here */
+			Assert(var->varlevelsup == 0);
+
+			/* Try to get data from RelOptInfo cache */
+			if (var->varno < root->simple_rel_array_size)
+			{
+				RelOptInfo *rel = root->simple_rel_array[var->varno];
+
+				if (rel != NULL &&
+					var->varattno >= rel->min_attr &&
+					var->varattno <= rel->max_attr)
+				{
+					int			ndx = var->varattno - rel->min_attr;
+
+					if (rel->attr_widths[ndx] > 0)
+					{
+						tuple_width += rel->attr_widths[ndx];
+						continue;
+					}
+				}
+			}
+
+			/*
+			 * No cached data available, so estimate using just the type info.
+			 */
+			item_width = get_typavgwidth(var->vartype, var->vartypmod);
+			Assert(item_width > 0);
+			tuple_width += item_width;
+		}
+		else
+		{
+			/*
+			 * Handle general expressions using type info.
+			 */
+			int32		item_width;
+			QualCost	cost;
+
+			item_width = get_typavgwidth(exprType(node), exprTypmod(node));
+			Assert(item_width > 0);
+			tuple_width += item_width;
+
+			/* Account for cost, too */
+			cost_qual_eval_node(&cost, node, root);
+			target->cost.startup += cost.startup;
+			target->cost.per_tuple += cost.per_tuple;
+		}
+	}
+
+	Assert(tuple_width >= 0);
+	target->width = tuple_width;
+
+	return target;
+}
+
+/*
+ * relation_byte_size
+ *	  Estimate the storage space in bytes for a given number of tuples
+ *	  of a given width (size in bytes).
+ */
+static double
+relation_byte_size(double tuples, int width)
+{
+	return tuples * (MAXALIGN(width) + MAXALIGN(SizeofHeapTupleHeader));
+}
+
+/*
+ * page_size
+ *	  Returns an estimate of the number of pages covered by a given
+ *	  number of tuples of a given width (size in bytes).
+ */
+static double
+page_size(double tuples, int width)
+{
+	return ceil(relation_byte_size(tuples, width) / BLCKSZ);
+}
+
+/*
+ * Estimate the fraction of the work that each worker will do given the
+ * number of workers budgeted for the path.
+ */
+static double
+get_parallel_divisor(Path *path)
+{
+	double		parallel_divisor = path->parallel_workers;
+
+	/*
+	 * Early experience with parallel query suggests that when there is only
+	 * one worker, the leader often makes a very substantial contribution to
+	 * executing the parallel portion of the plan, but as more workers are
+	 * added, it does less and less, because it's busy reading tuples from the
+	 * workers and doing whatever non-parallel post-processing is needed.  By
+	 * the time we reach 4 workers, the leader no longer makes a meaningful
+	 * contribution.  Thus, for now, estimate that the leader spends 30% of
+	 * its time servicing each worker, and the remainder executing the
+	 * parallel plan.
+	 */
+	if (parallel_leader_participation)
+	{
+		double		leader_contribution;
+
+		leader_contribution = 1.0 - (0.3 * path->parallel_workers);
+		if (leader_contribution > 0)
+			parallel_divisor += leader_contribution;
+	}
+
+	return parallel_divisor;
+}
+
+/*
+ * compute_bitmap_pages
+ *
+ * compute number of pages fetched from heap in bitmap heap scan.
+ */
+double
+compute_bitmap_pages(PlannerInfo *root, RelOptInfo *baserel, Path *bitmapqual,
+					 int loop_count, Cost *cost, double *tuple)
+{
+	Cost		indexTotalCost;
+	Selectivity indexSelectivity;
+	double		T;
+	double		pages_fetched;
+	double		tuples_fetched;
+	double		heap_pages;
+	long		maxentries;
+
+	/*
+	 * Fetch total cost of obtaining the bitmap, as well as its total
+	 * selectivity.
+	 */
+	cost_bitmap_tree_node(bitmapqual, &indexTotalCost, &indexSelectivity);
+
+	/*
+	 * Estimate number of main-table pages fetched.
+	 */
+	tuples_fetched = clamp_row_est(indexSelectivity * baserel->tuples);
+
+	T = (baserel->pages > 1) ? (double) baserel->pages : 1.0;
+
+	/*
+	 * For a single scan, the number of heap pages that need to be fetched is
+	 * the same as the Mackert and Lohman formula for the case T <= b (ie, no
+	 * re-reads needed).
+	 */
+	pages_fetched = (2.0 * T * tuples_fetched) / (2.0 * T + tuples_fetched);
+
+	/*
+	 * Calculate the number of pages fetched from the heap.  Then based on
+	 * current work_mem estimate get the estimated maxentries in the bitmap.
+	 * (Note that we always do this calculation based on the number of pages
+	 * that would be fetched in a single iteration, even if loop_count > 1.
+	 * That's correct, because only that number of entries will be stored in
+	 * the bitmap at one time.)
+	 */
+	heap_pages = Min(pages_fetched, baserel->pages);
+	maxentries = tbm_calculate_entries(work_mem * 1024L);
+
+	if (loop_count > 1)
+	{
+		/*
+		 * For repeated bitmap scans, scale up the number of tuples fetched in
+		 * the Mackert and Lohman formula by the number of scans, so that we
+		 * estimate the number of pages fetched by all the scans. Then
+		 * pro-rate for one scan.
+		 */
+		pages_fetched = index_pages_fetched(tuples_fetched * loop_count,
+											baserel->pages,
+											get_indexpath_pages(bitmapqual),
+											root);
+		pages_fetched /= loop_count;
+	}
+
+	if (pages_fetched >= T)
+		pages_fetched = T;
+	else
+		pages_fetched = ceil(pages_fetched);
+
+	if (maxentries < heap_pages)
+	{
+		double		exact_pages;
+		double		lossy_pages;
+
+		/*
+		 * Crude approximation of the number of lossy pages.  Because of the
+		 * way tbm_lossify() is coded, the number of lossy pages increases
+		 * very sharply as soon as we run short of memory; this formula has
+		 * that property and seems to perform adequately in testing, but it's
+		 * possible we could do better somehow.
+		 */
+		lossy_pages = Max(0, heap_pages - maxentries / 2);
+		exact_pages = heap_pages - lossy_pages;
+
+		/*
+		 * If there are lossy pages then recompute the  number of tuples
+		 * processed by the bitmap heap node.  We assume here that the chance
+		 * of a given tuple coming from an exact page is the same as the
+		 * chance that a given page is exact.  This might not be true, but
+		 * it's not clear how we can do any better.
+		 */
+		if (lossy_pages > 0)
+			tuples_fetched =
+				clamp_row_est(indexSelectivity *
+							  (exact_pages / heap_pages) * baserel->tuples +
+							  (lossy_pages / heap_pages) * baserel->tuples);
+	}
+
+	if (cost)
+		*cost = indexTotalCost;
+	if (tuple)
+		*tuple = tuples_fetched;
+
+	return pages_fetched;
+}
diff --git a/src/backend/optimizer/plan/planner.c b/src/backend/optimizer/plan/planner.c
index 65a9b0d..12344a3 100644
--- a/src/backend/optimizer/plan/planner.c
+++ b/src/backend/optimizer/plan/planner.c
@@ -64,6 +64,7 @@
 #include "utils/selfuncs.h"
 #include "utils/lsyscache.h"
 #include "utils/syscache.h"
+#include "lero/lero_extension.h"
 
 
 /* GUC parameters */
@@ -265,19 +266,30 @@ static int	common_prefix_cmp(const void *a, const void *b);
  *
  *****************************************************************************/
 PlannedStmt *
-planner(Query *parse, int cursorOptions, ParamListInfo boundParams)
+planner(Query *parse, const char *query_string, int cursorOptions, ParamListInfo boundParams)
 {
 	PlannedStmt *result;
 
+	// if (planner_hook)
+	// 	result = (*planner_hook) (parse, cursorOptions, boundParams);
+	// else
+	// 	result = standard_planner(parse, cursorOptions, boundParams);
+
 	if (planner_hook)
-		result = (*planner_hook) (parse, cursorOptions, boundParams);
-	else
-		result = standard_planner(parse, cursorOptions, boundParams);
+		result = (*planner_hook) (parse, query_string, cursorOptions, boundParams);
+	else {
+		if (enable_lero) {
+			result = lero_pgsysml_hook_planner(parse, query_string, cursorOptions, boundParams);
+		} else{
+			result = standard_planner(parse, query_string, cursorOptions, boundParams);
+		}
+	}
+
 	return result;
 }
 
 PlannedStmt *
-standard_planner(Query *parse, int cursorOptions, ParamListInfo boundParams)
+standard_planner(Query *parse, const char *query_string, int cursorOptions, ParamListInfo boundParams)
 {
 	PlannedStmt *result;
 	PlannerGlobal *glob;
diff --git a/src/backend/optimizer/plan/planner.c.orig b/src/backend/optimizer/plan/planner.c.orig
new file mode 100644
index 0000000..65a9b0d
--- /dev/null
+++ b/src/backend/optimizer/plan/planner.c.orig
@@ -0,0 +1,7406 @@
+/*-------------------------------------------------------------------------
+ *
+ * planner.c
+ *	  The query optimizer external interface.
+ *
+ * Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group
+ * Portions Copyright (c) 1994, Regents of the University of California
+ *
+ *
+ * IDENTIFICATION
+ *	  src/backend/optimizer/plan/planner.c
+ *
+ *-------------------------------------------------------------------------
+ */
+
+#include "postgres.h"
+
+#include <limits.h>
+#include <math.h>
+
+#include "access/genam.h"
+#include "access/htup_details.h"
+#include "access/parallel.h"
+#include "access/sysattr.h"
+#include "access/table.h"
+#include "access/xact.h"
+#include "catalog/pg_constraint.h"
+#include "catalog/pg_inherits.h"
+#include "catalog/pg_proc.h"
+#include "catalog/pg_type.h"
+#include "executor/executor.h"
+#include "executor/nodeAgg.h"
+#include "foreign/fdwapi.h"
+#include "miscadmin.h"
+#include "jit/jit.h"
+#include "lib/bipartite_match.h"
+#include "lib/knapsack.h"
+#include "nodes/makefuncs.h"
+#include "nodes/nodeFuncs.h"
+#ifdef OPTIMIZER_DEBUG
+#include "nodes/print.h"
+#endif
+#include "optimizer/appendinfo.h"
+#include "optimizer/clauses.h"
+#include "optimizer/cost.h"
+#include "optimizer/inherit.h"
+#include "optimizer/optimizer.h"
+#include "optimizer/paramassign.h"
+#include "optimizer/pathnode.h"
+#include "optimizer/paths.h"
+#include "optimizer/plancat.h"
+#include "optimizer/planmain.h"
+#include "optimizer/planner.h"
+#include "optimizer/prep.h"
+#include "optimizer/subselect.h"
+#include "optimizer/tlist.h"
+#include "parser/analyze.h"
+#include "parser/parsetree.h"
+#include "parser/parse_agg.h"
+#include "partitioning/partdesc.h"
+#include "rewrite/rewriteManip.h"
+#include "storage/dsm_impl.h"
+#include "utils/rel.h"
+#include "utils/selfuncs.h"
+#include "utils/lsyscache.h"
+#include "utils/syscache.h"
+
+
+/* GUC parameters */
+double		cursor_tuple_fraction = DEFAULT_CURSOR_TUPLE_FRACTION;
+int			force_parallel_mode = FORCE_PARALLEL_OFF;
+bool		parallel_leader_participation = true;
+
+/* Hook for plugins to get control in planner() */
+planner_hook_type planner_hook = NULL;
+
+/* Hook for plugins to get control when grouping_planner() plans upper rels */
+create_upper_paths_hook_type create_upper_paths_hook = NULL;
+
+
+/* Expression kind codes for preprocess_expression */
+#define EXPRKIND_QUAL				0
+#define EXPRKIND_TARGET				1
+#define EXPRKIND_RTFUNC				2
+#define EXPRKIND_RTFUNC_LATERAL		3
+#define EXPRKIND_VALUES				4
+#define EXPRKIND_VALUES_LATERAL		5
+#define EXPRKIND_LIMIT				6
+#define EXPRKIND_APPINFO			7
+#define EXPRKIND_PHV				8
+#define EXPRKIND_TABLESAMPLE		9
+#define EXPRKIND_ARBITER_ELEM		10
+#define EXPRKIND_TABLEFUNC			11
+#define EXPRKIND_TABLEFUNC_LATERAL	12
+
+/* Passthrough data for standard_qp_callback */
+typedef struct
+{
+	List	   *activeWindows;	/* active windows, if any */
+	List	   *groupClause;	/* overrides parse->groupClause */
+} standard_qp_extra;
+
+/*
+ * Data specific to grouping sets
+ */
+
+typedef struct
+{
+	List	   *rollups;
+	List	   *hash_sets_idx;
+	double		dNumHashGroups;
+	bool		any_hashable;
+	Bitmapset  *unsortable_refs;
+	Bitmapset  *unhashable_refs;
+	List	   *unsortable_sets;
+	int		   *tleref_to_colnum_map;
+} grouping_sets_data;
+
+/*
+ * Temporary structure for use during WindowClause reordering in order to be
+ * able to sort WindowClauses on partitioning/ordering prefix.
+ */
+typedef struct
+{
+	WindowClause *wc;
+	List	   *uniqueOrder;	/* A List of unique ordering/partitioning
+								 * clauses per Window */
+} WindowClauseSortData;
+
+/* Local functions */
+static Node *preprocess_expression(PlannerInfo *root, Node *expr, int kind);
+static void preprocess_qual_conditions(PlannerInfo *root, Node *jtnode);
+static void inheritance_planner(PlannerInfo *root);
+static void grouping_planner(PlannerInfo *root, bool inheritance_update,
+							 double tuple_fraction);
+static grouping_sets_data *preprocess_grouping_sets(PlannerInfo *root);
+static List *remap_to_groupclause_idx(List *groupClause, List *gsets,
+									  int *tleref_to_colnum_map);
+static void preprocess_rowmarks(PlannerInfo *root);
+static double preprocess_limit(PlannerInfo *root,
+							   double tuple_fraction,
+							   int64 *offset_est, int64 *count_est);
+static void remove_useless_groupby_columns(PlannerInfo *root);
+static List *preprocess_groupclause(PlannerInfo *root, List *force);
+static List *extract_rollup_sets(List *groupingSets);
+static List *reorder_grouping_sets(List *groupingSets, List *sortclause);
+static void standard_qp_callback(PlannerInfo *root, void *extra);
+static double get_number_of_groups(PlannerInfo *root,
+								   double path_rows,
+								   grouping_sets_data *gd,
+								   List *target_list);
+static RelOptInfo *create_grouping_paths(PlannerInfo *root,
+										 RelOptInfo *input_rel,
+										 PathTarget *target,
+										 bool target_parallel_safe,
+										 const AggClauseCosts *agg_costs,
+										 grouping_sets_data *gd);
+static bool is_degenerate_grouping(PlannerInfo *root);
+static void create_degenerate_grouping_paths(PlannerInfo *root,
+											 RelOptInfo *input_rel,
+											 RelOptInfo *grouped_rel);
+static RelOptInfo *make_grouping_rel(PlannerInfo *root, RelOptInfo *input_rel,
+									 PathTarget *target, bool target_parallel_safe,
+									 Node *havingQual);
+static void create_ordinary_grouping_paths(PlannerInfo *root,
+										   RelOptInfo *input_rel,
+										   RelOptInfo *grouped_rel,
+										   const AggClauseCosts *agg_costs,
+										   grouping_sets_data *gd,
+										   GroupPathExtraData *extra,
+										   RelOptInfo **partially_grouped_rel_p);
+static void consider_groupingsets_paths(PlannerInfo *root,
+										RelOptInfo *grouped_rel,
+										Path *path,
+										bool is_sorted,
+										bool can_hash,
+										grouping_sets_data *gd,
+										const AggClauseCosts *agg_costs,
+										double dNumGroups);
+static RelOptInfo *create_window_paths(PlannerInfo *root,
+									   RelOptInfo *input_rel,
+									   PathTarget *input_target,
+									   PathTarget *output_target,
+									   bool output_target_parallel_safe,
+									   WindowFuncLists *wflists,
+									   List *activeWindows);
+static void create_one_window_path(PlannerInfo *root,
+								   RelOptInfo *window_rel,
+								   Path *path,
+								   PathTarget *input_target,
+								   PathTarget *output_target,
+								   WindowFuncLists *wflists,
+								   List *activeWindows);
+static RelOptInfo *create_distinct_paths(PlannerInfo *root,
+										 RelOptInfo *input_rel);
+static RelOptInfo *create_ordered_paths(PlannerInfo *root,
+										RelOptInfo *input_rel,
+										PathTarget *target,
+										bool target_parallel_safe,
+										double limit_tuples);
+static PathTarget *make_group_input_target(PlannerInfo *root,
+										   PathTarget *final_target);
+static PathTarget *make_partial_grouping_target(PlannerInfo *root,
+												PathTarget *grouping_target,
+												Node *havingQual);
+static List *postprocess_setop_tlist(List *new_tlist, List *orig_tlist);
+static List *select_active_windows(PlannerInfo *root, WindowFuncLists *wflists);
+static PathTarget *make_window_input_target(PlannerInfo *root,
+											PathTarget *final_target,
+											List *activeWindows);
+static List *make_pathkeys_for_window(PlannerInfo *root, WindowClause *wc,
+									  List *tlist);
+static PathTarget *make_sort_input_target(PlannerInfo *root,
+										  PathTarget *final_target,
+										  bool *have_postponed_srfs);
+static void adjust_paths_for_srfs(PlannerInfo *root, RelOptInfo *rel,
+								  List *targets, List *targets_contain_srfs);
+static void add_paths_to_grouping_rel(PlannerInfo *root, RelOptInfo *input_rel,
+									  RelOptInfo *grouped_rel,
+									  RelOptInfo *partially_grouped_rel,
+									  const AggClauseCosts *agg_costs,
+									  grouping_sets_data *gd,
+									  double dNumGroups,
+									  GroupPathExtraData *extra);
+static RelOptInfo *create_partial_grouping_paths(PlannerInfo *root,
+												 RelOptInfo *grouped_rel,
+												 RelOptInfo *input_rel,
+												 grouping_sets_data *gd,
+												 GroupPathExtraData *extra,
+												 bool force_rel_creation);
+static void gather_grouping_paths(PlannerInfo *root, RelOptInfo *rel);
+static bool can_partial_agg(PlannerInfo *root,
+							const AggClauseCosts *agg_costs);
+static void apply_scanjoin_target_to_paths(PlannerInfo *root,
+										   RelOptInfo *rel,
+										   List *scanjoin_targets,
+										   List *scanjoin_targets_contain_srfs,
+										   bool scanjoin_target_parallel_safe,
+										   bool tlist_same_exprs);
+static void create_partitionwise_grouping_paths(PlannerInfo *root,
+												RelOptInfo *input_rel,
+												RelOptInfo *grouped_rel,
+												RelOptInfo *partially_grouped_rel,
+												const AggClauseCosts *agg_costs,
+												grouping_sets_data *gd,
+												PartitionwiseAggregateType patype,
+												GroupPathExtraData *extra);
+static bool group_by_has_partkey(RelOptInfo *input_rel,
+								 List *targetList,
+								 List *groupClause);
+static int	common_prefix_cmp(const void *a, const void *b);
+
+
+/*****************************************************************************
+ *
+ *	   Query optimizer entry point
+ *
+ * To support loadable plugins that monitor or modify planner behavior,
+ * we provide a hook variable that lets a plugin get control before and
+ * after the standard planning process.  The plugin would normally call
+ * standard_planner().
+ *
+ * Note to plugin authors: standard_planner() scribbles on its Query input,
+ * so you'd better copy that data structure if you want to plan more than once.
+ *
+ *****************************************************************************/
+PlannedStmt *
+planner(Query *parse, int cursorOptions, ParamListInfo boundParams)
+{
+	PlannedStmt *result;
+
+	if (planner_hook)
+		result = (*planner_hook) (parse, cursorOptions, boundParams);
+	else
+		result = standard_planner(parse, cursorOptions, boundParams);
+	return result;
+}
+
+PlannedStmt *
+standard_planner(Query *parse, int cursorOptions, ParamListInfo boundParams)
+{
+	PlannedStmt *result;
+	PlannerGlobal *glob;
+	double		tuple_fraction;
+	PlannerInfo *root;
+	RelOptInfo *final_rel;
+	Path	   *best_path;
+	Plan	   *top_plan;
+	ListCell   *lp,
+			   *lr;
+
+	/*
+	 * Set up global state for this planner invocation.  This data is needed
+	 * across all levels of sub-Query that might exist in the given command,
+	 * so we keep it in a separate struct that's linked to by each per-Query
+	 * PlannerInfo.
+	 */
+	glob = makeNode(PlannerGlobal);
+
+	glob->boundParams = boundParams;
+	glob->subplans = NIL;
+	glob->subroots = NIL;
+	glob->rewindPlanIDs = NULL;
+	glob->finalrtable = NIL;
+	glob->finalrowmarks = NIL;
+	glob->resultRelations = NIL;
+	glob->rootResultRelations = NIL;
+	glob->relationOids = NIL;
+	glob->invalItems = NIL;
+	glob->paramExecTypes = NIL;
+	glob->lastPHId = 0;
+	glob->lastRowMarkId = 0;
+	glob->lastPlanNodeId = 0;
+	glob->transientPlan = false;
+	glob->dependsOnRole = false;
+
+	/*
+	 * Assess whether it's feasible to use parallel mode for this query. We
+	 * can't do this in a standalone backend, or if the command will try to
+	 * modify any data, or if this is a cursor operation, or if GUCs are set
+	 * to values that don't permit parallelism, or if parallel-unsafe
+	 * functions are present in the query tree.
+	 *
+	 * (Note that we do allow CREATE TABLE AS, SELECT INTO, and CREATE
+	 * MATERIALIZED VIEW to use parallel plans, but this is safe only because
+	 * the command is writing into a completely new table which workers won't
+	 * be able to see.  If the workers could see the table, the fact that
+	 * group locking would cause them to ignore the leader's heavyweight
+	 * relation extension lock and GIN page locks would make this unsafe.
+	 * We'll have to fix that somehow if we want to allow parallel inserts in
+	 * general; updates and deletes have additional problems especially around
+	 * combo CIDs.)
+	 *
+	 * For now, we don't try to use parallel mode if we're running inside a
+	 * parallel worker.  We might eventually be able to relax this
+	 * restriction, but for now it seems best not to have parallel workers
+	 * trying to create their own parallel workers.
+	 */
+	if ((cursorOptions & CURSOR_OPT_PARALLEL_OK) != 0 &&
+		IsUnderPostmaster &&
+		parse->commandType == CMD_SELECT &&
+		!parse->hasModifyingCTE &&
+		max_parallel_workers_per_gather > 0 &&
+		!IsParallelWorker())
+	{
+		/* all the cheap tests pass, so scan the query tree */
+		glob->maxParallelHazard = max_parallel_hazard(parse);
+		glob->parallelModeOK = (glob->maxParallelHazard != PROPARALLEL_UNSAFE);
+	}
+	else
+	{
+		/* skip the query tree scan, just assume it's unsafe */
+		glob->maxParallelHazard = PROPARALLEL_UNSAFE;
+		glob->parallelModeOK = false;
+	}
+
+	/*
+	 * glob->parallelModeNeeded is normally set to false here and changed to
+	 * true during plan creation if a Gather or Gather Merge plan is actually
+	 * created (cf. create_gather_plan, create_gather_merge_plan).
+	 *
+	 * However, if force_parallel_mode = on or force_parallel_mode = regress,
+	 * then we impose parallel mode whenever it's safe to do so, even if the
+	 * final plan doesn't use parallelism.  It's not safe to do so if the
+	 * query contains anything parallel-unsafe; parallelModeOK will be false
+	 * in that case.  Note that parallelModeOK can't change after this point.
+	 * Otherwise, everything in the query is either parallel-safe or
+	 * parallel-restricted, and in either case it should be OK to impose
+	 * parallel-mode restrictions.  If that ends up breaking something, then
+	 * either some function the user included in the query is incorrectly
+	 * labelled as parallel-safe or parallel-restricted when in reality it's
+	 * parallel-unsafe, or else the query planner itself has a bug.
+	 */
+	glob->parallelModeNeeded = glob->parallelModeOK &&
+		(force_parallel_mode != FORCE_PARALLEL_OFF);
+
+	/* Determine what fraction of the plan is likely to be scanned */
+	if (cursorOptions & CURSOR_OPT_FAST_PLAN)
+	{
+		/*
+		 * We have no real idea how many tuples the user will ultimately FETCH
+		 * from a cursor, but it is often the case that he doesn't want 'em
+		 * all, or would prefer a fast-start plan anyway so that he can
+		 * process some of the tuples sooner.  Use a GUC parameter to decide
+		 * what fraction to optimize for.
+		 */
+		tuple_fraction = cursor_tuple_fraction;
+
+		/*
+		 * We document cursor_tuple_fraction as simply being a fraction, which
+		 * means the edge cases 0 and 1 have to be treated specially here.  We
+		 * convert 1 to 0 ("all the tuples") and 0 to a very small fraction.
+		 */
+		if (tuple_fraction >= 1.0)
+			tuple_fraction = 0.0;
+		else if (tuple_fraction <= 0.0)
+			tuple_fraction = 1e-10;
+	}
+	else
+	{
+		/* Default assumption is we need all the tuples */
+		tuple_fraction = 0.0;
+	}
+
+	/* primary planning entry point (may recurse for subqueries) */
+	root = subquery_planner(glob, parse, NULL,
+							false, tuple_fraction);
+
+	/* Select best Path and turn it into a Plan */
+	final_rel = fetch_upper_rel(root, UPPERREL_FINAL, NULL);
+	best_path = get_cheapest_fractional_path(final_rel, tuple_fraction);
+
+	top_plan = create_plan(root, best_path);
+
+	/*
+	 * If creating a plan for a scrollable cursor, make sure it can run
+	 * backwards on demand.  Add a Material node at the top at need.
+	 */
+	if (cursorOptions & CURSOR_OPT_SCROLL)
+	{
+		if (!ExecSupportsBackwardScan(top_plan))
+			top_plan = materialize_finished_plan(top_plan);
+	}
+
+	/*
+	 * Optionally add a Gather node for testing purposes, provided this is
+	 * actually a safe thing to do.
+	 */
+	if (force_parallel_mode != FORCE_PARALLEL_OFF && top_plan->parallel_safe)
+	{
+		Gather	   *gather = makeNode(Gather);
+
+		/*
+		 * Top plan must not have any initPlans, else it shouldn't have been
+		 * marked parallel-safe.
+		 */
+		Assert(top_plan->initPlan == NIL);
+
+		gather->plan.targetlist = top_plan->targetlist;
+		gather->plan.qual = NIL;
+		gather->plan.lefttree = top_plan;
+		gather->plan.righttree = NULL;
+		gather->num_workers = 1;
+		gather->single_copy = true;
+		gather->invisible = (force_parallel_mode == FORCE_PARALLEL_REGRESS);
+
+		/*
+		 * Since this Gather has no parallel-aware descendants to signal to,
+		 * we don't need a rescan Param.
+		 */
+		gather->rescan_param = -1;
+
+		/*
+		 * Ideally we'd use cost_gather here, but setting up dummy path data
+		 * to satisfy it doesn't seem much cleaner than knowing what it does.
+		 */
+		gather->plan.startup_cost = top_plan->startup_cost +
+			parallel_setup_cost;
+		gather->plan.total_cost = top_plan->total_cost +
+			parallel_setup_cost + parallel_tuple_cost * top_plan->plan_rows;
+		gather->plan.plan_rows = top_plan->plan_rows;
+		gather->plan.plan_width = top_plan->plan_width;
+		gather->plan.parallel_aware = false;
+		gather->plan.parallel_safe = false;
+
+		/* use parallel mode for parallel plans. */
+		root->glob->parallelModeNeeded = true;
+
+		top_plan = &gather->plan;
+	}
+
+	/*
+	 * If any Params were generated, run through the plan tree and compute
+	 * each plan node's extParam/allParam sets.  Ideally we'd merge this into
+	 * set_plan_references' tree traversal, but for now it has to be separate
+	 * because we need to visit subplans before not after main plan.
+	 */
+	if (glob->paramExecTypes != NIL)
+	{
+		Assert(list_length(glob->subplans) == list_length(glob->subroots));
+		forboth(lp, glob->subplans, lr, glob->subroots)
+		{
+			Plan	   *subplan = (Plan *) lfirst(lp);
+			PlannerInfo *subroot = lfirst_node(PlannerInfo, lr);
+
+			SS_finalize_plan(subroot, subplan);
+		}
+		SS_finalize_plan(root, top_plan);
+	}
+
+	/* final cleanup of the plan */
+	Assert(glob->finalrtable == NIL);
+	Assert(glob->finalrowmarks == NIL);
+	Assert(glob->resultRelations == NIL);
+	Assert(glob->rootResultRelations == NIL);
+	top_plan = set_plan_references(root, top_plan);
+	/* ... and the subplans (both regular subplans and initplans) */
+	Assert(list_length(glob->subplans) == list_length(glob->subroots));
+	forboth(lp, glob->subplans, lr, glob->subroots)
+	{
+		Plan	   *subplan = (Plan *) lfirst(lp);
+		PlannerInfo *subroot = lfirst_node(PlannerInfo, lr);
+
+		lfirst(lp) = set_plan_references(subroot, subplan);
+	}
+
+	/* build the PlannedStmt result */
+	result = makeNode(PlannedStmt);
+
+	result->commandType = parse->commandType;
+	result->queryId = parse->queryId;
+	result->hasReturning = (parse->returningList != NIL);
+	result->hasModifyingCTE = parse->hasModifyingCTE;
+	result->canSetTag = parse->canSetTag;
+	result->transientPlan = glob->transientPlan;
+	result->dependsOnRole = glob->dependsOnRole;
+	result->parallelModeNeeded = glob->parallelModeNeeded;
+	result->planTree = top_plan;
+	result->rtable = glob->finalrtable;
+	result->resultRelations = glob->resultRelations;
+	result->rootResultRelations = glob->rootResultRelations;
+	result->subplans = glob->subplans;
+	result->rewindPlanIDs = glob->rewindPlanIDs;
+	result->rowMarks = glob->finalrowmarks;
+	result->relationOids = glob->relationOids;
+	result->invalItems = glob->invalItems;
+	result->paramExecTypes = glob->paramExecTypes;
+	/* utilityStmt should be null, but we might as well copy it */
+	result->utilityStmt = parse->utilityStmt;
+	result->stmt_location = parse->stmt_location;
+	result->stmt_len = parse->stmt_len;
+
+	result->jitFlags = PGJIT_NONE;
+	if (jit_enabled && jit_above_cost >= 0 &&
+		top_plan->total_cost > jit_above_cost)
+	{
+		result->jitFlags |= PGJIT_PERFORM;
+
+		/*
+		 * Decide how much effort should be put into generating better code.
+		 */
+		if (jit_optimize_above_cost >= 0 &&
+			top_plan->total_cost > jit_optimize_above_cost)
+			result->jitFlags |= PGJIT_OPT3;
+		if (jit_inline_above_cost >= 0 &&
+			top_plan->total_cost > jit_inline_above_cost)
+			result->jitFlags |= PGJIT_INLINE;
+
+		/*
+		 * Decide which operations should be JITed.
+		 */
+		if (jit_expressions)
+			result->jitFlags |= PGJIT_EXPR;
+		if (jit_tuple_deforming)
+			result->jitFlags |= PGJIT_DEFORM;
+	}
+
+	if (glob->partition_directory != NULL)
+		DestroyPartitionDirectory(glob->partition_directory);
+
+	return result;
+}
+
+
+/*--------------------
+ * subquery_planner
+ *	  Invokes the planner on a subquery.  We recurse to here for each
+ *	  sub-SELECT found in the query tree.
+ *
+ * glob is the global state for the current planner run.
+ * parse is the querytree produced by the parser & rewriter.
+ * parent_root is the immediate parent Query's info (NULL at the top level).
+ * hasRecursion is true if this is a recursive WITH query.
+ * tuple_fraction is the fraction of tuples we expect will be retrieved.
+ * tuple_fraction is interpreted as explained for grouping_planner, below.
+ *
+ * Basically, this routine does the stuff that should only be done once
+ * per Query object.  It then calls grouping_planner.  At one time,
+ * grouping_planner could be invoked recursively on the same Query object;
+ * that's not currently true, but we keep the separation between the two
+ * routines anyway, in case we need it again someday.
+ *
+ * subquery_planner will be called recursively to handle sub-Query nodes
+ * found within the query's expressions and rangetable.
+ *
+ * Returns the PlannerInfo struct ("root") that contains all data generated
+ * while planning the subquery.  In particular, the Path(s) attached to
+ * the (UPPERREL_FINAL, NULL) upperrel represent our conclusions about the
+ * cheapest way(s) to implement the query.  The top level will select the
+ * best Path and pass it through createplan.c to produce a finished Plan.
+ *--------------------
+ */
+PlannerInfo *
+subquery_planner(PlannerGlobal *glob, Query *parse,
+				 PlannerInfo *parent_root,
+				 bool hasRecursion, double tuple_fraction)
+{
+	PlannerInfo *root;
+	List	   *newWithCheckOptions;
+	List	   *newHaving;
+	bool		hasOuterJoins;
+	bool		hasResultRTEs;
+	RelOptInfo *final_rel;
+	ListCell   *l;
+
+	/* Create a PlannerInfo data structure for this subquery */
+	root = makeNode(PlannerInfo);
+	root->parse = parse;
+	root->glob = glob;
+	root->query_level = parent_root ? parent_root->query_level + 1 : 1;
+	root->parent_root = parent_root;
+	root->plan_params = NIL;
+	root->outer_params = NULL;
+	root->planner_cxt = CurrentMemoryContext;
+	root->init_plans = NIL;
+	root->cte_plan_ids = NIL;
+	root->multiexpr_params = NIL;
+	root->eq_classes = NIL;
+	root->append_rel_list = NIL;
+	root->rowMarks = NIL;
+	memset(root->upper_rels, 0, sizeof(root->upper_rels));
+	memset(root->upper_targets, 0, sizeof(root->upper_targets));
+	root->processed_tlist = NIL;
+	root->grouping_map = NULL;
+	root->minmax_aggs = NIL;
+	root->qual_security_level = 0;
+	root->inhTargetKind = INHKIND_NONE;
+	root->hasRecursion = hasRecursion;
+	if (hasRecursion)
+		root->wt_param_id = assign_special_exec_param(root);
+	else
+		root->wt_param_id = -1;
+	root->non_recursive_path = NULL;
+	root->partColsUpdated = false;
+
+	/*
+	 * If there is a WITH list, process each WITH query and either convert it
+	 * to RTE_SUBQUERY RTE(s) or build an initplan SubPlan structure for it.
+	 */
+	if (parse->cteList)
+		SS_process_ctes(root);
+
+	/*
+	 * If the FROM clause is empty, replace it with a dummy RTE_RESULT RTE, so
+	 * that we don't need so many special cases to deal with that situation.
+	 */
+	replace_empty_jointree(parse);
+
+	/*
+	 * Look for ANY and EXISTS SubLinks in WHERE and JOIN/ON clauses, and try
+	 * to transform them into joins.  Note that this step does not descend
+	 * into subqueries; if we pull up any subqueries below, their SubLinks are
+	 * processed just before pulling them up.
+	 */
+	if (parse->hasSubLinks)
+		pull_up_sublinks(root);
+
+	/*
+	 * Scan the rangetable for set-returning functions, and inline them if
+	 * possible (producing subqueries that might get pulled up next).
+	 * Recursion issues here are handled in the same way as for SubLinks.
+	 */
+	inline_set_returning_functions(root);
+
+	/*
+	 * Check to see if any subqueries in the jointree can be merged into this
+	 * query.
+	 */
+	pull_up_subqueries(root);
+
+	/*
+	 * If this is a simple UNION ALL query, flatten it into an appendrel. We
+	 * do this now because it requires applying pull_up_subqueries to the leaf
+	 * queries of the UNION ALL, which weren't touched above because they
+	 * weren't referenced by the jointree (they will be after we do this).
+	 */
+	if (parse->setOperations)
+		flatten_simple_union_all(root);
+
+	/*
+	 * Survey the rangetable to see what kinds of entries are present.  We can
+	 * skip some later processing if relevant SQL features are not used; for
+	 * example if there are no JOIN RTEs we can avoid the expense of doing
+	 * flatten_join_alias_vars().  This must be done after we have finished
+	 * adding rangetable entries, of course.  (Note: actually, processing of
+	 * inherited or partitioned rels can cause RTEs for their child tables to
+	 * get added later; but those must all be RTE_RELATION entries, so they
+	 * don't invalidate the conclusions drawn here.)
+	 */
+	root->hasJoinRTEs = false;
+	root->hasLateralRTEs = false;
+	hasOuterJoins = false;
+	hasResultRTEs = false;
+	foreach(l, parse->rtable)
+	{
+		RangeTblEntry *rte = lfirst_node(RangeTblEntry, l);
+
+		switch (rte->rtekind)
+		{
+			case RTE_RELATION:
+				if (rte->inh)
+				{
+					/*
+					 * Check to see if the relation actually has any children;
+					 * if not, clear the inh flag so we can treat it as a
+					 * plain base relation.
+					 *
+					 * Note: this could give a false-positive result, if the
+					 * rel once had children but no longer does.  We used to
+					 * be able to clear rte->inh later on when we discovered
+					 * that, but no more; we have to handle such cases as
+					 * full-fledged inheritance.
+					 */
+					rte->inh = has_subclass(rte->relid);
+				}
+				break;
+			case RTE_JOIN:
+				root->hasJoinRTEs = true;
+				if (IS_OUTER_JOIN(rte->jointype))
+					hasOuterJoins = true;
+				break;
+			case RTE_RESULT:
+				hasResultRTEs = true;
+				break;
+			default:
+				/* No work here for other RTE types */
+				break;
+		}
+
+		if (rte->lateral)
+			root->hasLateralRTEs = true;
+
+		/*
+		 * We can also determine the maximum security level required for any
+		 * securityQuals now.  Addition of inheritance-child RTEs won't affect
+		 * this, because child tables don't have their own securityQuals; see
+		 * expand_single_inheritance_child().
+		 */
+		if (rte->securityQuals)
+			root->qual_security_level = Max(root->qual_security_level,
+											list_length(rte->securityQuals));
+	}
+
+	/*
+	 * Preprocess RowMark information.  We need to do this after subquery
+	 * pullup, so that all base relations are present.
+	 */
+	preprocess_rowmarks(root);
+
+	/*
+	 * Set hasHavingQual to remember if HAVING clause is present.  Needed
+	 * because preprocess_expression will reduce a constant-true condition to
+	 * an empty qual list ... but "HAVING TRUE" is not a semantic no-op.
+	 */
+	root->hasHavingQual = (parse->havingQual != NULL);
+
+	/* Clear this flag; might get set in distribute_qual_to_rels */
+	root->hasPseudoConstantQuals = false;
+
+	/*
+	 * Do expression preprocessing on targetlist and quals, as well as other
+	 * random expressions in the querytree.  Note that we do not need to
+	 * handle sort/group expressions explicitly, because they are actually
+	 * part of the targetlist.
+	 */
+	parse->targetList = (List *)
+		preprocess_expression(root, (Node *) parse->targetList,
+							  EXPRKIND_TARGET);
+
+	/* Constant-folding might have removed all set-returning functions */
+	if (parse->hasTargetSRFs)
+		parse->hasTargetSRFs = expression_returns_set((Node *) parse->targetList);
+
+	newWithCheckOptions = NIL;
+	foreach(l, parse->withCheckOptions)
+	{
+		WithCheckOption *wco = lfirst_node(WithCheckOption, l);
+
+		wco->qual = preprocess_expression(root, wco->qual,
+										  EXPRKIND_QUAL);
+		if (wco->qual != NULL)
+			newWithCheckOptions = lappend(newWithCheckOptions, wco);
+	}
+	parse->withCheckOptions = newWithCheckOptions;
+
+	parse->returningList = (List *)
+		preprocess_expression(root, (Node *) parse->returningList,
+							  EXPRKIND_TARGET);
+
+	preprocess_qual_conditions(root, (Node *) parse->jointree);
+
+	parse->havingQual = preprocess_expression(root, parse->havingQual,
+											  EXPRKIND_QUAL);
+
+	foreach(l, parse->windowClause)
+	{
+		WindowClause *wc = lfirst_node(WindowClause, l);
+
+		/* partitionClause/orderClause are sort/group expressions */
+		wc->startOffset = preprocess_expression(root, wc->startOffset,
+												EXPRKIND_LIMIT);
+		wc->endOffset = preprocess_expression(root, wc->endOffset,
+											  EXPRKIND_LIMIT);
+	}
+
+	parse->limitOffset = preprocess_expression(root, parse->limitOffset,
+											   EXPRKIND_LIMIT);
+	parse->limitCount = preprocess_expression(root, parse->limitCount,
+											  EXPRKIND_LIMIT);
+
+	if (parse->onConflict)
+	{
+		parse->onConflict->arbiterElems = (List *)
+			preprocess_expression(root,
+								  (Node *) parse->onConflict->arbiterElems,
+								  EXPRKIND_ARBITER_ELEM);
+		parse->onConflict->arbiterWhere =
+			preprocess_expression(root,
+								  parse->onConflict->arbiterWhere,
+								  EXPRKIND_QUAL);
+		parse->onConflict->onConflictSet = (List *)
+			preprocess_expression(root,
+								  (Node *) parse->onConflict->onConflictSet,
+								  EXPRKIND_TARGET);
+		parse->onConflict->onConflictWhere =
+			preprocess_expression(root,
+								  parse->onConflict->onConflictWhere,
+								  EXPRKIND_QUAL);
+		/* exclRelTlist contains only Vars, so no preprocessing needed */
+	}
+
+	root->append_rel_list = (List *)
+		preprocess_expression(root, (Node *) root->append_rel_list,
+							  EXPRKIND_APPINFO);
+
+	/* Also need to preprocess expressions within RTEs */
+	foreach(l, parse->rtable)
+	{
+		RangeTblEntry *rte = lfirst_node(RangeTblEntry, l);
+		int			kind;
+		ListCell   *lcsq;
+
+		if (rte->rtekind == RTE_RELATION)
+		{
+			if (rte->tablesample)
+				rte->tablesample = (TableSampleClause *)
+					preprocess_expression(root,
+										  (Node *) rte->tablesample,
+										  EXPRKIND_TABLESAMPLE);
+		}
+		else if (rte->rtekind == RTE_SUBQUERY)
+		{
+			/*
+			 * We don't want to do all preprocessing yet on the subquery's
+			 * expressions, since that will happen when we plan it.  But if it
+			 * contains any join aliases of our level, those have to get
+			 * expanded now, because planning of the subquery won't do it.
+			 * That's only possible if the subquery is LATERAL.
+			 */
+			if (rte->lateral && root->hasJoinRTEs)
+				rte->subquery = (Query *)
+					flatten_join_alias_vars(root->parse,
+											(Node *) rte->subquery);
+		}
+		else if (rte->rtekind == RTE_FUNCTION)
+		{
+			/* Preprocess the function expression(s) fully */
+			kind = rte->lateral ? EXPRKIND_RTFUNC_LATERAL : EXPRKIND_RTFUNC;
+			rte->functions = (List *)
+				preprocess_expression(root, (Node *) rte->functions, kind);
+		}
+		else if (rte->rtekind == RTE_TABLEFUNC)
+		{
+			/* Preprocess the function expression(s) fully */
+			kind = rte->lateral ? EXPRKIND_TABLEFUNC_LATERAL : EXPRKIND_TABLEFUNC;
+			rte->tablefunc = (TableFunc *)
+				preprocess_expression(root, (Node *) rte->tablefunc, kind);
+		}
+		else if (rte->rtekind == RTE_VALUES)
+		{
+			/* Preprocess the values lists fully */
+			kind = rte->lateral ? EXPRKIND_VALUES_LATERAL : EXPRKIND_VALUES;
+			rte->values_lists = (List *)
+				preprocess_expression(root, (Node *) rte->values_lists, kind);
+		}
+
+		/*
+		 * Process each element of the securityQuals list as if it were a
+		 * separate qual expression (as indeed it is).  We need to do it this
+		 * way to get proper canonicalization of AND/OR structure.  Note that
+		 * this converts each element into an implicit-AND sublist.
+		 */
+		foreach(lcsq, rte->securityQuals)
+		{
+			lfirst(lcsq) = preprocess_expression(root,
+												 (Node *) lfirst(lcsq),
+												 EXPRKIND_QUAL);
+		}
+	}
+
+	/*
+	 * Now that we are done preprocessing expressions, and in particular done
+	 * flattening join alias variables, get rid of the joinaliasvars lists.
+	 * They no longer match what expressions in the rest of the tree look
+	 * like, because we have not preprocessed expressions in those lists (and
+	 * do not want to; for example, expanding a SubLink there would result in
+	 * a useless unreferenced subplan).  Leaving them in place simply creates
+	 * a hazard for later scans of the tree.  We could try to prevent that by
+	 * using QTW_IGNORE_JOINALIASES in every tree scan done after this point,
+	 * but that doesn't sound very reliable.
+	 */
+	if (root->hasJoinRTEs)
+	{
+		foreach(l, parse->rtable)
+		{
+			RangeTblEntry *rte = lfirst_node(RangeTblEntry, l);
+
+			rte->joinaliasvars = NIL;
+		}
+	}
+
+	/*
+	 * In some cases we may want to transfer a HAVING clause into WHERE. We
+	 * cannot do so if the HAVING clause contains aggregates (obviously) or
+	 * volatile functions (since a HAVING clause is supposed to be executed
+	 * only once per group).  We also can't do this if there are any nonempty
+	 * grouping sets; moving such a clause into WHERE would potentially change
+	 * the results, if any referenced column isn't present in all the grouping
+	 * sets.  (If there are only empty grouping sets, then the HAVING clause
+	 * must be degenerate as discussed below.)
+	 *
+	 * Also, it may be that the clause is so expensive to execute that we're
+	 * better off doing it only once per group, despite the loss of
+	 * selectivity.  This is hard to estimate short of doing the entire
+	 * planning process twice, so we use a heuristic: clauses containing
+	 * subplans are left in HAVING.  Otherwise, we move or copy the HAVING
+	 * clause into WHERE, in hopes of eliminating tuples before aggregation
+	 * instead of after.
+	 *
+	 * If the query has explicit grouping then we can simply move such a
+	 * clause into WHERE; any group that fails the clause will not be in the
+	 * output because none of its tuples will reach the grouping or
+	 * aggregation stage.  Otherwise we must have a degenerate (variable-free)
+	 * HAVING clause, which we put in WHERE so that query_planner() can use it
+	 * in a gating Result node, but also keep in HAVING to ensure that we
+	 * don't emit a bogus aggregated row. (This could be done better, but it
+	 * seems not worth optimizing.)
+	 *
+	 * Note that both havingQual and parse->jointree->quals are in
+	 * implicitly-ANDed-list form at this point, even though they are declared
+	 * as Node *.
+	 */
+	newHaving = NIL;
+	foreach(l, (List *) parse->havingQual)
+	{
+		Node	   *havingclause = (Node *) lfirst(l);
+
+		if ((parse->groupClause && parse->groupingSets) ||
+			contain_agg_clause(havingclause) ||
+			contain_volatile_functions(havingclause) ||
+			contain_subplans(havingclause))
+		{
+			/* keep it in HAVING */
+			newHaving = lappend(newHaving, havingclause);
+		}
+		else if (parse->groupClause && !parse->groupingSets)
+		{
+			/* move it to WHERE */
+			parse->jointree->quals = (Node *)
+				lappend((List *) parse->jointree->quals, havingclause);
+		}
+		else
+		{
+			/* put a copy in WHERE, keep it in HAVING */
+			parse->jointree->quals = (Node *)
+				lappend((List *) parse->jointree->quals,
+						copyObject(havingclause));
+			newHaving = lappend(newHaving, havingclause);
+		}
+	}
+	parse->havingQual = (Node *) newHaving;
+
+	/* Remove any redundant GROUP BY columns */
+	remove_useless_groupby_columns(root);
+
+	/*
+	 * If we have any outer joins, try to reduce them to plain inner joins.
+	 * This step is most easily done after we've done expression
+	 * preprocessing.
+	 */
+	if (hasOuterJoins)
+		reduce_outer_joins(root);
+
+	/*
+	 * If we have any RTE_RESULT relations, see if they can be deleted from
+	 * the jointree.  This step is most effectively done after we've done
+	 * expression preprocessing and outer join reduction.
+	 */
+	if (hasResultRTEs)
+		remove_useless_result_rtes(root);
+
+	/*
+	 * Do the main planning.  If we have an inherited target relation, that
+	 * needs special processing, else go straight to grouping_planner.
+	 */
+	if (parse->resultRelation &&
+		rt_fetch(parse->resultRelation, parse->rtable)->inh)
+		inheritance_planner(root);
+	else
+		grouping_planner(root, false, tuple_fraction);
+
+	/*
+	 * Capture the set of outer-level param IDs we have access to, for use in
+	 * extParam/allParam calculations later.
+	 */
+	SS_identify_outer_params(root);
+
+	/*
+	 * If any initPlans were created in this query level, adjust the surviving
+	 * Paths' costs and parallel-safety flags to account for them.  The
+	 * initPlans won't actually get attached to the plan tree till
+	 * create_plan() runs, but we must include their effects now.
+	 */
+	final_rel = fetch_upper_rel(root, UPPERREL_FINAL, NULL);
+	SS_charge_for_initplans(root, final_rel);
+
+	/*
+	 * Make sure we've identified the cheapest Path for the final rel.  (By
+	 * doing this here not in grouping_planner, we include initPlan costs in
+	 * the decision, though it's unlikely that will change anything.)
+	 */
+	set_cheapest(final_rel);
+
+	return root;
+}
+
+/*
+ * preprocess_expression
+ *		Do subquery_planner's preprocessing work for an expression,
+ *		which can be a targetlist, a WHERE clause (including JOIN/ON
+ *		conditions), a HAVING clause, or a few other things.
+ */
+static Node *
+preprocess_expression(PlannerInfo *root, Node *expr, int kind)
+{
+	/*
+	 * Fall out quickly if expression is empty.  This occurs often enough to
+	 * be worth checking.  Note that null->null is the correct conversion for
+	 * implicit-AND result format, too.
+	 */
+	if (expr == NULL)
+		return NULL;
+
+	/*
+	 * If the query has any join RTEs, replace join alias variables with
+	 * base-relation variables.  We must do this first, since any expressions
+	 * we may extract from the joinaliasvars lists have not been preprocessed.
+	 * For example, if we did this after sublink processing, sublinks expanded
+	 * out from join aliases would not get processed.  But we can skip this in
+	 * non-lateral RTE functions, VALUES lists, and TABLESAMPLE clauses, since
+	 * they can't contain any Vars of the current query level.
+	 */
+	if (root->hasJoinRTEs &&
+		!(kind == EXPRKIND_RTFUNC ||
+		  kind == EXPRKIND_VALUES ||
+		  kind == EXPRKIND_TABLESAMPLE ||
+		  kind == EXPRKIND_TABLEFUNC))
+		expr = flatten_join_alias_vars(root->parse, expr);
+
+	/*
+	 * Simplify constant expressions.
+	 *
+	 * Note: an essential effect of this is to convert named-argument function
+	 * calls to positional notation and insert the current actual values of
+	 * any default arguments for functions.  To ensure that happens, we *must*
+	 * process all expressions here.  Previous PG versions sometimes skipped
+	 * const-simplification if it didn't seem worth the trouble, but we can't
+	 * do that anymore.
+	 *
+	 * Note: this also flattens nested AND and OR expressions into N-argument
+	 * form.  All processing of a qual expression after this point must be
+	 * careful to maintain AND/OR flatness --- that is, do not generate a tree
+	 * with AND directly under AND, nor OR directly under OR.
+	 */
+	expr = eval_const_expressions(root, expr);
+
+	/*
+	 * If it's a qual or havingQual, canonicalize it.
+	 */
+	if (kind == EXPRKIND_QUAL)
+	{
+		expr = (Node *) canonicalize_qual((Expr *) expr, false);
+
+#ifdef OPTIMIZER_DEBUG
+		printf("After canonicalize_qual()\n");
+		pprint(expr);
+#endif
+	}
+
+	/* Expand SubLinks to SubPlans */
+	if (root->parse->hasSubLinks)
+		expr = SS_process_sublinks(root, expr, (kind == EXPRKIND_QUAL));
+
+	/*
+	 * XXX do not insert anything here unless you have grokked the comments in
+	 * SS_replace_correlation_vars ...
+	 */
+
+	/* Replace uplevel vars with Param nodes (this IS possible in VALUES) */
+	if (root->query_level > 1)
+		expr = SS_replace_correlation_vars(root, expr);
+
+	/*
+	 * If it's a qual or havingQual, convert it to implicit-AND format. (We
+	 * don't want to do this before eval_const_expressions, since the latter
+	 * would be unable to simplify a top-level AND correctly. Also,
+	 * SS_process_sublinks expects explicit-AND format.)
+	 */
+	if (kind == EXPRKIND_QUAL)
+		expr = (Node *) make_ands_implicit((Expr *) expr);
+
+	return expr;
+}
+
+/*
+ * preprocess_qual_conditions
+ *		Recursively scan the query's jointree and do subquery_planner's
+ *		preprocessing work on each qual condition found therein.
+ */
+static void
+preprocess_qual_conditions(PlannerInfo *root, Node *jtnode)
+{
+	if (jtnode == NULL)
+		return;
+	if (IsA(jtnode, RangeTblRef))
+	{
+		/* nothing to do here */
+	}
+	else if (IsA(jtnode, FromExpr))
+	{
+		FromExpr   *f = (FromExpr *) jtnode;
+		ListCell   *l;
+
+		foreach(l, f->fromlist)
+			preprocess_qual_conditions(root, lfirst(l));
+
+		f->quals = preprocess_expression(root, f->quals, EXPRKIND_QUAL);
+	}
+	else if (IsA(jtnode, JoinExpr))
+	{
+		JoinExpr   *j = (JoinExpr *) jtnode;
+
+		preprocess_qual_conditions(root, j->larg);
+		preprocess_qual_conditions(root, j->rarg);
+
+		j->quals = preprocess_expression(root, j->quals, EXPRKIND_QUAL);
+	}
+	else
+		elog(ERROR, "unrecognized node type: %d",
+			 (int) nodeTag(jtnode));
+}
+
+/*
+ * preprocess_phv_expression
+ *	  Do preprocessing on a PlaceHolderVar expression that's been pulled up.
+ *
+ * If a LATERAL subquery references an output of another subquery, and that
+ * output must be wrapped in a PlaceHolderVar because of an intermediate outer
+ * join, then we'll push the PlaceHolderVar expression down into the subquery
+ * and later pull it back up during find_lateral_references, which runs after
+ * subquery_planner has preprocessed all the expressions that were in the
+ * current query level to start with.  So we need to preprocess it then.
+ */
+Expr *
+preprocess_phv_expression(PlannerInfo *root, Expr *expr)
+{
+	return (Expr *) preprocess_expression(root, (Node *) expr, EXPRKIND_PHV);
+}
+
+/*
+ * inheritance_planner
+ *	  Generate Paths in the case where the result relation is an
+ *	  inheritance set.
+ *
+ * We have to handle this case differently from cases where a source relation
+ * is an inheritance set. Source inheritance is expanded at the bottom of the
+ * plan tree (see allpaths.c), but target inheritance has to be expanded at
+ * the top.  The reason is that for UPDATE, each target relation needs a
+ * different targetlist matching its own column set.  Fortunately,
+ * the UPDATE/DELETE target can never be the nullable side of an outer join,
+ * so it's OK to generate the plan this way.
+ *
+ * Returns nothing; the useful output is in the Paths we attach to
+ * the (UPPERREL_FINAL, NULL) upperrel stored in *root.
+ *
+ * Note that we have not done set_cheapest() on the final rel; it's convenient
+ * to leave this to the caller.
+ */
+static void
+inheritance_planner(PlannerInfo *root)
+{
+	Query	   *parse = root->parse;
+	int			top_parentRTindex = parse->resultRelation;
+	List	   *select_rtable;
+	List	   *select_appinfos;
+	List	   *child_appinfos;
+	List	   *old_child_rtis;
+	List	   *new_child_rtis;
+	Bitmapset  *subqueryRTindexes;
+	Index		next_subquery_rti;
+	int			nominalRelation = -1;
+	Index		rootRelation = 0;
+	List	   *final_rtable = NIL;
+	List	   *final_rowmarks = NIL;
+	int			save_rel_array_size = 0;
+	RelOptInfo **save_rel_array = NULL;
+	AppendRelInfo **save_append_rel_array = NULL;
+	List	   *subpaths = NIL;
+	List	   *subroots = NIL;
+	List	   *resultRelations = NIL;
+	List	   *withCheckOptionLists = NIL;
+	List	   *returningLists = NIL;
+	List	   *rowMarks;
+	RelOptInfo *final_rel;
+	ListCell   *lc;
+	ListCell   *lc2;
+	Index		rti;
+	RangeTblEntry *parent_rte;
+	Bitmapset  *parent_relids;
+	Query	  **parent_parses;
+
+	/* Should only get here for UPDATE or DELETE */
+	Assert(parse->commandType == CMD_UPDATE ||
+		   parse->commandType == CMD_DELETE);
+
+	/*
+	 * We generate a modified instance of the original Query for each target
+	 * relation, plan that, and put all the plans into a list that will be
+	 * controlled by a single ModifyTable node.  All the instances share the
+	 * same rangetable, but each instance must have its own set of subquery
+	 * RTEs within the finished rangetable because (1) they are likely to get
+	 * scribbled on during planning, and (2) it's not inconceivable that
+	 * subqueries could get planned differently in different cases.  We need
+	 * not create duplicate copies of other RTE kinds, in particular not the
+	 * target relations, because they don't have either of those issues.  Not
+	 * having to duplicate the target relations is important because doing so
+	 * (1) would result in a rangetable of length O(N^2) for N targets, with
+	 * at least O(N^3) work expended here; and (2) would greatly complicate
+	 * management of the rowMarks list.
+	 *
+	 * To begin with, generate a bitmapset of the relids of the subquery RTEs.
+	 */
+	subqueryRTindexes = NULL;
+	rti = 1;
+	foreach(lc, parse->rtable)
+	{
+		RangeTblEntry *rte = lfirst_node(RangeTblEntry, lc);
+
+		if (rte->rtekind == RTE_SUBQUERY)
+			subqueryRTindexes = bms_add_member(subqueryRTindexes, rti);
+		rti++;
+	}
+
+	/*
+	 * If the parent RTE is a partitioned table, we should use that as the
+	 * nominal target relation, because the RTEs added for partitioned tables
+	 * (including the root parent) as child members of the inheritance set do
+	 * not appear anywhere else in the plan, so the confusion explained below
+	 * for non-partitioning inheritance cases is not possible.
+	 */
+	parent_rte = rt_fetch(top_parentRTindex, parse->rtable);
+	Assert(parent_rte->inh);
+	if (parent_rte->relkind == RELKIND_PARTITIONED_TABLE)
+	{
+		nominalRelation = top_parentRTindex;
+		rootRelation = top_parentRTindex;
+	}
+
+	/*
+	 * Before generating the real per-child-relation plans, do a cycle of
+	 * planning as though the query were a SELECT.  The objective here is to
+	 * find out which child relations need to be processed, using the same
+	 * expansion and pruning logic as for a SELECT.  We'll then pull out the
+	 * RangeTblEntry-s generated for the child rels, and make use of the
+	 * AppendRelInfo entries for them to guide the real planning.  (This is
+	 * rather inefficient; we could perhaps stop short of making a full Path
+	 * tree.  But this whole function is inefficient and slated for
+	 * destruction, so let's not contort query_planner for that.)
+	 */
+	{
+		PlannerInfo *subroot;
+
+		/*
+		 * Flat-copy the PlannerInfo to prevent modification of the original.
+		 */
+		subroot = makeNode(PlannerInfo);
+		memcpy(subroot, root, sizeof(PlannerInfo));
+
+		/*
+		 * Make a deep copy of the parsetree for this planning cycle to mess
+		 * around with, and change it to look like a SELECT.  (Hack alert: the
+		 * target RTE still has updatedCols set if this is an UPDATE, so that
+		 * expand_partitioned_rtentry will correctly update
+		 * subroot->partColsUpdated.)
+		 */
+		subroot->parse = copyObject(root->parse);
+
+		subroot->parse->commandType = CMD_SELECT;
+		subroot->parse->resultRelation = 0;
+
+		/*
+		 * Ensure the subroot has its own copy of the original
+		 * append_rel_list, since it'll be scribbled on.  (Note that at this
+		 * point, the list only contains AppendRelInfos for flattened UNION
+		 * ALL subqueries.)
+		 */
+		subroot->append_rel_list = copyObject(root->append_rel_list);
+
+		/*
+		 * Better make a private copy of the rowMarks, too.
+		 */
+		subroot->rowMarks = copyObject(root->rowMarks);
+
+		/* There shouldn't be any OJ info to translate, as yet */
+		Assert(subroot->join_info_list == NIL);
+		/* and we haven't created PlaceHolderInfos, either */
+		Assert(subroot->placeholder_list == NIL);
+
+		/* Generate Path(s) for accessing this result relation */
+		grouping_planner(subroot, true, 0.0 /* retrieve all tuples */ );
+
+		/* Extract the info we need. */
+		select_rtable = subroot->parse->rtable;
+		select_appinfos = subroot->append_rel_list;
+
+		/*
+		 * We need to propagate partColsUpdated back, too.  (The later
+		 * planning cycles will not set this because they won't run
+		 * expand_partitioned_rtentry for the UPDATE target.)
+		 */
+		root->partColsUpdated = subroot->partColsUpdated;
+	}
+
+	/*----------
+	 * Since only one rangetable can exist in the final plan, we need to make
+	 * sure that it contains all the RTEs needed for any child plan.  This is
+	 * complicated by the need to use separate subquery RTEs for each child.
+	 * We arrange the final rtable as follows:
+	 * 1. All original rtable entries (with their original RT indexes).
+	 * 2. All the relation RTEs generated for children of the target table.
+	 * 3. Subquery RTEs for children after the first.  We need N * (K - 1)
+	 *    RT slots for this, if there are N subqueries and K child tables.
+	 * 4. Additional RTEs generated during the child planning runs, such as
+	 *    children of inheritable RTEs other than the target table.
+	 * We assume that each child planning run will create an identical set
+	 * of type-4 RTEs.
+	 *
+	 * So the next thing to do is append the type-2 RTEs (the target table's
+	 * children) to the original rtable.  We look through select_appinfos
+	 * to find them.
+	 *
+	 * To identify which AppendRelInfos are relevant as we thumb through
+	 * select_appinfos, we need to look for both direct and indirect children
+	 * of top_parentRTindex, so we use a bitmap of known parent relids.
+	 * expand_inherited_rtentry() always processes a parent before any of that
+	 * parent's children, so we should see an intermediate parent before its
+	 * children.
+	 *----------
+	 */
+	child_appinfos = NIL;
+	old_child_rtis = NIL;
+	new_child_rtis = NIL;
+	parent_relids = bms_make_singleton(top_parentRTindex);
+	foreach(lc, select_appinfos)
+	{
+		AppendRelInfo *appinfo = lfirst_node(AppendRelInfo, lc);
+		RangeTblEntry *child_rte;
+
+		/* append_rel_list contains all append rels; ignore others */
+		if (!bms_is_member(appinfo->parent_relid, parent_relids))
+			continue;
+
+		/* remember relevant AppendRelInfos for use below */
+		child_appinfos = lappend(child_appinfos, appinfo);
+
+		/* extract RTE for this child rel */
+		child_rte = rt_fetch(appinfo->child_relid, select_rtable);
+
+		/* and append it to the original rtable */
+		parse->rtable = lappend(parse->rtable, child_rte);
+
+		/* remember child's index in the SELECT rtable */
+		old_child_rtis = lappend_int(old_child_rtis, appinfo->child_relid);
+
+		/* and its new index in the final rtable */
+		new_child_rtis = lappend_int(new_child_rtis, list_length(parse->rtable));
+
+		/* if child is itself partitioned, update parent_relids */
+		if (child_rte->inh)
+		{
+			Assert(child_rte->relkind == RELKIND_PARTITIONED_TABLE);
+			parent_relids = bms_add_member(parent_relids, appinfo->child_relid);
+		}
+	}
+
+	/*
+	 * It's possible that the RTIs we just assigned for the child rels in the
+	 * final rtable are different from what they were in the SELECT query.
+	 * Adjust the AppendRelInfos so that they will correctly map RT indexes to
+	 * the final indexes.  We can do this left-to-right since no child rel's
+	 * final RT index could be greater than what it had in the SELECT query.
+	 */
+	forboth(lc, old_child_rtis, lc2, new_child_rtis)
+	{
+		int			old_child_rti = lfirst_int(lc);
+		int			new_child_rti = lfirst_int(lc2);
+
+		if (old_child_rti == new_child_rti)
+			continue;			/* nothing to do */
+
+		Assert(old_child_rti > new_child_rti);
+
+		ChangeVarNodes((Node *) child_appinfos,
+					   old_child_rti, new_child_rti, 0);
+	}
+
+	/*
+	 * Now set up rangetable entries for subqueries for additional children
+	 * (the first child will just use the original ones).  These all have to
+	 * look more or less real, or EXPLAIN will get unhappy; so we just make
+	 * them all clones of the original subqueries.
+	 */
+	next_subquery_rti = list_length(parse->rtable) + 1;
+	if (subqueryRTindexes != NULL)
+	{
+		int			n_children = list_length(child_appinfos);
+
+		while (n_children-- > 1)
+		{
+			int			oldrti = -1;
+
+			while ((oldrti = bms_next_member(subqueryRTindexes, oldrti)) >= 0)
+			{
+				RangeTblEntry *subqrte;
+
+				subqrte = rt_fetch(oldrti, parse->rtable);
+				parse->rtable = lappend(parse->rtable, copyObject(subqrte));
+			}
+		}
+	}
+
+	/*
+	 * The query for each child is obtained by translating the query for its
+	 * immediate parent, since the AppendRelInfo data we have shows deltas
+	 * between parents and children.  We use the parent_parses array to
+	 * remember the appropriate query trees.  This is indexed by parent relid.
+	 * Since the maximum number of parents is limited by the number of RTEs in
+	 * the SELECT query, we use that number to allocate the array.  An extra
+	 * entry is needed since relids start from 1.
+	 */
+	parent_parses = (Query **) palloc0((list_length(select_rtable) + 1) *
+									   sizeof(Query *));
+	parent_parses[top_parentRTindex] = parse;
+
+	/*
+	 * And now we can get on with generating a plan for each child table.
+	 */
+	foreach(lc, child_appinfos)
+	{
+		AppendRelInfo *appinfo = lfirst_node(AppendRelInfo, lc);
+		Index		this_subquery_rti = next_subquery_rti;
+		Query	   *parent_parse;
+		PlannerInfo *subroot;
+		RangeTblEntry *child_rte;
+		RelOptInfo *sub_final_rel;
+		Path	   *subpath;
+
+		/*
+		 * expand_inherited_rtentry() always processes a parent before any of
+		 * that parent's children, so the parent query for this relation
+		 * should already be available.
+		 */
+		parent_parse = parent_parses[appinfo->parent_relid];
+		Assert(parent_parse != NULL);
+
+		/*
+		 * We need a working copy of the PlannerInfo so that we can control
+		 * propagation of information back to the main copy.
+		 */
+		subroot = makeNode(PlannerInfo);
+		memcpy(subroot, root, sizeof(PlannerInfo));
+
+		/*
+		 * Generate modified query with this rel as target.  We first apply
+		 * adjust_appendrel_attrs, which copies the Query and changes
+		 * references to the parent RTE to refer to the current child RTE,
+		 * then fool around with subquery RTEs.
+		 */
+		subroot->parse = (Query *)
+			adjust_appendrel_attrs(subroot,
+								   (Node *) parent_parse,
+								   1, &appinfo);
+
+		/*
+		 * If there are securityQuals attached to the parent, move them to the
+		 * child rel (they've already been transformed properly for that).
+		 */
+		parent_rte = rt_fetch(appinfo->parent_relid, subroot->parse->rtable);
+		child_rte = rt_fetch(appinfo->child_relid, subroot->parse->rtable);
+		child_rte->securityQuals = parent_rte->securityQuals;
+		parent_rte->securityQuals = NIL;
+
+		/*
+		 * HACK: setting this to a value other than INHKIND_NONE signals to
+		 * relation_excluded_by_constraints() to treat the result relation as
+		 * being an appendrel member.
+		 */
+		subroot->inhTargetKind =
+			(rootRelation != 0) ? INHKIND_PARTITIONED : INHKIND_INHERITED;
+
+		/*
+		 * If this child is further partitioned, remember it as a parent.
+		 * Since a partitioned table does not have any data, we don't need to
+		 * create a plan for it, and we can stop processing it here.  We do,
+		 * however, need to remember its modified PlannerInfo for use when
+		 * processing its children, since we'll update their varnos based on
+		 * the delta from immediate parent to child, not from top to child.
+		 *
+		 * Note: a very non-obvious point is that we have not yet added
+		 * duplicate subquery RTEs to the subroot's rtable.  We mustn't,
+		 * because then its children would have two sets of duplicates,
+		 * confusing matters.
+		 */
+		if (child_rte->inh)
+		{
+			Assert(child_rte->relkind == RELKIND_PARTITIONED_TABLE);
+			parent_parses[appinfo->child_relid] = subroot->parse;
+			continue;
+		}
+
+		/*
+		 * Set the nominal target relation of the ModifyTable node if not
+		 * already done.  If the target is a partitioned table, we already set
+		 * nominalRelation to refer to the partition root, above.  For
+		 * non-partitioned inheritance cases, we'll use the first child
+		 * relation (even if it's excluded) as the nominal target relation.
+		 * Because of the way expand_inherited_rtentry works, that should be
+		 * the RTE representing the parent table in its role as a simple
+		 * member of the inheritance set.
+		 *
+		 * It would be logically cleaner to *always* use the inheritance
+		 * parent RTE as the nominal relation; but that RTE is not otherwise
+		 * referenced in the plan in the non-partitioned inheritance case.
+		 * Instead the duplicate child RTE created by expand_inherited_rtentry
+		 * is used elsewhere in the plan, so using the original parent RTE
+		 * would give rise to confusing use of multiple aliases in EXPLAIN
+		 * output for what the user will think is the "same" table.  OTOH,
+		 * it's not a problem in the partitioned inheritance case, because
+		 * there is no duplicate RTE for the parent.
+		 */
+		if (nominalRelation < 0)
+			nominalRelation = appinfo->child_relid;
+
+		/*
+		 * As above, each child plan run needs its own append_rel_list and
+		 * rowmarks, which should start out as pristine copies of the
+		 * originals.  There can't be any references to UPDATE/DELETE target
+		 * rels in them; but there could be subquery references, which we'll
+		 * fix up in a moment.
+		 */
+		subroot->append_rel_list = copyObject(root->append_rel_list);
+		subroot->rowMarks = copyObject(root->rowMarks);
+
+		/*
+		 * If this isn't the first child Query, adjust Vars and jointree
+		 * entries to reference the appropriate set of subquery RTEs.
+		 */
+		if (final_rtable != NIL && subqueryRTindexes != NULL)
+		{
+			int			oldrti = -1;
+
+			while ((oldrti = bms_next_member(subqueryRTindexes, oldrti)) >= 0)
+			{
+				Index		newrti = next_subquery_rti++;
+
+				ChangeVarNodes((Node *) subroot->parse, oldrti, newrti, 0);
+				ChangeVarNodes((Node *) subroot->append_rel_list,
+							   oldrti, newrti, 0);
+				ChangeVarNodes((Node *) subroot->rowMarks, oldrti, newrti, 0);
+			}
+		}
+
+		/* There shouldn't be any OJ info to translate, as yet */
+		Assert(subroot->join_info_list == NIL);
+		/* and we haven't created PlaceHolderInfos, either */
+		Assert(subroot->placeholder_list == NIL);
+
+		/* Generate Path(s) for accessing this result relation */
+		grouping_planner(subroot, true, 0.0 /* retrieve all tuples */ );
+
+		/*
+		 * Select cheapest path in case there's more than one.  We always run
+		 * modification queries to conclusion, so we care only for the
+		 * cheapest-total path.
+		 */
+		sub_final_rel = fetch_upper_rel(subroot, UPPERREL_FINAL, NULL);
+		set_cheapest(sub_final_rel);
+		subpath = sub_final_rel->cheapest_total_path;
+
+		/*
+		 * If this child rel was excluded by constraint exclusion, exclude it
+		 * from the result plan.
+		 */
+		if (IS_DUMMY_REL(sub_final_rel))
+			continue;
+
+		/*
+		 * If this is the first non-excluded child, its post-planning rtable
+		 * becomes the initial contents of final_rtable; otherwise, copy its
+		 * modified subquery RTEs into final_rtable, to ensure we have sane
+		 * copies of those.  Also save the first non-excluded child's version
+		 * of the rowmarks list; we assume all children will end up with
+		 * equivalent versions of that.
+		 */
+		if (final_rtable == NIL)
+		{
+			final_rtable = subroot->parse->rtable;
+			final_rowmarks = subroot->rowMarks;
+		}
+		else
+		{
+			Assert(list_length(final_rtable) ==
+				   list_length(subroot->parse->rtable));
+			if (subqueryRTindexes != NULL)
+			{
+				int			oldrti = -1;
+
+				while ((oldrti = bms_next_member(subqueryRTindexes, oldrti)) >= 0)
+				{
+					Index		newrti = this_subquery_rti++;
+					RangeTblEntry *subqrte;
+					ListCell   *newrticell;
+
+					subqrte = rt_fetch(newrti, subroot->parse->rtable);
+					newrticell = list_nth_cell(final_rtable, newrti - 1);
+					lfirst(newrticell) = subqrte;
+				}
+			}
+		}
+
+		/*
+		 * We need to collect all the RelOptInfos from all child plans into
+		 * the main PlannerInfo, since setrefs.c will need them.  We use the
+		 * last child's simple_rel_array, so we have to propagate forward the
+		 * RelOptInfos that were already built in previous children.
+		 */
+		Assert(subroot->simple_rel_array_size >= save_rel_array_size);
+		for (rti = 1; rti < save_rel_array_size; rti++)
+		{
+			RelOptInfo *brel = save_rel_array[rti];
+
+			if (brel)
+				subroot->simple_rel_array[rti] = brel;
+		}
+		save_rel_array_size = subroot->simple_rel_array_size;
+		save_rel_array = subroot->simple_rel_array;
+		save_append_rel_array = subroot->append_rel_array;
+
+		/*
+		 * Make sure any initplans from this rel get into the outer list. Note
+		 * we're effectively assuming all children generate the same
+		 * init_plans.
+		 */
+		root->init_plans = subroot->init_plans;
+
+		/* Build list of sub-paths */
+		subpaths = lappend(subpaths, subpath);
+
+		/* Build list of modified subroots, too */
+		subroots = lappend(subroots, subroot);
+
+		/* Build list of target-relation RT indexes */
+		resultRelations = lappend_int(resultRelations, appinfo->child_relid);
+
+		/* Build lists of per-relation WCO and RETURNING targetlists */
+		if (parse->withCheckOptions)
+			withCheckOptionLists = lappend(withCheckOptionLists,
+										   subroot->parse->withCheckOptions);
+		if (parse->returningList)
+			returningLists = lappend(returningLists,
+									 subroot->parse->returningList);
+
+		Assert(!parse->onConflict);
+	}
+
+	/* Result path must go into outer query's FINAL upperrel */
+	final_rel = fetch_upper_rel(root, UPPERREL_FINAL, NULL);
+
+	/*
+	 * We don't currently worry about setting final_rel's consider_parallel
+	 * flag in this case, nor about allowing FDWs or create_upper_paths_hook
+	 * to get control here.
+	 */
+
+	if (subpaths == NIL)
+	{
+		/*
+		 * We managed to exclude every child rel, so generate a dummy path
+		 * representing the empty set.  Although it's clear that no data will
+		 * be updated or deleted, we will still need to have a ModifyTable
+		 * node so that any statement triggers are executed.  (This could be
+		 * cleaner if we fixed nodeModifyTable.c to support zero child nodes,
+		 * but that probably wouldn't be a net win.)
+		 */
+		Path	   *dummy_path;
+
+		/* tlist processing never got done, either */
+		root->processed_tlist = preprocess_targetlist(root);
+		final_rel->reltarget = create_pathtarget(root, root->processed_tlist);
+
+		/* Make a dummy path, cf set_dummy_rel_pathlist() */
+		dummy_path = (Path *) create_append_path(NULL, final_rel, NIL, NIL,
+												 NIL, NULL, 0, false,
+												 NIL, -1);
+
+		/* These lists must be nonempty to make a valid ModifyTable node */
+		subpaths = list_make1(dummy_path);
+		subroots = list_make1(root);
+		resultRelations = list_make1_int(parse->resultRelation);
+		if (parse->withCheckOptions)
+			withCheckOptionLists = list_make1(parse->withCheckOptions);
+		if (parse->returningList)
+			returningLists = list_make1(parse->returningList);
+		/* Disable tuple routing, too, just to be safe */
+		root->partColsUpdated = false;
+	}
+	else
+	{
+		/*
+		 * Put back the final adjusted rtable into the master copy of the
+		 * Query.  (We mustn't do this if we found no non-excluded children,
+		 * since we never saved an adjusted rtable at all.)
+		 */
+		parse->rtable = final_rtable;
+		root->simple_rel_array_size = save_rel_array_size;
+		root->simple_rel_array = save_rel_array;
+		root->append_rel_array = save_append_rel_array;
+
+		/* Must reconstruct master's simple_rte_array, too */
+		root->simple_rte_array = (RangeTblEntry **)
+			palloc0((list_length(final_rtable) + 1) * sizeof(RangeTblEntry *));
+		rti = 1;
+		foreach(lc, final_rtable)
+		{
+			RangeTblEntry *rte = lfirst_node(RangeTblEntry, lc);
+
+			root->simple_rte_array[rti++] = rte;
+		}
+
+		/* Put back adjusted rowmarks, too */
+		root->rowMarks = final_rowmarks;
+	}
+
+	/*
+	 * If there was a FOR [KEY] UPDATE/SHARE clause, the LockRows node will
+	 * have dealt with fetching non-locked marked rows, else we need to have
+	 * ModifyTable do that.
+	 */
+	if (parse->rowMarks)
+		rowMarks = NIL;
+	else
+		rowMarks = root->rowMarks;
+
+	/* Create Path representing a ModifyTable to do the UPDATE/DELETE work */
+	add_path(final_rel, (Path *)
+			 create_modifytable_path(root, final_rel,
+									 parse->commandType,
+									 parse->canSetTag,
+									 nominalRelation,
+									 rootRelation,
+									 root->partColsUpdated,
+									 resultRelations,
+									 subpaths,
+									 subroots,
+									 withCheckOptionLists,
+									 returningLists,
+									 rowMarks,
+									 NULL,
+									 assign_special_exec_param(root)));
+}
+
+/*--------------------
+ * grouping_planner
+ *	  Perform planning steps related to grouping, aggregation, etc.
+ *
+ * This function adds all required top-level processing to the scan/join
+ * Path(s) produced by query_planner.
+ *
+ * If inheritance_update is true, we're being called from inheritance_planner
+ * and should not include a ModifyTable step in the resulting Path(s).
+ * (inheritance_planner will create a single ModifyTable node covering all the
+ * target tables.)
+ *
+ * tuple_fraction is the fraction of tuples we expect will be retrieved.
+ * tuple_fraction is interpreted as follows:
+ *	  0: expect all tuples to be retrieved (normal case)
+ *	  0 < tuple_fraction < 1: expect the given fraction of tuples available
+ *		from the plan to be retrieved
+ *	  tuple_fraction >= 1: tuple_fraction is the absolute number of tuples
+ *		expected to be retrieved (ie, a LIMIT specification)
+ *
+ * Returns nothing; the useful output is in the Paths we attach to the
+ * (UPPERREL_FINAL, NULL) upperrel in *root.  In addition,
+ * root->processed_tlist contains the final processed targetlist.
+ *
+ * Note that we have not done set_cheapest() on the final rel; it's convenient
+ * to leave this to the caller.
+ *--------------------
+ */
+static void
+grouping_planner(PlannerInfo *root, bool inheritance_update,
+				 double tuple_fraction)
+{
+	Query	   *parse = root->parse;
+	int64		offset_est = 0;
+	int64		count_est = 0;
+	double		limit_tuples = -1.0;
+	bool		have_postponed_srfs = false;
+	PathTarget *final_target;
+	List	   *final_targets;
+	List	   *final_targets_contain_srfs;
+	bool		final_target_parallel_safe;
+	RelOptInfo *current_rel;
+	RelOptInfo *final_rel;
+	FinalPathExtraData extra;
+	ListCell   *lc;
+
+	/* Tweak caller-supplied tuple_fraction if have LIMIT/OFFSET */
+	if (parse->limitCount || parse->limitOffset)
+	{
+		tuple_fraction = preprocess_limit(root, tuple_fraction,
+										  &offset_est, &count_est);
+
+		/*
+		 * If we have a known LIMIT, and don't have an unknown OFFSET, we can
+		 * estimate the effects of using a bounded sort.
+		 */
+		if (count_est > 0 && offset_est >= 0)
+			limit_tuples = (double) count_est + (double) offset_est;
+	}
+
+	/* Make tuple_fraction accessible to lower-level routines */
+	root->tuple_fraction = tuple_fraction;
+
+	if (parse->setOperations)
+	{
+		/*
+		 * If there's a top-level ORDER BY, assume we have to fetch all the
+		 * tuples.  This might be too simplistic given all the hackery below
+		 * to possibly avoid the sort; but the odds of accurate estimates here
+		 * are pretty low anyway.  XXX try to get rid of this in favor of
+		 * letting plan_set_operations generate both fast-start and
+		 * cheapest-total paths.
+		 */
+		if (parse->sortClause)
+			root->tuple_fraction = 0.0;
+
+		/*
+		 * Construct Paths for set operations.  The results will not need any
+		 * work except perhaps a top-level sort and/or LIMIT.  Note that any
+		 * special work for recursive unions is the responsibility of
+		 * plan_set_operations.
+		 */
+		current_rel = plan_set_operations(root);
+
+		/*
+		 * We should not need to call preprocess_targetlist, since we must be
+		 * in a SELECT query node.  Instead, use the processed_tlist returned
+		 * by plan_set_operations (since this tells whether it returned any
+		 * resjunk columns!), and transfer any sort key information from the
+		 * original tlist.
+		 */
+		Assert(parse->commandType == CMD_SELECT);
+
+		/* for safety, copy processed_tlist instead of modifying in-place */
+		root->processed_tlist =
+			postprocess_setop_tlist(copyObject(root->processed_tlist),
+									parse->targetList);
+
+		/* Also extract the PathTarget form of the setop result tlist */
+		final_target = current_rel->cheapest_total_path->pathtarget;
+
+		/* And check whether it's parallel safe */
+		final_target_parallel_safe =
+			is_parallel_safe(root, (Node *) final_target->exprs);
+
+		/* The setop result tlist couldn't contain any SRFs */
+		Assert(!parse->hasTargetSRFs);
+		final_targets = final_targets_contain_srfs = NIL;
+
+		/*
+		 * Can't handle FOR [KEY] UPDATE/SHARE here (parser should have
+		 * checked already, but let's make sure).
+		 */
+		if (parse->rowMarks)
+			ereport(ERROR,
+					(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+			/*------
+			  translator: %s is a SQL row locking clause such as FOR UPDATE */
+					 errmsg("%s is not allowed with UNION/INTERSECT/EXCEPT",
+							LCS_asString(linitial_node(RowMarkClause,
+													   parse->rowMarks)->strength))));
+
+		/*
+		 * Calculate pathkeys that represent result ordering requirements
+		 */
+		Assert(parse->distinctClause == NIL);
+		root->sort_pathkeys = make_pathkeys_for_sortclauses(root,
+															parse->sortClause,
+															root->processed_tlist);
+	}
+	else
+	{
+		/* No set operations, do regular planning */
+		PathTarget *sort_input_target;
+		List	   *sort_input_targets;
+		List	   *sort_input_targets_contain_srfs;
+		bool		sort_input_target_parallel_safe;
+		PathTarget *grouping_target;
+		List	   *grouping_targets;
+		List	   *grouping_targets_contain_srfs;
+		bool		grouping_target_parallel_safe;
+		PathTarget *scanjoin_target;
+		List	   *scanjoin_targets;
+		List	   *scanjoin_targets_contain_srfs;
+		bool		scanjoin_target_parallel_safe;
+		bool		scanjoin_target_same_exprs;
+		bool		have_grouping;
+		AggClauseCosts agg_costs;
+		WindowFuncLists *wflists = NULL;
+		List	   *activeWindows = NIL;
+		grouping_sets_data *gset_data = NULL;
+		standard_qp_extra qp_extra;
+
+		/* A recursive query should always have setOperations */
+		Assert(!root->hasRecursion);
+
+		/* Preprocess grouping sets and GROUP BY clause, if any */
+		if (parse->groupingSets)
+		{
+			gset_data = preprocess_grouping_sets(root);
+		}
+		else
+		{
+			/* Preprocess regular GROUP BY clause, if any */
+			if (parse->groupClause)
+				parse->groupClause = preprocess_groupclause(root, NIL);
+		}
+
+		/*
+		 * Preprocess targetlist.  Note that much of the remaining planning
+		 * work will be done with the PathTarget representation of tlists, but
+		 * we must also maintain the full representation of the final tlist so
+		 * that we can transfer its decoration (resnames etc) to the topmost
+		 * tlist of the finished Plan.  This is kept in processed_tlist.
+		 */
+		root->processed_tlist = preprocess_targetlist(root);
+
+		/*
+		 * Collect statistics about aggregates for estimating costs, and mark
+		 * all the aggregates with resolved aggtranstypes.  We must do this
+		 * before slicing and dicing the tlist into various pathtargets, else
+		 * some copies of the Aggref nodes might escape being marked with the
+		 * correct transtypes.
+		 *
+		 * Note: currently, we do not detect duplicate aggregates here.  This
+		 * may result in somewhat-overestimated cost, which is fine for our
+		 * purposes since all Paths will get charged the same.  But at some
+		 * point we might wish to do that detection in the planner, rather
+		 * than during executor startup.
+		 */
+		MemSet(&agg_costs, 0, sizeof(AggClauseCosts));
+		if (parse->hasAggs)
+		{
+			get_agg_clause_costs(root, (Node *) root->processed_tlist,
+								 AGGSPLIT_SIMPLE, &agg_costs);
+			get_agg_clause_costs(root, parse->havingQual, AGGSPLIT_SIMPLE,
+								 &agg_costs);
+		}
+
+		/*
+		 * Locate any window functions in the tlist.  (We don't need to look
+		 * anywhere else, since expressions used in ORDER BY will be in there
+		 * too.)  Note that they could all have been eliminated by constant
+		 * folding, in which case we don't need to do any more work.
+		 */
+		if (parse->hasWindowFuncs)
+		{
+			wflists = find_window_functions((Node *) root->processed_tlist,
+											list_length(parse->windowClause));
+			if (wflists->numWindowFuncs > 0)
+				activeWindows = select_active_windows(root, wflists);
+			else
+				parse->hasWindowFuncs = false;
+		}
+
+		/*
+		 * Preprocess MIN/MAX aggregates, if any.  Note: be careful about
+		 * adding logic between here and the query_planner() call.  Anything
+		 * that is needed in MIN/MAX-optimizable cases will have to be
+		 * duplicated in planagg.c.
+		 */
+		if (parse->hasAggs)
+			preprocess_minmax_aggregates(root);
+
+		/*
+		 * Figure out whether there's a hard limit on the number of rows that
+		 * query_planner's result subplan needs to return.  Even if we know a
+		 * hard limit overall, it doesn't apply if the query has any
+		 * grouping/aggregation operations, or SRFs in the tlist.
+		 */
+		if (parse->groupClause ||
+			parse->groupingSets ||
+			parse->distinctClause ||
+			parse->hasAggs ||
+			parse->hasWindowFuncs ||
+			parse->hasTargetSRFs ||
+			root->hasHavingQual)
+			root->limit_tuples = -1.0;
+		else
+			root->limit_tuples = limit_tuples;
+
+		/* Set up data needed by standard_qp_callback */
+		qp_extra.activeWindows = activeWindows;
+		qp_extra.groupClause = (gset_data
+								? (gset_data->rollups ? linitial_node(RollupData, gset_data->rollups)->groupClause : NIL)
+								: parse->groupClause);
+
+		/*
+		 * Generate the best unsorted and presorted paths for the scan/join
+		 * portion of this Query, ie the processing represented by the
+		 * FROM/WHERE clauses.  (Note there may not be any presorted paths.)
+		 * We also generate (in standard_qp_callback) pathkey representations
+		 * of the query's sort clause, distinct clause, etc.
+		 */
+		current_rel = query_planner(root, standard_qp_callback, &qp_extra);
+
+		/*
+		 * Convert the query's result tlist into PathTarget format.
+		 *
+		 * Note: this cannot be done before query_planner() has performed
+		 * appendrel expansion, because that might add resjunk entries to
+		 * root->processed_tlist.  Waiting till afterwards is also helpful
+		 * because the target width estimates can use per-Var width numbers
+		 * that were obtained within query_planner().
+		 */
+		final_target = create_pathtarget(root, root->processed_tlist);
+		final_target_parallel_safe =
+			is_parallel_safe(root, (Node *) final_target->exprs);
+
+		/*
+		 * If ORDER BY was given, consider whether we should use a post-sort
+		 * projection, and compute the adjusted target for preceding steps if
+		 * so.
+		 */
+		if (parse->sortClause)
+		{
+			sort_input_target = make_sort_input_target(root,
+													   final_target,
+													   &have_postponed_srfs);
+			sort_input_target_parallel_safe =
+				is_parallel_safe(root, (Node *) sort_input_target->exprs);
+		}
+		else
+		{
+			sort_input_target = final_target;
+			sort_input_target_parallel_safe = final_target_parallel_safe;
+		}
+
+		/*
+		 * If we have window functions to deal with, the output from any
+		 * grouping step needs to be what the window functions want;
+		 * otherwise, it should be sort_input_target.
+		 */
+		if (activeWindows)
+		{
+			grouping_target = make_window_input_target(root,
+													   final_target,
+													   activeWindows);
+			grouping_target_parallel_safe =
+				is_parallel_safe(root, (Node *) grouping_target->exprs);
+		}
+		else
+		{
+			grouping_target = sort_input_target;
+			grouping_target_parallel_safe = sort_input_target_parallel_safe;
+		}
+
+		/*
+		 * If we have grouping or aggregation to do, the topmost scan/join
+		 * plan node must emit what the grouping step wants; otherwise, it
+		 * should emit grouping_target.
+		 */
+		have_grouping = (parse->groupClause || parse->groupingSets ||
+						 parse->hasAggs || root->hasHavingQual);
+		if (have_grouping)
+		{
+			scanjoin_target = make_group_input_target(root, final_target);
+			scanjoin_target_parallel_safe =
+				is_parallel_safe(root, (Node *) scanjoin_target->exprs);
+		}
+		else
+		{
+			scanjoin_target = grouping_target;
+			scanjoin_target_parallel_safe = grouping_target_parallel_safe;
+		}
+
+		/*
+		 * If there are any SRFs in the targetlist, we must separate each of
+		 * these PathTargets into SRF-computing and SRF-free targets.  Replace
+		 * each of the named targets with a SRF-free version, and remember the
+		 * list of additional projection steps we need to add afterwards.
+		 */
+		if (parse->hasTargetSRFs)
+		{
+			/* final_target doesn't recompute any SRFs in sort_input_target */
+			split_pathtarget_at_srfs(root, final_target, sort_input_target,
+									 &final_targets,
+									 &final_targets_contain_srfs);
+			final_target = linitial_node(PathTarget, final_targets);
+			Assert(!linitial_int(final_targets_contain_srfs));
+			/* likewise for sort_input_target vs. grouping_target */
+			split_pathtarget_at_srfs(root, sort_input_target, grouping_target,
+									 &sort_input_targets,
+									 &sort_input_targets_contain_srfs);
+			sort_input_target = linitial_node(PathTarget, sort_input_targets);
+			Assert(!linitial_int(sort_input_targets_contain_srfs));
+			/* likewise for grouping_target vs. scanjoin_target */
+			split_pathtarget_at_srfs(root, grouping_target, scanjoin_target,
+									 &grouping_targets,
+									 &grouping_targets_contain_srfs);
+			grouping_target = linitial_node(PathTarget, grouping_targets);
+			Assert(!linitial_int(grouping_targets_contain_srfs));
+			/* scanjoin_target will not have any SRFs precomputed for it */
+			split_pathtarget_at_srfs(root, scanjoin_target, NULL,
+									 &scanjoin_targets,
+									 &scanjoin_targets_contain_srfs);
+			scanjoin_target = linitial_node(PathTarget, scanjoin_targets);
+			Assert(!linitial_int(scanjoin_targets_contain_srfs));
+		}
+		else
+		{
+			/* initialize lists; for most of these, dummy values are OK */
+			final_targets = final_targets_contain_srfs = NIL;
+			sort_input_targets = sort_input_targets_contain_srfs = NIL;
+			grouping_targets = grouping_targets_contain_srfs = NIL;
+			scanjoin_targets = list_make1(scanjoin_target);
+			scanjoin_targets_contain_srfs = NIL;
+		}
+
+		/* Apply scan/join target. */
+		scanjoin_target_same_exprs = list_length(scanjoin_targets) == 1
+			&& equal(scanjoin_target->exprs, current_rel->reltarget->exprs);
+		apply_scanjoin_target_to_paths(root, current_rel, scanjoin_targets,
+									   scanjoin_targets_contain_srfs,
+									   scanjoin_target_parallel_safe,
+									   scanjoin_target_same_exprs);
+
+		/*
+		 * Save the various upper-rel PathTargets we just computed into
+		 * root->upper_targets[].  The core code doesn't use this, but it
+		 * provides a convenient place for extensions to get at the info.  For
+		 * consistency, we save all the intermediate targets, even though some
+		 * of the corresponding upperrels might not be needed for this query.
+		 */
+		root->upper_targets[UPPERREL_FINAL] = final_target;
+		root->upper_targets[UPPERREL_ORDERED] = final_target;
+		root->upper_targets[UPPERREL_DISTINCT] = sort_input_target;
+		root->upper_targets[UPPERREL_WINDOW] = sort_input_target;
+		root->upper_targets[UPPERREL_GROUP_AGG] = grouping_target;
+
+		/*
+		 * If we have grouping and/or aggregation, consider ways to implement
+		 * that.  We build a new upperrel representing the output of this
+		 * phase.
+		 */
+		if (have_grouping)
+		{
+			current_rel = create_grouping_paths(root,
+												current_rel,
+												grouping_target,
+												grouping_target_parallel_safe,
+												&agg_costs,
+												gset_data);
+			/* Fix things up if grouping_target contains SRFs */
+			if (parse->hasTargetSRFs)
+				adjust_paths_for_srfs(root, current_rel,
+									  grouping_targets,
+									  grouping_targets_contain_srfs);
+		}
+
+		/*
+		 * If we have window functions, consider ways to implement those.  We
+		 * build a new upperrel representing the output of this phase.
+		 */
+		if (activeWindows)
+		{
+			current_rel = create_window_paths(root,
+											  current_rel,
+											  grouping_target,
+											  sort_input_target,
+											  sort_input_target_parallel_safe,
+											  wflists,
+											  activeWindows);
+			/* Fix things up if sort_input_target contains SRFs */
+			if (parse->hasTargetSRFs)
+				adjust_paths_for_srfs(root, current_rel,
+									  sort_input_targets,
+									  sort_input_targets_contain_srfs);
+		}
+
+		/*
+		 * If there is a DISTINCT clause, consider ways to implement that. We
+		 * build a new upperrel representing the output of this phase.
+		 */
+		if (parse->distinctClause)
+		{
+			current_rel = create_distinct_paths(root,
+												current_rel);
+		}
+	}							/* end of if (setOperations) */
+
+	/*
+	 * If ORDER BY was given, consider ways to implement that, and generate a
+	 * new upperrel containing only paths that emit the correct ordering and
+	 * project the correct final_target.  We can apply the original
+	 * limit_tuples limit in sort costing here, but only if there are no
+	 * postponed SRFs.
+	 */
+	if (parse->sortClause)
+	{
+		current_rel = create_ordered_paths(root,
+										   current_rel,
+										   final_target,
+										   final_target_parallel_safe,
+										   have_postponed_srfs ? -1.0 :
+										   limit_tuples);
+		/* Fix things up if final_target contains SRFs */
+		if (parse->hasTargetSRFs)
+			adjust_paths_for_srfs(root, current_rel,
+								  final_targets,
+								  final_targets_contain_srfs);
+	}
+
+	/*
+	 * Now we are prepared to build the final-output upperrel.
+	 */
+	final_rel = fetch_upper_rel(root, UPPERREL_FINAL, NULL);
+
+	/*
+	 * If the input rel is marked consider_parallel and there's nothing that's
+	 * not parallel-safe in the LIMIT clause, then the final_rel can be marked
+	 * consider_parallel as well.  Note that if the query has rowMarks or is
+	 * not a SELECT, consider_parallel will be false for every relation in the
+	 * query.
+	 */
+	if (current_rel->consider_parallel &&
+		is_parallel_safe(root, parse->limitOffset) &&
+		is_parallel_safe(root, parse->limitCount))
+		final_rel->consider_parallel = true;
+
+	/*
+	 * If the current_rel belongs to a single FDW, so does the final_rel.
+	 */
+	final_rel->serverid = current_rel->serverid;
+	final_rel->userid = current_rel->userid;
+	final_rel->useridiscurrent = current_rel->useridiscurrent;
+	final_rel->fdwroutine = current_rel->fdwroutine;
+
+	/*
+	 * Generate paths for the final_rel.  Insert all surviving paths, with
+	 * LockRows, Limit, and/or ModifyTable steps added if needed.
+	 */
+	foreach(lc, current_rel->pathlist)
+	{
+		Path	   *path = (Path *) lfirst(lc);
+
+		/*
+		 * If there is a FOR [KEY] UPDATE/SHARE clause, add the LockRows node.
+		 * (Note: we intentionally test parse->rowMarks not root->rowMarks
+		 * here.  If there are only non-locking rowmarks, they should be
+		 * handled by the ModifyTable node instead.  However, root->rowMarks
+		 * is what goes into the LockRows node.)
+		 */
+		if (parse->rowMarks)
+		{
+			path = (Path *) create_lockrows_path(root, final_rel, path,
+												 root->rowMarks,
+												 assign_special_exec_param(root));
+		}
+
+		/*
+		 * If there is a LIMIT/OFFSET clause, add the LIMIT node.
+		 */
+		if (limit_needed(parse))
+		{
+			path = (Path *) create_limit_path(root, final_rel, path,
+											  parse->limitOffset,
+											  parse->limitCount,
+											  offset_est, count_est);
+		}
+
+		/*
+		 * If this is an INSERT/UPDATE/DELETE, and we're not being called from
+		 * inheritance_planner, add the ModifyTable node.
+		 */
+		if (parse->commandType != CMD_SELECT && !inheritance_update)
+		{
+			Index		rootRelation;
+			List	   *withCheckOptionLists;
+			List	   *returningLists;
+			List	   *rowMarks;
+
+			/*
+			 * If target is a partition root table, we need to mark the
+			 * ModifyTable node appropriately for that.
+			 */
+			if (rt_fetch(parse->resultRelation, parse->rtable)->relkind ==
+				RELKIND_PARTITIONED_TABLE)
+				rootRelation = parse->resultRelation;
+			else
+				rootRelation = 0;
+
+			/*
+			 * Set up the WITH CHECK OPTION and RETURNING lists-of-lists, if
+			 * needed.
+			 */
+			if (parse->withCheckOptions)
+				withCheckOptionLists = list_make1(parse->withCheckOptions);
+			else
+				withCheckOptionLists = NIL;
+
+			if (parse->returningList)
+				returningLists = list_make1(parse->returningList);
+			else
+				returningLists = NIL;
+
+			/*
+			 * If there was a FOR [KEY] UPDATE/SHARE clause, the LockRows node
+			 * will have dealt with fetching non-locked marked rows, else we
+			 * need to have ModifyTable do that.
+			 */
+			if (parse->rowMarks)
+				rowMarks = NIL;
+			else
+				rowMarks = root->rowMarks;
+
+			path = (Path *)
+				create_modifytable_path(root, final_rel,
+										parse->commandType,
+										parse->canSetTag,
+										parse->resultRelation,
+										rootRelation,
+										false,
+										list_make1_int(parse->resultRelation),
+										list_make1(path),
+										list_make1(root),
+										withCheckOptionLists,
+										returningLists,
+										rowMarks,
+										parse->onConflict,
+										assign_special_exec_param(root));
+		}
+
+		/* And shove it into final_rel */
+		add_path(final_rel, path);
+	}
+
+	/*
+	 * Generate partial paths for final_rel, too, if outer query levels might
+	 * be able to make use of them.
+	 */
+	if (final_rel->consider_parallel && root->query_level > 1 &&
+		!limit_needed(parse))
+	{
+		Assert(!parse->rowMarks && parse->commandType == CMD_SELECT);
+		foreach(lc, current_rel->partial_pathlist)
+		{
+			Path	   *partial_path = (Path *) lfirst(lc);
+
+			add_partial_path(final_rel, partial_path);
+		}
+	}
+
+	extra.limit_needed = limit_needed(parse);
+	extra.limit_tuples = limit_tuples;
+	extra.count_est = count_est;
+	extra.offset_est = offset_est;
+
+	/*
+	 * If there is an FDW that's responsible for all baserels of the query,
+	 * let it consider adding ForeignPaths.
+	 */
+	if (final_rel->fdwroutine &&
+		final_rel->fdwroutine->GetForeignUpperPaths)
+		final_rel->fdwroutine->GetForeignUpperPaths(root, UPPERREL_FINAL,
+													current_rel, final_rel,
+													&extra);
+
+	/* Let extensions possibly add some more paths */
+	if (create_upper_paths_hook)
+		(*create_upper_paths_hook) (root, UPPERREL_FINAL,
+									current_rel, final_rel, &extra);
+
+	/* Note: currently, we leave it to callers to do set_cheapest() */
+}
+
+/*
+ * Do preprocessing for groupingSets clause and related data.  This handles the
+ * preliminary steps of expanding the grouping sets, organizing them into lists
+ * of rollups, and preparing annotations which will later be filled in with
+ * size estimates.
+ */
+static grouping_sets_data *
+preprocess_grouping_sets(PlannerInfo *root)
+{
+	Query	   *parse = root->parse;
+	List	   *sets;
+	int			maxref = 0;
+	ListCell   *lc;
+	ListCell   *lc_set;
+	grouping_sets_data *gd = palloc0(sizeof(grouping_sets_data));
+
+	parse->groupingSets = expand_grouping_sets(parse->groupingSets, -1);
+
+	gd->any_hashable = false;
+	gd->unhashable_refs = NULL;
+	gd->unsortable_refs = NULL;
+	gd->unsortable_sets = NIL;
+
+	if (parse->groupClause)
+	{
+		ListCell   *lc;
+
+		foreach(lc, parse->groupClause)
+		{
+			SortGroupClause *gc = lfirst_node(SortGroupClause, lc);
+			Index		ref = gc->tleSortGroupRef;
+
+			if (ref > maxref)
+				maxref = ref;
+
+			if (!gc->hashable)
+				gd->unhashable_refs = bms_add_member(gd->unhashable_refs, ref);
+
+			if (!OidIsValid(gc->sortop))
+				gd->unsortable_refs = bms_add_member(gd->unsortable_refs, ref);
+		}
+	}
+
+	/* Allocate workspace array for remapping */
+	gd->tleref_to_colnum_map = (int *) palloc((maxref + 1) * sizeof(int));
+
+	/*
+	 * If we have any unsortable sets, we must extract them before trying to
+	 * prepare rollups. Unsortable sets don't go through
+	 * reorder_grouping_sets, so we must apply the GroupingSetData annotation
+	 * here.
+	 */
+	if (!bms_is_empty(gd->unsortable_refs))
+	{
+		List	   *sortable_sets = NIL;
+
+		foreach(lc, parse->groupingSets)
+		{
+			List	   *gset = (List *) lfirst(lc);
+
+			if (bms_overlap_list(gd->unsortable_refs, gset))
+			{
+				GroupingSetData *gs = makeNode(GroupingSetData);
+
+				gs->set = gset;
+				gd->unsortable_sets = lappend(gd->unsortable_sets, gs);
+
+				/*
+				 * We must enforce here that an unsortable set is hashable;
+				 * later code assumes this.  Parse analysis only checks that
+				 * every individual column is either hashable or sortable.
+				 *
+				 * Note that passing this test doesn't guarantee we can
+				 * generate a plan; there might be other showstoppers.
+				 */
+				if (bms_overlap_list(gd->unhashable_refs, gset))
+					ereport(ERROR,
+							(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+							 errmsg("could not implement GROUP BY"),
+							 errdetail("Some of the datatypes only support hashing, while others only support sorting.")));
+			}
+			else
+				sortable_sets = lappend(sortable_sets, gset);
+		}
+
+		if (sortable_sets)
+			sets = extract_rollup_sets(sortable_sets);
+		else
+			sets = NIL;
+	}
+	else
+		sets = extract_rollup_sets(parse->groupingSets);
+
+	foreach(lc_set, sets)
+	{
+		List	   *current_sets = (List *) lfirst(lc_set);
+		RollupData *rollup = makeNode(RollupData);
+		GroupingSetData *gs;
+
+		/*
+		 * Reorder the current list of grouping sets into correct prefix
+		 * order.  If only one aggregation pass is needed, try to make the
+		 * list match the ORDER BY clause; if more than one pass is needed, we
+		 * don't bother with that.
+		 *
+		 * Note that this reorders the sets from smallest-member-first to
+		 * largest-member-first, and applies the GroupingSetData annotations,
+		 * though the data will be filled in later.
+		 */
+		current_sets = reorder_grouping_sets(current_sets,
+											 (list_length(sets) == 1
+											  ? parse->sortClause
+											  : NIL));
+
+		/*
+		 * Get the initial (and therefore largest) grouping set.
+		 */
+		gs = linitial_node(GroupingSetData, current_sets);
+
+		/*
+		 * Order the groupClause appropriately.  If the first grouping set is
+		 * empty, then the groupClause must also be empty; otherwise we have
+		 * to force the groupClause to match that grouping set's order.
+		 *
+		 * (The first grouping set can be empty even though parse->groupClause
+		 * is not empty only if all non-empty grouping sets are unsortable.
+		 * The groupClauses for hashed grouping sets are built later on.)
+		 */
+		if (gs->set)
+			rollup->groupClause = preprocess_groupclause(root, gs->set);
+		else
+			rollup->groupClause = NIL;
+
+		/*
+		 * Is it hashable? We pretend empty sets are hashable even though we
+		 * actually force them not to be hashed later. But don't bother if
+		 * there's nothing but empty sets (since in that case we can't hash
+		 * anything).
+		 */
+		if (gs->set &&
+			!bms_overlap_list(gd->unhashable_refs, gs->set))
+		{
+			rollup->hashable = true;
+			gd->any_hashable = true;
+		}
+
+		/*
+		 * Now that we've pinned down an order for the groupClause for this
+		 * list of grouping sets, we need to remap the entries in the grouping
+		 * sets from sortgrouprefs to plain indices (0-based) into the
+		 * groupClause for this collection of grouping sets. We keep the
+		 * original form for later use, though.
+		 */
+		rollup->gsets = remap_to_groupclause_idx(rollup->groupClause,
+												 current_sets,
+												 gd->tleref_to_colnum_map);
+		rollup->gsets_data = current_sets;
+
+		gd->rollups = lappend(gd->rollups, rollup);
+	}
+
+	if (gd->unsortable_sets)
+	{
+		/*
+		 * We have not yet pinned down a groupclause for this, but we will
+		 * need index-based lists for estimation purposes. Construct
+		 * hash_sets_idx based on the entire original groupclause for now.
+		 */
+		gd->hash_sets_idx = remap_to_groupclause_idx(parse->groupClause,
+													 gd->unsortable_sets,
+													 gd->tleref_to_colnum_map);
+		gd->any_hashable = true;
+	}
+
+	return gd;
+}
+
+/*
+ * Given a groupclause and a list of GroupingSetData, return equivalent sets
+ * (without annotation) mapped to indexes into the given groupclause.
+ */
+static List *
+remap_to_groupclause_idx(List *groupClause,
+						 List *gsets,
+						 int *tleref_to_colnum_map)
+{
+	int			ref = 0;
+	List	   *result = NIL;
+	ListCell   *lc;
+
+	foreach(lc, groupClause)
+	{
+		SortGroupClause *gc = lfirst_node(SortGroupClause, lc);
+
+		tleref_to_colnum_map[gc->tleSortGroupRef] = ref++;
+	}
+
+	foreach(lc, gsets)
+	{
+		List	   *set = NIL;
+		ListCell   *lc2;
+		GroupingSetData *gs = lfirst_node(GroupingSetData, lc);
+
+		foreach(lc2, gs->set)
+		{
+			set = lappend_int(set, tleref_to_colnum_map[lfirst_int(lc2)]);
+		}
+
+		result = lappend(result, set);
+	}
+
+	return result;
+}
+
+
+/*
+ * preprocess_rowmarks - set up PlanRowMarks if needed
+ */
+static void
+preprocess_rowmarks(PlannerInfo *root)
+{
+	Query	   *parse = root->parse;
+	Bitmapset  *rels;
+	List	   *prowmarks;
+	ListCell   *l;
+	int			i;
+
+	if (parse->rowMarks)
+	{
+		/*
+		 * We've got trouble if FOR [KEY] UPDATE/SHARE appears inside
+		 * grouping, since grouping renders a reference to individual tuple
+		 * CTIDs invalid.  This is also checked at parse time, but that's
+		 * insufficient because of rule substitution, query pullup, etc.
+		 */
+		CheckSelectLocking(parse, linitial_node(RowMarkClause,
+												parse->rowMarks)->strength);
+	}
+	else
+	{
+		/*
+		 * We only need rowmarks for UPDATE, DELETE, or FOR [KEY]
+		 * UPDATE/SHARE.
+		 */
+		if (parse->commandType != CMD_UPDATE &&
+			parse->commandType != CMD_DELETE)
+			return;
+	}
+
+	/*
+	 * We need to have rowmarks for all base relations except the target. We
+	 * make a bitmapset of all base rels and then remove the items we don't
+	 * need or have FOR [KEY] UPDATE/SHARE marks for.
+	 */
+	rels = get_relids_in_jointree((Node *) parse->jointree, false);
+	if (parse->resultRelation)
+		rels = bms_del_member(rels, parse->resultRelation);
+
+	/*
+	 * Convert RowMarkClauses to PlanRowMark representation.
+	 */
+	prowmarks = NIL;
+	foreach(l, parse->rowMarks)
+	{
+		RowMarkClause *rc = lfirst_node(RowMarkClause, l);
+		RangeTblEntry *rte = rt_fetch(rc->rti, parse->rtable);
+		PlanRowMark *newrc;
+
+		/*
+		 * Currently, it is syntactically impossible to have FOR UPDATE et al
+		 * applied to an update/delete target rel.  If that ever becomes
+		 * possible, we should drop the target from the PlanRowMark list.
+		 */
+		Assert(rc->rti != parse->resultRelation);
+
+		/*
+		 * Ignore RowMarkClauses for subqueries; they aren't real tables and
+		 * can't support true locking.  Subqueries that got flattened into the
+		 * main query should be ignored completely.  Any that didn't will get
+		 * ROW_MARK_COPY items in the next loop.
+		 */
+		if (rte->rtekind != RTE_RELATION)
+			continue;
+
+		rels = bms_del_member(rels, rc->rti);
+
+		newrc = makeNode(PlanRowMark);
+		newrc->rti = newrc->prti = rc->rti;
+		newrc->rowmarkId = ++(root->glob->lastRowMarkId);
+		newrc->markType = select_rowmark_type(rte, rc->strength);
+		newrc->allMarkTypes = (1 << newrc->markType);
+		newrc->strength = rc->strength;
+		newrc->waitPolicy = rc->waitPolicy;
+		newrc->isParent = false;
+
+		prowmarks = lappend(prowmarks, newrc);
+	}
+
+	/*
+	 * Now, add rowmarks for any non-target, non-locked base relations.
+	 */
+	i = 0;
+	foreach(l, parse->rtable)
+	{
+		RangeTblEntry *rte = lfirst_node(RangeTblEntry, l);
+		PlanRowMark *newrc;
+
+		i++;
+		if (!bms_is_member(i, rels))
+			continue;
+
+		newrc = makeNode(PlanRowMark);
+		newrc->rti = newrc->prti = i;
+		newrc->rowmarkId = ++(root->glob->lastRowMarkId);
+		newrc->markType = select_rowmark_type(rte, LCS_NONE);
+		newrc->allMarkTypes = (1 << newrc->markType);
+		newrc->strength = LCS_NONE;
+		newrc->waitPolicy = LockWaitBlock;	/* doesn't matter */
+		newrc->isParent = false;
+
+		prowmarks = lappend(prowmarks, newrc);
+	}
+
+	root->rowMarks = prowmarks;
+}
+
+/*
+ * Select RowMarkType to use for a given table
+ */
+RowMarkType
+select_rowmark_type(RangeTblEntry *rte, LockClauseStrength strength)
+{
+	if (rte->rtekind != RTE_RELATION)
+	{
+		/* If it's not a table at all, use ROW_MARK_COPY */
+		return ROW_MARK_COPY;
+	}
+	else if (rte->relkind == RELKIND_FOREIGN_TABLE)
+	{
+		/* Let the FDW select the rowmark type, if it wants to */
+		FdwRoutine *fdwroutine = GetFdwRoutineByRelId(rte->relid);
+
+		if (fdwroutine->GetForeignRowMarkType != NULL)
+			return fdwroutine->GetForeignRowMarkType(rte, strength);
+		/* Otherwise, use ROW_MARK_COPY by default */
+		return ROW_MARK_COPY;
+	}
+	else
+	{
+		/* Regular table, apply the appropriate lock type */
+		switch (strength)
+		{
+			case LCS_NONE:
+
+				/*
+				 * We don't need a tuple lock, only the ability to re-fetch
+				 * the row.
+				 */
+				return ROW_MARK_REFERENCE;
+				break;
+			case LCS_FORKEYSHARE:
+				return ROW_MARK_KEYSHARE;
+				break;
+			case LCS_FORSHARE:
+				return ROW_MARK_SHARE;
+				break;
+			case LCS_FORNOKEYUPDATE:
+				return ROW_MARK_NOKEYEXCLUSIVE;
+				break;
+			case LCS_FORUPDATE:
+				return ROW_MARK_EXCLUSIVE;
+				break;
+		}
+		elog(ERROR, "unrecognized LockClauseStrength %d", (int) strength);
+		return ROW_MARK_EXCLUSIVE;	/* keep compiler quiet */
+	}
+}
+
+/*
+ * preprocess_limit - do pre-estimation for LIMIT and/or OFFSET clauses
+ *
+ * We try to estimate the values of the LIMIT/OFFSET clauses, and pass the
+ * results back in *count_est and *offset_est.  These variables are set to
+ * 0 if the corresponding clause is not present, and -1 if it's present
+ * but we couldn't estimate the value for it.  (The "0" convention is OK
+ * for OFFSET but a little bit bogus for LIMIT: effectively we estimate
+ * LIMIT 0 as though it were LIMIT 1.  But this is in line with the planner's
+ * usual practice of never estimating less than one row.)  These values will
+ * be passed to create_limit_path, which see if you change this code.
+ *
+ * The return value is the suitably adjusted tuple_fraction to use for
+ * planning the query.  This adjustment is not overridable, since it reflects
+ * plan actions that grouping_planner() will certainly take, not assumptions
+ * about context.
+ */
+static double
+preprocess_limit(PlannerInfo *root, double tuple_fraction,
+				 int64 *offset_est, int64 *count_est)
+{
+	Query	   *parse = root->parse;
+	Node	   *est;
+	double		limit_fraction;
+
+	/* Should not be called unless LIMIT or OFFSET */
+	Assert(parse->limitCount || parse->limitOffset);
+
+	/*
+	 * Try to obtain the clause values.  We use estimate_expression_value
+	 * primarily because it can sometimes do something useful with Params.
+	 */
+	if (parse->limitCount)
+	{
+		est = estimate_expression_value(root, parse->limitCount);
+		if (est && IsA(est, Const))
+		{
+			if (((Const *) est)->constisnull)
+			{
+				/* NULL indicates LIMIT ALL, ie, no limit */
+				*count_est = 0; /* treat as not present */
+			}
+			else
+			{
+				*count_est = DatumGetInt64(((Const *) est)->constvalue);
+				if (*count_est <= 0)
+					*count_est = 1; /* force to at least 1 */
+			}
+		}
+		else
+			*count_est = -1;	/* can't estimate */
+	}
+	else
+		*count_est = 0;			/* not present */
+
+	if (parse->limitOffset)
+	{
+		est = estimate_expression_value(root, parse->limitOffset);
+		if (est && IsA(est, Const))
+		{
+			if (((Const *) est)->constisnull)
+			{
+				/* Treat NULL as no offset; the executor will too */
+				*offset_est = 0;	/* treat as not present */
+			}
+			else
+			{
+				*offset_est = DatumGetInt64(((Const *) est)->constvalue);
+				if (*offset_est < 0)
+					*offset_est = 0;	/* treat as not present */
+			}
+		}
+		else
+			*offset_est = -1;	/* can't estimate */
+	}
+	else
+		*offset_est = 0;		/* not present */
+
+	if (*count_est != 0)
+	{
+		/*
+		 * A LIMIT clause limits the absolute number of tuples returned.
+		 * However, if it's not a constant LIMIT then we have to guess; for
+		 * lack of a better idea, assume 10% of the plan's result is wanted.
+		 */
+		if (*count_est < 0 || *offset_est < 0)
+		{
+			/* LIMIT or OFFSET is an expression ... punt ... */
+			limit_fraction = 0.10;
+		}
+		else
+		{
+			/* LIMIT (plus OFFSET, if any) is max number of tuples needed */
+			limit_fraction = (double) *count_est + (double) *offset_est;
+		}
+
+		/*
+		 * If we have absolute limits from both caller and LIMIT, use the
+		 * smaller value; likewise if they are both fractional.  If one is
+		 * fractional and the other absolute, we can't easily determine which
+		 * is smaller, but we use the heuristic that the absolute will usually
+		 * be smaller.
+		 */
+		if (tuple_fraction >= 1.0)
+		{
+			if (limit_fraction >= 1.0)
+			{
+				/* both absolute */
+				tuple_fraction = Min(tuple_fraction, limit_fraction);
+			}
+			else
+			{
+				/* caller absolute, limit fractional; use caller's value */
+			}
+		}
+		else if (tuple_fraction > 0.0)
+		{
+			if (limit_fraction >= 1.0)
+			{
+				/* caller fractional, limit absolute; use limit */
+				tuple_fraction = limit_fraction;
+			}
+			else
+			{
+				/* both fractional */
+				tuple_fraction = Min(tuple_fraction, limit_fraction);
+			}
+		}
+		else
+		{
+			/* no info from caller, just use limit */
+			tuple_fraction = limit_fraction;
+		}
+	}
+	else if (*offset_est != 0 && tuple_fraction > 0.0)
+	{
+		/*
+		 * We have an OFFSET but no LIMIT.  This acts entirely differently
+		 * from the LIMIT case: here, we need to increase rather than decrease
+		 * the caller's tuple_fraction, because the OFFSET acts to cause more
+		 * tuples to be fetched instead of fewer.  This only matters if we got
+		 * a tuple_fraction > 0, however.
+		 *
+		 * As above, use 10% if OFFSET is present but unestimatable.
+		 */
+		if (*offset_est < 0)
+			limit_fraction = 0.10;
+		else
+			limit_fraction = (double) *offset_est;
+
+		/*
+		 * If we have absolute counts from both caller and OFFSET, add them
+		 * together; likewise if they are both fractional.  If one is
+		 * fractional and the other absolute, we want to take the larger, and
+		 * we heuristically assume that's the fractional one.
+		 */
+		if (tuple_fraction >= 1.0)
+		{
+			if (limit_fraction >= 1.0)
+			{
+				/* both absolute, so add them together */
+				tuple_fraction += limit_fraction;
+			}
+			else
+			{
+				/* caller absolute, limit fractional; use limit */
+				tuple_fraction = limit_fraction;
+			}
+		}
+		else
+		{
+			if (limit_fraction >= 1.0)
+			{
+				/* caller fractional, limit absolute; use caller's value */
+			}
+			else
+			{
+				/* both fractional, so add them together */
+				tuple_fraction += limit_fraction;
+				if (tuple_fraction >= 1.0)
+					tuple_fraction = 0.0;	/* assume fetch all */
+			}
+		}
+	}
+
+	return tuple_fraction;
+}
+
+/*
+ * limit_needed - do we actually need a Limit plan node?
+ *
+ * If we have constant-zero OFFSET and constant-null LIMIT, we can skip adding
+ * a Limit node.  This is worth checking for because "OFFSET 0" is a common
+ * locution for an optimization fence.  (Because other places in the planner
+ * merely check whether parse->limitOffset isn't NULL, it will still work as
+ * an optimization fence --- we're just suppressing unnecessary run-time
+ * overhead.)
+ *
+ * This might look like it could be merged into preprocess_limit, but there's
+ * a key distinction: here we need hard constants in OFFSET/LIMIT, whereas
+ * in preprocess_limit it's good enough to consider estimated values.
+ */
+bool
+limit_needed(Query *parse)
+{
+	Node	   *node;
+
+	node = parse->limitCount;
+	if (node)
+	{
+		if (IsA(node, Const))
+		{
+			/* NULL indicates LIMIT ALL, ie, no limit */
+			if (!((Const *) node)->constisnull)
+				return true;	/* LIMIT with a constant value */
+		}
+		else
+			return true;		/* non-constant LIMIT */
+	}
+
+	node = parse->limitOffset;
+	if (node)
+	{
+		if (IsA(node, Const))
+		{
+			/* Treat NULL as no offset; the executor would too */
+			if (!((Const *) node)->constisnull)
+			{
+				int64		offset = DatumGetInt64(((Const *) node)->constvalue);
+
+				if (offset != 0)
+					return true;	/* OFFSET with a nonzero value */
+			}
+		}
+		else
+			return true;		/* non-constant OFFSET */
+	}
+
+	return false;				/* don't need a Limit plan node */
+}
+
+
+/*
+ * remove_useless_groupby_columns
+ *		Remove any columns in the GROUP BY clause that are redundant due to
+ *		being functionally dependent on other GROUP BY columns.
+ *
+ * Since some other DBMSes do not allow references to ungrouped columns, it's
+ * not unusual to find all columns listed in GROUP BY even though listing the
+ * primary-key columns would be sufficient.  Deleting such excess columns
+ * avoids redundant sorting work, so it's worth doing.  When we do this, we
+ * must mark the plan as dependent on the pkey constraint (compare the
+ * parser's check_ungrouped_columns() and check_functional_grouping()).
+ *
+ * In principle, we could treat any NOT-NULL columns appearing in a UNIQUE
+ * index as the determining columns.  But as with check_functional_grouping(),
+ * there's currently no way to represent dependency on a NOT NULL constraint,
+ * so we consider only the pkey for now.
+ */
+static void
+remove_useless_groupby_columns(PlannerInfo *root)
+{
+	Query	   *parse = root->parse;
+	Bitmapset **groupbyattnos;
+	Bitmapset **surplusvars;
+	ListCell   *lc;
+	int			relid;
+
+	/* No chance to do anything if there are less than two GROUP BY items */
+	if (list_length(parse->groupClause) < 2)
+		return;
+
+	/* Don't fiddle with the GROUP BY clause if the query has grouping sets */
+	if (parse->groupingSets)
+		return;
+
+	/*
+	 * Scan the GROUP BY clause to find GROUP BY items that are simple Vars.
+	 * Fill groupbyattnos[k] with a bitmapset of the column attnos of RTE k
+	 * that are GROUP BY items.
+	 */
+	groupbyattnos = (Bitmapset **) palloc0(sizeof(Bitmapset *) *
+										   (list_length(parse->rtable) + 1));
+	foreach(lc, parse->groupClause)
+	{
+		SortGroupClause *sgc = lfirst_node(SortGroupClause, lc);
+		TargetEntry *tle = get_sortgroupclause_tle(sgc, parse->targetList);
+		Var		   *var = (Var *) tle->expr;
+
+		/*
+		 * Ignore non-Vars and Vars from other query levels.
+		 *
+		 * XXX in principle, stable expressions containing Vars could also be
+		 * removed, if all the Vars are functionally dependent on other GROUP
+		 * BY items.  But it's not clear that such cases occur often enough to
+		 * be worth troubling over.
+		 */
+		if (!IsA(var, Var) ||
+			var->varlevelsup > 0)
+			continue;
+
+		/* OK, remember we have this Var */
+		relid = var->varno;
+		Assert(relid <= list_length(parse->rtable));
+		groupbyattnos[relid] = bms_add_member(groupbyattnos[relid],
+											  var->varattno - FirstLowInvalidHeapAttributeNumber);
+	}
+
+	/*
+	 * Consider each relation and see if it is possible to remove some of its
+	 * Vars from GROUP BY.  For simplicity and speed, we do the actual removal
+	 * in a separate pass.  Here, we just fill surplusvars[k] with a bitmapset
+	 * of the column attnos of RTE k that are removable GROUP BY items.
+	 */
+	surplusvars = NULL;			/* don't allocate array unless required */
+	relid = 0;
+	foreach(lc, parse->rtable)
+	{
+		RangeTblEntry *rte = lfirst_node(RangeTblEntry, lc);
+		Bitmapset  *relattnos;
+		Bitmapset  *pkattnos;
+		Oid			constraintOid;
+
+		relid++;
+
+		/* Only plain relations could have primary-key constraints */
+		if (rte->rtekind != RTE_RELATION)
+			continue;
+
+		/*
+		 * We must skip inheritance parent tables as some of the child rels
+		 * may cause duplicate rows.  This cannot happen with partitioned
+		 * tables, however.
+		 */
+		if (rte->inh && rte->relkind != RELKIND_PARTITIONED_TABLE)
+			continue;
+
+		/* Nothing to do unless this rel has multiple Vars in GROUP BY */
+		relattnos = groupbyattnos[relid];
+		if (bms_membership(relattnos) != BMS_MULTIPLE)
+			continue;
+
+		/*
+		 * Can't remove any columns for this rel if there is no suitable
+		 * (i.e., nondeferrable) primary key constraint.
+		 */
+		pkattnos = get_primary_key_attnos(rte->relid, false, &constraintOid);
+		if (pkattnos == NULL)
+			continue;
+
+		/*
+		 * If the primary key is a proper subset of relattnos then we have
+		 * some items in the GROUP BY that can be removed.
+		 */
+		if (bms_subset_compare(pkattnos, relattnos) == BMS_SUBSET1)
+		{
+			/*
+			 * To easily remember whether we've found anything to do, we don't
+			 * allocate the surplusvars[] array until we find something.
+			 */
+			if (surplusvars == NULL)
+				surplusvars = (Bitmapset **) palloc0(sizeof(Bitmapset *) *
+													 (list_length(parse->rtable) + 1));
+
+			/* Remember the attnos of the removable columns */
+			surplusvars[relid] = bms_difference(relattnos, pkattnos);
+
+			/* Also, mark the resulting plan as dependent on this constraint */
+			parse->constraintDeps = lappend_oid(parse->constraintDeps,
+												constraintOid);
+		}
+	}
+
+	/*
+	 * If we found any surplus Vars, build a new GROUP BY clause without them.
+	 * (Note: this may leave some TLEs with unreferenced ressortgroupref
+	 * markings, but that's harmless.)
+	 */
+	if (surplusvars != NULL)
+	{
+		List	   *new_groupby = NIL;
+
+		foreach(lc, parse->groupClause)
+		{
+			SortGroupClause *sgc = lfirst_node(SortGroupClause, lc);
+			TargetEntry *tle = get_sortgroupclause_tle(sgc, parse->targetList);
+			Var		   *var = (Var *) tle->expr;
+
+			/*
+			 * New list must include non-Vars, outer Vars, and anything not
+			 * marked as surplus.
+			 */
+			if (!IsA(var, Var) ||
+				var->varlevelsup > 0 ||
+				!bms_is_member(var->varattno - FirstLowInvalidHeapAttributeNumber,
+							   surplusvars[var->varno]))
+				new_groupby = lappend(new_groupby, sgc);
+		}
+
+		parse->groupClause = new_groupby;
+	}
+}
+
+/*
+ * preprocess_groupclause - do preparatory work on GROUP BY clause
+ *
+ * The idea here is to adjust the ordering of the GROUP BY elements
+ * (which in itself is semantically insignificant) to match ORDER BY,
+ * thereby allowing a single sort operation to both implement the ORDER BY
+ * requirement and set up for a Unique step that implements GROUP BY.
+ *
+ * In principle it might be interesting to consider other orderings of the
+ * GROUP BY elements, which could match the sort ordering of other
+ * possible plans (eg an indexscan) and thereby reduce cost.  We don't
+ * bother with that, though.  Hashed grouping will frequently win anyway.
+ *
+ * Note: we need no comparable processing of the distinctClause because
+ * the parser already enforced that that matches ORDER BY.
+ *
+ * For grouping sets, the order of items is instead forced to agree with that
+ * of the grouping set (and items not in the grouping set are skipped). The
+ * work of sorting the order of grouping set elements to match the ORDER BY if
+ * possible is done elsewhere.
+ */
+static List *
+preprocess_groupclause(PlannerInfo *root, List *force)
+{
+	Query	   *parse = root->parse;
+	List	   *new_groupclause = NIL;
+	bool		partial_match;
+	ListCell   *sl;
+	ListCell   *gl;
+
+	/* For grouping sets, we need to force the ordering */
+	if (force)
+	{
+		foreach(sl, force)
+		{
+			Index		ref = lfirst_int(sl);
+			SortGroupClause *cl = get_sortgroupref_clause(ref, parse->groupClause);
+
+			new_groupclause = lappend(new_groupclause, cl);
+		}
+
+		return new_groupclause;
+	}
+
+	/* If no ORDER BY, nothing useful to do here */
+	if (parse->sortClause == NIL)
+		return parse->groupClause;
+
+	/*
+	 * Scan the ORDER BY clause and construct a list of matching GROUP BY
+	 * items, but only as far as we can make a matching prefix.
+	 *
+	 * This code assumes that the sortClause contains no duplicate items.
+	 */
+	foreach(sl, parse->sortClause)
+	{
+		SortGroupClause *sc = lfirst_node(SortGroupClause, sl);
+
+		foreach(gl, parse->groupClause)
+		{
+			SortGroupClause *gc = lfirst_node(SortGroupClause, gl);
+
+			if (equal(gc, sc))
+			{
+				new_groupclause = lappend(new_groupclause, gc);
+				break;
+			}
+		}
+		if (gl == NULL)
+			break;				/* no match, so stop scanning */
+	}
+
+	/* Did we match all of the ORDER BY list, or just some of it? */
+	partial_match = (sl != NULL);
+
+	/* If no match at all, no point in reordering GROUP BY */
+	if (new_groupclause == NIL)
+		return parse->groupClause;
+
+	/*
+	 * Add any remaining GROUP BY items to the new list, but only if we were
+	 * able to make a complete match.  In other words, we only rearrange the
+	 * GROUP BY list if the result is that one list is a prefix of the other
+	 * --- otherwise there's no possibility of a common sort.  Also, give up
+	 * if there are any non-sortable GROUP BY items, since then there's no
+	 * hope anyway.
+	 */
+	foreach(gl, parse->groupClause)
+	{
+		SortGroupClause *gc = lfirst_node(SortGroupClause, gl);
+
+		if (list_member_ptr(new_groupclause, gc))
+			continue;			/* it matched an ORDER BY item */
+		if (partial_match)
+			return parse->groupClause;	/* give up, no common sort possible */
+		if (!OidIsValid(gc->sortop))
+			return parse->groupClause;	/* give up, GROUP BY can't be sorted */
+		new_groupclause = lappend(new_groupclause, gc);
+	}
+
+	/* Success --- install the rearranged GROUP BY list */
+	Assert(list_length(parse->groupClause) == list_length(new_groupclause));
+	return new_groupclause;
+}
+
+/*
+ * Extract lists of grouping sets that can be implemented using a single
+ * rollup-type aggregate pass each. Returns a list of lists of grouping sets.
+ *
+ * Input must be sorted with smallest sets first. Result has each sublist
+ * sorted with smallest sets first.
+ *
+ * We want to produce the absolute minimum possible number of lists here to
+ * avoid excess sorts. Fortunately, there is an algorithm for this; the problem
+ * of finding the minimal partition of a partially-ordered set into chains
+ * (which is what we need, taking the list of grouping sets as a poset ordered
+ * by set inclusion) can be mapped to the problem of finding the maximum
+ * cardinality matching on a bipartite graph, which is solvable in polynomial
+ * time with a worst case of no worse than O(n^2.5) and usually much
+ * better. Since our N is at most 4096, we don't need to consider fallbacks to
+ * heuristic or approximate methods.  (Planning time for a 12-d cube is under
+ * half a second on my modest system even with optimization off and assertions
+ * on.)
+ */
+static List *
+extract_rollup_sets(List *groupingSets)
+{
+	int			num_sets_raw = list_length(groupingSets);
+	int			num_empty = 0;
+	int			num_sets = 0;	/* distinct sets */
+	int			num_chains = 0;
+	List	   *result = NIL;
+	List	  **results;
+	List	  **orig_sets;
+	Bitmapset **set_masks;
+	int		   *chains;
+	short	  **adjacency;
+	short	   *adjacency_buf;
+	BipartiteMatchState *state;
+	int			i;
+	int			j;
+	int			j_size;
+	ListCell   *lc1 = list_head(groupingSets);
+	ListCell   *lc;
+
+	/*
+	 * Start by stripping out empty sets.  The algorithm doesn't require this,
+	 * but the planner currently needs all empty sets to be returned in the
+	 * first list, so we strip them here and add them back after.
+	 */
+	while (lc1 && lfirst(lc1) == NIL)
+	{
+		++num_empty;
+		lc1 = lnext(lc1);
+	}
+
+	/* bail out now if it turns out that all we had were empty sets. */
+	if (!lc1)
+		return list_make1(groupingSets);
+
+	/*----------
+	 * We don't strictly need to remove duplicate sets here, but if we don't,
+	 * they tend to become scattered through the result, which is a bit
+	 * confusing (and irritating if we ever decide to optimize them out).
+	 * So we remove them here and add them back after.
+	 *
+	 * For each non-duplicate set, we fill in the following:
+	 *
+	 * orig_sets[i] = list of the original set lists
+	 * set_masks[i] = bitmapset for testing inclusion
+	 * adjacency[i] = array [n, v1, v2, ... vn] of adjacency indices
+	 *
+	 * chains[i] will be the result group this set is assigned to.
+	 *
+	 * We index all of these from 1 rather than 0 because it is convenient
+	 * to leave 0 free for the NIL node in the graph algorithm.
+	 *----------
+	 */
+	orig_sets = palloc0((num_sets_raw + 1) * sizeof(List *));
+	set_masks = palloc0((num_sets_raw + 1) * sizeof(Bitmapset *));
+	adjacency = palloc0((num_sets_raw + 1) * sizeof(short *));
+	adjacency_buf = palloc((num_sets_raw + 1) * sizeof(short));
+
+	j_size = 0;
+	j = 0;
+	i = 1;
+
+	for_each_cell(lc, lc1)
+	{
+		List	   *candidate = (List *) lfirst(lc);
+		Bitmapset  *candidate_set = NULL;
+		ListCell   *lc2;
+		int			dup_of = 0;
+
+		foreach(lc2, candidate)
+		{
+			candidate_set = bms_add_member(candidate_set, lfirst_int(lc2));
+		}
+
+		/* we can only be a dup if we're the same length as a previous set */
+		if (j_size == list_length(candidate))
+		{
+			int			k;
+
+			for (k = j; k < i; ++k)
+			{
+				if (bms_equal(set_masks[k], candidate_set))
+				{
+					dup_of = k;
+					break;
+				}
+			}
+		}
+		else if (j_size < list_length(candidate))
+		{
+			j_size = list_length(candidate);
+			j = i;
+		}
+
+		if (dup_of > 0)
+		{
+			orig_sets[dup_of] = lappend(orig_sets[dup_of], candidate);
+			bms_free(candidate_set);
+		}
+		else
+		{
+			int			k;
+			int			n_adj = 0;
+
+			orig_sets[i] = list_make1(candidate);
+			set_masks[i] = candidate_set;
+
+			/* fill in adjacency list; no need to compare equal-size sets */
+
+			for (k = j - 1; k > 0; --k)
+			{
+				if (bms_is_subset(set_masks[k], candidate_set))
+					adjacency_buf[++n_adj] = k;
+			}
+
+			if (n_adj > 0)
+			{
+				adjacency_buf[0] = n_adj;
+				adjacency[i] = palloc((n_adj + 1) * sizeof(short));
+				memcpy(adjacency[i], adjacency_buf, (n_adj + 1) * sizeof(short));
+			}
+			else
+				adjacency[i] = NULL;
+
+			++i;
+		}
+	}
+
+	num_sets = i - 1;
+
+	/*
+	 * Apply the graph matching algorithm to do the work.
+	 */
+	state = BipartiteMatch(num_sets, num_sets, adjacency);
+
+	/*
+	 * Now, the state->pair* fields have the info we need to assign sets to
+	 * chains. Two sets (u,v) belong to the same chain if pair_uv[u] = v or
+	 * pair_vu[v] = u (both will be true, but we check both so that we can do
+	 * it in one pass)
+	 */
+	chains = palloc0((num_sets + 1) * sizeof(int));
+
+	for (i = 1; i <= num_sets; ++i)
+	{
+		int			u = state->pair_vu[i];
+		int			v = state->pair_uv[i];
+
+		if (u > 0 && u < i)
+			chains[i] = chains[u];
+		else if (v > 0 && v < i)
+			chains[i] = chains[v];
+		else
+			chains[i] = ++num_chains;
+	}
+
+	/* build result lists. */
+	results = palloc0((num_chains + 1) * sizeof(List *));
+
+	for (i = 1; i <= num_sets; ++i)
+	{
+		int			c = chains[i];
+
+		Assert(c > 0);
+
+		results[c] = list_concat(results[c], orig_sets[i]);
+	}
+
+	/* push any empty sets back on the first list. */
+	while (num_empty-- > 0)
+		results[1] = lcons(NIL, results[1]);
+
+	/* make result list */
+	for (i = 1; i <= num_chains; ++i)
+		result = lappend(result, results[i]);
+
+	/*
+	 * Free all the things.
+	 *
+	 * (This is over-fussy for small sets but for large sets we could have
+	 * tied up a nontrivial amount of memory.)
+	 */
+	BipartiteMatchFree(state);
+	pfree(results);
+	pfree(chains);
+	for (i = 1; i <= num_sets; ++i)
+		if (adjacency[i])
+			pfree(adjacency[i]);
+	pfree(adjacency);
+	pfree(adjacency_buf);
+	pfree(orig_sets);
+	for (i = 1; i <= num_sets; ++i)
+		bms_free(set_masks[i]);
+	pfree(set_masks);
+
+	return result;
+}
+
+/*
+ * Reorder the elements of a list of grouping sets such that they have correct
+ * prefix relationships. Also inserts the GroupingSetData annotations.
+ *
+ * The input must be ordered with smallest sets first; the result is returned
+ * with largest sets first.  Note that the result shares no list substructure
+ * with the input, so it's safe for the caller to modify it later.
+ *
+ * If we're passed in a sortclause, we follow its order of columns to the
+ * extent possible, to minimize the chance that we add unnecessary sorts.
+ * (We're trying here to ensure that GROUPING SETS ((a,b,c),(c)) ORDER BY c,b,a
+ * gets implemented in one pass.)
+ */
+static List *
+reorder_grouping_sets(List *groupingsets, List *sortclause)
+{
+	ListCell   *lc;
+	List	   *previous = NIL;
+	List	   *result = NIL;
+
+	foreach(lc, groupingsets)
+	{
+		List	   *candidate = (List *) lfirst(lc);
+		List	   *new_elems = list_difference_int(candidate, previous);
+		GroupingSetData *gs = makeNode(GroupingSetData);
+
+		while (list_length(sortclause) > list_length(previous) &&
+			   list_length(new_elems) > 0)
+		{
+			SortGroupClause *sc = list_nth(sortclause, list_length(previous));
+			int			ref = sc->tleSortGroupRef;
+
+			if (list_member_int(new_elems, ref))
+			{
+				previous = lappend_int(previous, ref);
+				new_elems = list_delete_int(new_elems, ref);
+			}
+			else
+			{
+				/* diverged from the sortclause; give up on it */
+				sortclause = NIL;
+				break;
+			}
+		}
+
+		/*
+		 * Safe to use list_concat (which shares cells of the second arg)
+		 * because we know that new_elems does not share cells with anything.
+		 */
+		previous = list_concat(previous, new_elems);
+
+		gs->set = list_copy(previous);
+		result = lcons(gs, result);
+	}
+
+	list_free(previous);
+
+	return result;
+}
+
+/*
+ * Compute query_pathkeys and other pathkeys during plan generation
+ */
+static void
+standard_qp_callback(PlannerInfo *root, void *extra)
+{
+	Query	   *parse = root->parse;
+	standard_qp_extra *qp_extra = (standard_qp_extra *) extra;
+	List	   *tlist = root->processed_tlist;
+	List	   *activeWindows = qp_extra->activeWindows;
+
+	/*
+	 * Calculate pathkeys that represent grouping/ordering requirements.  The
+	 * sortClause is certainly sort-able, but GROUP BY and DISTINCT might not
+	 * be, in which case we just leave their pathkeys empty.
+	 */
+	if (qp_extra->groupClause &&
+		grouping_is_sortable(qp_extra->groupClause))
+		root->group_pathkeys =
+			make_pathkeys_for_sortclauses(root,
+										  qp_extra->groupClause,
+										  tlist);
+	else
+		root->group_pathkeys = NIL;
+
+	/* We consider only the first (bottom) window in pathkeys logic */
+	if (activeWindows != NIL)
+	{
+		WindowClause *wc = linitial_node(WindowClause, activeWindows);
+
+		root->window_pathkeys = make_pathkeys_for_window(root,
+														 wc,
+														 tlist);
+	}
+	else
+		root->window_pathkeys = NIL;
+
+	if (parse->distinctClause &&
+		grouping_is_sortable(parse->distinctClause))
+		root->distinct_pathkeys =
+			make_pathkeys_for_sortclauses(root,
+										  parse->distinctClause,
+										  tlist);
+	else
+		root->distinct_pathkeys = NIL;
+
+	root->sort_pathkeys =
+		make_pathkeys_for_sortclauses(root,
+									  parse->sortClause,
+									  tlist);
+
+	/*
+	 * Figure out whether we want a sorted result from query_planner.
+	 *
+	 * If we have a sortable GROUP BY clause, then we want a result sorted
+	 * properly for grouping.  Otherwise, if we have window functions to
+	 * evaluate, we try to sort for the first window.  Otherwise, if there's a
+	 * sortable DISTINCT clause that's more rigorous than the ORDER BY clause,
+	 * we try to produce output that's sufficiently well sorted for the
+	 * DISTINCT.  Otherwise, if there is an ORDER BY clause, we want to sort
+	 * by the ORDER BY clause.
+	 *
+	 * Note: if we have both ORDER BY and GROUP BY, and ORDER BY is a superset
+	 * of GROUP BY, it would be tempting to request sort by ORDER BY --- but
+	 * that might just leave us failing to exploit an available sort order at
+	 * all.  Needs more thought.  The choice for DISTINCT versus ORDER BY is
+	 * much easier, since we know that the parser ensured that one is a
+	 * superset of the other.
+	 */
+	if (root->group_pathkeys)
+		root->query_pathkeys = root->group_pathkeys;
+	else if (root->window_pathkeys)
+		root->query_pathkeys = root->window_pathkeys;
+	else if (list_length(root->distinct_pathkeys) >
+			 list_length(root->sort_pathkeys))
+		root->query_pathkeys = root->distinct_pathkeys;
+	else if (root->sort_pathkeys)
+		root->query_pathkeys = root->sort_pathkeys;
+	else
+		root->query_pathkeys = NIL;
+}
+
+/*
+ * Estimate number of groups produced by grouping clauses (1 if not grouping)
+ *
+ * path_rows: number of output rows from scan/join step
+ * gd: grouping sets data including list of grouping sets and their clauses
+ * target_list: target list containing group clause references
+ *
+ * If doing grouping sets, we also annotate the gsets data with the estimates
+ * for each set and each individual rollup list, with a view to later
+ * determining whether some combination of them could be hashed instead.
+ */
+static double
+get_number_of_groups(PlannerInfo *root,
+					 double path_rows,
+					 grouping_sets_data *gd,
+					 List *target_list)
+{
+	Query	   *parse = root->parse;
+	double		dNumGroups;
+
+	if (parse->groupClause)
+	{
+		List	   *groupExprs;
+
+		if (parse->groupingSets)
+		{
+			/* Add up the estimates for each grouping set */
+			ListCell   *lc;
+			ListCell   *lc2;
+
+			Assert(gd);			/* keep Coverity happy */
+
+			dNumGroups = 0;
+
+			foreach(lc, gd->rollups)
+			{
+				RollupData *rollup = lfirst_node(RollupData, lc);
+				ListCell   *lc;
+
+				groupExprs = get_sortgrouplist_exprs(rollup->groupClause,
+													 target_list);
+
+				rollup->numGroups = 0.0;
+
+				forboth(lc, rollup->gsets, lc2, rollup->gsets_data)
+				{
+					List	   *gset = (List *) lfirst(lc);
+					GroupingSetData *gs = lfirst_node(GroupingSetData, lc2);
+					double		numGroups = estimate_num_groups(root,
+																groupExprs,
+																path_rows,
+																&gset);
+
+					gs->numGroups = numGroups;
+					rollup->numGroups += numGroups;
+				}
+
+				dNumGroups += rollup->numGroups;
+			}
+
+			if (gd->hash_sets_idx)
+			{
+				ListCell   *lc;
+
+				gd->dNumHashGroups = 0;
+
+				groupExprs = get_sortgrouplist_exprs(parse->groupClause,
+													 target_list);
+
+				forboth(lc, gd->hash_sets_idx, lc2, gd->unsortable_sets)
+				{
+					List	   *gset = (List *) lfirst(lc);
+					GroupingSetData *gs = lfirst_node(GroupingSetData, lc2);
+					double		numGroups = estimate_num_groups(root,
+																groupExprs,
+																path_rows,
+																&gset);
+
+					gs->numGroups = numGroups;
+					gd->dNumHashGroups += numGroups;
+				}
+
+				dNumGroups += gd->dNumHashGroups;
+			}
+		}
+		else
+		{
+			/* Plain GROUP BY */
+			groupExprs = get_sortgrouplist_exprs(parse->groupClause,
+												 target_list);
+
+			dNumGroups = estimate_num_groups(root, groupExprs, path_rows,
+											 NULL);
+		}
+	}
+	else if (parse->groupingSets)
+	{
+		/* Empty grouping sets ... one result row for each one */
+		dNumGroups = list_length(parse->groupingSets);
+	}
+	else if (parse->hasAggs || root->hasHavingQual)
+	{
+		/* Plain aggregation, one result row */
+		dNumGroups = 1;
+	}
+	else
+	{
+		/* Not grouping */
+		dNumGroups = 1;
+	}
+
+	return dNumGroups;
+}
+
+/*
+ * create_grouping_paths
+ *
+ * Build a new upperrel containing Paths for grouping and/or aggregation.
+ * Along the way, we also build an upperrel for Paths which are partially
+ * grouped and/or aggregated.  A partially grouped and/or aggregated path
+ * needs a FinalizeAggregate node to complete the aggregation.  Currently,
+ * the only partially grouped paths we build are also partial paths; that
+ * is, they need a Gather and then a FinalizeAggregate.
+ *
+ * input_rel: contains the source-data Paths
+ * target: the pathtarget for the result Paths to compute
+ * agg_costs: cost info about all aggregates in query (in AGGSPLIT_SIMPLE mode)
+ * gd: grouping sets data including list of grouping sets and their clauses
+ *
+ * Note: all Paths in input_rel are expected to return the target computed
+ * by make_group_input_target.
+ */
+static RelOptInfo *
+create_grouping_paths(PlannerInfo *root,
+					  RelOptInfo *input_rel,
+					  PathTarget *target,
+					  bool target_parallel_safe,
+					  const AggClauseCosts *agg_costs,
+					  grouping_sets_data *gd)
+{
+	Query	   *parse = root->parse;
+	RelOptInfo *grouped_rel;
+	RelOptInfo *partially_grouped_rel;
+
+	/*
+	 * Create grouping relation to hold fully aggregated grouping and/or
+	 * aggregation paths.
+	 */
+	grouped_rel = make_grouping_rel(root, input_rel, target,
+									target_parallel_safe, parse->havingQual);
+
+	/*
+	 * Create either paths for a degenerate grouping or paths for ordinary
+	 * grouping, as appropriate.
+	 */
+	if (is_degenerate_grouping(root))
+		create_degenerate_grouping_paths(root, input_rel, grouped_rel);
+	else
+	{
+		int			flags = 0;
+		GroupPathExtraData extra;
+
+		/*
+		 * Determine whether it's possible to perform sort-based
+		 * implementations of grouping.  (Note that if groupClause is empty,
+		 * grouping_is_sortable() is trivially true, and all the
+		 * pathkeys_contained_in() tests will succeed too, so that we'll
+		 * consider every surviving input path.)
+		 *
+		 * If we have grouping sets, we might be able to sort some but not all
+		 * of them; in this case, we need can_sort to be true as long as we
+		 * must consider any sorted-input plan.
+		 */
+		if ((gd && gd->rollups != NIL)
+			|| grouping_is_sortable(parse->groupClause))
+			flags |= GROUPING_CAN_USE_SORT;
+
+		/*
+		 * Determine whether we should consider hash-based implementations of
+		 * grouping.
+		 *
+		 * Hashed aggregation only applies if we're grouping. If we have
+		 * grouping sets, some groups might be hashable but others not; in
+		 * this case we set can_hash true as long as there is nothing globally
+		 * preventing us from hashing (and we should therefore consider plans
+		 * with hashes).
+		 *
+		 * Executor doesn't support hashed aggregation with DISTINCT or ORDER
+		 * BY aggregates.  (Doing so would imply storing *all* the input
+		 * values in the hash table, and/or running many sorts in parallel,
+		 * either of which seems like a certain loser.)  We similarly don't
+		 * support ordered-set aggregates in hashed aggregation, but that case
+		 * is also included in the numOrderedAggs count.
+		 *
+		 * Note: grouping_is_hashable() is much more expensive to check than
+		 * the other gating conditions, so we want to do it last.
+		 */
+		if ((parse->groupClause != NIL &&
+			 agg_costs->numOrderedAggs == 0 &&
+			 (gd ? gd->any_hashable : grouping_is_hashable(parse->groupClause))))
+			flags |= GROUPING_CAN_USE_HASH;
+
+		/*
+		 * Determine whether partial aggregation is possible.
+		 */
+		if (can_partial_agg(root, agg_costs))
+			flags |= GROUPING_CAN_PARTIAL_AGG;
+
+		extra.flags = flags;
+		extra.target_parallel_safe = target_parallel_safe;
+		extra.havingQual = parse->havingQual;
+		extra.targetList = parse->targetList;
+		extra.partial_costs_set = false;
+
+		/*
+		 * Determine whether partitionwise aggregation is in theory possible.
+		 * It can be disabled by the user, and for now, we don't try to
+		 * support grouping sets.  create_ordinary_grouping_paths() will check
+		 * additional conditions, such as whether input_rel is partitioned.
+		 */
+		if (enable_partitionwise_aggregate && !parse->groupingSets)
+			extra.patype = PARTITIONWISE_AGGREGATE_FULL;
+		else
+			extra.patype = PARTITIONWISE_AGGREGATE_NONE;
+
+		create_ordinary_grouping_paths(root, input_rel, grouped_rel,
+									   agg_costs, gd, &extra,
+									   &partially_grouped_rel);
+	}
+
+	set_cheapest(grouped_rel);
+	return grouped_rel;
+}
+
+/*
+ * make_grouping_rel
+ *
+ * Create a new grouping rel and set basic properties.
+ *
+ * input_rel represents the underlying scan/join relation.
+ * target is the output expected from the grouping relation.
+ */
+static RelOptInfo *
+make_grouping_rel(PlannerInfo *root, RelOptInfo *input_rel,
+				  PathTarget *target, bool target_parallel_safe,
+				  Node *havingQual)
+{
+	RelOptInfo *grouped_rel;
+
+	if (IS_OTHER_REL(input_rel))
+	{
+		grouped_rel = fetch_upper_rel(root, UPPERREL_GROUP_AGG,
+									  input_rel->relids);
+		grouped_rel->reloptkind = RELOPT_OTHER_UPPER_REL;
+	}
+	else
+	{
+		/*
+		 * By tradition, the relids set for the main grouping relation is
+		 * NULL.  (This could be changed, but might require adjustments
+		 * elsewhere.)
+		 */
+		grouped_rel = fetch_upper_rel(root, UPPERREL_GROUP_AGG, NULL);
+	}
+
+	/* Set target. */
+	grouped_rel->reltarget = target;
+
+	/*
+	 * If the input relation is not parallel-safe, then the grouped relation
+	 * can't be parallel-safe, either.  Otherwise, it's parallel-safe if the
+	 * target list and HAVING quals are parallel-safe.
+	 */
+	if (input_rel->consider_parallel && target_parallel_safe &&
+		is_parallel_safe(root, (Node *) havingQual))
+		grouped_rel->consider_parallel = true;
+
+	/*
+	 * If the input rel belongs to a single FDW, so does the grouped rel.
+	 */
+	grouped_rel->serverid = input_rel->serverid;
+	grouped_rel->userid = input_rel->userid;
+	grouped_rel->useridiscurrent = input_rel->useridiscurrent;
+	grouped_rel->fdwroutine = input_rel->fdwroutine;
+
+	return grouped_rel;
+}
+
+/*
+ * is_degenerate_grouping
+ *
+ * A degenerate grouping is one in which the query has a HAVING qual and/or
+ * grouping sets, but no aggregates and no GROUP BY (which implies that the
+ * grouping sets are all empty).
+ */
+static bool
+is_degenerate_grouping(PlannerInfo *root)
+{
+	Query	   *parse = root->parse;
+
+	return (root->hasHavingQual || parse->groupingSets) &&
+		!parse->hasAggs && parse->groupClause == NIL;
+}
+
+/*
+ * create_degenerate_grouping_paths
+ *
+ * When the grouping is degenerate (see is_degenerate_grouping), we are
+ * supposed to emit either zero or one row for each grouping set depending on
+ * whether HAVING succeeds.  Furthermore, there cannot be any variables in
+ * either HAVING or the targetlist, so we actually do not need the FROM table
+ * at all! We can just throw away the plan-so-far and generate a Result node.
+ * This is a sufficiently unusual corner case that it's not worth contorting
+ * the structure of this module to avoid having to generate the earlier paths
+ * in the first place.
+ */
+static void
+create_degenerate_grouping_paths(PlannerInfo *root, RelOptInfo *input_rel,
+								 RelOptInfo *grouped_rel)
+{
+	Query	   *parse = root->parse;
+	int			nrows;
+	Path	   *path;
+
+	nrows = list_length(parse->groupingSets);
+	if (nrows > 1)
+	{
+		/*
+		 * Doesn't seem worthwhile writing code to cons up a generate_series
+		 * or a values scan to emit multiple rows. Instead just make N clones
+		 * and append them.  (With a volatile HAVING clause, this means you
+		 * might get between 0 and N output rows. Offhand I think that's
+		 * desired.)
+		 */
+		List	   *paths = NIL;
+
+		while (--nrows >= 0)
+		{
+			path = (Path *)
+				create_group_result_path(root, grouped_rel,
+										 grouped_rel->reltarget,
+										 (List *) parse->havingQual);
+			paths = lappend(paths, path);
+		}
+		path = (Path *)
+			create_append_path(root,
+							   grouped_rel,
+							   paths,
+							   NIL,
+							   NIL,
+							   NULL,
+							   0,
+							   false,
+							   NIL,
+							   -1);
+	}
+	else
+	{
+		/* No grouping sets, or just one, so one output row */
+		path = (Path *)
+			create_group_result_path(root, grouped_rel,
+									 grouped_rel->reltarget,
+									 (List *) parse->havingQual);
+	}
+
+	add_path(grouped_rel, path);
+}
+
+/*
+ * create_ordinary_grouping_paths
+ *
+ * Create grouping paths for the ordinary (that is, non-degenerate) case.
+ *
+ * We need to consider sorted and hashed aggregation in the same function,
+ * because otherwise (1) it would be harder to throw an appropriate error
+ * message if neither way works, and (2) we should not allow hashtable size
+ * considerations to dissuade us from using hashing if sorting is not possible.
+ *
+ * *partially_grouped_rel_p will be set to the partially grouped rel which this
+ * function creates, or to NULL if it doesn't create one.
+ */
+static void
+create_ordinary_grouping_paths(PlannerInfo *root, RelOptInfo *input_rel,
+							   RelOptInfo *grouped_rel,
+							   const AggClauseCosts *agg_costs,
+							   grouping_sets_data *gd,
+							   GroupPathExtraData *extra,
+							   RelOptInfo **partially_grouped_rel_p)
+{
+	Path	   *cheapest_path = input_rel->cheapest_total_path;
+	RelOptInfo *partially_grouped_rel = NULL;
+	double		dNumGroups;
+	PartitionwiseAggregateType patype = PARTITIONWISE_AGGREGATE_NONE;
+
+	/*
+	 * If this is the topmost grouping relation or if the parent relation is
+	 * doing some form of partitionwise aggregation, then we may be able to do
+	 * it at this level also.  However, if the input relation is not
+	 * partitioned, partitionwise aggregate is impossible.
+	 */
+	if (extra->patype != PARTITIONWISE_AGGREGATE_NONE &&
+		IS_PARTITIONED_REL(input_rel))
+	{
+		/*
+		 * If this is the topmost relation or if the parent relation is doing
+		 * full partitionwise aggregation, then we can do full partitionwise
+		 * aggregation provided that the GROUP BY clause contains all of the
+		 * partitioning columns at this level.  Otherwise, we can do at most
+		 * partial partitionwise aggregation.  But if partial aggregation is
+		 * not supported in general then we can't use it for partitionwise
+		 * aggregation either.
+		 */
+		if (extra->patype == PARTITIONWISE_AGGREGATE_FULL &&
+			group_by_has_partkey(input_rel, extra->targetList,
+								 root->parse->groupClause))
+			patype = PARTITIONWISE_AGGREGATE_FULL;
+		else if ((extra->flags & GROUPING_CAN_PARTIAL_AGG) != 0)
+			patype = PARTITIONWISE_AGGREGATE_PARTIAL;
+		else
+			patype = PARTITIONWISE_AGGREGATE_NONE;
+	}
+
+	/*
+	 * Before generating paths for grouped_rel, we first generate any possible
+	 * partially grouped paths; that way, later code can easily consider both
+	 * parallel and non-parallel approaches to grouping.
+	 */
+	if ((extra->flags & GROUPING_CAN_PARTIAL_AGG) != 0)
+	{
+		bool		force_rel_creation;
+
+		/*
+		 * If we're doing partitionwise aggregation at this level, force
+		 * creation of a partially_grouped_rel so we can add partitionwise
+		 * paths to it.
+		 */
+		force_rel_creation = (patype == PARTITIONWISE_AGGREGATE_PARTIAL);
+
+		partially_grouped_rel =
+			create_partial_grouping_paths(root,
+										  grouped_rel,
+										  input_rel,
+										  gd,
+										  extra,
+										  force_rel_creation);
+	}
+
+	/* Set out parameter. */
+	*partially_grouped_rel_p = partially_grouped_rel;
+
+	/* Apply partitionwise aggregation technique, if possible. */
+	if (patype != PARTITIONWISE_AGGREGATE_NONE)
+		create_partitionwise_grouping_paths(root, input_rel, grouped_rel,
+											partially_grouped_rel, agg_costs,
+											gd, patype, extra);
+
+	/* If we are doing partial aggregation only, return. */
+	if (extra->patype == PARTITIONWISE_AGGREGATE_PARTIAL)
+	{
+		Assert(partially_grouped_rel);
+
+		if (partially_grouped_rel->pathlist)
+			set_cheapest(partially_grouped_rel);
+
+		return;
+	}
+
+	/* Gather any partially grouped partial paths. */
+	if (partially_grouped_rel && partially_grouped_rel->partial_pathlist)
+	{
+		gather_grouping_paths(root, partially_grouped_rel);
+		set_cheapest(partially_grouped_rel);
+	}
+
+	/*
+	 * Estimate number of groups.
+	 */
+	dNumGroups = get_number_of_groups(root,
+									  cheapest_path->rows,
+									  gd,
+									  extra->targetList);
+
+	/* Build final grouping paths */
+	add_paths_to_grouping_rel(root, input_rel, grouped_rel,
+							  partially_grouped_rel, agg_costs, gd,
+							  dNumGroups, extra);
+
+	/* Give a helpful error if we failed to find any implementation */
+	if (grouped_rel->pathlist == NIL)
+		ereport(ERROR,
+				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+				 errmsg("could not implement GROUP BY"),
+				 errdetail("Some of the datatypes only support hashing, while others only support sorting.")));
+
+	/*
+	 * If there is an FDW that's responsible for all baserels of the query,
+	 * let it consider adding ForeignPaths.
+	 */
+	if (grouped_rel->fdwroutine &&
+		grouped_rel->fdwroutine->GetForeignUpperPaths)
+		grouped_rel->fdwroutine->GetForeignUpperPaths(root, UPPERREL_GROUP_AGG,
+													  input_rel, grouped_rel,
+													  extra);
+
+	/* Let extensions possibly add some more paths */
+	if (create_upper_paths_hook)
+		(*create_upper_paths_hook) (root, UPPERREL_GROUP_AGG,
+									input_rel, grouped_rel,
+									extra);
+}
+
+/*
+ * For a given input path, consider the possible ways of doing grouping sets on
+ * it, by combinations of hashing and sorting.  This can be called multiple
+ * times, so it's important that it not scribble on input.  No result is
+ * returned, but any generated paths are added to grouped_rel.
+ */
+static void
+consider_groupingsets_paths(PlannerInfo *root,
+							RelOptInfo *grouped_rel,
+							Path *path,
+							bool is_sorted,
+							bool can_hash,
+							grouping_sets_data *gd,
+							const AggClauseCosts *agg_costs,
+							double dNumGroups)
+{
+	Query	   *parse = root->parse;
+
+	/*
+	 * If we're not being offered sorted input, then only consider plans that
+	 * can be done entirely by hashing.
+	 *
+	 * We can hash everything if it looks like it'll fit in work_mem. But if
+	 * the input is actually sorted despite not being advertised as such, we
+	 * prefer to make use of that in order to use less memory.
+	 *
+	 * If none of the grouping sets are sortable, then ignore the work_mem
+	 * limit and generate a path anyway, since otherwise we'll just fail.
+	 */
+	if (!is_sorted)
+	{
+		List	   *new_rollups = NIL;
+		RollupData *unhashed_rollup = NULL;
+		List	   *sets_data;
+		List	   *empty_sets_data = NIL;
+		List	   *empty_sets = NIL;
+		ListCell   *lc;
+		ListCell   *l_start = list_head(gd->rollups);
+		AggStrategy strat = AGG_HASHED;
+		double		hashsize;
+		double		exclude_groups = 0.0;
+
+		Assert(can_hash);
+
+		/*
+		 * If the input is coincidentally sorted usefully (which can happen
+		 * even if is_sorted is false, since that only means that our caller
+		 * has set up the sorting for us), then save some hashtable space by
+		 * making use of that. But we need to watch out for degenerate cases:
+		 *
+		 * 1) If there are any empty grouping sets, then group_pathkeys might
+		 * be NIL if all non-empty grouping sets are unsortable. In this case,
+		 * there will be a rollup containing only empty groups, and the
+		 * pathkeys_contained_in test is vacuously true; this is ok.
+		 *
+		 * XXX: the above relies on the fact that group_pathkeys is generated
+		 * from the first rollup. If we add the ability to consider multiple
+		 * sort orders for grouping input, this assumption might fail.
+		 *
+		 * 2) If there are no empty sets and only unsortable sets, then the
+		 * rollups list will be empty (and thus l_start == NULL), and
+		 * group_pathkeys will be NIL; we must ensure that the vacuously-true
+		 * pathkeys_contain_in test doesn't cause us to crash.
+		 */
+		if (l_start != NULL &&
+			pathkeys_contained_in(root->group_pathkeys, path->pathkeys))
+		{
+			unhashed_rollup = lfirst_node(RollupData, l_start);
+			exclude_groups = unhashed_rollup->numGroups;
+			l_start = lnext(l_start);
+		}
+
+		hashsize = estimate_hashagg_tablesize(path,
+											  agg_costs,
+											  dNumGroups - exclude_groups);
+
+		/*
+		 * gd->rollups is empty if we have only unsortable columns to work
+		 * with.  Override work_mem in that case; otherwise, we'll rely on the
+		 * sorted-input case to generate usable mixed paths.
+		 */
+		if (hashsize > work_mem * 1024L && gd->rollups)
+			return;				/* nope, won't fit */
+
+		/*
+		 * We need to burst the existing rollups list into individual grouping
+		 * sets and recompute a groupClause for each set.
+		 */
+		sets_data = list_copy(gd->unsortable_sets);
+
+		for_each_cell(lc, l_start)
+		{
+			RollupData *rollup = lfirst_node(RollupData, lc);
+
+			/*
+			 * If we find an unhashable rollup that's not been skipped by the
+			 * "actually sorted" check above, we can't cope; we'd need sorted
+			 * input (with a different sort order) but we can't get that here.
+			 * So bail out; we'll get a valid path from the is_sorted case
+			 * instead.
+			 *
+			 * The mere presence of empty grouping sets doesn't make a rollup
+			 * unhashable (see preprocess_grouping_sets), we handle those
+			 * specially below.
+			 */
+			if (!rollup->hashable)
+				return;
+			else
+				sets_data = list_concat(sets_data, list_copy(rollup->gsets_data));
+		}
+		foreach(lc, sets_data)
+		{
+			GroupingSetData *gs = lfirst_node(GroupingSetData, lc);
+			List	   *gset = gs->set;
+			RollupData *rollup;
+
+			if (gset == NIL)
+			{
+				/* Empty grouping sets can't be hashed. */
+				empty_sets_data = lappend(empty_sets_data, gs);
+				empty_sets = lappend(empty_sets, NIL);
+			}
+			else
+			{
+				rollup = makeNode(RollupData);
+
+				rollup->groupClause = preprocess_groupclause(root, gset);
+				rollup->gsets_data = list_make1(gs);
+				rollup->gsets = remap_to_groupclause_idx(rollup->groupClause,
+														 rollup->gsets_data,
+														 gd->tleref_to_colnum_map);
+				rollup->numGroups = gs->numGroups;
+				rollup->hashable = true;
+				rollup->is_hashed = true;
+				new_rollups = lappend(new_rollups, rollup);
+			}
+		}
+
+		/*
+		 * If we didn't find anything nonempty to hash, then bail.  We'll
+		 * generate a path from the is_sorted case.
+		 */
+		if (new_rollups == NIL)
+			return;
+
+		/*
+		 * If there were empty grouping sets they should have been in the
+		 * first rollup.
+		 */
+		Assert(!unhashed_rollup || !empty_sets);
+
+		if (unhashed_rollup)
+		{
+			new_rollups = lappend(new_rollups, unhashed_rollup);
+			strat = AGG_MIXED;
+		}
+		else if (empty_sets)
+		{
+			RollupData *rollup = makeNode(RollupData);
+
+			rollup->groupClause = NIL;
+			rollup->gsets_data = empty_sets_data;
+			rollup->gsets = empty_sets;
+			rollup->numGroups = list_length(empty_sets);
+			rollup->hashable = false;
+			rollup->is_hashed = false;
+			new_rollups = lappend(new_rollups, rollup);
+			strat = AGG_MIXED;
+		}
+
+		add_path(grouped_rel, (Path *)
+				 create_groupingsets_path(root,
+										  grouped_rel,
+										  path,
+										  (List *) parse->havingQual,
+										  strat,
+										  new_rollups,
+										  agg_costs,
+										  dNumGroups));
+		return;
+	}
+
+	/*
+	 * If we have sorted input but nothing we can do with it, bail.
+	 */
+	if (list_length(gd->rollups) == 0)
+		return;
+
+	/*
+	 * Given sorted input, we try and make two paths: one sorted and one mixed
+	 * sort/hash. (We need to try both because hashagg might be disabled, or
+	 * some columns might not be sortable.)
+	 *
+	 * can_hash is passed in as false if some obstacle elsewhere (such as
+	 * ordered aggs) means that we shouldn't consider hashing at all.
+	 */
+	if (can_hash && gd->any_hashable)
+	{
+		List	   *rollups = NIL;
+		List	   *hash_sets = list_copy(gd->unsortable_sets);
+		double		availspace = (work_mem * 1024.0);
+		ListCell   *lc;
+
+		/*
+		 * Account first for space needed for groups we can't sort at all.
+		 */
+		availspace -= estimate_hashagg_tablesize(path,
+												 agg_costs,
+												 gd->dNumHashGroups);
+
+		if (availspace > 0 && list_length(gd->rollups) > 1)
+		{
+			double		scale;
+			int			num_rollups = list_length(gd->rollups);
+			int			k_capacity;
+			int		   *k_weights = palloc(num_rollups * sizeof(int));
+			Bitmapset  *hash_items = NULL;
+			int			i;
+
+			/*
+			 * We treat this as a knapsack problem: the knapsack capacity
+			 * represents work_mem, the item weights are the estimated memory
+			 * usage of the hashtables needed to implement a single rollup,
+			 * and we really ought to use the cost saving as the item value;
+			 * however, currently the costs assigned to sort nodes don't
+			 * reflect the comparison costs well, and so we treat all items as
+			 * of equal value (each rollup we hash instead saves us one sort).
+			 *
+			 * To use the discrete knapsack, we need to scale the values to a
+			 * reasonably small bounded range.  We choose to allow a 5% error
+			 * margin; we have no more than 4096 rollups in the worst possible
+			 * case, which with a 5% error margin will require a bit over 42MB
+			 * of workspace. (Anyone wanting to plan queries that complex had
+			 * better have the memory for it.  In more reasonable cases, with
+			 * no more than a couple of dozen rollups, the memory usage will
+			 * be negligible.)
+			 *
+			 * k_capacity is naturally bounded, but we clamp the values for
+			 * scale and weight (below) to avoid overflows or underflows (or
+			 * uselessly trying to use a scale factor less than 1 byte).
+			 */
+			scale = Max(availspace / (20.0 * num_rollups), 1.0);
+			k_capacity = (int) floor(availspace / scale);
+
+			/*
+			 * We leave the first rollup out of consideration since it's the
+			 * one that matches the input sort order.  We assign indexes "i"
+			 * to only those entries considered for hashing; the second loop,
+			 * below, must use the same condition.
+			 */
+			i = 0;
+			for_each_cell(lc, lnext(list_head(gd->rollups)))
+			{
+				RollupData *rollup = lfirst_node(RollupData, lc);
+
+				if (rollup->hashable)
+				{
+					double		sz = estimate_hashagg_tablesize(path,
+																agg_costs,
+																rollup->numGroups);
+
+					/*
+					 * If sz is enormous, but work_mem (and hence scale) is
+					 * small, avoid integer overflow here.
+					 */
+					k_weights[i] = (int) Min(floor(sz / scale),
+											 k_capacity + 1.0);
+					++i;
+				}
+			}
+
+			/*
+			 * Apply knapsack algorithm; compute the set of items which
+			 * maximizes the value stored (in this case the number of sorts
+			 * saved) while keeping the total size (approximately) within
+			 * capacity.
+			 */
+			if (i > 0)
+				hash_items = DiscreteKnapsack(k_capacity, i, k_weights, NULL);
+
+			if (!bms_is_empty(hash_items))
+			{
+				rollups = list_make1(linitial(gd->rollups));
+
+				i = 0;
+				for_each_cell(lc, lnext(list_head(gd->rollups)))
+				{
+					RollupData *rollup = lfirst_node(RollupData, lc);
+
+					if (rollup->hashable)
+					{
+						if (bms_is_member(i, hash_items))
+							hash_sets = list_concat(hash_sets,
+													list_copy(rollup->gsets_data));
+						else
+							rollups = lappend(rollups, rollup);
+						++i;
+					}
+					else
+						rollups = lappend(rollups, rollup);
+				}
+			}
+		}
+
+		if (!rollups && hash_sets)
+			rollups = list_copy(gd->rollups);
+
+		foreach(lc, hash_sets)
+		{
+			GroupingSetData *gs = lfirst_node(GroupingSetData, lc);
+			RollupData *rollup = makeNode(RollupData);
+
+			Assert(gs->set != NIL);
+
+			rollup->groupClause = preprocess_groupclause(root, gs->set);
+			rollup->gsets_data = list_make1(gs);
+			rollup->gsets = remap_to_groupclause_idx(rollup->groupClause,
+													 rollup->gsets_data,
+													 gd->tleref_to_colnum_map);
+			rollup->numGroups = gs->numGroups;
+			rollup->hashable = true;
+			rollup->is_hashed = true;
+			rollups = lcons(rollup, rollups);
+		}
+
+		if (rollups)
+		{
+			add_path(grouped_rel, (Path *)
+					 create_groupingsets_path(root,
+											  grouped_rel,
+											  path,
+											  (List *) parse->havingQual,
+											  AGG_MIXED,
+											  rollups,
+											  agg_costs,
+											  dNumGroups));
+		}
+	}
+
+	/*
+	 * Now try the simple sorted case.
+	 */
+	if (!gd->unsortable_sets)
+		add_path(grouped_rel, (Path *)
+				 create_groupingsets_path(root,
+										  grouped_rel,
+										  path,
+										  (List *) parse->havingQual,
+										  AGG_SORTED,
+										  gd->rollups,
+										  agg_costs,
+										  dNumGroups));
+}
+
+/*
+ * create_window_paths
+ *
+ * Build a new upperrel containing Paths for window-function evaluation.
+ *
+ * input_rel: contains the source-data Paths
+ * input_target: result of make_window_input_target
+ * output_target: what the topmost WindowAggPath should return
+ * wflists: result of find_window_functions
+ * activeWindows: result of select_active_windows
+ *
+ * Note: all Paths in input_rel are expected to return input_target.
+ */
+static RelOptInfo *
+create_window_paths(PlannerInfo *root,
+					RelOptInfo *input_rel,
+					PathTarget *input_target,
+					PathTarget *output_target,
+					bool output_target_parallel_safe,
+					WindowFuncLists *wflists,
+					List *activeWindows)
+{
+	RelOptInfo *window_rel;
+	ListCell   *lc;
+
+	/* For now, do all work in the (WINDOW, NULL) upperrel */
+	window_rel = fetch_upper_rel(root, UPPERREL_WINDOW, NULL);
+
+	/*
+	 * If the input relation is not parallel-safe, then the window relation
+	 * can't be parallel-safe, either.  Otherwise, we need to examine the
+	 * target list and active windows for non-parallel-safe constructs.
+	 */
+	if (input_rel->consider_parallel && output_target_parallel_safe &&
+		is_parallel_safe(root, (Node *) activeWindows))
+		window_rel->consider_parallel = true;
+
+	/*
+	 * If the input rel belongs to a single FDW, so does the window rel.
+	 */
+	window_rel->serverid = input_rel->serverid;
+	window_rel->userid = input_rel->userid;
+	window_rel->useridiscurrent = input_rel->useridiscurrent;
+	window_rel->fdwroutine = input_rel->fdwroutine;
+
+	/*
+	 * Consider computing window functions starting from the existing
+	 * cheapest-total path (which will likely require a sort) as well as any
+	 * existing paths that satisfy root->window_pathkeys (which won't).
+	 */
+	foreach(lc, input_rel->pathlist)
+	{
+		Path	   *path = (Path *) lfirst(lc);
+
+		if (path == input_rel->cheapest_total_path ||
+			pathkeys_contained_in(root->window_pathkeys, path->pathkeys))
+			create_one_window_path(root,
+								   window_rel,
+								   path,
+								   input_target,
+								   output_target,
+								   wflists,
+								   activeWindows);
+	}
+
+	/*
+	 * If there is an FDW that's responsible for all baserels of the query,
+	 * let it consider adding ForeignPaths.
+	 */
+	if (window_rel->fdwroutine &&
+		window_rel->fdwroutine->GetForeignUpperPaths)
+		window_rel->fdwroutine->GetForeignUpperPaths(root, UPPERREL_WINDOW,
+													 input_rel, window_rel,
+													 NULL);
+
+	/* Let extensions possibly add some more paths */
+	if (create_upper_paths_hook)
+		(*create_upper_paths_hook) (root, UPPERREL_WINDOW,
+									input_rel, window_rel, NULL);
+
+	/* Now choose the best path(s) */
+	set_cheapest(window_rel);
+
+	return window_rel;
+}
+
+/*
+ * Stack window-function implementation steps atop the given Path, and
+ * add the result to window_rel.
+ *
+ * window_rel: upperrel to contain result
+ * path: input Path to use (must return input_target)
+ * input_target: result of make_window_input_target
+ * output_target: what the topmost WindowAggPath should return
+ * wflists: result of find_window_functions
+ * activeWindows: result of select_active_windows
+ */
+static void
+create_one_window_path(PlannerInfo *root,
+					   RelOptInfo *window_rel,
+					   Path *path,
+					   PathTarget *input_target,
+					   PathTarget *output_target,
+					   WindowFuncLists *wflists,
+					   List *activeWindows)
+{
+	PathTarget *window_target;
+	ListCell   *l;
+
+	/*
+	 * Since each window clause could require a different sort order, we stack
+	 * up a WindowAgg node for each clause, with sort steps between them as
+	 * needed.  (We assume that select_active_windows chose a good order for
+	 * executing the clauses in.)
+	 *
+	 * input_target should contain all Vars and Aggs needed for the result.
+	 * (In some cases we wouldn't need to propagate all of these all the way
+	 * to the top, since they might only be needed as inputs to WindowFuncs.
+	 * It's probably not worth trying to optimize that though.)  It must also
+	 * contain all window partitioning and sorting expressions, to ensure
+	 * they're computed only once at the bottom of the stack (that's critical
+	 * for volatile functions).  As we climb up the stack, we'll add outputs
+	 * for the WindowFuncs computed at each level.
+	 */
+	window_target = input_target;
+
+	foreach(l, activeWindows)
+	{
+		WindowClause *wc = lfirst_node(WindowClause, l);
+		List	   *window_pathkeys;
+
+		window_pathkeys = make_pathkeys_for_window(root,
+												   wc,
+												   root->processed_tlist);
+
+		/* Sort if necessary */
+		if (!pathkeys_contained_in(window_pathkeys, path->pathkeys))
+		{
+			path = (Path *) create_sort_path(root, window_rel,
+											 path,
+											 window_pathkeys,
+											 -1.0);
+		}
+
+		if (lnext(l))
+		{
+			/*
+			 * Add the current WindowFuncs to the output target for this
+			 * intermediate WindowAggPath.  We must copy window_target to
+			 * avoid changing the previous path's target.
+			 *
+			 * Note: a WindowFunc adds nothing to the target's eval costs; but
+			 * we do need to account for the increase in tlist width.
+			 */
+			ListCell   *lc2;
+
+			window_target = copy_pathtarget(window_target);
+			foreach(lc2, wflists->windowFuncs[wc->winref])
+			{
+				WindowFunc *wfunc = lfirst_node(WindowFunc, lc2);
+
+				add_column_to_pathtarget(window_target, (Expr *) wfunc, 0);
+				window_target->width += get_typavgwidth(wfunc->wintype, -1);
+			}
+		}
+		else
+		{
+			/* Install the goal target in the topmost WindowAgg */
+			window_target = output_target;
+		}
+
+		path = (Path *)
+			create_windowagg_path(root, window_rel, path, window_target,
+								  wflists->windowFuncs[wc->winref],
+								  wc);
+	}
+
+	add_path(window_rel, path);
+}
+
+/*
+ * create_distinct_paths
+ *
+ * Build a new upperrel containing Paths for SELECT DISTINCT evaluation.
+ *
+ * input_rel: contains the source-data Paths
+ *
+ * Note: input paths should already compute the desired pathtarget, since
+ * Sort/Unique won't project anything.
+ */
+static RelOptInfo *
+create_distinct_paths(PlannerInfo *root,
+					  RelOptInfo *input_rel)
+{
+	Query	   *parse = root->parse;
+	Path	   *cheapest_input_path = input_rel->cheapest_total_path;
+	RelOptInfo *distinct_rel;
+	double		numDistinctRows;
+	bool		allow_hash;
+	Path	   *path;
+	ListCell   *lc;
+
+	/* For now, do all work in the (DISTINCT, NULL) upperrel */
+	distinct_rel = fetch_upper_rel(root, UPPERREL_DISTINCT, NULL);
+
+	/*
+	 * We don't compute anything at this level, so distinct_rel will be
+	 * parallel-safe if the input rel is parallel-safe.  In particular, if
+	 * there is a DISTINCT ON (...) clause, any path for the input_rel will
+	 * output those expressions, and will not be parallel-safe unless those
+	 * expressions are parallel-safe.
+	 */
+	distinct_rel->consider_parallel = input_rel->consider_parallel;
+
+	/*
+	 * If the input rel belongs to a single FDW, so does the distinct_rel.
+	 */
+	distinct_rel->serverid = input_rel->serverid;
+	distinct_rel->userid = input_rel->userid;
+	distinct_rel->useridiscurrent = input_rel->useridiscurrent;
+	distinct_rel->fdwroutine = input_rel->fdwroutine;
+
+	/* Estimate number of distinct rows there will be */
+	if (parse->groupClause || parse->groupingSets || parse->hasAggs ||
+		root->hasHavingQual)
+	{
+		/*
+		 * If there was grouping or aggregation, use the number of input rows
+		 * as the estimated number of DISTINCT rows (ie, assume the input is
+		 * already mostly unique).
+		 */
+		numDistinctRows = cheapest_input_path->rows;
+	}
+	else
+	{
+		/*
+		 * Otherwise, the UNIQUE filter has effects comparable to GROUP BY.
+		 */
+		List	   *distinctExprs;
+
+		distinctExprs = get_sortgrouplist_exprs(parse->distinctClause,
+												parse->targetList);
+		numDistinctRows = estimate_num_groups(root, distinctExprs,
+											  cheapest_input_path->rows,
+											  NULL);
+	}
+
+	/*
+	 * Consider sort-based implementations of DISTINCT, if possible.
+	 */
+	if (grouping_is_sortable(parse->distinctClause))
+	{
+		/*
+		 * First, if we have any adequately-presorted paths, just stick a
+		 * Unique node on those.  Then consider doing an explicit sort of the
+		 * cheapest input path and Unique'ing that.
+		 *
+		 * When we have DISTINCT ON, we must sort by the more rigorous of
+		 * DISTINCT and ORDER BY, else it won't have the desired behavior.
+		 * Also, if we do have to do an explicit sort, we might as well use
+		 * the more rigorous ordering to avoid a second sort later.  (Note
+		 * that the parser will have ensured that one clause is a prefix of
+		 * the other.)
+		 */
+		List	   *needed_pathkeys;
+
+		if (parse->hasDistinctOn &&
+			list_length(root->distinct_pathkeys) <
+			list_length(root->sort_pathkeys))
+			needed_pathkeys = root->sort_pathkeys;
+		else
+			needed_pathkeys = root->distinct_pathkeys;
+
+		foreach(lc, input_rel->pathlist)
+		{
+			Path	   *path = (Path *) lfirst(lc);
+
+			if (pathkeys_contained_in(needed_pathkeys, path->pathkeys))
+			{
+				add_path(distinct_rel, (Path *)
+						 create_upper_unique_path(root, distinct_rel,
+												  path,
+												  list_length(root->distinct_pathkeys),
+												  numDistinctRows));
+			}
+		}
+
+		/* For explicit-sort case, always use the more rigorous clause */
+		if (list_length(root->distinct_pathkeys) <
+			list_length(root->sort_pathkeys))
+		{
+			needed_pathkeys = root->sort_pathkeys;
+			/* Assert checks that parser didn't mess up... */
+			Assert(pathkeys_contained_in(root->distinct_pathkeys,
+										 needed_pathkeys));
+		}
+		else
+			needed_pathkeys = root->distinct_pathkeys;
+
+		path = cheapest_input_path;
+		if (!pathkeys_contained_in(needed_pathkeys, path->pathkeys))
+			path = (Path *) create_sort_path(root, distinct_rel,
+											 path,
+											 needed_pathkeys,
+											 -1.0);
+
+		add_path(distinct_rel, (Path *)
+				 create_upper_unique_path(root, distinct_rel,
+										  path,
+										  list_length(root->distinct_pathkeys),
+										  numDistinctRows));
+	}
+
+	/*
+	 * Consider hash-based implementations of DISTINCT, if possible.
+	 *
+	 * If we were not able to make any other types of path, we *must* hash or
+	 * die trying.  If we do have other choices, there are several things that
+	 * should prevent selection of hashing: if the query uses DISTINCT ON
+	 * (because it won't really have the expected behavior if we hash), or if
+	 * enable_hashagg is off, or if it looks like the hashtable will exceed
+	 * work_mem.
+	 *
+	 * Note: grouping_is_hashable() is much more expensive to check than the
+	 * other gating conditions, so we want to do it last.
+	 */
+	if (distinct_rel->pathlist == NIL)
+		allow_hash = true;		/* we have no alternatives */
+	else if (parse->hasDistinctOn || !enable_hashagg)
+		allow_hash = false;		/* policy-based decision not to hash */
+	else
+	{
+		Size		hashentrysize;
+
+		/* Estimate per-hash-entry space at tuple width... */
+		hashentrysize = MAXALIGN(cheapest_input_path->pathtarget->width) +
+			MAXALIGN(SizeofMinimalTupleHeader);
+		/* plus the per-hash-entry overhead */
+		hashentrysize += hash_agg_entry_size(0);
+
+		/* Allow hashing only if hashtable is predicted to fit in work_mem */
+		allow_hash = (hashentrysize * numDistinctRows <= work_mem * 1024L);
+	}
+
+	if (allow_hash && grouping_is_hashable(parse->distinctClause))
+	{
+		/* Generate hashed aggregate path --- no sort needed */
+		add_path(distinct_rel, (Path *)
+				 create_agg_path(root,
+								 distinct_rel,
+								 cheapest_input_path,
+								 cheapest_input_path->pathtarget,
+								 AGG_HASHED,
+								 AGGSPLIT_SIMPLE,
+								 parse->distinctClause,
+								 NIL,
+								 NULL,
+								 numDistinctRows));
+	}
+
+	/* Give a helpful error if we failed to find any implementation */
+	if (distinct_rel->pathlist == NIL)
+		ereport(ERROR,
+				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+				 errmsg("could not implement DISTINCT"),
+				 errdetail("Some of the datatypes only support hashing, while others only support sorting.")));
+
+	/*
+	 * If there is an FDW that's responsible for all baserels of the query,
+	 * let it consider adding ForeignPaths.
+	 */
+	if (distinct_rel->fdwroutine &&
+		distinct_rel->fdwroutine->GetForeignUpperPaths)
+		distinct_rel->fdwroutine->GetForeignUpperPaths(root, UPPERREL_DISTINCT,
+													   input_rel, distinct_rel,
+													   NULL);
+
+	/* Let extensions possibly add some more paths */
+	if (create_upper_paths_hook)
+		(*create_upper_paths_hook) (root, UPPERREL_DISTINCT,
+									input_rel, distinct_rel, NULL);
+
+	/* Now choose the best path(s) */
+	set_cheapest(distinct_rel);
+
+	return distinct_rel;
+}
+
+/*
+ * create_ordered_paths
+ *
+ * Build a new upperrel containing Paths for ORDER BY evaluation.
+ *
+ * All paths in the result must satisfy the ORDER BY ordering.
+ * The only new path we need consider is an explicit sort on the
+ * cheapest-total existing path.
+ *
+ * input_rel: contains the source-data Paths
+ * target: the output tlist the result Paths must emit
+ * limit_tuples: estimated bound on the number of output tuples,
+ *		or -1 if no LIMIT or couldn't estimate
+ */
+static RelOptInfo *
+create_ordered_paths(PlannerInfo *root,
+					 RelOptInfo *input_rel,
+					 PathTarget *target,
+					 bool target_parallel_safe,
+					 double limit_tuples)
+{
+	Path	   *cheapest_input_path = input_rel->cheapest_total_path;
+	RelOptInfo *ordered_rel;
+	ListCell   *lc;
+
+	/* For now, do all work in the (ORDERED, NULL) upperrel */
+	ordered_rel = fetch_upper_rel(root, UPPERREL_ORDERED, NULL);
+
+	/*
+	 * If the input relation is not parallel-safe, then the ordered relation
+	 * can't be parallel-safe, either.  Otherwise, it's parallel-safe if the
+	 * target list is parallel-safe.
+	 */
+	if (input_rel->consider_parallel && target_parallel_safe)
+		ordered_rel->consider_parallel = true;
+
+	/*
+	 * If the input rel belongs to a single FDW, so does the ordered_rel.
+	 */
+	ordered_rel->serverid = input_rel->serverid;
+	ordered_rel->userid = input_rel->userid;
+	ordered_rel->useridiscurrent = input_rel->useridiscurrent;
+	ordered_rel->fdwroutine = input_rel->fdwroutine;
+
+	foreach(lc, input_rel->pathlist)
+	{
+		Path	   *path = (Path *) lfirst(lc);
+		bool		is_sorted;
+
+		is_sorted = pathkeys_contained_in(root->sort_pathkeys,
+										  path->pathkeys);
+		if (path == cheapest_input_path || is_sorted)
+		{
+			if (!is_sorted)
+			{
+				/* An explicit sort here can take advantage of LIMIT */
+				path = (Path *) create_sort_path(root,
+												 ordered_rel,
+												 path,
+												 root->sort_pathkeys,
+												 limit_tuples);
+			}
+
+			/* Add projection step if needed */
+			if (path->pathtarget != target)
+				path = apply_projection_to_path(root, ordered_rel,
+												path, target);
+
+			add_path(ordered_rel, path);
+		}
+	}
+
+	/*
+	 * generate_gather_paths() will have already generated a simple Gather
+	 * path for the best parallel path, if any, and the loop above will have
+	 * considered sorting it.  Similarly, generate_gather_paths() will also
+	 * have generated order-preserving Gather Merge plans which can be used
+	 * without sorting if they happen to match the sort_pathkeys, and the loop
+	 * above will have handled those as well.  However, there's one more
+	 * possibility: it may make sense to sort the cheapest partial path
+	 * according to the required output order and then use Gather Merge.
+	 */
+	if (ordered_rel->consider_parallel && root->sort_pathkeys != NIL &&
+		input_rel->partial_pathlist != NIL)
+	{
+		Path	   *cheapest_partial_path;
+
+		cheapest_partial_path = linitial(input_rel->partial_pathlist);
+
+		/*
+		 * If cheapest partial path doesn't need a sort, this is redundant
+		 * with what's already been tried.
+		 */
+		if (!pathkeys_contained_in(root->sort_pathkeys,
+								   cheapest_partial_path->pathkeys))
+		{
+			Path	   *path;
+			double		total_groups;
+
+			path = (Path *) create_sort_path(root,
+											 ordered_rel,
+											 cheapest_partial_path,
+											 root->sort_pathkeys,
+											 limit_tuples);
+
+			total_groups = cheapest_partial_path->rows *
+				cheapest_partial_path->parallel_workers;
+			path = (Path *)
+				create_gather_merge_path(root, ordered_rel,
+										 path,
+										 path->pathtarget,
+										 root->sort_pathkeys, NULL,
+										 &total_groups);
+
+			/* Add projection step if needed */
+			if (path->pathtarget != target)
+				path = apply_projection_to_path(root, ordered_rel,
+												path, target);
+
+			add_path(ordered_rel, path);
+		}
+	}
+
+	/*
+	 * If there is an FDW that's responsible for all baserels of the query,
+	 * let it consider adding ForeignPaths.
+	 */
+	if (ordered_rel->fdwroutine &&
+		ordered_rel->fdwroutine->GetForeignUpperPaths)
+		ordered_rel->fdwroutine->GetForeignUpperPaths(root, UPPERREL_ORDERED,
+													  input_rel, ordered_rel,
+													  NULL);
+
+	/* Let extensions possibly add some more paths */
+	if (create_upper_paths_hook)
+		(*create_upper_paths_hook) (root, UPPERREL_ORDERED,
+									input_rel, ordered_rel, NULL);
+
+	/*
+	 * No need to bother with set_cheapest here; grouping_planner does not
+	 * need us to do it.
+	 */
+	Assert(ordered_rel->pathlist != NIL);
+
+	return ordered_rel;
+}
+
+
+/*
+ * make_group_input_target
+ *	  Generate appropriate PathTarget for initial input to grouping nodes.
+ *
+ * If there is grouping or aggregation, the scan/join subplan cannot emit
+ * the query's final targetlist; for example, it certainly can't emit any
+ * aggregate function calls.  This routine generates the correct target
+ * for the scan/join subplan.
+ *
+ * The query target list passed from the parser already contains entries
+ * for all ORDER BY and GROUP BY expressions, but it will not have entries
+ * for variables used only in HAVING clauses; so we need to add those
+ * variables to the subplan target list.  Also, we flatten all expressions
+ * except GROUP BY items into their component variables; other expressions
+ * will be computed by the upper plan nodes rather than by the subplan.
+ * For example, given a query like
+ *		SELECT a+b,SUM(c+d) FROM table GROUP BY a+b;
+ * we want to pass this targetlist to the subplan:
+ *		a+b,c,d
+ * where the a+b target will be used by the Sort/Group steps, and the
+ * other targets will be used for computing the final results.
+ *
+ * 'final_target' is the query's final target list (in PathTarget form)
+ *
+ * The result is the PathTarget to be computed by the Paths returned from
+ * query_planner().
+ */
+static PathTarget *
+make_group_input_target(PlannerInfo *root, PathTarget *final_target)
+{
+	Query	   *parse = root->parse;
+	PathTarget *input_target;
+	List	   *non_group_cols;
+	List	   *non_group_vars;
+	int			i;
+	ListCell   *lc;
+
+	/*
+	 * We must build a target containing all grouping columns, plus any other
+	 * Vars mentioned in the query's targetlist and HAVING qual.
+	 */
+	input_target = create_empty_pathtarget();
+	non_group_cols = NIL;
+
+	i = 0;
+	foreach(lc, final_target->exprs)
+	{
+		Expr	   *expr = (Expr *) lfirst(lc);
+		Index		sgref = get_pathtarget_sortgroupref(final_target, i);
+
+		if (sgref && parse->groupClause &&
+			get_sortgroupref_clause_noerr(sgref, parse->groupClause) != NULL)
+		{
+			/*
+			 * It's a grouping column, so add it to the input target as-is.
+			 */
+			add_column_to_pathtarget(input_target, expr, sgref);
+		}
+		else
+		{
+			/*
+			 * Non-grouping column, so just remember the expression for later
+			 * call to pull_var_clause.
+			 */
+			non_group_cols = lappend(non_group_cols, expr);
+		}
+
+		i++;
+	}
+
+	/*
+	 * If there's a HAVING clause, we'll need the Vars it uses, too.
+	 */
+	if (parse->havingQual)
+		non_group_cols = lappend(non_group_cols, parse->havingQual);
+
+	/*
+	 * Pull out all the Vars mentioned in non-group cols (plus HAVING), and
+	 * add them to the input target if not already present.  (A Var used
+	 * directly as a GROUP BY item will be present already.)  Note this
+	 * includes Vars used in resjunk items, so we are covering the needs of
+	 * ORDER BY and window specifications.  Vars used within Aggrefs and
+	 * WindowFuncs will be pulled out here, too.
+	 */
+	non_group_vars = pull_var_clause((Node *) non_group_cols,
+									 PVC_RECURSE_AGGREGATES |
+									 PVC_RECURSE_WINDOWFUNCS |
+									 PVC_INCLUDE_PLACEHOLDERS);
+	add_new_columns_to_pathtarget(input_target, non_group_vars);
+
+	/* clean up cruft */
+	list_free(non_group_vars);
+	list_free(non_group_cols);
+
+	/* XXX this causes some redundant cost calculation ... */
+	return set_pathtarget_cost_width(root, input_target);
+}
+
+/*
+ * make_partial_grouping_target
+ *	  Generate appropriate PathTarget for output of partial aggregate
+ *	  (or partial grouping, if there are no aggregates) nodes.
+ *
+ * A partial aggregation node needs to emit all the same aggregates that
+ * a regular aggregation node would, plus any aggregates used in HAVING;
+ * except that the Aggref nodes should be marked as partial aggregates.
+ *
+ * In addition, we'd better emit any Vars and PlaceholderVars that are
+ * used outside of Aggrefs in the aggregation tlist and HAVING.  (Presumably,
+ * these would be Vars that are grouped by or used in grouping expressions.)
+ *
+ * grouping_target is the tlist to be emitted by the topmost aggregation step.
+ * havingQual represents the HAVING clause.
+ */
+static PathTarget *
+make_partial_grouping_target(PlannerInfo *root,
+							 PathTarget *grouping_target,
+							 Node *havingQual)
+{
+	Query	   *parse = root->parse;
+	PathTarget *partial_target;
+	List	   *non_group_cols;
+	List	   *non_group_exprs;
+	int			i;
+	ListCell   *lc;
+
+	partial_target = create_empty_pathtarget();
+	non_group_cols = NIL;
+
+	i = 0;
+	foreach(lc, grouping_target->exprs)
+	{
+		Expr	   *expr = (Expr *) lfirst(lc);
+		Index		sgref = get_pathtarget_sortgroupref(grouping_target, i);
+
+		if (sgref && parse->groupClause &&
+			get_sortgroupref_clause_noerr(sgref, parse->groupClause) != NULL)
+		{
+			/*
+			 * It's a grouping column, so add it to the partial_target as-is.
+			 * (This allows the upper agg step to repeat the grouping calcs.)
+			 */
+			add_column_to_pathtarget(partial_target, expr, sgref);
+		}
+		else
+		{
+			/*
+			 * Non-grouping column, so just remember the expression for later
+			 * call to pull_var_clause.
+			 */
+			non_group_cols = lappend(non_group_cols, expr);
+		}
+
+		i++;
+	}
+
+	/*
+	 * If there's a HAVING clause, we'll need the Vars/Aggrefs it uses, too.
+	 */
+	if (havingQual)
+		non_group_cols = lappend(non_group_cols, havingQual);
+
+	/*
+	 * Pull out all the Vars, PlaceHolderVars, and Aggrefs mentioned in
+	 * non-group cols (plus HAVING), and add them to the partial_target if not
+	 * already present.  (An expression used directly as a GROUP BY item will
+	 * be present already.)  Note this includes Vars used in resjunk items, so
+	 * we are covering the needs of ORDER BY and window specifications.
+	 */
+	non_group_exprs = pull_var_clause((Node *) non_group_cols,
+									  PVC_INCLUDE_AGGREGATES |
+									  PVC_RECURSE_WINDOWFUNCS |
+									  PVC_INCLUDE_PLACEHOLDERS);
+
+	add_new_columns_to_pathtarget(partial_target, non_group_exprs);
+
+	/*
+	 * Adjust Aggrefs to put them in partial mode.  At this point all Aggrefs
+	 * are at the top level of the target list, so we can just scan the list
+	 * rather than recursing through the expression trees.
+	 */
+	foreach(lc, partial_target->exprs)
+	{
+		Aggref	   *aggref = (Aggref *) lfirst(lc);
+
+		if (IsA(aggref, Aggref))
+		{
+			Aggref	   *newaggref;
+
+			/*
+			 * We shouldn't need to copy the substructure of the Aggref node,
+			 * but flat-copy the node itself to avoid damaging other trees.
+			 */
+			newaggref = makeNode(Aggref);
+			memcpy(newaggref, aggref, sizeof(Aggref));
+
+			/* For now, assume serialization is required */
+			mark_partial_aggref(newaggref, AGGSPLIT_INITIAL_SERIAL);
+
+			lfirst(lc) = newaggref;
+		}
+	}
+
+	/* clean up cruft */
+	list_free(non_group_exprs);
+	list_free(non_group_cols);
+
+	/* XXX this causes some redundant cost calculation ... */
+	return set_pathtarget_cost_width(root, partial_target);
+}
+
+/*
+ * mark_partial_aggref
+ *	  Adjust an Aggref to make it represent a partial-aggregation step.
+ *
+ * The Aggref node is modified in-place; caller must do any copying required.
+ */
+void
+mark_partial_aggref(Aggref *agg, AggSplit aggsplit)
+{
+	/* aggtranstype should be computed by this point */
+	Assert(OidIsValid(agg->aggtranstype));
+	/* ... but aggsplit should still be as the parser left it */
+	Assert(agg->aggsplit == AGGSPLIT_SIMPLE);
+
+	/* Mark the Aggref with the intended partial-aggregation mode */
+	agg->aggsplit = aggsplit;
+
+	/*
+	 * Adjust result type if needed.  Normally, a partial aggregate returns
+	 * the aggregate's transition type; but if that's INTERNAL and we're
+	 * serializing, it returns BYTEA instead.
+	 */
+	if (DO_AGGSPLIT_SKIPFINAL(aggsplit))
+	{
+		if (agg->aggtranstype == INTERNALOID && DO_AGGSPLIT_SERIALIZE(aggsplit))
+			agg->aggtype = BYTEAOID;
+		else
+			agg->aggtype = agg->aggtranstype;
+	}
+}
+
+/*
+ * postprocess_setop_tlist
+ *	  Fix up targetlist returned by plan_set_operations().
+ *
+ * We need to transpose sort key info from the orig_tlist into new_tlist.
+ * NOTE: this would not be good enough if we supported resjunk sort keys
+ * for results of set operations --- then, we'd need to project a whole
+ * new tlist to evaluate the resjunk columns.  For now, just ereport if we
+ * find any resjunk columns in orig_tlist.
+ */
+static List *
+postprocess_setop_tlist(List *new_tlist, List *orig_tlist)
+{
+	ListCell   *l;
+	ListCell   *orig_tlist_item = list_head(orig_tlist);
+
+	foreach(l, new_tlist)
+	{
+		TargetEntry *new_tle = lfirst_node(TargetEntry, l);
+		TargetEntry *orig_tle;
+
+		/* ignore resjunk columns in setop result */
+		if (new_tle->resjunk)
+			continue;
+
+		Assert(orig_tlist_item != NULL);
+		orig_tle = lfirst_node(TargetEntry, orig_tlist_item);
+		orig_tlist_item = lnext(orig_tlist_item);
+		if (orig_tle->resjunk)	/* should not happen */
+			elog(ERROR, "resjunk output columns are not implemented");
+		Assert(new_tle->resno == orig_tle->resno);
+		new_tle->ressortgroupref = orig_tle->ressortgroupref;
+	}
+	if (orig_tlist_item != NULL)
+		elog(ERROR, "resjunk output columns are not implemented");
+	return new_tlist;
+}
+
+/*
+ * select_active_windows
+ *		Create a list of the "active" window clauses (ie, those referenced
+ *		by non-deleted WindowFuncs) in the order they are to be executed.
+ */
+static List *
+select_active_windows(PlannerInfo *root, WindowFuncLists *wflists)
+{
+	List	   *windowClause = root->parse->windowClause;
+	List	   *result = NIL;
+	ListCell   *lc;
+	int			nActive = 0;
+	WindowClauseSortData *actives = palloc(sizeof(WindowClauseSortData)
+										   * list_length(windowClause));
+
+	/* First, construct an array of the active windows */
+	foreach(lc, windowClause)
+	{
+		WindowClause *wc = lfirst_node(WindowClause, lc);
+
+		/* It's only active if wflists shows some related WindowFuncs */
+		Assert(wc->winref <= wflists->maxWinRef);
+		if (wflists->windowFuncs[wc->winref] == NIL)
+			continue;
+
+		actives[nActive].wc = wc;	/* original clause */
+
+		/*
+		 * For sorting, we want the list of partition keys followed by the
+		 * list of sort keys. But pathkeys construction will remove duplicates
+		 * between the two, so we can as well (even though we can't detect all
+		 * of the duplicates, since some may come from ECs - that might mean
+		 * we miss optimization chances here). We must, however, ensure that
+		 * the order of entries is preserved with respect to the ones we do
+		 * keep.
+		 *
+		 * partitionClause and orderClause had their own duplicates removed in
+		 * parse analysis, so we're only concerned here with removing
+		 * orderClause entries that also appear in partitionClause.
+		 */
+		actives[nActive].uniqueOrder =
+			list_concat_unique(list_copy(wc->partitionClause),
+							   wc->orderClause);
+		nActive++;
+	}
+
+	/*
+	 * Sort active windows by their partitioning/ordering clauses, ignoring
+	 * any framing clauses, so that the windows that need the same sorting are
+	 * adjacent in the list. When we come to generate paths, this will avoid
+	 * inserting additional Sort nodes.
+	 *
+	 * This is how we implement a specific requirement from the SQL standard,
+	 * which says that when two or more windows are order-equivalent (i.e.
+	 * have matching partition and order clauses, even if their names or
+	 * framing clauses differ), then all peer rows must be presented in the
+	 * same order in all of them. If we allowed multiple sort nodes for such
+	 * cases, we'd risk having the peer rows end up in different orders in
+	 * equivalent windows due to sort instability. (See General Rule 4 of
+	 * <window clause> in SQL2008 - SQL2016.)
+	 *
+	 * Additionally, if the entire list of clauses of one window is a prefix
+	 * of another, put first the window with stronger sorting requirements.
+	 * This way we will first sort for stronger window, and won't have to sort
+	 * again for the weaker one.
+	 */
+	qsort(actives, nActive, sizeof(WindowClauseSortData), common_prefix_cmp);
+
+	/* build ordered list of the original WindowClause nodes */
+	for (int i = 0; i < nActive; i++)
+		result = lappend(result, actives[i].wc);
+
+	pfree(actives);
+
+	return result;
+}
+
+/*
+ * common_prefix_cmp
+ *	  QSort comparison function for WindowClauseSortData
+ *
+ * Sort the windows by the required sorting clauses. First, compare the sort
+ * clauses themselves. Second, if one window's clauses are a prefix of another
+ * one's clauses, put the window with more sort clauses first.
+ */
+static int
+common_prefix_cmp(const void *a, const void *b)
+{
+	const WindowClauseSortData *wcsa = a;
+	const WindowClauseSortData *wcsb = b;
+	ListCell   *item_a;
+	ListCell   *item_b;
+
+	forboth(item_a, wcsa->uniqueOrder, item_b, wcsb->uniqueOrder)
+	{
+		SortGroupClause *sca = lfirst_node(SortGroupClause, item_a);
+		SortGroupClause *scb = lfirst_node(SortGroupClause, item_b);
+
+		if (sca->tleSortGroupRef > scb->tleSortGroupRef)
+			return -1;
+		else if (sca->tleSortGroupRef < scb->tleSortGroupRef)
+			return 1;
+		else if (sca->sortop > scb->sortop)
+			return -1;
+		else if (sca->sortop < scb->sortop)
+			return 1;
+		else if (sca->nulls_first && !scb->nulls_first)
+			return -1;
+		else if (!sca->nulls_first && scb->nulls_first)
+			return 1;
+		/* no need to compare eqop, since it is fully determined by sortop */
+	}
+
+	if (list_length(wcsa->uniqueOrder) > list_length(wcsb->uniqueOrder))
+		return -1;
+	else if (list_length(wcsa->uniqueOrder) < list_length(wcsb->uniqueOrder))
+		return 1;
+
+	return 0;
+}
+
+/*
+ * make_window_input_target
+ *	  Generate appropriate PathTarget for initial input to WindowAgg nodes.
+ *
+ * When the query has window functions, this function computes the desired
+ * target to be computed by the node just below the first WindowAgg.
+ * This tlist must contain all values needed to evaluate the window functions,
+ * compute the final target list, and perform any required final sort step.
+ * If multiple WindowAggs are needed, each intermediate one adds its window
+ * function results onto this base tlist; only the topmost WindowAgg computes
+ * the actual desired target list.
+ *
+ * This function is much like make_group_input_target, though not quite enough
+ * like it to share code.  As in that function, we flatten most expressions
+ * into their component variables.  But we do not want to flatten window
+ * PARTITION BY/ORDER BY clauses, since that might result in multiple
+ * evaluations of them, which would be bad (possibly even resulting in
+ * inconsistent answers, if they contain volatile functions).
+ * Also, we must not flatten GROUP BY clauses that were left unflattened by
+ * make_group_input_target, because we may no longer have access to the
+ * individual Vars in them.
+ *
+ * Another key difference from make_group_input_target is that we don't
+ * flatten Aggref expressions, since those are to be computed below the
+ * window functions and just referenced like Vars above that.
+ *
+ * 'final_target' is the query's final target list (in PathTarget form)
+ * 'activeWindows' is the list of active windows previously identified by
+ *			select_active_windows.
+ *
+ * The result is the PathTarget to be computed by the plan node immediately
+ * below the first WindowAgg node.
+ */
+static PathTarget *
+make_window_input_target(PlannerInfo *root,
+						 PathTarget *final_target,
+						 List *activeWindows)
+{
+	Query	   *parse = root->parse;
+	PathTarget *input_target;
+	Bitmapset  *sgrefs;
+	List	   *flattenable_cols;
+	List	   *flattenable_vars;
+	int			i;
+	ListCell   *lc;
+
+	Assert(parse->hasWindowFuncs);
+
+	/*
+	 * Collect the sortgroupref numbers of window PARTITION/ORDER BY clauses
+	 * into a bitmapset for convenient reference below.
+	 */
+	sgrefs = NULL;
+	foreach(lc, activeWindows)
+	{
+		WindowClause *wc = lfirst_node(WindowClause, lc);
+		ListCell   *lc2;
+
+		foreach(lc2, wc->partitionClause)
+		{
+			SortGroupClause *sortcl = lfirst_node(SortGroupClause, lc2);
+
+			sgrefs = bms_add_member(sgrefs, sortcl->tleSortGroupRef);
+		}
+		foreach(lc2, wc->orderClause)
+		{
+			SortGroupClause *sortcl = lfirst_node(SortGroupClause, lc2);
+
+			sgrefs = bms_add_member(sgrefs, sortcl->tleSortGroupRef);
+		}
+	}
+
+	/* Add in sortgroupref numbers of GROUP BY clauses, too */
+	foreach(lc, parse->groupClause)
+	{
+		SortGroupClause *grpcl = lfirst_node(SortGroupClause, lc);
+
+		sgrefs = bms_add_member(sgrefs, grpcl->tleSortGroupRef);
+	}
+
+	/*
+	 * Construct a target containing all the non-flattenable targetlist items,
+	 * and save aside the others for a moment.
+	 */
+	input_target = create_empty_pathtarget();
+	flattenable_cols = NIL;
+
+	i = 0;
+	foreach(lc, final_target->exprs)
+	{
+		Expr	   *expr = (Expr *) lfirst(lc);
+		Index		sgref = get_pathtarget_sortgroupref(final_target, i);
+
+		/*
+		 * Don't want to deconstruct window clauses or GROUP BY items.  (Note
+		 * that such items can't contain window functions, so it's okay to
+		 * compute them below the WindowAgg nodes.)
+		 */
+		if (sgref != 0 && bms_is_member(sgref, sgrefs))
+		{
+			/*
+			 * Don't want to deconstruct this value, so add it to the input
+			 * target as-is.
+			 */
+			add_column_to_pathtarget(input_target, expr, sgref);
+		}
+		else
+		{
+			/*
+			 * Column is to be flattened, so just remember the expression for
+			 * later call to pull_var_clause.
+			 */
+			flattenable_cols = lappend(flattenable_cols, expr);
+		}
+
+		i++;
+	}
+
+	/*
+	 * Pull out all the Vars and Aggrefs mentioned in flattenable columns, and
+	 * add them to the input target if not already present.  (Some might be
+	 * there already because they're used directly as window/group clauses.)
+	 *
+	 * Note: it's essential to use PVC_INCLUDE_AGGREGATES here, so that any
+	 * Aggrefs are placed in the Agg node's tlist and not left to be computed
+	 * at higher levels.  On the other hand, we should recurse into
+	 * WindowFuncs to make sure their input expressions are available.
+	 */
+	flattenable_vars = pull_var_clause((Node *) flattenable_cols,
+									   PVC_INCLUDE_AGGREGATES |
+									   PVC_RECURSE_WINDOWFUNCS |
+									   PVC_INCLUDE_PLACEHOLDERS);
+	add_new_columns_to_pathtarget(input_target, flattenable_vars);
+
+	/* clean up cruft */
+	list_free(flattenable_vars);
+	list_free(flattenable_cols);
+
+	/* XXX this causes some redundant cost calculation ... */
+	return set_pathtarget_cost_width(root, input_target);
+}
+
+/*
+ * make_pathkeys_for_window
+ *		Create a pathkeys list describing the required input ordering
+ *		for the given WindowClause.
+ *
+ * The required ordering is first the PARTITION keys, then the ORDER keys.
+ * In the future we might try to implement windowing using hashing, in which
+ * case the ordering could be relaxed, but for now we always sort.
+ */
+static List *
+make_pathkeys_for_window(PlannerInfo *root, WindowClause *wc,
+						 List *tlist)
+{
+	List	   *window_pathkeys;
+	List	   *window_sortclauses;
+
+	/* Throw error if can't sort */
+	if (!grouping_is_sortable(wc->partitionClause))
+		ereport(ERROR,
+				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+				 errmsg("could not implement window PARTITION BY"),
+				 errdetail("Window partitioning columns must be of sortable datatypes.")));
+	if (!grouping_is_sortable(wc->orderClause))
+		ereport(ERROR,
+				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+				 errmsg("could not implement window ORDER BY"),
+				 errdetail("Window ordering columns must be of sortable datatypes.")));
+
+	/* Okay, make the combined pathkeys */
+	window_sortclauses = list_concat(list_copy(wc->partitionClause),
+									 list_copy(wc->orderClause));
+	window_pathkeys = make_pathkeys_for_sortclauses(root,
+													window_sortclauses,
+													tlist);
+	list_free(window_sortclauses);
+	return window_pathkeys;
+}
+
+/*
+ * make_sort_input_target
+ *	  Generate appropriate PathTarget for initial input to Sort step.
+ *
+ * If the query has ORDER BY, this function chooses the target to be computed
+ * by the node just below the Sort (and DISTINCT, if any, since Unique can't
+ * project) steps.  This might or might not be identical to the query's final
+ * output target.
+ *
+ * The main argument for keeping the sort-input tlist the same as the final
+ * is that we avoid a separate projection node (which will be needed if
+ * they're different, because Sort can't project).  However, there are also
+ * advantages to postponing tlist evaluation till after the Sort: it ensures
+ * a consistent order of evaluation for any volatile functions in the tlist,
+ * and if there's also a LIMIT, we can stop the query without ever computing
+ * tlist functions for later rows, which is beneficial for both volatile and
+ * expensive functions.
+ *
+ * Our current policy is to postpone volatile expressions till after the sort
+ * unconditionally (assuming that that's possible, ie they are in plain tlist
+ * columns and not ORDER BY/GROUP BY/DISTINCT columns).  We also prefer to
+ * postpone set-returning expressions, because running them beforehand would
+ * bloat the sort dataset, and because it might cause unexpected output order
+ * if the sort isn't stable.  However there's a constraint on that: all SRFs
+ * in the tlist should be evaluated at the same plan step, so that they can
+ * run in sync in nodeProjectSet.  So if any SRFs are in sort columns, we
+ * mustn't postpone any SRFs.  (Note that in principle that policy should
+ * probably get applied to the group/window input targetlists too, but we
+ * have not done that historically.)  Lastly, expensive expressions are
+ * postponed if there is a LIMIT, or if root->tuple_fraction shows that
+ * partial evaluation of the query is possible (if neither is true, we expect
+ * to have to evaluate the expressions for every row anyway), or if there are
+ * any volatile or set-returning expressions (since once we've put in a
+ * projection at all, it won't cost any more to postpone more stuff).
+ *
+ * Another issue that could potentially be considered here is that
+ * evaluating tlist expressions could result in data that's either wider
+ * or narrower than the input Vars, thus changing the volume of data that
+ * has to go through the Sort.  However, we usually have only a very bad
+ * idea of the output width of any expression more complex than a Var,
+ * so for now it seems too risky to try to optimize on that basis.
+ *
+ * Note that if we do produce a modified sort-input target, and then the
+ * query ends up not using an explicit Sort, no particular harm is done:
+ * we'll initially use the modified target for the preceding path nodes,
+ * but then change them to the final target with apply_projection_to_path.
+ * Moreover, in such a case the guarantees about evaluation order of
+ * volatile functions still hold, since the rows are sorted already.
+ *
+ * This function has some things in common with make_group_input_target and
+ * make_window_input_target, though the detailed rules for what to do are
+ * different.  We never flatten/postpone any grouping or ordering columns;
+ * those are needed before the sort.  If we do flatten a particular
+ * expression, we leave Aggref and WindowFunc nodes alone, since those were
+ * computed earlier.
+ *
+ * 'final_target' is the query's final target list (in PathTarget form)
+ * 'have_postponed_srfs' is an output argument, see below
+ *
+ * The result is the PathTarget to be computed by the plan node immediately
+ * below the Sort step (and the Distinct step, if any).  This will be
+ * exactly final_target if we decide a projection step wouldn't be helpful.
+ *
+ * In addition, *have_postponed_srfs is set to true if we choose to postpone
+ * any set-returning functions to after the Sort.
+ */
+static PathTarget *
+make_sort_input_target(PlannerInfo *root,
+					   PathTarget *final_target,
+					   bool *have_postponed_srfs)
+{
+	Query	   *parse = root->parse;
+	PathTarget *input_target;
+	int			ncols;
+	bool	   *col_is_srf;
+	bool	   *postpone_col;
+	bool		have_srf;
+	bool		have_volatile;
+	bool		have_expensive;
+	bool		have_srf_sortcols;
+	bool		postpone_srfs;
+	List	   *postponable_cols;
+	List	   *postponable_vars;
+	int			i;
+	ListCell   *lc;
+
+	/* Shouldn't get here unless query has ORDER BY */
+	Assert(parse->sortClause);
+
+	*have_postponed_srfs = false;	/* default result */
+
+	/* Inspect tlist and collect per-column information */
+	ncols = list_length(final_target->exprs);
+	col_is_srf = (bool *) palloc0(ncols * sizeof(bool));
+	postpone_col = (bool *) palloc0(ncols * sizeof(bool));
+	have_srf = have_volatile = have_expensive = have_srf_sortcols = false;
+
+	i = 0;
+	foreach(lc, final_target->exprs)
+	{
+		Expr	   *expr = (Expr *) lfirst(lc);
+
+		/*
+		 * If the column has a sortgroupref, assume it has to be evaluated
+		 * before sorting.  Generally such columns would be ORDER BY, GROUP
+		 * BY, etc targets.  One exception is columns that were removed from
+		 * GROUP BY by remove_useless_groupby_columns() ... but those would
+		 * only be Vars anyway.  There don't seem to be any cases where it
+		 * would be worth the trouble to double-check.
+		 */
+		if (get_pathtarget_sortgroupref(final_target, i) == 0)
+		{
+			/*
+			 * Check for SRF or volatile functions.  Check the SRF case first
+			 * because we must know whether we have any postponed SRFs.
+			 */
+			if (parse->hasTargetSRFs &&
+				expression_returns_set((Node *) expr))
+			{
+				/* We'll decide below whether these are postponable */
+				col_is_srf[i] = true;
+				have_srf = true;
+			}
+			else if (contain_volatile_functions((Node *) expr))
+			{
+				/* Unconditionally postpone */
+				postpone_col[i] = true;
+				have_volatile = true;
+			}
+			else
+			{
+				/*
+				 * Else check the cost.  XXX it's annoying to have to do this
+				 * when set_pathtarget_cost_width() just did it.  Refactor to
+				 * allow sharing the work?
+				 */
+				QualCost	cost;
+
+				cost_qual_eval_node(&cost, (Node *) expr, root);
+
+				/*
+				 * We arbitrarily define "expensive" as "more than 10X
+				 * cpu_operator_cost".  Note this will take in any PL function
+				 * with default cost.
+				 */
+				if (cost.per_tuple > 10 * cpu_operator_cost)
+				{
+					postpone_col[i] = true;
+					have_expensive = true;
+				}
+			}
+		}
+		else
+		{
+			/* For sortgroupref cols, just check if any contain SRFs */
+			if (!have_srf_sortcols &&
+				parse->hasTargetSRFs &&
+				expression_returns_set((Node *) expr))
+				have_srf_sortcols = true;
+		}
+
+		i++;
+	}
+
+	/*
+	 * We can postpone SRFs if we have some but none are in sortgroupref cols.
+	 */
+	postpone_srfs = (have_srf && !have_srf_sortcols);
+
+	/*
+	 * If we don't need a post-sort projection, just return final_target.
+	 */
+	if (!(postpone_srfs || have_volatile ||
+		  (have_expensive &&
+		   (parse->limitCount || root->tuple_fraction > 0))))
+		return final_target;
+
+	/*
+	 * Report whether the post-sort projection will contain set-returning
+	 * functions.  This is important because it affects whether the Sort can
+	 * rely on the query's LIMIT (if any) to bound the number of rows it needs
+	 * to return.
+	 */
+	*have_postponed_srfs = postpone_srfs;
+
+	/*
+	 * Construct the sort-input target, taking all non-postponable columns and
+	 * then adding Vars, PlaceHolderVars, Aggrefs, and WindowFuncs found in
+	 * the postponable ones.
+	 */
+	input_target = create_empty_pathtarget();
+	postponable_cols = NIL;
+
+	i = 0;
+	foreach(lc, final_target->exprs)
+	{
+		Expr	   *expr = (Expr *) lfirst(lc);
+
+		if (postpone_col[i] || (postpone_srfs && col_is_srf[i]))
+			postponable_cols = lappend(postponable_cols, expr);
+		else
+			add_column_to_pathtarget(input_target, expr,
+									 get_pathtarget_sortgroupref(final_target, i));
+
+		i++;
+	}
+
+	/*
+	 * Pull out all the Vars, Aggrefs, and WindowFuncs mentioned in
+	 * postponable columns, and add them to the sort-input target if not
+	 * already present.  (Some might be there already.)  We mustn't
+	 * deconstruct Aggrefs or WindowFuncs here, since the projection node
+	 * would be unable to recompute them.
+	 */
+	postponable_vars = pull_var_clause((Node *) postponable_cols,
+									   PVC_INCLUDE_AGGREGATES |
+									   PVC_INCLUDE_WINDOWFUNCS |
+									   PVC_INCLUDE_PLACEHOLDERS);
+	add_new_columns_to_pathtarget(input_target, postponable_vars);
+
+	/* clean up cruft */
+	list_free(postponable_vars);
+	list_free(postponable_cols);
+
+	/* XXX this represents even more redundant cost calculation ... */
+	return set_pathtarget_cost_width(root, input_target);
+}
+
+/*
+ * get_cheapest_fractional_path
+ *	  Find the cheapest path for retrieving a specified fraction of all
+ *	  the tuples expected to be returned by the given relation.
+ *
+ * We interpret tuple_fraction the same way as grouping_planner.
+ *
+ * We assume set_cheapest() has been run on the given rel.
+ */
+Path *
+get_cheapest_fractional_path(RelOptInfo *rel, double tuple_fraction)
+{
+	Path	   *best_path = rel->cheapest_total_path;
+	ListCell   *l;
+
+	/* If all tuples will be retrieved, just return the cheapest-total path */
+	if (tuple_fraction <= 0.0)
+		return best_path;
+
+	/* Convert absolute # of tuples to a fraction; no need to clamp to 0..1 */
+	if (tuple_fraction >= 1.0 && best_path->rows > 0)
+		tuple_fraction /= best_path->rows;
+
+	foreach(l, rel->pathlist)
+	{
+		Path	   *path = (Path *) lfirst(l);
+
+		if (path == rel->cheapest_total_path ||
+			compare_fractional_path_costs(best_path, path, tuple_fraction) <= 0)
+			continue;
+
+		best_path = path;
+	}
+
+	return best_path;
+}
+
+/*
+ * adjust_paths_for_srfs
+ *		Fix up the Paths of the given upperrel to handle tSRFs properly.
+ *
+ * The executor can only handle set-returning functions that appear at the
+ * top level of the targetlist of a ProjectSet plan node.  If we have any SRFs
+ * that are not at top level, we need to split up the evaluation into multiple
+ * plan levels in which each level satisfies this constraint.  This function
+ * modifies each Path of an upperrel that (might) compute any SRFs in its
+ * output tlist to insert appropriate projection steps.
+ *
+ * The given targets and targets_contain_srfs lists are from
+ * split_pathtarget_at_srfs().  We assume the existing Paths emit the first
+ * target in targets.
+ */
+static void
+adjust_paths_for_srfs(PlannerInfo *root, RelOptInfo *rel,
+					  List *targets, List *targets_contain_srfs)
+{
+	ListCell   *lc;
+
+	Assert(list_length(targets) == list_length(targets_contain_srfs));
+	Assert(!linitial_int(targets_contain_srfs));
+
+	/* If no SRFs appear at this plan level, nothing to do */
+	if (list_length(targets) == 1)
+		return;
+
+	/*
+	 * Stack SRF-evaluation nodes atop each path for the rel.
+	 *
+	 * In principle we should re-run set_cheapest() here to identify the
+	 * cheapest path, but it seems unlikely that adding the same tlist eval
+	 * costs to all the paths would change that, so we don't bother. Instead,
+	 * just assume that the cheapest-startup and cheapest-total paths remain
+	 * so.  (There should be no parameterized paths anymore, so we needn't
+	 * worry about updating cheapest_parameterized_paths.)
+	 */
+	foreach(lc, rel->pathlist)
+	{
+		Path	   *subpath = (Path *) lfirst(lc);
+		Path	   *newpath = subpath;
+		ListCell   *lc1,
+				   *lc2;
+
+		Assert(subpath->param_info == NULL);
+		forboth(lc1, targets, lc2, targets_contain_srfs)
+		{
+			PathTarget *thistarget = lfirst_node(PathTarget, lc1);
+			bool		contains_srfs = (bool) lfirst_int(lc2);
+
+			/* If this level doesn't contain SRFs, do regular projection */
+			if (contains_srfs)
+				newpath = (Path *) create_set_projection_path(root,
+															  rel,
+															  newpath,
+															  thistarget);
+			else
+				newpath = (Path *) apply_projection_to_path(root,
+															rel,
+															newpath,
+															thistarget);
+		}
+		lfirst(lc) = newpath;
+		if (subpath == rel->cheapest_startup_path)
+			rel->cheapest_startup_path = newpath;
+		if (subpath == rel->cheapest_total_path)
+			rel->cheapest_total_path = newpath;
+	}
+
+	/* Likewise for partial paths, if any */
+	foreach(lc, rel->partial_pathlist)
+	{
+		Path	   *subpath = (Path *) lfirst(lc);
+		Path	   *newpath = subpath;
+		ListCell   *lc1,
+				   *lc2;
+
+		Assert(subpath->param_info == NULL);
+		forboth(lc1, targets, lc2, targets_contain_srfs)
+		{
+			PathTarget *thistarget = lfirst_node(PathTarget, lc1);
+			bool		contains_srfs = (bool) lfirst_int(lc2);
+
+			/* If this level doesn't contain SRFs, do regular projection */
+			if (contains_srfs)
+				newpath = (Path *) create_set_projection_path(root,
+															  rel,
+															  newpath,
+															  thistarget);
+			else
+			{
+				/* avoid apply_projection_to_path, in case of multiple refs */
+				newpath = (Path *) create_projection_path(root,
+														  rel,
+														  newpath,
+														  thistarget);
+			}
+		}
+		lfirst(lc) = newpath;
+	}
+}
+
+/*
+ * expression_planner
+ *		Perform planner's transformations on a standalone expression.
+ *
+ * Various utility commands need to evaluate expressions that are not part
+ * of a plannable query.  They can do so using the executor's regular
+ * expression-execution machinery, but first the expression has to be fed
+ * through here to transform it from parser output to something executable.
+ *
+ * Currently, we disallow sublinks in standalone expressions, so there's no
+ * real "planning" involved here.  (That might not always be true though.)
+ * What we must do is run eval_const_expressions to ensure that any function
+ * calls are converted to positional notation and function default arguments
+ * get inserted.  The fact that constant subexpressions get simplified is a
+ * side-effect that is useful when the expression will get evaluated more than
+ * once.  Also, we must fix operator function IDs.
+ *
+ * This does not return any information about dependencies of the expression.
+ * Hence callers should use the results only for the duration of the current
+ * query.  Callers that would like to cache the results for longer should use
+ * expression_planner_with_deps, probably via the plancache.
+ *
+ * Note: this must not make any damaging changes to the passed-in expression
+ * tree.  (It would actually be okay to apply fix_opfuncids to it, but since
+ * we first do an expression_tree_mutator-based walk, what is returned will
+ * be a new node tree.)  The result is constructed in the current memory
+ * context; beware that this can leak a lot of additional stuff there, too.
+ */
+Expr *
+expression_planner(Expr *expr)
+{
+	Node	   *result;
+
+	/*
+	 * Convert named-argument function calls, insert default arguments and
+	 * simplify constant subexprs
+	 */
+	result = eval_const_expressions(NULL, (Node *) expr);
+
+	/* Fill in opfuncid values if missing */
+	fix_opfuncids(result);
+
+	return (Expr *) result;
+}
+
+/*
+ * expression_planner_with_deps
+ *		Perform planner's transformations on a standalone expression,
+ *		returning expression dependency information along with the result.
+ *
+ * This is identical to expression_planner() except that it also returns
+ * information about possible dependencies of the expression, ie identities of
+ * objects whose definitions affect the result.  As in a PlannedStmt, these
+ * are expressed as a list of relation Oids and a list of PlanInvalItems.
+ */
+Expr *
+expression_planner_with_deps(Expr *expr,
+							 List **relationOids,
+							 List **invalItems)
+{
+	Node	   *result;
+	PlannerGlobal glob;
+	PlannerInfo root;
+
+	/* Make up dummy planner state so we can use setrefs machinery */
+	MemSet(&glob, 0, sizeof(glob));
+	glob.type = T_PlannerGlobal;
+	glob.relationOids = NIL;
+	glob.invalItems = NIL;
+
+	MemSet(&root, 0, sizeof(root));
+	root.type = T_PlannerInfo;
+	root.glob = &glob;
+
+	/*
+	 * Convert named-argument function calls, insert default arguments and
+	 * simplify constant subexprs.  Collect identities of inlined functions
+	 * and elided domains, too.
+	 */
+	result = eval_const_expressions(&root, (Node *) expr);
+
+	/* Fill in opfuncid values if missing */
+	fix_opfuncids(result);
+
+	/*
+	 * Now walk the finished expression to find anything else we ought to
+	 * record as an expression dependency.
+	 */
+	(void) extract_query_dependencies_walker(result, &root);
+
+	*relationOids = glob.relationOids;
+	*invalItems = glob.invalItems;
+
+	return (Expr *) result;
+}
+
+
+/*
+ * plan_cluster_use_sort
+ *		Use the planner to decide how CLUSTER should implement sorting
+ *
+ * tableOid is the OID of a table to be clustered on its index indexOid
+ * (which is already known to be a btree index).  Decide whether it's
+ * cheaper to do an indexscan or a seqscan-plus-sort to execute the CLUSTER.
+ * Return true to use sorting, false to use an indexscan.
+ *
+ * Note: caller had better already hold some type of lock on the table.
+ */
+bool
+plan_cluster_use_sort(Oid tableOid, Oid indexOid)
+{
+	PlannerInfo *root;
+	Query	   *query;
+	PlannerGlobal *glob;
+	RangeTblEntry *rte;
+	RelOptInfo *rel;
+	IndexOptInfo *indexInfo;
+	QualCost	indexExprCost;
+	Cost		comparisonCost;
+	Path	   *seqScanPath;
+	Path		seqScanAndSortPath;
+	IndexPath  *indexScanPath;
+	ListCell   *lc;
+
+	/* We can short-circuit the cost comparison if indexscans are disabled */
+	if (!enable_indexscan)
+		return true;			/* use sort */
+
+	/* Set up mostly-dummy planner state */
+	query = makeNode(Query);
+	query->commandType = CMD_SELECT;
+
+	glob = makeNode(PlannerGlobal);
+
+	root = makeNode(PlannerInfo);
+	root->parse = query;
+	root->glob = glob;
+	root->query_level = 1;
+	root->planner_cxt = CurrentMemoryContext;
+	root->wt_param_id = -1;
+
+	/* Build a minimal RTE for the rel */
+	rte = makeNode(RangeTblEntry);
+	rte->rtekind = RTE_RELATION;
+	rte->relid = tableOid;
+	rte->relkind = RELKIND_RELATION;	/* Don't be too picky. */
+	rte->rellockmode = AccessShareLock;
+	rte->lateral = false;
+	rte->inh = false;
+	rte->inFromCl = true;
+	query->rtable = list_make1(rte);
+
+	/* Set up RTE/RelOptInfo arrays */
+	setup_simple_rel_arrays(root);
+
+	/* Build RelOptInfo */
+	rel = build_simple_rel(root, 1, NULL);
+
+	/* Locate IndexOptInfo for the target index */
+	indexInfo = NULL;
+	foreach(lc, rel->indexlist)
+	{
+		indexInfo = lfirst_node(IndexOptInfo, lc);
+		if (indexInfo->indexoid == indexOid)
+			break;
+	}
+
+	/*
+	 * It's possible that get_relation_info did not generate an IndexOptInfo
+	 * for the desired index; this could happen if it's not yet reached its
+	 * indcheckxmin usability horizon, or if it's a system index and we're
+	 * ignoring system indexes.  In such cases we should tell CLUSTER to not
+	 * trust the index contents but use seqscan-and-sort.
+	 */
+	if (lc == NULL)				/* not in the list? */
+		return true;			/* use sort */
+
+	/*
+	 * Rather than doing all the pushups that would be needed to use
+	 * set_baserel_size_estimates, just do a quick hack for rows and width.
+	 */
+	rel->rows = rel->tuples;
+	rel->reltarget->width = get_relation_data_width(tableOid, NULL);
+
+	root->total_table_pages = rel->pages;
+
+	/*
+	 * Determine eval cost of the index expressions, if any.  We need to
+	 * charge twice that amount for each tuple comparison that happens during
+	 * the sort, since tuplesort.c will have to re-evaluate the index
+	 * expressions each time.  (XXX that's pretty inefficient...)
+	 */
+	cost_qual_eval(&indexExprCost, indexInfo->indexprs, root);
+	comparisonCost = 2.0 * (indexExprCost.startup + indexExprCost.per_tuple);
+
+	/* Estimate the cost of seq scan + sort */
+	seqScanPath = create_seqscan_path(root, rel, NULL, 0);
+	cost_sort(&seqScanAndSortPath, root, NIL,
+			  seqScanPath->total_cost, rel->tuples, rel->reltarget->width,
+			  comparisonCost, maintenance_work_mem, -1.0);
+
+	/* Estimate the cost of index scan */
+	indexScanPath = create_index_path(root, indexInfo,
+									  NIL, NIL, NIL, NIL,
+									  ForwardScanDirection, false,
+									  NULL, 1.0, false);
+
+	return (seqScanAndSortPath.total_cost < indexScanPath->path.total_cost);
+}
+
+/*
+ * plan_create_index_workers
+ *		Use the planner to decide how many parallel worker processes
+ *		CREATE INDEX should request for use
+ *
+ * tableOid is the table on which the index is to be built.  indexOid is the
+ * OID of an index to be created or reindexed (which must be a btree index).
+ *
+ * Return value is the number of parallel worker processes to request.  It
+ * may be unsafe to proceed if this is 0.  Note that this does not include the
+ * leader participating as a worker (value is always a number of parallel
+ * worker processes).
+ *
+ * Note: caller had better already hold some type of lock on the table and
+ * index.
+ */
+int
+plan_create_index_workers(Oid tableOid, Oid indexOid)
+{
+	PlannerInfo *root;
+	Query	   *query;
+	PlannerGlobal *glob;
+	RangeTblEntry *rte;
+	Relation	heap;
+	Relation	index;
+	RelOptInfo *rel;
+	int			parallel_workers;
+	BlockNumber heap_blocks;
+	double		reltuples;
+	double		allvisfrac;
+
+	/*
+	 * We don't allow performing parallel operation in standalone backend or
+	 * when parallelism is disabled.
+	 */
+	if (!IsUnderPostmaster || max_parallel_maintenance_workers == 0)
+		return 0;
+
+	/* Set up largely-dummy planner state */
+	query = makeNode(Query);
+	query->commandType = CMD_SELECT;
+
+	glob = makeNode(PlannerGlobal);
+
+	root = makeNode(PlannerInfo);
+	root->parse = query;
+	root->glob = glob;
+	root->query_level = 1;
+	root->planner_cxt = CurrentMemoryContext;
+	root->wt_param_id = -1;
+
+	/*
+	 * Build a minimal RTE.
+	 *
+	 * Mark the RTE with inh = true.  This is a kludge to prevent
+	 * get_relation_info() from fetching index info, which is necessary
+	 * because it does not expect that any IndexOptInfo is currently
+	 * undergoing REINDEX.
+	 */
+	rte = makeNode(RangeTblEntry);
+	rte->rtekind = RTE_RELATION;
+	rte->relid = tableOid;
+	rte->relkind = RELKIND_RELATION;	/* Don't be too picky. */
+	rte->rellockmode = AccessShareLock;
+	rte->lateral = false;
+	rte->inh = true;
+	rte->inFromCl = true;
+	query->rtable = list_make1(rte);
+
+	/* Set up RTE/RelOptInfo arrays */
+	setup_simple_rel_arrays(root);
+
+	/* Build RelOptInfo */
+	rel = build_simple_rel(root, 1, NULL);
+
+	/* Rels are assumed already locked by the caller */
+	heap = table_open(tableOid, NoLock);
+	index = index_open(indexOid, NoLock);
+
+	/*
+	 * Determine if it's safe to proceed.
+	 *
+	 * Currently, parallel workers can't access the leader's temporary tables.
+	 * Furthermore, any index predicate or index expressions must be parallel
+	 * safe.
+	 */
+	if (heap->rd_rel->relpersistence == RELPERSISTENCE_TEMP ||
+		!is_parallel_safe(root, (Node *) RelationGetIndexExpressions(index)) ||
+		!is_parallel_safe(root, (Node *) RelationGetIndexPredicate(index)))
+	{
+		parallel_workers = 0;
+		goto done;
+	}
+
+	/*
+	 * If parallel_workers storage parameter is set for the table, accept that
+	 * as the number of parallel worker processes to launch (though still cap
+	 * at max_parallel_maintenance_workers).  Note that we deliberately do not
+	 * consider any other factor when parallel_workers is set. (e.g., memory
+	 * use by workers.)
+	 */
+	if (rel->rel_parallel_workers != -1)
+	{
+		parallel_workers = Min(rel->rel_parallel_workers,
+							   max_parallel_maintenance_workers);
+		goto done;
+	}
+
+	/*
+	 * Estimate heap relation size ourselves, since rel->pages cannot be
+	 * trusted (heap RTE was marked as inheritance parent)
+	 */
+	estimate_rel_size(heap, NULL, &heap_blocks, &reltuples, &allvisfrac);
+
+	/*
+	 * Determine number of workers to scan the heap relation using generic
+	 * model
+	 */
+	parallel_workers = compute_parallel_worker(rel, heap_blocks, -1,
+											   max_parallel_maintenance_workers);
+
+	/*
+	 * Cap workers based on available maintenance_work_mem as needed.
+	 *
+	 * Note that each tuplesort participant receives an even share of the
+	 * total maintenance_work_mem budget.  Aim to leave participants
+	 * (including the leader as a participant) with no less than 32MB of
+	 * memory.  This leaves cases where maintenance_work_mem is set to 64MB
+	 * immediately past the threshold of being capable of launching a single
+	 * parallel worker to sort.
+	 */
+	while (parallel_workers > 0 &&
+		   maintenance_work_mem / (parallel_workers + 1) < 32768L)
+		parallel_workers--;
+
+done:
+	index_close(index, NoLock);
+	table_close(heap, NoLock);
+
+	return parallel_workers;
+}
+
+/*
+ * add_paths_to_grouping_rel
+ *
+ * Add non-partial paths to grouping relation.
+ */
+static void
+add_paths_to_grouping_rel(PlannerInfo *root, RelOptInfo *input_rel,
+						  RelOptInfo *grouped_rel,
+						  RelOptInfo *partially_grouped_rel,
+						  const AggClauseCosts *agg_costs,
+						  grouping_sets_data *gd, double dNumGroups,
+						  GroupPathExtraData *extra)
+{
+	Query	   *parse = root->parse;
+	Path	   *cheapest_path = input_rel->cheapest_total_path;
+	ListCell   *lc;
+	bool		can_hash = (extra->flags & GROUPING_CAN_USE_HASH) != 0;
+	bool		can_sort = (extra->flags & GROUPING_CAN_USE_SORT) != 0;
+	List	   *havingQual = (List *) extra->havingQual;
+	AggClauseCosts *agg_final_costs = &extra->agg_final_costs;
+
+	if (can_sort)
+	{
+		/*
+		 * Use any available suitably-sorted path as input, and also consider
+		 * sorting the cheapest-total path.
+		 */
+		foreach(lc, input_rel->pathlist)
+		{
+			Path	   *path = (Path *) lfirst(lc);
+			bool		is_sorted;
+
+			is_sorted = pathkeys_contained_in(root->group_pathkeys,
+											  path->pathkeys);
+			if (path == cheapest_path || is_sorted)
+			{
+				/* Sort the cheapest-total path if it isn't already sorted */
+				if (!is_sorted)
+					path = (Path *) create_sort_path(root,
+													 grouped_rel,
+													 path,
+													 root->group_pathkeys,
+													 -1.0);
+
+				/* Now decide what to stick atop it */
+				if (parse->groupingSets)
+				{
+					consider_groupingsets_paths(root, grouped_rel,
+												path, true, can_hash,
+												gd, agg_costs, dNumGroups);
+				}
+				else if (parse->hasAggs)
+				{
+					/*
+					 * We have aggregation, possibly with plain GROUP BY. Make
+					 * an AggPath.
+					 */
+					add_path(grouped_rel, (Path *)
+							 create_agg_path(root,
+											 grouped_rel,
+											 path,
+											 grouped_rel->reltarget,
+											 parse->groupClause ? AGG_SORTED : AGG_PLAIN,
+											 AGGSPLIT_SIMPLE,
+											 parse->groupClause,
+											 havingQual,
+											 agg_costs,
+											 dNumGroups));
+				}
+				else if (parse->groupClause)
+				{
+					/*
+					 * We have GROUP BY without aggregation or grouping sets.
+					 * Make a GroupPath.
+					 */
+					add_path(grouped_rel, (Path *)
+							 create_group_path(root,
+											   grouped_rel,
+											   path,
+											   parse->groupClause,
+											   havingQual,
+											   dNumGroups));
+				}
+				else
+				{
+					/* Other cases should have been handled above */
+					Assert(false);
+				}
+			}
+		}
+
+		/*
+		 * Instead of operating directly on the input relation, we can
+		 * consider finalizing a partially aggregated path.
+		 */
+		if (partially_grouped_rel != NULL)
+		{
+			foreach(lc, partially_grouped_rel->pathlist)
+			{
+				Path	   *path = (Path *) lfirst(lc);
+
+				/*
+				 * Insert a Sort node, if required.  But there's no point in
+				 * sorting anything but the cheapest path.
+				 */
+				if (!pathkeys_contained_in(root->group_pathkeys, path->pathkeys))
+				{
+					if (path != partially_grouped_rel->cheapest_total_path)
+						continue;
+					path = (Path *) create_sort_path(root,
+													 grouped_rel,
+													 path,
+													 root->group_pathkeys,
+													 -1.0);
+				}
+
+				if (parse->hasAggs)
+					add_path(grouped_rel, (Path *)
+							 create_agg_path(root,
+											 grouped_rel,
+											 path,
+											 grouped_rel->reltarget,
+											 parse->groupClause ? AGG_SORTED : AGG_PLAIN,
+											 AGGSPLIT_FINAL_DESERIAL,
+											 parse->groupClause,
+											 havingQual,
+											 agg_final_costs,
+											 dNumGroups));
+				else
+					add_path(grouped_rel, (Path *)
+							 create_group_path(root,
+											   grouped_rel,
+											   path,
+											   parse->groupClause,
+											   havingQual,
+											   dNumGroups));
+			}
+		}
+	}
+
+	if (can_hash)
+	{
+		double		hashaggtablesize;
+
+		if (parse->groupingSets)
+		{
+			/*
+			 * Try for a hash-only groupingsets path over unsorted input.
+			 */
+			consider_groupingsets_paths(root, grouped_rel,
+										cheapest_path, false, true,
+										gd, agg_costs, dNumGroups);
+		}
+		else
+		{
+			hashaggtablesize = estimate_hashagg_tablesize(cheapest_path,
+														  agg_costs,
+														  dNumGroups);
+
+			/*
+			 * Provided that the estimated size of the hashtable does not
+			 * exceed work_mem, we'll generate a HashAgg Path, although if we
+			 * were unable to sort above, then we'd better generate a Path, so
+			 * that we at least have one.
+			 */
+			if (hashaggtablesize < work_mem * 1024L ||
+				grouped_rel->pathlist == NIL)
+			{
+				/*
+				 * We just need an Agg over the cheapest-total input path,
+				 * since input order won't matter.
+				 */
+				add_path(grouped_rel, (Path *)
+						 create_agg_path(root, grouped_rel,
+										 cheapest_path,
+										 grouped_rel->reltarget,
+										 AGG_HASHED,
+										 AGGSPLIT_SIMPLE,
+										 parse->groupClause,
+										 havingQual,
+										 agg_costs,
+										 dNumGroups));
+			}
+		}
+
+		/*
+		 * Generate a Finalize HashAgg Path atop of the cheapest partially
+		 * grouped path, assuming there is one. Once again, we'll only do this
+		 * if it looks as though the hash table won't exceed work_mem.
+		 */
+		if (partially_grouped_rel && partially_grouped_rel->pathlist)
+		{
+			Path	   *path = partially_grouped_rel->cheapest_total_path;
+
+			hashaggtablesize = estimate_hashagg_tablesize(path,
+														  agg_final_costs,
+														  dNumGroups);
+
+			if (hashaggtablesize < work_mem * 1024L)
+				add_path(grouped_rel, (Path *)
+						 create_agg_path(root,
+										 grouped_rel,
+										 path,
+										 grouped_rel->reltarget,
+										 AGG_HASHED,
+										 AGGSPLIT_FINAL_DESERIAL,
+										 parse->groupClause,
+										 havingQual,
+										 agg_final_costs,
+										 dNumGroups));
+		}
+	}
+
+	/*
+	 * When partitionwise aggregate is used, we might have fully aggregated
+	 * paths in the partial pathlist, because add_paths_to_append_rel() will
+	 * consider a path for grouped_rel consisting of a Parallel Append of
+	 * non-partial paths from each child.
+	 */
+	if (grouped_rel->partial_pathlist != NIL)
+		gather_grouping_paths(root, grouped_rel);
+}
+
+/*
+ * create_partial_grouping_paths
+ *
+ * Create a new upper relation representing the result of partial aggregation
+ * and populate it with appropriate paths.  Note that we don't finalize the
+ * lists of paths here, so the caller can add additional partial or non-partial
+ * paths and must afterward call gather_grouping_paths and set_cheapest on
+ * the returned upper relation.
+ *
+ * All paths for this new upper relation -- both partial and non-partial --
+ * have been partially aggregated but require a subsequent FinalizeAggregate
+ * step.
+ *
+ * NB: This function is allowed to return NULL if it determines that there is
+ * no real need to create a new RelOptInfo.
+ */
+static RelOptInfo *
+create_partial_grouping_paths(PlannerInfo *root,
+							  RelOptInfo *grouped_rel,
+							  RelOptInfo *input_rel,
+							  grouping_sets_data *gd,
+							  GroupPathExtraData *extra,
+							  bool force_rel_creation)
+{
+	Query	   *parse = root->parse;
+	RelOptInfo *partially_grouped_rel;
+	AggClauseCosts *agg_partial_costs = &extra->agg_partial_costs;
+	AggClauseCosts *agg_final_costs = &extra->agg_final_costs;
+	Path	   *cheapest_partial_path = NULL;
+	Path	   *cheapest_total_path = NULL;
+	double		dNumPartialGroups = 0;
+	double		dNumPartialPartialGroups = 0;
+	ListCell   *lc;
+	bool		can_hash = (extra->flags & GROUPING_CAN_USE_HASH) != 0;
+	bool		can_sort = (extra->flags & GROUPING_CAN_USE_SORT) != 0;
+
+	/*
+	 * Consider whether we should generate partially aggregated non-partial
+	 * paths.  We can only do this if we have a non-partial path, and only if
+	 * the parent of the input rel is performing partial partitionwise
+	 * aggregation.  (Note that extra->patype is the type of partitionwise
+	 * aggregation being used at the parent level, not this level.)
+	 */
+	if (input_rel->pathlist != NIL &&
+		extra->patype == PARTITIONWISE_AGGREGATE_PARTIAL)
+		cheapest_total_path = input_rel->cheapest_total_path;
+
+	/*
+	 * If parallelism is possible for grouped_rel, then we should consider
+	 * generating partially-grouped partial paths.  However, if the input rel
+	 * has no partial paths, then we can't.
+	 */
+	if (grouped_rel->consider_parallel && input_rel->partial_pathlist != NIL)
+		cheapest_partial_path = linitial(input_rel->partial_pathlist);
+
+	/*
+	 * If we can't partially aggregate partial paths, and we can't partially
+	 * aggregate non-partial paths, then don't bother creating the new
+	 * RelOptInfo at all, unless the caller specified force_rel_creation.
+	 */
+	if (cheapest_total_path == NULL &&
+		cheapest_partial_path == NULL &&
+		!force_rel_creation)
+		return NULL;
+
+	/*
+	 * Build a new upper relation to represent the result of partially
+	 * aggregating the rows from the input relation.
+	 */
+	partially_grouped_rel = fetch_upper_rel(root,
+											UPPERREL_PARTIAL_GROUP_AGG,
+											grouped_rel->relids);
+	partially_grouped_rel->consider_parallel =
+		grouped_rel->consider_parallel;
+	partially_grouped_rel->reloptkind = grouped_rel->reloptkind;
+	partially_grouped_rel->serverid = grouped_rel->serverid;
+	partially_grouped_rel->userid = grouped_rel->userid;
+	partially_grouped_rel->useridiscurrent = grouped_rel->useridiscurrent;
+	partially_grouped_rel->fdwroutine = grouped_rel->fdwroutine;
+
+	/*
+	 * Build target list for partial aggregate paths.  These paths cannot just
+	 * emit the same tlist as regular aggregate paths, because (1) we must
+	 * include Vars and Aggrefs needed in HAVING, which might not appear in
+	 * the result tlist, and (2) the Aggrefs must be set in partial mode.
+	 */
+	partially_grouped_rel->reltarget =
+		make_partial_grouping_target(root, grouped_rel->reltarget,
+									 extra->havingQual);
+
+	if (!extra->partial_costs_set)
+	{
+		/*
+		 * Collect statistics about aggregates for estimating costs of
+		 * performing aggregation in parallel.
+		 */
+		MemSet(agg_partial_costs, 0, sizeof(AggClauseCosts));
+		MemSet(agg_final_costs, 0, sizeof(AggClauseCosts));
+		if (parse->hasAggs)
+		{
+			List	   *partial_target_exprs;
+
+			/* partial phase */
+			partial_target_exprs = partially_grouped_rel->reltarget->exprs;
+			get_agg_clause_costs(root, (Node *) partial_target_exprs,
+								 AGGSPLIT_INITIAL_SERIAL,
+								 agg_partial_costs);
+
+			/* final phase */
+			get_agg_clause_costs(root, (Node *) grouped_rel->reltarget->exprs,
+								 AGGSPLIT_FINAL_DESERIAL,
+								 agg_final_costs);
+			get_agg_clause_costs(root, extra->havingQual,
+								 AGGSPLIT_FINAL_DESERIAL,
+								 agg_final_costs);
+		}
+
+		extra->partial_costs_set = true;
+	}
+
+	/* Estimate number of partial groups. */
+	if (cheapest_total_path != NULL)
+		dNumPartialGroups =
+			get_number_of_groups(root,
+								 cheapest_total_path->rows,
+								 gd,
+								 extra->targetList);
+	if (cheapest_partial_path != NULL)
+		dNumPartialPartialGroups =
+			get_number_of_groups(root,
+								 cheapest_partial_path->rows,
+								 gd,
+								 extra->targetList);
+
+	if (can_sort && cheapest_total_path != NULL)
+	{
+		/* This should have been checked previously */
+		Assert(parse->hasAggs || parse->groupClause);
+
+		/*
+		 * Use any available suitably-sorted path as input, and also consider
+		 * sorting the cheapest partial path.
+		 */
+		foreach(lc, input_rel->pathlist)
+		{
+			Path	   *path = (Path *) lfirst(lc);
+			bool		is_sorted;
+
+			is_sorted = pathkeys_contained_in(root->group_pathkeys,
+											  path->pathkeys);
+			if (path == cheapest_total_path || is_sorted)
+			{
+				/* Sort the cheapest partial path, if it isn't already */
+				if (!is_sorted)
+					path = (Path *) create_sort_path(root,
+													 partially_grouped_rel,
+													 path,
+													 root->group_pathkeys,
+													 -1.0);
+
+				if (parse->hasAggs)
+					add_path(partially_grouped_rel, (Path *)
+							 create_agg_path(root,
+											 partially_grouped_rel,
+											 path,
+											 partially_grouped_rel->reltarget,
+											 parse->groupClause ? AGG_SORTED : AGG_PLAIN,
+											 AGGSPLIT_INITIAL_SERIAL,
+											 parse->groupClause,
+											 NIL,
+											 agg_partial_costs,
+											 dNumPartialGroups));
+				else
+					add_path(partially_grouped_rel, (Path *)
+							 create_group_path(root,
+											   partially_grouped_rel,
+											   path,
+											   parse->groupClause,
+											   NIL,
+											   dNumPartialGroups));
+			}
+		}
+	}
+
+	if (can_sort && cheapest_partial_path != NULL)
+	{
+		/* Similar to above logic, but for partial paths. */
+		foreach(lc, input_rel->partial_pathlist)
+		{
+			Path	   *path = (Path *) lfirst(lc);
+			bool		is_sorted;
+
+			is_sorted = pathkeys_contained_in(root->group_pathkeys,
+											  path->pathkeys);
+			if (path == cheapest_partial_path || is_sorted)
+			{
+				/* Sort the cheapest partial path, if it isn't already */
+				if (!is_sorted)
+					path = (Path *) create_sort_path(root,
+													 partially_grouped_rel,
+													 path,
+													 root->group_pathkeys,
+													 -1.0);
+
+				if (parse->hasAggs)
+					add_partial_path(partially_grouped_rel, (Path *)
+									 create_agg_path(root,
+													 partially_grouped_rel,
+													 path,
+													 partially_grouped_rel->reltarget,
+													 parse->groupClause ? AGG_SORTED : AGG_PLAIN,
+													 AGGSPLIT_INITIAL_SERIAL,
+													 parse->groupClause,
+													 NIL,
+													 agg_partial_costs,
+													 dNumPartialPartialGroups));
+				else
+					add_partial_path(partially_grouped_rel, (Path *)
+									 create_group_path(root,
+													   partially_grouped_rel,
+													   path,
+													   parse->groupClause,
+													   NIL,
+													   dNumPartialPartialGroups));
+			}
+		}
+	}
+
+	if (can_hash && cheapest_total_path != NULL)
+	{
+		double		hashaggtablesize;
+
+		/* Checked above */
+		Assert(parse->hasAggs || parse->groupClause);
+
+		hashaggtablesize =
+			estimate_hashagg_tablesize(cheapest_total_path,
+									   agg_partial_costs,
+									   dNumPartialGroups);
+
+		/*
+		 * Tentatively produce a partial HashAgg Path, depending on if it
+		 * looks as if the hash table will fit in work_mem.
+		 */
+		if (hashaggtablesize < work_mem * 1024L &&
+			cheapest_total_path != NULL)
+		{
+			add_path(partially_grouped_rel, (Path *)
+					 create_agg_path(root,
+									 partially_grouped_rel,
+									 cheapest_total_path,
+									 partially_grouped_rel->reltarget,
+									 AGG_HASHED,
+									 AGGSPLIT_INITIAL_SERIAL,
+									 parse->groupClause,
+									 NIL,
+									 agg_partial_costs,
+									 dNumPartialGroups));
+		}
+	}
+
+	if (can_hash && cheapest_partial_path != NULL)
+	{
+		double		hashaggtablesize;
+
+		hashaggtablesize =
+			estimate_hashagg_tablesize(cheapest_partial_path,
+									   agg_partial_costs,
+									   dNumPartialPartialGroups);
+
+		/* Do the same for partial paths. */
+		if (hashaggtablesize < work_mem * 1024L &&
+			cheapest_partial_path != NULL)
+		{
+			add_partial_path(partially_grouped_rel, (Path *)
+							 create_agg_path(root,
+											 partially_grouped_rel,
+											 cheapest_partial_path,
+											 partially_grouped_rel->reltarget,
+											 AGG_HASHED,
+											 AGGSPLIT_INITIAL_SERIAL,
+											 parse->groupClause,
+											 NIL,
+											 agg_partial_costs,
+											 dNumPartialPartialGroups));
+		}
+	}
+
+	/*
+	 * If there is an FDW that's responsible for all baserels of the query,
+	 * let it consider adding partially grouped ForeignPaths.
+	 */
+	if (partially_grouped_rel->fdwroutine &&
+		partially_grouped_rel->fdwroutine->GetForeignUpperPaths)
+	{
+		FdwRoutine *fdwroutine = partially_grouped_rel->fdwroutine;
+
+		fdwroutine->GetForeignUpperPaths(root,
+										 UPPERREL_PARTIAL_GROUP_AGG,
+										 input_rel, partially_grouped_rel,
+										 extra);
+	}
+
+	return partially_grouped_rel;
+}
+
+/*
+ * Generate Gather and Gather Merge paths for a grouping relation or partial
+ * grouping relation.
+ *
+ * generate_gather_paths does most of the work, but we also consider a special
+ * case: we could try sorting the data by the group_pathkeys and then applying
+ * Gather Merge.
+ *
+ * NB: This function shouldn't be used for anything other than a grouped or
+ * partially grouped relation not only because of the fact that it explicitly
+ * references group_pathkeys but we pass "true" as the third argument to
+ * generate_gather_paths().
+ */
+static void
+gather_grouping_paths(PlannerInfo *root, RelOptInfo *rel)
+{
+	Path	   *cheapest_partial_path;
+
+	/* Try Gather for unordered paths and Gather Merge for ordered ones. */
+	generate_gather_paths(root, rel, true);
+
+	/* Try cheapest partial path + explicit Sort + Gather Merge. */
+	cheapest_partial_path = linitial(rel->partial_pathlist);
+	if (!pathkeys_contained_in(root->group_pathkeys,
+							   cheapest_partial_path->pathkeys))
+	{
+		Path	   *path;
+		double		total_groups;
+
+		total_groups =
+			cheapest_partial_path->rows * cheapest_partial_path->parallel_workers;
+		path = (Path *) create_sort_path(root, rel, cheapest_partial_path,
+										 root->group_pathkeys,
+										 -1.0);
+		path = (Path *)
+			create_gather_merge_path(root,
+									 rel,
+									 path,
+									 rel->reltarget,
+									 root->group_pathkeys,
+									 NULL,
+									 &total_groups);
+
+		add_path(rel, path);
+	}
+}
+
+/*
+ * can_partial_agg
+ *
+ * Determines whether or not partial grouping and/or aggregation is possible.
+ * Returns true when possible, false otherwise.
+ */
+static bool
+can_partial_agg(PlannerInfo *root, const AggClauseCosts *agg_costs)
+{
+	Query	   *parse = root->parse;
+
+	if (!parse->hasAggs && parse->groupClause == NIL)
+	{
+		/*
+		 * We don't know how to do parallel aggregation unless we have either
+		 * some aggregates or a grouping clause.
+		 */
+		return false;
+	}
+	else if (parse->groupingSets)
+	{
+		/* We don't know how to do grouping sets in parallel. */
+		return false;
+	}
+	else if (agg_costs->hasNonPartial || agg_costs->hasNonSerial)
+	{
+		/* Insufficient support for partial mode. */
+		return false;
+	}
+
+	/* Everything looks good. */
+	return true;
+}
+
+/*
+ * apply_scanjoin_target_to_paths
+ *
+ * Adjust the final scan/join relation, and recursively all of its children,
+ * to generate the final scan/join target.  It would be more correct to model
+ * this as a separate planning step with a new RelOptInfo at the toplevel and
+ * for each child relation, but doing it this way is noticeably cheaper.
+ * Maybe that problem can be solved at some point, but for now we do this.
+ *
+ * If tlist_same_exprs is true, then the scan/join target to be applied has
+ * the same expressions as the existing reltarget, so we need only insert the
+ * appropriate sortgroupref information.  By avoiding the creation of
+ * projection paths we save effort both immediately and at plan creation time.
+ */
+static void
+apply_scanjoin_target_to_paths(PlannerInfo *root,
+							   RelOptInfo *rel,
+							   List *scanjoin_targets,
+							   List *scanjoin_targets_contain_srfs,
+							   bool scanjoin_target_parallel_safe,
+							   bool tlist_same_exprs)
+{
+	bool		rel_is_partitioned = IS_PARTITIONED_REL(rel);
+	PathTarget *scanjoin_target;
+	ListCell   *lc;
+
+	/* This recurses, so be paranoid. */
+	check_stack_depth();
+
+	/*
+	 * If the rel is partitioned, we want to drop its existing paths and
+	 * generate new ones.  This function would still be correct if we kept the
+	 * existing paths: we'd modify them to generate the correct target above
+	 * the partitioning Append, and then they'd compete on cost with paths
+	 * generating the target below the Append.  However, in our current cost
+	 * model the latter way is always the same or cheaper cost, so modifying
+	 * the existing paths would just be useless work.  Moreover, when the cost
+	 * is the same, varying roundoff errors might sometimes allow an existing
+	 * path to be picked, resulting in undesirable cross-platform plan
+	 * variations.  So we drop old paths and thereby force the work to be done
+	 * below the Append, except in the case of a non-parallel-safe target.
+	 *
+	 * Some care is needed, because we have to allow generate_gather_paths to
+	 * see the old partial paths in the next stanza.  Hence, zap the main
+	 * pathlist here, then allow generate_gather_paths to add path(s) to the
+	 * main list, and finally zap the partial pathlist.
+	 */
+	if (rel_is_partitioned)
+		rel->pathlist = NIL;
+
+	/*
+	 * If the scan/join target is not parallel-safe, partial paths cannot
+	 * generate it.
+	 */
+	if (!scanjoin_target_parallel_safe)
+	{
+		/*
+		 * Since we can't generate the final scan/join target in parallel
+		 * workers, this is our last opportunity to use any partial paths that
+		 * exist; so build Gather path(s) that use them and emit whatever the
+		 * current reltarget is.  We don't do this in the case where the
+		 * target is parallel-safe, since we will be able to generate superior
+		 * paths by doing it after the final scan/join target has been
+		 * applied.
+		 */
+		generate_gather_paths(root, rel, false);
+
+		/* Can't use parallel query above this level. */
+		rel->partial_pathlist = NIL;
+		rel->consider_parallel = false;
+	}
+
+	/* Finish dropping old paths for a partitioned rel, per comment above */
+	if (rel_is_partitioned)
+		rel->partial_pathlist = NIL;
+
+	/* Extract SRF-free scan/join target. */
+	scanjoin_target = linitial_node(PathTarget, scanjoin_targets);
+
+	/*
+	 * Apply the SRF-free scan/join target to each existing path.
+	 *
+	 * If the tlist exprs are the same, we can just inject the sortgroupref
+	 * information into the existing pathtargets.  Otherwise, replace each
+	 * path with a projection path that generates the SRF-free scan/join
+	 * target.  This can't change the ordering of paths within rel->pathlist,
+	 * so we just modify the list in place.
+	 */
+	foreach(lc, rel->pathlist)
+	{
+		Path	   *subpath = (Path *) lfirst(lc);
+
+		/* Shouldn't have any parameterized paths anymore */
+		Assert(subpath->param_info == NULL);
+
+		if (tlist_same_exprs)
+			subpath->pathtarget->sortgrouprefs =
+				scanjoin_target->sortgrouprefs;
+		else
+		{
+			Path	   *newpath;
+
+			newpath = (Path *) create_projection_path(root, rel, subpath,
+													  scanjoin_target);
+			lfirst(lc) = newpath;
+		}
+	}
+
+	/* Likewise adjust the targets for any partial paths. */
+	foreach(lc, rel->partial_pathlist)
+	{
+		Path	   *subpath = (Path *) lfirst(lc);
+
+		/* Shouldn't have any parameterized paths anymore */
+		Assert(subpath->param_info == NULL);
+
+		if (tlist_same_exprs)
+			subpath->pathtarget->sortgrouprefs =
+				scanjoin_target->sortgrouprefs;
+		else
+		{
+			Path	   *newpath;
+
+			newpath = (Path *) create_projection_path(root, rel, subpath,
+													  scanjoin_target);
+			lfirst(lc) = newpath;
+		}
+	}
+
+	/*
+	 * Now, if final scan/join target contains SRFs, insert ProjectSetPath(s)
+	 * atop each existing path.  (Note that this function doesn't look at the
+	 * cheapest-path fields, which is a good thing because they're bogus right
+	 * now.)
+	 */
+	if (root->parse->hasTargetSRFs)
+		adjust_paths_for_srfs(root, rel,
+							  scanjoin_targets,
+							  scanjoin_targets_contain_srfs);
+
+	/*
+	 * Update the rel's target to be the final (with SRFs) scan/join target.
+	 * This now matches the actual output of all the paths, and we might get
+	 * confused in createplan.c if they don't agree.  We must do this now so
+	 * that any append paths made in the next part will use the correct
+	 * pathtarget (cf. create_append_path).
+	 *
+	 * Note that this is also necessary if GetForeignUpperPaths() gets called
+	 * on the final scan/join relation or on any of its children, since the
+	 * FDW might look at the rel's target to create ForeignPaths.
+	 */
+	rel->reltarget = llast_node(PathTarget, scanjoin_targets);
+
+	/*
+	 * If the relation is partitioned, recursively apply the scan/join target
+	 * to all partitions, and generate brand-new Append paths in which the
+	 * scan/join target is computed below the Append rather than above it.
+	 * Since Append is not projection-capable, that might save a separate
+	 * Result node, and it also is important for partitionwise aggregate.
+	 */
+	if (rel_is_partitioned)
+	{
+		List	   *live_children = NIL;
+		int			partition_idx;
+
+		/* Adjust each partition. */
+		for (partition_idx = 0; partition_idx < rel->nparts; partition_idx++)
+		{
+			RelOptInfo *child_rel = rel->part_rels[partition_idx];
+			AppendRelInfo **appinfos;
+			int			nappinfos;
+			List	   *child_scanjoin_targets = NIL;
+			ListCell   *lc;
+
+			/* Pruned or dummy children can be ignored. */
+			if (child_rel == NULL || IS_DUMMY_REL(child_rel))
+				continue;
+
+			/* Translate scan/join targets for this child. */
+			appinfos = find_appinfos_by_relids(root, child_rel->relids,
+											   &nappinfos);
+			foreach(lc, scanjoin_targets)
+			{
+				PathTarget *target = lfirst_node(PathTarget, lc);
+
+				target = copy_pathtarget(target);
+				target->exprs = (List *)
+					adjust_appendrel_attrs(root,
+										   (Node *) target->exprs,
+										   nappinfos, appinfos);
+				child_scanjoin_targets = lappend(child_scanjoin_targets,
+												 target);
+			}
+			pfree(appinfos);
+
+			/* Recursion does the real work. */
+			apply_scanjoin_target_to_paths(root, child_rel,
+										   child_scanjoin_targets,
+										   scanjoin_targets_contain_srfs,
+										   scanjoin_target_parallel_safe,
+										   tlist_same_exprs);
+
+			/* Save non-dummy children for Append paths. */
+			if (!IS_DUMMY_REL(child_rel))
+				live_children = lappend(live_children, child_rel);
+		}
+
+		/* Build new paths for this relation by appending child paths. */
+		add_paths_to_append_rel(root, rel, live_children);
+	}
+
+	/*
+	 * Consider generating Gather or Gather Merge paths.  We must only do this
+	 * if the relation is parallel safe, and we don't do it for child rels to
+	 * avoid creating multiple Gather nodes within the same plan. We must do
+	 * this after all paths have been generated and before set_cheapest, since
+	 * one of the generated paths may turn out to be the cheapest one.
+	 */
+	if (rel->consider_parallel && !IS_OTHER_REL(rel))
+		generate_gather_paths(root, rel, false);
+
+	/*
+	 * Reassess which paths are the cheapest, now that we've potentially added
+	 * new Gather (or Gather Merge) and/or Append (or MergeAppend) paths to
+	 * this relation.
+	 */
+	set_cheapest(rel);
+}
+
+/*
+ * create_partitionwise_grouping_paths
+ *
+ * If the partition keys of input relation are part of the GROUP BY clause, all
+ * the rows belonging to a given group come from a single partition.  This
+ * allows aggregation/grouping over a partitioned relation to be broken down
+ * into aggregation/grouping on each partition.  This should be no worse, and
+ * often better, than the normal approach.
+ *
+ * However, if the GROUP BY clause does not contain all the partition keys,
+ * rows from a given group may be spread across multiple partitions. In that
+ * case, we perform partial aggregation for each group, append the results,
+ * and then finalize aggregation.  This is less certain to win than the
+ * previous case.  It may win if the PartialAggregate stage greatly reduces
+ * the number of groups, because fewer rows will pass through the Append node.
+ * It may lose if we have lots of small groups.
+ */
+static void
+create_partitionwise_grouping_paths(PlannerInfo *root,
+									RelOptInfo *input_rel,
+									RelOptInfo *grouped_rel,
+									RelOptInfo *partially_grouped_rel,
+									const AggClauseCosts *agg_costs,
+									grouping_sets_data *gd,
+									PartitionwiseAggregateType patype,
+									GroupPathExtraData *extra)
+{
+	int			nparts = input_rel->nparts;
+	int			cnt_parts;
+	List	   *grouped_live_children = NIL;
+	List	   *partially_grouped_live_children = NIL;
+	PathTarget *target = grouped_rel->reltarget;
+	bool		partial_grouping_valid = true;
+
+	Assert(patype != PARTITIONWISE_AGGREGATE_NONE);
+	Assert(patype != PARTITIONWISE_AGGREGATE_PARTIAL ||
+		   partially_grouped_rel != NULL);
+
+	/* Add paths for partitionwise aggregation/grouping. */
+	for (cnt_parts = 0; cnt_parts < nparts; cnt_parts++)
+	{
+		RelOptInfo *child_input_rel = input_rel->part_rels[cnt_parts];
+		PathTarget *child_target = copy_pathtarget(target);
+		AppendRelInfo **appinfos;
+		int			nappinfos;
+		GroupPathExtraData child_extra;
+		RelOptInfo *child_grouped_rel;
+		RelOptInfo *child_partially_grouped_rel;
+
+		/* Pruned or dummy children can be ignored. */
+		if (child_input_rel == NULL || IS_DUMMY_REL(child_input_rel))
+			continue;
+
+		/*
+		 * Copy the given "extra" structure as is and then override the
+		 * members specific to this child.
+		 */
+		memcpy(&child_extra, extra, sizeof(child_extra));
+
+		appinfos = find_appinfos_by_relids(root, child_input_rel->relids,
+										   &nappinfos);
+
+		child_target->exprs = (List *)
+			adjust_appendrel_attrs(root,
+								   (Node *) target->exprs,
+								   nappinfos, appinfos);
+
+		/* Translate havingQual and targetList. */
+		child_extra.havingQual = (Node *)
+			adjust_appendrel_attrs(root,
+								   extra->havingQual,
+								   nappinfos, appinfos);
+		child_extra.targetList = (List *)
+			adjust_appendrel_attrs(root,
+								   (Node *) extra->targetList,
+								   nappinfos, appinfos);
+
+		/*
+		 * extra->patype was the value computed for our parent rel; patype is
+		 * the value for this relation.  For the child, our value is its
+		 * parent rel's value.
+		 */
+		child_extra.patype = patype;
+
+		/*
+		 * Create grouping relation to hold fully aggregated grouping and/or
+		 * aggregation paths for the child.
+		 */
+		child_grouped_rel = make_grouping_rel(root, child_input_rel,
+											  child_target,
+											  extra->target_parallel_safe,
+											  child_extra.havingQual);
+
+		/* Create grouping paths for this child relation. */
+		create_ordinary_grouping_paths(root, child_input_rel,
+									   child_grouped_rel,
+									   agg_costs, gd, &child_extra,
+									   &child_partially_grouped_rel);
+
+		if (child_partially_grouped_rel)
+		{
+			partially_grouped_live_children =
+				lappend(partially_grouped_live_children,
+						child_partially_grouped_rel);
+		}
+		else
+			partial_grouping_valid = false;
+
+		if (patype == PARTITIONWISE_AGGREGATE_FULL)
+		{
+			set_cheapest(child_grouped_rel);
+			grouped_live_children = lappend(grouped_live_children,
+											child_grouped_rel);
+		}
+
+		pfree(appinfos);
+	}
+
+	/*
+	 * Try to create append paths for partially grouped children. For full
+	 * partitionwise aggregation, we might have paths in the partial_pathlist
+	 * if parallel aggregation is possible.  For partial partitionwise
+	 * aggregation, we may have paths in both pathlist and partial_pathlist.
+	 *
+	 * NB: We must have a partially grouped path for every child in order to
+	 * generate a partially grouped path for this relation.
+	 */
+	if (partially_grouped_rel && partial_grouping_valid)
+	{
+		Assert(partially_grouped_live_children != NIL);
+
+		add_paths_to_append_rel(root, partially_grouped_rel,
+								partially_grouped_live_children);
+
+		/*
+		 * We need call set_cheapest, since the finalization step will use the
+		 * cheapest path from the rel.
+		 */
+		if (partially_grouped_rel->pathlist)
+			set_cheapest(partially_grouped_rel);
+	}
+
+	/* If possible, create append paths for fully grouped children. */
+	if (patype == PARTITIONWISE_AGGREGATE_FULL)
+	{
+		Assert(grouped_live_children != NIL);
+
+		add_paths_to_append_rel(root, grouped_rel, grouped_live_children);
+	}
+}
+
+/*
+ * group_by_has_partkey
+ *
+ * Returns true, if all the partition keys of the given relation are part of
+ * the GROUP BY clauses, false otherwise.
+ */
+static bool
+group_by_has_partkey(RelOptInfo *input_rel,
+					 List *targetList,
+					 List *groupClause)
+{
+	List	   *groupexprs = get_sortgrouplist_exprs(groupClause, targetList);
+	int			cnt = 0;
+	int			partnatts;
+
+	/* Input relation should be partitioned. */
+	Assert(input_rel->part_scheme);
+
+	/* Rule out early, if there are no partition keys present. */
+	if (!input_rel->partexprs)
+		return false;
+
+	partnatts = input_rel->part_scheme->partnatts;
+
+	for (cnt = 0; cnt < partnatts; cnt++)
+	{
+		List	   *partexprs = input_rel->partexprs[cnt];
+		ListCell   *lc;
+		bool		found = false;
+
+		foreach(lc, partexprs)
+		{
+			Expr	   *partexpr = lfirst(lc);
+
+			if (list_member(groupexprs, partexpr))
+			{
+				found = true;
+				break;
+			}
+		}
+
+		/*
+		 * If none of the partition key expressions match with any of the
+		 * GROUP BY expression, return false.
+		 */
+		if (!found)
+			return false;
+	}
+
+	return true;
+}
diff --git a/src/backend/optimizer/plan/planner.c.rej b/src/backend/optimizer/plan/planner.c.rej
new file mode 100644
index 0000000..a50e03b
--- /dev/null
+++ b/src/backend/optimizer/plan/planner.c.rej
@@ -0,0 +1,18 @@
+--- src/backend/optimizer/plan/planner.c
++++ src/backend/optimizer/plan/planner.c
+@@ -272,8 +273,13 @@ planner(Query *parse, const char *query_string, int cursorOptions,
+ 
+ 	if (planner_hook)
+ 		result = (*planner_hook) (parse, query_string, cursorOptions, boundParams);
+-	else
+-		result = standard_planner(parse, query_string, cursorOptions, boundParams);
++	else {
++		if (enable_lero) {
++			result = lero_pgsysml_hook_planner(parse, query_string, cursorOptions, boundParams);
++		} else{
++			result = standard_planner(parse, query_string, cursorOptions, boundParams);
++		}
++	}
+ 	return result;
+ }
+ 
diff --git a/src/backend/tcop/postgres.c b/src/backend/tcop/postgres.c
index ea41039..414871d 100644
--- a/src/backend/tcop/postgres.c
+++ b/src/backend/tcop/postgres.c
@@ -860,7 +860,7 @@ pg_rewrite_query(Query *query)
  * This is a thin wrapper around planner() and takes the same parameters.
  */
 PlannedStmt *
-pg_plan_query(Query *querytree, int cursorOptions, ParamListInfo boundParams)
+pg_plan_query(Query *querytree, const char *query_string, int cursorOptions, ParamListInfo boundParams)
 {
 	PlannedStmt *plan;
 
@@ -877,7 +877,7 @@ pg_plan_query(Query *querytree, int cursorOptions, ParamListInfo boundParams)
 		ResetUsage();
 
 	/* call the optimizer */
-	plan = planner(querytree, cursorOptions, boundParams);
+	plan = planner(querytree, query_string, cursorOptions, boundParams);
 
 	if (log_planner_stats)
 		ShowUsage("PLANNER STATISTICS");
@@ -945,7 +945,7 @@ pg_plan_query(Query *querytree, int cursorOptions, ParamListInfo boundParams)
  * The result is a list of PlannedStmt nodes.
  */
 List *
-pg_plan_queries(List *querytrees, int cursorOptions, ParamListInfo boundParams)
+pg_plan_queries(List *querytrees, const char *query_string, int cursorOptions, ParamListInfo boundParams)
 {
 	List	   *stmt_list = NIL;
 	ListCell   *query_list;
@@ -967,7 +967,7 @@ pg_plan_queries(List *querytrees, int cursorOptions, ParamListInfo boundParams)
 		}
 		else
 		{
-			stmt = pg_plan_query(query, cursorOptions, boundParams);
+			stmt = pg_plan_query(query, query_string, cursorOptions, boundParams);
 		}
 
 		stmt_list = lappend(stmt_list, stmt);
@@ -1142,7 +1142,7 @@ exec_simple_query(const char *query_string)
 		querytree_list = pg_analyze_and_rewrite(parsetree, query_string,
 												NULL, 0, NULL);
 
-		plantree_list = pg_plan_queries(querytree_list,
+		plantree_list = pg_plan_queries(querytree_list, query_string,
 										CURSOR_OPT_PARALLEL_OK, NULL);
 
 		/* Done with the snapshot used for parsing/planning */
diff --git a/src/backend/utils/cache/plancache.c b/src/backend/utils/cache/plancache.c
index 51c1131..2004361 100644
--- a/src/backend/utils/cache/plancache.c
+++ b/src/backend/utils/cache/plancache.c
@@ -934,7 +934,7 @@ BuildCachedPlan(CachedPlanSource *plansource, List *qlist,
 	/*
 	 * Generate the plan.
 	 */
-	plist = pg_plan_queries(qlist, plansource->cursor_options, boundParams);
+	plist = pg_plan_queries(qlist, plansource->query_string, plansource->cursor_options, boundParams);
 
 	/* Release snapshot if we got one */
 	if (snapshot_set)
diff --git a/src/backend/utils/misc/guc.c b/src/backend/utils/misc/guc.c
index b0c29a1..39250c4 100644
--- a/src/backend/utils/misc/guc.c
+++ b/src/backend/utils/misc/guc.c
@@ -96,6 +96,7 @@
 #include "utils/tzparser.h"
 #include "utils/varlena.h"
 #include "utils/xml.h"
+#include "lero/lero_extension.h"
 
 #ifndef PG_KRB_SRVTAB
 #define PG_KRB_SRVTAB ""
@@ -1965,6 +1966,16 @@ static struct config_bool ConfigureNamesBool[] =
 		NULL, NULL, NULL
 	},
 
+	{
+		{"enable_lero", PGC_USERSET, UNGROUPED,
+			gettext_noop("Enable Lero."),
+			NULL
+		},
+		&enable_lero,
+		false,
+		NULL, NULL, NULL
+    },
+
 	/* End-of-list marker */
 	{
 		{NULL, 0, 0, NULL, NULL}, NULL, false, NULL, NULL, NULL
@@ -3207,6 +3218,16 @@ static struct config_int ConfigureNamesInt[] =
 		NULL, assign_tcp_user_timeout, show_tcp_user_timeout
 	},
 
+	{
+		{"lero_server_port", PGC_USERSET, UNGROUPED,
+			gettext_noop("Sets the port of Lero server."),
+			NULL
+		},
+		&lero_server_port,
+		14567, 0, 65535,
+		NULL, NULL, NULL
+    },
+
 	/* End-of-list marker */
 	{
 		{NULL, 0, 0, NULL, NULL}, NULL, 0, 0, 0, NULL, NULL, NULL
@@ -4223,6 +4244,28 @@ static struct config_string ConfigureNamesString[] =
 		check_restrict_nonsystem_relation_kind, assign_restrict_nonsystem_relation_kind, NULL
 	},
 
+	{
+		{"lero_joinest_fname", PGC_USERSET, UNGROUPED,
+				gettext_noop("Sets the file name of ML-based join size estimation."),
+				NULL,
+				GUC_IS_NAME
+		},
+		&lero_joinest_fname,
+		"",
+		check_cluster_name, NULL, NULL
+    },
+
+	{
+		{"lero_server_host", PGC_USERSET, UNGROUPED,
+				gettext_noop("Sets the host of Lero server."),
+				NULL,
+				GUC_IS_NAME
+		},
+		&lero_server_host,
+		"localhost",
+		check_cluster_name, NULL, NULL
+    },	
+
 	/* End-of-list marker */
 	{
 		{NULL, 0, 0, NULL, NULL}, NULL, NULL, NULL, NULL, NULL
diff --git a/src/backend/utils/misc/guc.c.orig b/src/backend/utils/misc/guc.c.orig
new file mode 100644
index 0000000..b0c29a1
--- /dev/null
+++ b/src/backend/utils/misc/guc.c.orig
@@ -0,0 +1,11822 @@
+/*--------------------------------------------------------------------
+ * guc.c
+ *
+ * Support for grand unified configuration scheme, including SET
+ * command, configuration file, and command line options.
+ * See src/backend/utils/misc/README for more information.
+ *
+ *
+ * Copyright (c) 2000-2019, PostgreSQL Global Development Group
+ * Written by Peter Eisentraut <peter_e@gmx.net>.
+ *
+ * IDENTIFICATION
+ *	  src/backend/utils/misc/guc.c
+ *
+ *--------------------------------------------------------------------
+ */
+#include "postgres.h"
+
+#include <ctype.h>
+#include <float.h>
+#include <math.h>
+#include <limits.h>
+#include <unistd.h>
+#include <sys/stat.h>
+#ifdef HAVE_SYSLOG
+#include <syslog.h>
+#endif
+
+#include "access/commit_ts.h"
+#include "access/gin.h"
+#include "access/rmgr.h"
+#include "access/tableam.h"
+#include "access/transam.h"
+#include "access/twophase.h"
+#include "access/xact.h"
+#include "access/xlog_internal.h"
+#include "catalog/namespace.h"
+#include "catalog/pg_authid.h"
+#include "commands/async.h"
+#include "commands/prepare.h"
+#include "commands/tablespace.h"
+#include "commands/user.h"
+#include "commands/vacuum.h"
+#include "commands/variable.h"
+#include "commands/trigger.h"
+#include "common/string.h"
+#include "funcapi.h"
+#include "jit/jit.h"
+#include "libpq/auth.h"
+#include "libpq/libpq.h"
+#include "libpq/pqformat.h"
+#include "miscadmin.h"
+#include "optimizer/cost.h"
+#include "optimizer/geqo.h"
+#include "optimizer/optimizer.h"
+#include "optimizer/paths.h"
+#include "optimizer/planmain.h"
+#include "parser/parse_expr.h"
+#include "parser/parse_type.h"
+#include "parser/parser.h"
+#include "parser/scansup.h"
+#include "pgstat.h"
+#include "postmaster/autovacuum.h"
+#include "postmaster/bgworker_internals.h"
+#include "postmaster/bgwriter.h"
+#include "postmaster/postmaster.h"
+#include "postmaster/syslogger.h"
+#include "postmaster/walwriter.h"
+#include "replication/logicallauncher.h"
+#include "replication/slot.h"
+#include "replication/syncrep.h"
+#include "replication/walreceiver.h"
+#include "replication/walsender.h"
+#include "storage/bufmgr.h"
+#include "storage/dsm_impl.h"
+#include "storage/standby.h"
+#include "storage/fd.h"
+#include "storage/large_object.h"
+#include "storage/pg_shmem.h"
+#include "storage/proc.h"
+#include "storage/predicate.h"
+#include "tcop/tcopprot.h"
+#include "tsearch/ts_cache.h"
+#include "utils/builtins.h"
+#include "utils/bytea.h"
+#include "utils/guc_tables.h"
+#include "utils/float.h"
+#include "utils/memutils.h"
+#include "utils/pg_locale.h"
+#include "utils/pg_lsn.h"
+#include "utils/plancache.h"
+#include "utils/portal.h"
+#include "utils/ps_status.h"
+#include "utils/rls.h"
+#include "utils/snapmgr.h"
+#include "utils/tzparser.h"
+#include "utils/varlena.h"
+#include "utils/xml.h"
+
+#ifndef PG_KRB_SRVTAB
+#define PG_KRB_SRVTAB ""
+#endif
+
+#define CONFIG_FILENAME "postgresql.conf"
+#define HBA_FILENAME	"pg_hba.conf"
+#define IDENT_FILENAME	"pg_ident.conf"
+
+#ifdef EXEC_BACKEND
+#define CONFIG_EXEC_PARAMS "global/config_exec_params"
+#define CONFIG_EXEC_PARAMS_NEW "global/config_exec_params.new"
+#endif
+
+/*
+ * Precision with which REAL type guc values are to be printed for GUC
+ * serialization.
+ */
+#define REALTYPE_PRECISION 17
+
+/* XXX these should appear in other modules' header files */
+extern bool Log_disconnections;
+extern int	CommitDelay;
+extern int	CommitSiblings;
+extern char *default_tablespace;
+extern char *temp_tablespaces;
+extern bool ignore_checksum_failure;
+extern bool synchronize_seqscans;
+
+#ifdef TRACE_SYNCSCAN
+extern bool trace_syncscan;
+#endif
+#ifdef DEBUG_BOUNDED_SORT
+extern bool optimize_bounded_sort;
+#endif
+
+static int	GUC_check_errcode_value;
+
+/* global variables for check hook support */
+char	   *GUC_check_errmsg_string;
+char	   *GUC_check_errdetail_string;
+char	   *GUC_check_errhint_string;
+
+static void do_serialize(char **destptr, Size *maxbytes, const char *fmt,...) pg_attribute_printf(3, 4);
+
+static void set_config_sourcefile(const char *name, char *sourcefile,
+								  int sourceline);
+static bool call_bool_check_hook(struct config_bool *conf, bool *newval,
+								 void **extra, GucSource source, int elevel);
+static bool call_int_check_hook(struct config_int *conf, int *newval,
+								void **extra, GucSource source, int elevel);
+static bool call_real_check_hook(struct config_real *conf, double *newval,
+								 void **extra, GucSource source, int elevel);
+static bool call_string_check_hook(struct config_string *conf, char **newval,
+								   void **extra, GucSource source, int elevel);
+static bool call_enum_check_hook(struct config_enum *conf, int *newval,
+								 void **extra, GucSource source, int elevel);
+
+static bool check_log_destination(char **newval, void **extra, GucSource source);
+static void assign_log_destination(const char *newval, void *extra);
+
+static bool check_wal_consistency_checking(char **newval, void **extra,
+										   GucSource source);
+static void assign_wal_consistency_checking(const char *newval, void *extra);
+
+#ifdef HAVE_SYSLOG
+static int	syslog_facility = LOG_LOCAL0;
+#else
+static int	syslog_facility = 0;
+#endif
+
+static void assign_syslog_facility(int newval, void *extra);
+static void assign_syslog_ident(const char *newval, void *extra);
+static void assign_session_replication_role(int newval, void *extra);
+static bool check_temp_buffers(int *newval, void **extra, GucSource source);
+static bool check_bonjour(bool *newval, void **extra, GucSource source);
+static bool check_ssl(bool *newval, void **extra, GucSource source);
+static bool check_stage_log_stats(bool *newval, void **extra, GucSource source);
+static bool check_log_stats(bool *newval, void **extra, GucSource source);
+static bool check_canonical_path(char **newval, void **extra, GucSource source);
+static bool check_timezone_abbreviations(char **newval, void **extra, GucSource source);
+static void assign_timezone_abbreviations(const char *newval, void *extra);
+static void pg_timezone_abbrev_initialize(void);
+static const char *show_archive_command(void);
+static void assign_tcp_keepalives_idle(int newval, void *extra);
+static void assign_tcp_keepalives_interval(int newval, void *extra);
+static void assign_tcp_keepalives_count(int newval, void *extra);
+static void assign_tcp_user_timeout(int newval, void *extra);
+static const char *show_tcp_keepalives_idle(void);
+static const char *show_tcp_keepalives_interval(void);
+static const char *show_tcp_keepalives_count(void);
+static const char *show_tcp_user_timeout(void);
+static bool check_maxconnections(int *newval, void **extra, GucSource source);
+static bool check_max_worker_processes(int *newval, void **extra, GucSource source);
+static bool check_autovacuum_max_workers(int *newval, void **extra, GucSource source);
+static bool check_max_wal_senders(int *newval, void **extra, GucSource source);
+static bool check_autovacuum_work_mem(int *newval, void **extra, GucSource source);
+static bool check_effective_io_concurrency(int *newval, void **extra, GucSource source);
+static void assign_effective_io_concurrency(int newval, void *extra);
+static void assign_pgstat_temp_directory(const char *newval, void *extra);
+static bool check_application_name(char **newval, void **extra, GucSource source);
+static void assign_application_name(const char *newval, void *extra);
+static bool check_cluster_name(char **newval, void **extra, GucSource source);
+static const char *show_unix_socket_permissions(void);
+static const char *show_log_file_mode(void);
+static const char *show_data_directory_mode(void);
+static bool check_recovery_target_timeline(char **newval, void **extra, GucSource source);
+static void assign_recovery_target_timeline(const char *newval, void *extra);
+static bool check_recovery_target(char **newval, void **extra, GucSource source);
+static void assign_recovery_target(const char *newval, void *extra);
+static bool check_recovery_target_xid(char **newval, void **extra, GucSource source);
+static void assign_recovery_target_xid(const char *newval, void *extra);
+static bool check_recovery_target_time(char **newval, void **extra, GucSource source);
+static void assign_recovery_target_time(const char *newval, void *extra);
+static bool check_recovery_target_name(char **newval, void **extra, GucSource source);
+static void assign_recovery_target_name(const char *newval, void *extra);
+static bool check_recovery_target_lsn(char **newval, void **extra, GucSource source);
+static void assign_recovery_target_lsn(const char *newval, void *extra);
+static bool check_primary_slot_name(char **newval, void **extra, GucSource source);
+static bool check_default_with_oids(bool *newval, void **extra, GucSource source);
+
+/* Private functions in guc-file.l that need to be called from guc.c */
+static ConfigVariable *ProcessConfigFileInternal(GucContext context,
+												 bool applySettings, int elevel);
+
+
+/*
+ * Options for enum values defined in this module.
+ *
+ * NOTE! Option values may not contain double quotes!
+ */
+
+static const struct config_enum_entry bytea_output_options[] = {
+	{"escape", BYTEA_OUTPUT_ESCAPE, false},
+	{"hex", BYTEA_OUTPUT_HEX, false},
+	{NULL, 0, false}
+};
+
+/*
+ * We have different sets for client and server message level options because
+ * they sort slightly different (see "log" level), and because "fatal"/"panic"
+ * aren't sensible for client_min_messages.
+ */
+static const struct config_enum_entry client_message_level_options[] = {
+	{"debug5", DEBUG5, false},
+	{"debug4", DEBUG4, false},
+	{"debug3", DEBUG3, false},
+	{"debug2", DEBUG2, false},
+	{"debug1", DEBUG1, false},
+	{"debug", DEBUG2, true},
+	{"log", LOG, false},
+	{"info", INFO, true},
+	{"notice", NOTICE, false},
+	{"warning", WARNING, false},
+	{"error", ERROR, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry server_message_level_options[] = {
+	{"debug5", DEBUG5, false},
+	{"debug4", DEBUG4, false},
+	{"debug3", DEBUG3, false},
+	{"debug2", DEBUG2, false},
+	{"debug1", DEBUG1, false},
+	{"debug", DEBUG2, true},
+	{"info", INFO, false},
+	{"notice", NOTICE, false},
+	{"warning", WARNING, false},
+	{"error", ERROR, false},
+	{"log", LOG, false},
+	{"fatal", FATAL, false},
+	{"panic", PANIC, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry intervalstyle_options[] = {
+	{"postgres", INTSTYLE_POSTGRES, false},
+	{"postgres_verbose", INTSTYLE_POSTGRES_VERBOSE, false},
+	{"sql_standard", INTSTYLE_SQL_STANDARD, false},
+	{"iso_8601", INTSTYLE_ISO_8601, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry log_error_verbosity_options[] = {
+	{"terse", PGERROR_TERSE, false},
+	{"default", PGERROR_DEFAULT, false},
+	{"verbose", PGERROR_VERBOSE, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry log_statement_options[] = {
+	{"none", LOGSTMT_NONE, false},
+	{"ddl", LOGSTMT_DDL, false},
+	{"mod", LOGSTMT_MOD, false},
+	{"all", LOGSTMT_ALL, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry isolation_level_options[] = {
+	{"serializable", XACT_SERIALIZABLE, false},
+	{"repeatable read", XACT_REPEATABLE_READ, false},
+	{"read committed", XACT_READ_COMMITTED, false},
+	{"read uncommitted", XACT_READ_UNCOMMITTED, false},
+	{NULL, 0}
+};
+
+static const struct config_enum_entry session_replication_role_options[] = {
+	{"origin", SESSION_REPLICATION_ROLE_ORIGIN, false},
+	{"replica", SESSION_REPLICATION_ROLE_REPLICA, false},
+	{"local", SESSION_REPLICATION_ROLE_LOCAL, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry syslog_facility_options[] = {
+#ifdef HAVE_SYSLOG
+	{"local0", LOG_LOCAL0, false},
+	{"local1", LOG_LOCAL1, false},
+	{"local2", LOG_LOCAL2, false},
+	{"local3", LOG_LOCAL3, false},
+	{"local4", LOG_LOCAL4, false},
+	{"local5", LOG_LOCAL5, false},
+	{"local6", LOG_LOCAL6, false},
+	{"local7", LOG_LOCAL7, false},
+#else
+	{"none", 0, false},
+#endif
+	{NULL, 0}
+};
+
+static const struct config_enum_entry track_function_options[] = {
+	{"none", TRACK_FUNC_OFF, false},
+	{"pl", TRACK_FUNC_PL, false},
+	{"all", TRACK_FUNC_ALL, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry xmlbinary_options[] = {
+	{"base64", XMLBINARY_BASE64, false},
+	{"hex", XMLBINARY_HEX, false},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry xmloption_options[] = {
+	{"content", XMLOPTION_CONTENT, false},
+	{"document", XMLOPTION_DOCUMENT, false},
+	{NULL, 0, false}
+};
+
+/*
+ * Although only "on", "off", and "safe_encoding" are documented, we
+ * accept all the likely variants of "on" and "off".
+ */
+static const struct config_enum_entry backslash_quote_options[] = {
+	{"safe_encoding", BACKSLASH_QUOTE_SAFE_ENCODING, false},
+	{"on", BACKSLASH_QUOTE_ON, false},
+	{"off", BACKSLASH_QUOTE_OFF, false},
+	{"true", BACKSLASH_QUOTE_ON, true},
+	{"false", BACKSLASH_QUOTE_OFF, true},
+	{"yes", BACKSLASH_QUOTE_ON, true},
+	{"no", BACKSLASH_QUOTE_OFF, true},
+	{"1", BACKSLASH_QUOTE_ON, true},
+	{"0", BACKSLASH_QUOTE_OFF, true},
+	{NULL, 0, false}
+};
+
+/*
+ * Although only "on", "off", and "partition" are documented, we
+ * accept all the likely variants of "on" and "off".
+ */
+static const struct config_enum_entry constraint_exclusion_options[] = {
+	{"partition", CONSTRAINT_EXCLUSION_PARTITION, false},
+	{"on", CONSTRAINT_EXCLUSION_ON, false},
+	{"off", CONSTRAINT_EXCLUSION_OFF, false},
+	{"true", CONSTRAINT_EXCLUSION_ON, true},
+	{"false", CONSTRAINT_EXCLUSION_OFF, true},
+	{"yes", CONSTRAINT_EXCLUSION_ON, true},
+	{"no", CONSTRAINT_EXCLUSION_OFF, true},
+	{"1", CONSTRAINT_EXCLUSION_ON, true},
+	{"0", CONSTRAINT_EXCLUSION_OFF, true},
+	{NULL, 0, false}
+};
+
+/*
+ * Although only "on", "off", "remote_apply", "remote_write", and "local" are
+ * documented, we accept all the likely variants of "on" and "off".
+ */
+static const struct config_enum_entry synchronous_commit_options[] = {
+	{"local", SYNCHRONOUS_COMMIT_LOCAL_FLUSH, false},
+	{"remote_write", SYNCHRONOUS_COMMIT_REMOTE_WRITE, false},
+	{"remote_apply", SYNCHRONOUS_COMMIT_REMOTE_APPLY, false},
+	{"on", SYNCHRONOUS_COMMIT_ON, false},
+	{"off", SYNCHRONOUS_COMMIT_OFF, false},
+	{"true", SYNCHRONOUS_COMMIT_ON, true},
+	{"false", SYNCHRONOUS_COMMIT_OFF, true},
+	{"yes", SYNCHRONOUS_COMMIT_ON, true},
+	{"no", SYNCHRONOUS_COMMIT_OFF, true},
+	{"1", SYNCHRONOUS_COMMIT_ON, true},
+	{"0", SYNCHRONOUS_COMMIT_OFF, true},
+	{NULL, 0, false}
+};
+
+/*
+ * Although only "on", "off", "try" are documented, we accept all the likely
+ * variants of "on" and "off".
+ */
+static const struct config_enum_entry huge_pages_options[] = {
+	{"off", HUGE_PAGES_OFF, false},
+	{"on", HUGE_PAGES_ON, false},
+	{"try", HUGE_PAGES_TRY, false},
+	{"true", HUGE_PAGES_ON, true},
+	{"false", HUGE_PAGES_OFF, true},
+	{"yes", HUGE_PAGES_ON, true},
+	{"no", HUGE_PAGES_OFF, true},
+	{"1", HUGE_PAGES_ON, true},
+	{"0", HUGE_PAGES_OFF, true},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry force_parallel_mode_options[] = {
+	{"off", FORCE_PARALLEL_OFF, false},
+	{"on", FORCE_PARALLEL_ON, false},
+	{"regress", FORCE_PARALLEL_REGRESS, false},
+	{"true", FORCE_PARALLEL_ON, true},
+	{"false", FORCE_PARALLEL_OFF, true},
+	{"yes", FORCE_PARALLEL_ON, true},
+	{"no", FORCE_PARALLEL_OFF, true},
+	{"1", FORCE_PARALLEL_ON, true},
+	{"0", FORCE_PARALLEL_OFF, true},
+	{NULL, 0, false}
+};
+
+static const struct config_enum_entry plan_cache_mode_options[] = {
+	{"auto", PLAN_CACHE_MODE_AUTO, false},
+	{"force_generic_plan", PLAN_CACHE_MODE_FORCE_GENERIC_PLAN, false},
+	{"force_custom_plan", PLAN_CACHE_MODE_FORCE_CUSTOM_PLAN, false},
+	{NULL, 0, false}
+};
+
+/*
+ * password_encryption used to be a boolean, so accept all the likely
+ * variants of "on", too. "off" used to store passwords in plaintext,
+ * but we don't support that anymore.
+ */
+static const struct config_enum_entry password_encryption_options[] = {
+	{"md5", PASSWORD_TYPE_MD5, false},
+	{"scram-sha-256", PASSWORD_TYPE_SCRAM_SHA_256, false},
+	{"on", PASSWORD_TYPE_MD5, true},
+	{"true", PASSWORD_TYPE_MD5, true},
+	{"yes", PASSWORD_TYPE_MD5, true},
+	{"1", PASSWORD_TYPE_MD5, true},
+	{NULL, 0, false}
+};
+
+const struct config_enum_entry ssl_protocol_versions_info[] = {
+	{"", PG_TLS_ANY, false},
+	{"TLSv1", PG_TLS1_VERSION, false},
+	{"TLSv1.1", PG_TLS1_1_VERSION, false},
+	{"TLSv1.2", PG_TLS1_2_VERSION, false},
+	{"TLSv1.3", PG_TLS1_3_VERSION, false},
+	{NULL, 0, false}
+};
+
+static struct config_enum_entry shared_memory_options[] = {
+#ifndef WIN32
+	{"sysv", SHMEM_TYPE_SYSV, false},
+#endif
+#ifndef EXEC_BACKEND
+	{"mmap", SHMEM_TYPE_MMAP, false},
+#endif
+#ifdef WIN32
+	{"windows", SHMEM_TYPE_WINDOWS, false},
+#endif
+	{NULL, 0, false}
+};
+
+/*
+ * Options for enum values stored in other modules
+ */
+extern const struct config_enum_entry wal_level_options[];
+extern const struct config_enum_entry archive_mode_options[];
+extern const struct config_enum_entry recovery_target_action_options[];
+extern const struct config_enum_entry sync_method_options[];
+extern const struct config_enum_entry dynamic_shared_memory_options[];
+
+/*
+ * GUC option variables that are exported from this module
+ */
+bool		log_duration = false;
+bool		Debug_print_plan = false;
+bool		Debug_print_parse = false;
+bool		Debug_print_rewritten = false;
+bool		Debug_pretty_print = true;
+
+bool		log_parser_stats = false;
+bool		log_planner_stats = false;
+bool		log_executor_stats = false;
+bool		log_statement_stats = false;	/* this is sort of all three above
+											 * together */
+bool		log_btree_build_stats = false;
+char	   *event_source;
+
+bool		row_security;
+bool		check_function_bodies = true;
+
+/*
+ * This GUC exists solely for backward compatibility, check its definition for
+ * details.
+ */
+bool		default_with_oids = false;
+bool		session_auth_is_superuser;
+
+int			log_min_error_statement = ERROR;
+int			log_min_messages = WARNING;
+int			client_min_messages = NOTICE;
+int			log_min_duration_statement = -1;
+int			log_temp_files = -1;
+double		log_xact_sample_rate = 0;
+int			trace_recovery_messages = LOG;
+
+int			temp_file_limit = -1;
+
+int			num_temp_buffers = 1024;
+
+char	   *cluster_name = "";
+char	   *ConfigFileName;
+char	   *HbaFileName;
+char	   *IdentFileName;
+char	   *external_pid_file;
+
+char	   *pgstat_temp_directory;
+
+char	   *application_name;
+
+int			tcp_keepalives_idle;
+int			tcp_keepalives_interval;
+int			tcp_keepalives_count;
+int			tcp_user_timeout;
+
+/*
+ * SSL renegotiation was been removed in PostgreSQL 9.5, but we tolerate it
+ * being set to zero (meaning never renegotiate) for backward compatibility.
+ * This avoids breaking compatibility with clients that have never supported
+ * renegotiation and therefore always try to zero it.
+ */
+int			ssl_renegotiation_limit;
+
+/*
+ * This really belongs in pg_shmem.c, but is defined here so that it doesn't
+ * need to be duplicated in all the different implementations of pg_shmem.c.
+ */
+int			huge_pages;
+
+/*
+ * These variables are all dummies that don't do anything, except in some
+ * cases provide the value for SHOW to display.  The real state is elsewhere
+ * and is kept in sync by assign_hooks.
+ */
+static char *syslog_ident_str;
+static double phony_random_seed;
+static char *client_encoding_string;
+static char *datestyle_string;
+static char *locale_collate;
+static char *locale_ctype;
+static char *server_encoding_string;
+static char *server_version_string;
+static int	server_version_num;
+static char *timezone_string;
+static char *log_timezone_string;
+static char *timezone_abbreviations_string;
+static char *data_directory;
+static char *session_authorization_string;
+static int	max_function_args;
+static int	max_index_keys;
+static int	max_identifier_length;
+static int	block_size;
+static int	segment_size;
+static int	wal_block_size;
+static bool data_checksums;
+static bool integer_datetimes;
+static bool assert_enabled;
+static char *recovery_target_timeline_string;
+static char *recovery_target_string;
+static char *recovery_target_xid_string;
+static char *recovery_target_name_string;
+static char *recovery_target_lsn_string;
+static char *restrict_nonsystem_relation_kind_string;
+
+
+/* should be static, but commands/variable.c needs to get at this */
+char	   *role_string;
+
+
+/*
+ * Displayable names for context types (enum GucContext)
+ *
+ * Note: these strings are deliberately not localized.
+ */
+const char *const GucContext_Names[] =
+{
+	 /* PGC_INTERNAL */ "internal",
+	 /* PGC_POSTMASTER */ "postmaster",
+	 /* PGC_SIGHUP */ "sighup",
+	 /* PGC_SU_BACKEND */ "superuser-backend",
+	 /* PGC_BACKEND */ "backend",
+	 /* PGC_SUSET */ "superuser",
+	 /* PGC_USERSET */ "user"
+};
+
+/*
+ * Displayable names for source types (enum GucSource)
+ *
+ * Note: these strings are deliberately not localized.
+ */
+const char *const GucSource_Names[] =
+{
+	 /* PGC_S_DEFAULT */ "default",
+	 /* PGC_S_DYNAMIC_DEFAULT */ "default",
+	 /* PGC_S_ENV_VAR */ "environment variable",
+	 /* PGC_S_FILE */ "configuration file",
+	 /* PGC_S_ARGV */ "command line",
+	 /* PGC_S_GLOBAL */ "global",
+	 /* PGC_S_DATABASE */ "database",
+	 /* PGC_S_USER */ "user",
+	 /* PGC_S_DATABASE_USER */ "database user",
+	 /* PGC_S_CLIENT */ "client",
+	 /* PGC_S_OVERRIDE */ "override",
+	 /* PGC_S_INTERACTIVE */ "interactive",
+	 /* PGC_S_TEST */ "test",
+	 /* PGC_S_SESSION */ "session"
+};
+
+/*
+ * Displayable names for the groupings defined in enum config_group
+ */
+const char *const config_group_names[] =
+{
+	/* UNGROUPED */
+	gettext_noop("Ungrouped"),
+	/* FILE_LOCATIONS */
+	gettext_noop("File Locations"),
+	/* CONN_AUTH */
+	gettext_noop("Connections and Authentication"),
+	/* CONN_AUTH_SETTINGS */
+	gettext_noop("Connections and Authentication / Connection Settings"),
+	/* CONN_AUTH_AUTH */
+	gettext_noop("Connections and Authentication / Authentication"),
+	/* CONN_AUTH_SSL */
+	gettext_noop("Connections and Authentication / SSL"),
+	/* RESOURCES */
+	gettext_noop("Resource Usage"),
+	/* RESOURCES_MEM */
+	gettext_noop("Resource Usage / Memory"),
+	/* RESOURCES_DISK */
+	gettext_noop("Resource Usage / Disk"),
+	/* RESOURCES_KERNEL */
+	gettext_noop("Resource Usage / Kernel Resources"),
+	/* RESOURCES_VACUUM_DELAY */
+	gettext_noop("Resource Usage / Cost-Based Vacuum Delay"),
+	/* RESOURCES_BGWRITER */
+	gettext_noop("Resource Usage / Background Writer"),
+	/* RESOURCES_ASYNCHRONOUS */
+	gettext_noop("Resource Usage / Asynchronous Behavior"),
+	/* WAL */
+	gettext_noop("Write-Ahead Log"),
+	/* WAL_SETTINGS */
+	gettext_noop("Write-Ahead Log / Settings"),
+	/* WAL_CHECKPOINTS */
+	gettext_noop("Write-Ahead Log / Checkpoints"),
+	/* WAL_ARCHIVING */
+	gettext_noop("Write-Ahead Log / Archiving"),
+	/* WAL_ARCHIVE_RECOVERY */
+	gettext_noop("Write-Ahead Log / Archive Recovery"),
+	/* WAL_RECOVERY_TARGET */
+	gettext_noop("Write-Ahead Log / Recovery Target"),
+	/* REPLICATION */
+	gettext_noop("Replication"),
+	/* REPLICATION_SENDING */
+	gettext_noop("Replication / Sending Servers"),
+	/* REPLICATION_MASTER */
+	gettext_noop("Replication / Master Server"),
+	/* REPLICATION_STANDBY */
+	gettext_noop("Replication / Standby Servers"),
+	/* REPLICATION_SUBSCRIBERS */
+	gettext_noop("Replication / Subscribers"),
+	/* QUERY_TUNING */
+	gettext_noop("Query Tuning"),
+	/* QUERY_TUNING_METHOD */
+	gettext_noop("Query Tuning / Planner Method Configuration"),
+	/* QUERY_TUNING_COST */
+	gettext_noop("Query Tuning / Planner Cost Constants"),
+	/* QUERY_TUNING_GEQO */
+	gettext_noop("Query Tuning / Genetic Query Optimizer"),
+	/* QUERY_TUNING_OTHER */
+	gettext_noop("Query Tuning / Other Planner Options"),
+	/* LOGGING */
+	gettext_noop("Reporting and Logging"),
+	/* LOGGING_WHERE */
+	gettext_noop("Reporting and Logging / Where to Log"),
+	/* LOGGING_WHEN */
+	gettext_noop("Reporting and Logging / When to Log"),
+	/* LOGGING_WHAT */
+	gettext_noop("Reporting and Logging / What to Log"),
+	/* PROCESS_TITLE */
+	gettext_noop("Process Title"),
+	/* STATS */
+	gettext_noop("Statistics"),
+	/* STATS_MONITORING */
+	gettext_noop("Statistics / Monitoring"),
+	/* STATS_COLLECTOR */
+	gettext_noop("Statistics / Query and Index Statistics Collector"),
+	/* AUTOVACUUM */
+	gettext_noop("Autovacuum"),
+	/* CLIENT_CONN */
+	gettext_noop("Client Connection Defaults"),
+	/* CLIENT_CONN_STATEMENT */
+	gettext_noop("Client Connection Defaults / Statement Behavior"),
+	/* CLIENT_CONN_LOCALE */
+	gettext_noop("Client Connection Defaults / Locale and Formatting"),
+	/* CLIENT_CONN_PRELOAD */
+	gettext_noop("Client Connection Defaults / Shared Library Preloading"),
+	/* CLIENT_CONN_OTHER */
+	gettext_noop("Client Connection Defaults / Other Defaults"),
+	/* LOCK_MANAGEMENT */
+	gettext_noop("Lock Management"),
+	/* COMPAT_OPTIONS */
+	gettext_noop("Version and Platform Compatibility"),
+	/* COMPAT_OPTIONS_PREVIOUS */
+	gettext_noop("Version and Platform Compatibility / Previous PostgreSQL Versions"),
+	/* COMPAT_OPTIONS_CLIENT */
+	gettext_noop("Version and Platform Compatibility / Other Platforms and Clients"),
+	/* ERROR_HANDLING */
+	gettext_noop("Error Handling"),
+	/* PRESET_OPTIONS */
+	gettext_noop("Preset Options"),
+	/* CUSTOM_OPTIONS */
+	gettext_noop("Customized Options"),
+	/* DEVELOPER_OPTIONS */
+	gettext_noop("Developer Options"),
+	/* help_config wants this array to be null-terminated */
+	NULL
+};
+
+/*
+ * Displayable names for GUC variable types (enum config_type)
+ *
+ * Note: these strings are deliberately not localized.
+ */
+const char *const config_type_names[] =
+{
+	 /* PGC_BOOL */ "bool",
+	 /* PGC_INT */ "integer",
+	 /* PGC_REAL */ "real",
+	 /* PGC_STRING */ "string",
+	 /* PGC_ENUM */ "enum"
+};
+
+/*
+ * Unit conversion tables.
+ *
+ * There are two tables, one for memory units, and another for time units.
+ * For each supported conversion from one unit to another, we have an entry
+ * in the table.
+ *
+ * To keep things simple, and to avoid possible roundoff error,
+ * conversions are never chained.  There needs to be a direct conversion
+ * between all units (of the same type).
+ *
+ * The conversions for each base unit must be kept in order from greatest to
+ * smallest human-friendly unit; convert_xxx_from_base_unit() rely on that.
+ * (The order of the base-unit groups does not matter.)
+ */
+#define MAX_UNIT_LEN		3	/* length of longest recognized unit string */
+
+typedef struct
+{
+	char		unit[MAX_UNIT_LEN + 1]; /* unit, as a string, like "kB" or
+										 * "min" */
+	int			base_unit;		/* GUC_UNIT_XXX */
+	double		multiplier;		/* Factor for converting unit -> base_unit */
+} unit_conversion;
+
+/* Ensure that the constants in the tables don't overflow or underflow */
+#if BLCKSZ < 1024 || BLCKSZ > (1024*1024)
+#error BLCKSZ must be between 1KB and 1MB
+#endif
+#if XLOG_BLCKSZ < 1024 || XLOG_BLCKSZ > (1024*1024)
+#error XLOG_BLCKSZ must be between 1KB and 1MB
+#endif
+
+static const char *memory_units_hint = gettext_noop("Valid units for this parameter are \"B\", \"kB\", \"MB\", \"GB\", and \"TB\".");
+
+static const unit_conversion memory_unit_conversion_table[] =
+{
+	{"TB", GUC_UNIT_BYTE, 1024.0 * 1024.0 * 1024.0 * 1024.0},
+	{"GB", GUC_UNIT_BYTE, 1024.0 * 1024.0 * 1024.0},
+	{"MB", GUC_UNIT_BYTE, 1024.0 * 1024.0},
+	{"kB", GUC_UNIT_BYTE, 1024.0},
+	{"B", GUC_UNIT_BYTE, 1.0},
+
+	{"TB", GUC_UNIT_KB, 1024.0 * 1024.0 * 1024.0},
+	{"GB", GUC_UNIT_KB, 1024.0 * 1024.0},
+	{"MB", GUC_UNIT_KB, 1024.0},
+	{"kB", GUC_UNIT_KB, 1.0},
+	{"B", GUC_UNIT_KB, 1.0 / 1024.0},
+
+	{"TB", GUC_UNIT_MB, 1024.0 * 1024.0},
+	{"GB", GUC_UNIT_MB, 1024.0},
+	{"MB", GUC_UNIT_MB, 1.0},
+	{"kB", GUC_UNIT_MB, 1.0 / 1024.0},
+	{"B", GUC_UNIT_MB, 1.0 / (1024.0 * 1024.0)},
+
+	{"TB", GUC_UNIT_BLOCKS, (1024.0 * 1024.0 * 1024.0) / (BLCKSZ / 1024)},
+	{"GB", GUC_UNIT_BLOCKS, (1024.0 * 1024.0) / (BLCKSZ / 1024)},
+	{"MB", GUC_UNIT_BLOCKS, 1024.0 / (BLCKSZ / 1024)},
+	{"kB", GUC_UNIT_BLOCKS, 1.0 / (BLCKSZ / 1024)},
+	{"B", GUC_UNIT_BLOCKS, 1.0 / BLCKSZ},
+
+	{"TB", GUC_UNIT_XBLOCKS, (1024.0 * 1024.0 * 1024.0) / (XLOG_BLCKSZ / 1024)},
+	{"GB", GUC_UNIT_XBLOCKS, (1024.0 * 1024.0) / (XLOG_BLCKSZ / 1024)},
+	{"MB", GUC_UNIT_XBLOCKS, 1024.0 / (XLOG_BLCKSZ / 1024)},
+	{"kB", GUC_UNIT_XBLOCKS, 1.0 / (XLOG_BLCKSZ / 1024)},
+	{"B", GUC_UNIT_XBLOCKS, 1.0 / XLOG_BLCKSZ},
+
+	{""}						/* end of table marker */
+};
+
+static const char *time_units_hint = gettext_noop("Valid units for this parameter are \"us\", \"ms\", \"s\", \"min\", \"h\", and \"d\".");
+
+static const unit_conversion time_unit_conversion_table[] =
+{
+	{"d", GUC_UNIT_MS, 1000 * 60 * 60 * 24},
+	{"h", GUC_UNIT_MS, 1000 * 60 * 60},
+	{"min", GUC_UNIT_MS, 1000 * 60},
+	{"s", GUC_UNIT_MS, 1000},
+	{"ms", GUC_UNIT_MS, 1},
+	{"us", GUC_UNIT_MS, 1.0 / 1000},
+
+	{"d", GUC_UNIT_S, 60 * 60 * 24},
+	{"h", GUC_UNIT_S, 60 * 60},
+	{"min", GUC_UNIT_S, 60},
+	{"s", GUC_UNIT_S, 1},
+	{"ms", GUC_UNIT_S, 1.0 / 1000},
+	{"us", GUC_UNIT_S, 1.0 / (1000 * 1000)},
+
+	{"d", GUC_UNIT_MIN, 60 * 24},
+	{"h", GUC_UNIT_MIN, 60},
+	{"min", GUC_UNIT_MIN, 1},
+	{"s", GUC_UNIT_MIN, 1.0 / 60},
+	{"ms", GUC_UNIT_MIN, 1.0 / (1000 * 60)},
+	{"us", GUC_UNIT_MIN, 1.0 / (1000 * 1000 * 60)},
+
+	{""}						/* end of table marker */
+};
+
+/*
+ * Contents of GUC tables
+ *
+ * See src/backend/utils/misc/README for design notes.
+ *
+ * TO ADD AN OPTION:
+ *
+ * 1. Declare a global variable of type bool, int, double, or char*
+ *	  and make use of it.
+ *
+ * 2. Decide at what times it's safe to set the option. See guc.h for
+ *	  details.
+ *
+ * 3. Decide on a name, a default value, upper and lower bounds (if
+ *	  applicable), etc.
+ *
+ * 4. Add a record below.
+ *
+ * 5. Add it to src/backend/utils/misc/postgresql.conf.sample, if
+ *	  appropriate.
+ *
+ * 6. Don't forget to document the option (at least in config.sgml).
+ *
+ * 7. If it's a new GUC_LIST_QUOTE option, you must add it to
+ *	  variable_is_guc_list_quote() in src/bin/pg_dump/dumputils.c.
+ */
+
+
+/******** option records follow ********/
+
+static struct config_bool ConfigureNamesBool[] =
+{
+	{
+		{"enable_seqscan", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of sequential-scan plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_seqscan,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_indexscan", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of index-scan plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_indexscan,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_indexonlyscan", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of index-only-scan plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_indexonlyscan,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_bitmapscan", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of bitmap-scan plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_bitmapscan,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_tidscan", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of TID scan plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_tidscan,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_sort", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of explicit sort steps."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_sort,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_hashagg", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of hashed aggregation plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_hashagg,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_material", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of materialization."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_material,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_nestloop", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of nested-loop join plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_nestloop,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_mergejoin", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of merge join plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_mergejoin,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_hashjoin", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of hash join plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_hashjoin,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_gathermerge", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of gather merge plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_gathermerge,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_partitionwise_join", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables partitionwise join."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_partitionwise_join,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_partitionwise_aggregate", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables partitionwise aggregation and grouping."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_partitionwise_aggregate,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_parallel_append", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of parallel append plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_parallel_append,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_parallel_hash", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables the planner's use of parallel hash plans."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&enable_parallel_hash,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"enable_partition_pruning", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enables plan-time and run-time partition pruning."),
+			gettext_noop("Allows the query planner and executor to compare partition "
+						 "bounds to conditions in the query to determine which "
+						 "partitions must be scanned."),
+			GUC_EXPLAIN
+		},
+		&enable_partition_pruning,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"geqo", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("Enables genetic query optimization."),
+			gettext_noop("This algorithm attempts to do planning without "
+						 "exhaustive searching."),
+			GUC_EXPLAIN
+		},
+		&enable_geqo,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		/* Not for general use --- used by SET SESSION AUTHORIZATION */
+		{"is_superuser", PGC_INTERNAL, UNGROUPED,
+			gettext_noop("Shows whether the current user is a superuser."),
+			NULL,
+			GUC_REPORT | GUC_NO_SHOW_ALL | GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&session_auth_is_superuser,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"bonjour", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Enables advertising the server via Bonjour."),
+			NULL
+		},
+		&enable_bonjour,
+		false,
+		check_bonjour, NULL, NULL
+	},
+	{
+		{"track_commit_timestamp", PGC_POSTMASTER, REPLICATION,
+			gettext_noop("Collects transaction commit time."),
+			NULL
+		},
+		&track_commit_timestamp,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"ssl", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Enables SSL connections."),
+			NULL
+		},
+		&EnableSSL,
+		false,
+		check_ssl, NULL, NULL
+	},
+	{
+		{"ssl_passphrase_command_supports_reload", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Also use ssl_passphrase_command during server reload."),
+			NULL
+		},
+		&ssl_passphrase_command_supports_reload,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"ssl_prefer_server_ciphers", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Give priority to server ciphersuite order."),
+			NULL
+		},
+		&SSLPreferServerCiphers,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"fsync", PGC_SIGHUP, WAL_SETTINGS,
+			gettext_noop("Forces synchronization of updates to disk."),
+			gettext_noop("The server will use the fsync() system call in several places to make "
+						 "sure that updates are physically written to disk. This ensures "
+						 "that a database cluster will recover to a consistent state after "
+						 "an operating system or hardware crash.")
+		},
+		&enableFsync,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"ignore_checksum_failure", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Continues processing after a checksum failure."),
+			gettext_noop("Detection of a checksum failure normally causes PostgreSQL to "
+						 "report an error, aborting the current transaction. Setting "
+						 "ignore_checksum_failure to true causes the system to ignore the failure "
+						 "(but still report a warning), and continue processing. This "
+						 "behavior could cause crashes or other serious problems. Only "
+						 "has an effect if checksums are enabled."),
+			GUC_NOT_IN_SAMPLE
+		},
+		&ignore_checksum_failure,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"zero_damaged_pages", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Continues processing past damaged page headers."),
+			gettext_noop("Detection of a damaged page header normally causes PostgreSQL to "
+						 "report an error, aborting the current transaction. Setting "
+						 "zero_damaged_pages to true causes the system to instead report a "
+						 "warning, zero out the damaged page, and continue processing. This "
+						 "behavior will destroy data, namely all the rows on the damaged page."),
+			GUC_NOT_IN_SAMPLE
+		},
+		&zero_damaged_pages,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"full_page_writes", PGC_SIGHUP, WAL_SETTINGS,
+			gettext_noop("Writes full pages to WAL when first modified after a checkpoint."),
+			gettext_noop("A page write in process during an operating system crash might be "
+						 "only partially written to disk.  During recovery, the row changes "
+						 "stored in WAL are not enough to recover.  This option writes "
+						 "pages when first modified after a checkpoint to WAL so full recovery "
+						 "is possible.")
+		},
+		&fullPageWrites,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_log_hints", PGC_POSTMASTER, WAL_SETTINGS,
+			gettext_noop("Writes full pages to WAL when first modified after a checkpoint, even for a non-critical modification."),
+			NULL
+		},
+		&wal_log_hints,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_compression", PGC_SUSET, WAL_SETTINGS,
+			gettext_noop("Compresses full-page writes written in WAL file."),
+			NULL
+		},
+		&wal_compression,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_init_zero", PGC_SUSET, WAL_SETTINGS,
+			gettext_noop("Writes zeroes to new WAL files before first use."),
+			NULL
+		},
+		&wal_init_zero,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_recycle", PGC_SUSET, WAL_SETTINGS,
+			gettext_noop("Recycles WAL files by renaming them."),
+			NULL
+		},
+		&wal_recycle,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_checkpoints", PGC_SIGHUP, LOGGING_WHAT,
+			gettext_noop("Logs each checkpoint."),
+			NULL
+		},
+		&log_checkpoints,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"log_connections", PGC_SU_BACKEND, LOGGING_WHAT,
+			gettext_noop("Logs each successful connection."),
+			NULL
+		},
+		&Log_connections,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"log_disconnections", PGC_SU_BACKEND, LOGGING_WHAT,
+			gettext_noop("Logs end of a session, including duration."),
+			NULL
+		},
+		&Log_disconnections,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"log_replication_commands", PGC_SUSET, LOGGING_WHAT,
+			gettext_noop("Logs each replication command."),
+			NULL
+		},
+		&log_replication_commands,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"debug_assertions", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows whether the running server has assertion checks enabled."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&assert_enabled,
+#ifdef USE_ASSERT_CHECKING
+		true,
+#else
+		false,
+#endif
+		NULL, NULL, NULL
+	},
+
+	{
+		{"exit_on_error", PGC_USERSET, ERROR_HANDLING_OPTIONS,
+			gettext_noop("Terminate session on any error."),
+			NULL
+		},
+		&ExitOnAnyError,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"restart_after_crash", PGC_SIGHUP, ERROR_HANDLING_OPTIONS,
+			gettext_noop("Reinitialize server after backend crash."),
+			NULL
+		},
+		&restart_after_crash,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_duration", PGC_SUSET, LOGGING_WHAT,
+			gettext_noop("Logs the duration of each completed SQL statement."),
+			NULL
+		},
+		&log_duration,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"debug_print_parse", PGC_USERSET, LOGGING_WHAT,
+			gettext_noop("Logs each query's parse tree."),
+			NULL
+		},
+		&Debug_print_parse,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"debug_print_rewritten", PGC_USERSET, LOGGING_WHAT,
+			gettext_noop("Logs each query's rewritten parse tree."),
+			NULL
+		},
+		&Debug_print_rewritten,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"debug_print_plan", PGC_USERSET, LOGGING_WHAT,
+			gettext_noop("Logs each query's execution plan."),
+			NULL
+		},
+		&Debug_print_plan,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"debug_pretty_print", PGC_USERSET, LOGGING_WHAT,
+			gettext_noop("Indents parse and plan tree displays."),
+			NULL
+		},
+		&Debug_pretty_print,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"log_parser_stats", PGC_SUSET, STATS_MONITORING,
+			gettext_noop("Writes parser performance statistics to the server log."),
+			NULL
+		},
+		&log_parser_stats,
+		false,
+		check_stage_log_stats, NULL, NULL
+	},
+	{
+		{"log_planner_stats", PGC_SUSET, STATS_MONITORING,
+			gettext_noop("Writes planner performance statistics to the server log."),
+			NULL
+		},
+		&log_planner_stats,
+		false,
+		check_stage_log_stats, NULL, NULL
+	},
+	{
+		{"log_executor_stats", PGC_SUSET, STATS_MONITORING,
+			gettext_noop("Writes executor performance statistics to the server log."),
+			NULL
+		},
+		&log_executor_stats,
+		false,
+		check_stage_log_stats, NULL, NULL
+	},
+	{
+		{"log_statement_stats", PGC_SUSET, STATS_MONITORING,
+			gettext_noop("Writes cumulative performance statistics to the server log."),
+			NULL
+		},
+		&log_statement_stats,
+		false,
+		check_log_stats, NULL, NULL
+	},
+#ifdef BTREE_BUILD_STATS
+	{
+		{"log_btree_build_stats", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Logs system resource usage statistics (memory and CPU) on various B-tree operations."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&log_btree_build_stats,
+		false,
+		NULL, NULL, NULL
+	},
+#endif
+
+	{
+		{"track_activities", PGC_SUSET, STATS_COLLECTOR,
+			gettext_noop("Collects information about executing commands."),
+			gettext_noop("Enables the collection of information on the currently "
+						 "executing command of each session, along with "
+						 "the time at which that command began execution.")
+		},
+		&pgstat_track_activities,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"track_counts", PGC_SUSET, STATS_COLLECTOR,
+			gettext_noop("Collects statistics on database activity."),
+			NULL
+		},
+		&pgstat_track_counts,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"track_io_timing", PGC_SUSET, STATS_COLLECTOR,
+			gettext_noop("Collects timing statistics for database I/O activity."),
+			NULL
+		},
+		&track_io_timing,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"update_process_title", PGC_SUSET, PROCESS_TITLE,
+			gettext_noop("Updates the process title to show the active SQL command."),
+			gettext_noop("Enables updating of the process title every time a new SQL command is received by the server.")
+		},
+		&update_process_title,
+#ifdef WIN32
+		false,
+#else
+		true,
+#endif
+		NULL, NULL, NULL
+	},
+
+	{
+		{"autovacuum", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Starts the autovacuum subprocess."),
+			NULL
+		},
+		&autovacuum_start_daemon,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"trace_notify", PGC_USERSET, DEVELOPER_OPTIONS,
+			gettext_noop("Generates debugging output for LISTEN and NOTIFY."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&Trace_notify,
+		false,
+		NULL, NULL, NULL
+	},
+
+#ifdef LOCK_DEBUG
+	{
+		{"trace_locks", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Emits information about lock usage."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&Trace_locks,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"trace_userlocks", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Emits information about user lock usage."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&Trace_userlocks,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"trace_lwlocks", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Emits information about lightweight lock usage."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&Trace_lwlocks,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"debug_deadlocks", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Dumps information about all current locks when a deadlock timeout occurs."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&Debug_deadlocks,
+		false,
+		NULL, NULL, NULL
+	},
+#endif
+
+	{
+		{"log_lock_waits", PGC_SUSET, LOGGING_WHAT,
+			gettext_noop("Logs long lock waits."),
+			NULL
+		},
+		&log_lock_waits,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_hostname", PGC_SIGHUP, LOGGING_WHAT,
+			gettext_noop("Logs the host name in the connection logs."),
+			gettext_noop("By default, connection logs only show the IP address "
+						 "of the connecting host. If you want them to show the host name you "
+						 "can turn this on, but depending on your host name resolution "
+						 "setup it might impose a non-negligible performance penalty.")
+		},
+		&log_hostname,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"transform_null_equals", PGC_USERSET, COMPAT_OPTIONS_CLIENT,
+			gettext_noop("Treats \"expr=NULL\" as \"expr IS NULL\"."),
+			gettext_noop("When turned on, expressions of the form expr = NULL "
+						 "(or NULL = expr) are treated as expr IS NULL, that is, they "
+						 "return true if expr evaluates to the null value, and false "
+						 "otherwise. The correct behavior of expr = NULL is to always "
+						 "return null (unknown).")
+		},
+		&Transform_null_equals,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"db_user_namespace", PGC_SIGHUP, CONN_AUTH_AUTH,
+			gettext_noop("Enables per-database user names."),
+			NULL
+		},
+		&Db_user_namespace,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"default_transaction_read_only", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the default read-only status of new transactions."),
+			NULL
+		},
+		&DefaultXactReadOnly,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"transaction_read_only", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the current transaction's read-only status."),
+			NULL,
+			GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&XactReadOnly,
+		false,
+		check_transaction_read_only, NULL, NULL
+	},
+	{
+		{"default_transaction_deferrable", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the default deferrable status of new transactions."),
+			NULL
+		},
+		&DefaultXactDeferrable,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"transaction_deferrable", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Whether to defer a read-only serializable transaction until it can be executed with no possible serialization failures."),
+			NULL,
+			GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&XactDeferrable,
+		false,
+		check_transaction_deferrable, NULL, NULL
+	},
+	{
+		{"row_security", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Enable row security."),
+			gettext_noop("When enabled, row security will be applied to all users.")
+		},
+		&row_security,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"check_function_bodies", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Check function bodies during CREATE FUNCTION."),
+			NULL
+		},
+		&check_function_bodies,
+		true,
+		NULL, NULL, NULL
+	},
+	{
+		{"array_nulls", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Enable input of NULL elements in arrays."),
+			gettext_noop("When turned on, unquoted NULL in an array input "
+						 "value means a null value; "
+						 "otherwise it is taken literally.")
+		},
+		&Array_nulls,
+		true,
+		NULL, NULL, NULL
+	},
+
+	/*
+	 * WITH OIDS support, and consequently default_with_oids, was removed in
+	 * PostgreSQL 12, but we tolerate the parameter being set to false to
+	 * avoid unnecessarily breaking older dump files.
+	 */
+	{
+		{"default_with_oids", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("WITH OIDS is no longer supported; this can only be false."),
+			NULL,
+			GUC_NO_SHOW_ALL | GUC_NOT_IN_SAMPLE
+		},
+		&default_with_oids,
+		false,
+		check_default_with_oids, NULL, NULL
+	},
+	{
+		{"logging_collector", PGC_POSTMASTER, LOGGING_WHERE,
+			gettext_noop("Start a subprocess to capture stderr output and/or csvlogs into log files."),
+			NULL
+		},
+		&Logging_collector,
+		false,
+		NULL, NULL, NULL
+	},
+	{
+		{"log_truncate_on_rotation", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Truncate existing log files of same name during log rotation."),
+			NULL
+		},
+		&Log_truncate_on_rotation,
+		false,
+		NULL, NULL, NULL
+	},
+
+#ifdef TRACE_SORT
+	{
+		{"trace_sort", PGC_USERSET, DEVELOPER_OPTIONS,
+			gettext_noop("Emit information about resource usage in sorting."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&trace_sort,
+		false,
+		NULL, NULL, NULL
+	},
+#endif
+
+#ifdef TRACE_SYNCSCAN
+	/* this is undocumented because not exposed in a standard build */
+	{
+		{"trace_syncscan", PGC_USERSET, DEVELOPER_OPTIONS,
+			gettext_noop("Generate debugging output for synchronized scanning."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&trace_syncscan,
+		false,
+		NULL, NULL, NULL
+	},
+#endif
+
+#ifdef DEBUG_BOUNDED_SORT
+	/* this is undocumented because not exposed in a standard build */
+	{
+		{
+			"optimize_bounded_sort", PGC_USERSET, QUERY_TUNING_METHOD,
+			gettext_noop("Enable bounded sorting using heap sort."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_EXPLAIN
+		},
+		&optimize_bounded_sort,
+		true,
+		NULL, NULL, NULL
+	},
+#endif
+
+#ifdef WAL_DEBUG
+	{
+		{"wal_debug", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Emit WAL-related debugging output."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&XLOG_DEBUG,
+		false,
+		NULL, NULL, NULL
+	},
+#endif
+
+	{
+		{"integer_datetimes", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Datetimes are integer based."),
+			NULL,
+			GUC_REPORT | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&integer_datetimes,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"krb_caseins_users", PGC_SIGHUP, CONN_AUTH_AUTH,
+			gettext_noop("Sets whether Kerberos and GSSAPI user names should be treated as case-insensitive."),
+			NULL
+		},
+		&pg_krb_caseins_users,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"escape_string_warning", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Warn about backslash escapes in ordinary string literals."),
+			NULL
+		},
+		&escape_string_warning,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"standard_conforming_strings", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Causes '...' strings to treat backslashes literally."),
+			NULL,
+			GUC_REPORT
+		},
+		&standard_conforming_strings,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"synchronize_seqscans", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Enable synchronized sequential scans."),
+			NULL
+		},
+		&synchronize_seqscans,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"recovery_target_inclusive", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Sets whether to include or exclude transaction with recovery target."),
+			NULL
+		},
+		&recoveryTargetInclusive,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"hot_standby", PGC_POSTMASTER, REPLICATION_STANDBY,
+			gettext_noop("Allows connections and queries during recovery."),
+			NULL
+		},
+		&EnableHotStandby,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"hot_standby_feedback", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Allows feedback from a hot standby to the primary that will avoid query conflicts."),
+			NULL
+		},
+		&hot_standby_feedback,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"allow_system_table_mods", PGC_POSTMASTER, DEVELOPER_OPTIONS,
+			gettext_noop("Allows modifications of the structure of system tables."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&allowSystemTableMods,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ignore_system_indexes", PGC_BACKEND, DEVELOPER_OPTIONS,
+			gettext_noop("Disables reading from system indexes."),
+			gettext_noop("It does not prevent updating the indexes, so it is safe "
+						 "to use.  The worst consequence is slowness."),
+			GUC_NOT_IN_SAMPLE
+		},
+		&IgnoreSystemIndexes,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"allow_in_place_tablespaces", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Allows tablespaces directly inside pg_tblspc, for testing."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&allow_in_place_tablespaces,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"lo_compat_privileges", PGC_SUSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Enables backward compatibility mode for privilege checks on large objects."),
+			gettext_noop("Skips privilege checks when reading or modifying large objects, "
+						 "for compatibility with PostgreSQL releases prior to 9.0.")
+		},
+		&lo_compat_privileges,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"operator_precedence_warning", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Emit a warning for constructs that changed meaning since PostgreSQL 9.4."),
+			NULL,
+		},
+		&operator_precedence_warning,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"quote_all_identifiers", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("When generating SQL fragments, quote all identifiers."),
+			NULL,
+		},
+		&quote_all_identifiers,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"data_checksums", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows whether data checksums are turned on for this cluster."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&data_checksums,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"syslog_sequence_numbers", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Add sequence number to syslog messages to avoid duplicate suppression."),
+			NULL
+		},
+		&syslog_sequence_numbers,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"syslog_split_messages", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Split messages sent to syslog by lines and to fit into 1024 bytes."),
+			NULL
+		},
+		&syslog_split_messages,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"parallel_leader_participation", PGC_USERSET, RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Controls whether Gather and Gather Merge also run subplans."),
+			gettext_noop("Should gather nodes also run subplans, or just gather tuples?"),
+			GUC_EXPLAIN
+		},
+		&parallel_leader_participation,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Allow JIT compilation."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&jit_enabled,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_debugging_support", PGC_SU_BACKEND, DEVELOPER_OPTIONS,
+			gettext_noop("Register JIT compiled function with debugger."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&jit_debugging_support,
+		false,
+
+		/*
+		 * This is not guaranteed to be available, but given it's a developer
+		 * oriented option, it doesn't seem worth adding code checking
+		 * availability.
+		 */
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_dump_bitcode", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Write out LLVM bitcode to facilitate JIT debugging."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&jit_dump_bitcode,
+		false,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_expressions", PGC_USERSET, DEVELOPER_OPTIONS,
+			gettext_noop("Allow JIT compilation of expressions."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&jit_expressions,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_profiling_support", PGC_SU_BACKEND, DEVELOPER_OPTIONS,
+			gettext_noop("Register JIT compiled function with perf profiler."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&jit_profiling_support,
+		false,
+
+		/*
+		 * This is not guaranteed to be available, but given it's a developer
+		 * oriented option, it doesn't seem worth adding code checking
+		 * availability.
+		 */
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_tuple_deforming", PGC_USERSET, DEVELOPER_OPTIONS,
+			gettext_noop("Allow JIT compilation of tuple deforming."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&jit_tuple_deforming,
+		true,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"data_sync_retry", PGC_POSTMASTER, ERROR_HANDLING_OPTIONS,
+			gettext_noop("Whether to continue running after a failure to sync data files."),
+		},
+		&data_sync_retry,
+		false,
+		NULL, NULL, NULL
+	},
+
+	/* End-of-list marker */
+	{
+		{NULL, 0, 0, NULL, NULL}, NULL, false, NULL, NULL, NULL
+	}
+};
+
+
+static struct config_int ConfigureNamesInt[] =
+{
+	{
+		{"archive_timeout", PGC_SIGHUP, WAL_ARCHIVING,
+			gettext_noop("Forces a switch to the next WAL file if a "
+						 "new file has not been started within N seconds."),
+			NULL,
+			GUC_UNIT_S
+		},
+		&XLogArchiveTimeout,
+		0, 0, INT_MAX / 2,
+		NULL, NULL, NULL
+	},
+	{
+		{"post_auth_delay", PGC_BACKEND, DEVELOPER_OPTIONS,
+			gettext_noop("Waits N seconds on connection startup after authentication."),
+			gettext_noop("This allows attaching a debugger to the process."),
+			GUC_NOT_IN_SAMPLE | GUC_UNIT_S
+		},
+		&PostAuthDelay,
+		0, 0, INT_MAX / 1000000,
+		NULL, NULL, NULL
+	},
+	{
+		{"default_statistics_target", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Sets the default statistics target."),
+			gettext_noop("This applies to table columns that have not had a "
+						 "column-specific target set via ALTER TABLE SET STATISTICS.")
+		},
+		&default_statistics_target,
+		100, 1, 10000,
+		NULL, NULL, NULL
+	},
+	{
+		{"from_collapse_limit", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Sets the FROM-list size beyond which subqueries "
+						 "are not collapsed."),
+			gettext_noop("The planner will merge subqueries into upper "
+						 "queries if the resulting FROM list would have no more than "
+						 "this many items."),
+			GUC_EXPLAIN
+		},
+		&from_collapse_limit,
+		8, 1, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"join_collapse_limit", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Sets the FROM-list size beyond which JOIN "
+						 "constructs are not flattened."),
+			gettext_noop("The planner will flatten explicit JOIN "
+						 "constructs into lists of FROM items whenever a "
+						 "list of no more than this many items would result."),
+			GUC_EXPLAIN
+		},
+		&join_collapse_limit,
+		8, 1, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"geqo_threshold", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("Sets the threshold of FROM items beyond which GEQO is used."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&geqo_threshold,
+		12, 2, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"geqo_effort", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("GEQO: effort is used to set the default for other GEQO parameters."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&Geqo_effort,
+		DEFAULT_GEQO_EFFORT, MIN_GEQO_EFFORT, MAX_GEQO_EFFORT,
+		NULL, NULL, NULL
+	},
+	{
+		{"geqo_pool_size", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("GEQO: number of individuals in the population."),
+			gettext_noop("Zero selects a suitable default value."),
+			GUC_EXPLAIN
+		},
+		&Geqo_pool_size,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"geqo_generations", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("GEQO: number of iterations of the algorithm."),
+			gettext_noop("Zero selects a suitable default value."),
+			GUC_EXPLAIN
+		},
+		&Geqo_generations,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		/* This is PGC_SUSET to prevent hiding from log_lock_waits. */
+		{"deadlock_timeout", PGC_SUSET, LOCK_MANAGEMENT,
+			gettext_noop("Sets the time to wait on a lock before checking for deadlock."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&DeadlockTimeout,
+		1000, 1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_standby_archive_delay", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Sets the maximum delay before canceling queries when a hot standby server is processing archived WAL data."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&max_standby_archive_delay,
+		30 * 1000, -1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_standby_streaming_delay", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Sets the maximum delay before canceling queries when a hot standby server is processing streamed WAL data."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&max_standby_streaming_delay,
+		30 * 1000, -1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"recovery_min_apply_delay", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Sets the minimum delay for applying changes during recovery."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&recovery_min_apply_delay,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_receiver_status_interval", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Sets the maximum interval between WAL receiver status reports to the sending server."),
+			NULL,
+			GUC_UNIT_S
+		},
+		&wal_receiver_status_interval,
+		10, 0, INT_MAX / 1000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_receiver_timeout", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Sets the maximum wait time to receive data from the sending server."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&wal_receiver_timeout,
+		60 * 1000, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_connections", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the maximum number of concurrent connections."),
+			NULL
+		},
+		&MaxConnections,
+		100, 1, MAX_BACKENDS,
+		check_maxconnections, NULL, NULL
+	},
+
+	{
+		/* see max_connections */
+		{"superuser_reserved_connections", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the number of connection slots reserved for superusers."),
+			NULL
+		},
+		&ReservedBackends,
+		3, 0, MAX_BACKENDS,
+		NULL, NULL, NULL
+	},
+
+	/*
+	 * We sometimes multiply the number of shared buffers by two without
+	 * checking for overflow, so we mustn't allow more than INT_MAX / 2.
+	 */
+	{
+		{"shared_buffers", PGC_POSTMASTER, RESOURCES_MEM,
+			gettext_noop("Sets the number of shared memory buffers used by the server."),
+			NULL,
+			GUC_UNIT_BLOCKS
+		},
+		&NBuffers,
+		1024, 16, INT_MAX / 2,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"temp_buffers", PGC_USERSET, RESOURCES_MEM,
+			gettext_noop("Sets the maximum number of temporary buffers used by each session."),
+			NULL,
+			GUC_UNIT_BLOCKS | GUC_EXPLAIN
+		},
+		&num_temp_buffers,
+		1024, 100, INT_MAX / 2,
+		check_temp_buffers, NULL, NULL
+	},
+
+	{
+		{"port", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the TCP port the server listens on."),
+			NULL
+		},
+		&PostPortNumber,
+		DEF_PGPORT, 1, 65535,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"unix_socket_permissions", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the access permissions of the Unix-domain socket."),
+			gettext_noop("Unix-domain sockets use the usual Unix file system "
+						 "permission set. The parameter value is expected "
+						 "to be a numeric mode specification in the form "
+						 "accepted by the chmod and umask system calls. "
+						 "(To use the customary octal format the number must "
+						 "start with a 0 (zero).)")
+		},
+		&Unix_socket_permissions,
+		0777, 0000, 0777,
+		NULL, NULL, show_unix_socket_permissions
+	},
+
+	{
+		{"log_file_mode", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Sets the file permissions for log files."),
+			gettext_noop("The parameter value is expected "
+						 "to be a numeric mode specification in the form "
+						 "accepted by the chmod and umask system calls. "
+						 "(To use the customary octal format the number must "
+						 "start with a 0 (zero).)")
+		},
+		&Log_file_mode,
+		0600, 0000, 0777,
+		NULL, NULL, show_log_file_mode
+	},
+
+
+	{
+		{"data_directory_mode", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Mode of the data directory."),
+			gettext_noop("The parameter value is a numeric mode specification "
+						 "in the form accepted by the chmod and umask system "
+						 "calls. (To use the customary octal format the number "
+						 "must start with a 0 (zero).)"),
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&data_directory_mode,
+		0700, 0000, 0777,
+		NULL, NULL, show_data_directory_mode
+	},
+
+	{
+		{"work_mem", PGC_USERSET, RESOURCES_MEM,
+			gettext_noop("Sets the maximum memory to be used for query workspaces."),
+			gettext_noop("This much memory can be used by each internal "
+						 "sort operation and hash table before switching to "
+						 "temporary disk files."),
+			GUC_UNIT_KB | GUC_EXPLAIN
+		},
+		&work_mem,
+		4096, 64, MAX_KILOBYTES,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"maintenance_work_mem", PGC_USERSET, RESOURCES_MEM,
+			gettext_noop("Sets the maximum memory to be used for maintenance operations."),
+			gettext_noop("This includes operations such as VACUUM and CREATE INDEX."),
+			GUC_UNIT_KB
+		},
+		&maintenance_work_mem,
+		65536, 1024, MAX_KILOBYTES,
+		NULL, NULL, NULL
+	},
+
+	/*
+	 * We use the hopefully-safely-small value of 100kB as the compiled-in
+	 * default for max_stack_depth.  InitializeGUCOptions will increase it if
+	 * possible, depending on the actual platform-specific stack limit.
+	 */
+	{
+		{"max_stack_depth", PGC_SUSET, RESOURCES_MEM,
+			gettext_noop("Sets the maximum stack depth, in kilobytes."),
+			NULL,
+			GUC_UNIT_KB
+		},
+		&max_stack_depth,
+		100, 100, MAX_KILOBYTES,
+		check_max_stack_depth, assign_max_stack_depth, NULL
+	},
+
+	{
+		{"temp_file_limit", PGC_SUSET, RESOURCES_DISK,
+			gettext_noop("Limits the total size of all temporary files used by each process."),
+			gettext_noop("-1 means no limit."),
+			GUC_UNIT_KB
+		},
+		&temp_file_limit,
+		-1, -1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_cost_page_hit", PGC_USERSET, RESOURCES_VACUUM_DELAY,
+			gettext_noop("Vacuum cost for a page found in the buffer cache."),
+			NULL
+		},
+		&VacuumCostPageHit,
+		1, 0, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_cost_page_miss", PGC_USERSET, RESOURCES_VACUUM_DELAY,
+			gettext_noop("Vacuum cost for a page not found in the buffer cache."),
+			NULL
+		},
+		&VacuumCostPageMiss,
+		10, 0, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_cost_page_dirty", PGC_USERSET, RESOURCES_VACUUM_DELAY,
+			gettext_noop("Vacuum cost for a page dirtied by vacuum."),
+			NULL
+		},
+		&VacuumCostPageDirty,
+		20, 0, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_cost_limit", PGC_USERSET, RESOURCES_VACUUM_DELAY,
+			gettext_noop("Vacuum cost amount available before napping."),
+			NULL
+		},
+		&VacuumCostLimit,
+		200, 1, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"autovacuum_vacuum_cost_limit", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Vacuum cost amount available before napping, for autovacuum."),
+			NULL
+		},
+		&autovacuum_vac_cost_limit,
+		-1, -1, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_files_per_process", PGC_POSTMASTER, RESOURCES_KERNEL,
+			gettext_noop("Sets the maximum number of simultaneously open files for each server process."),
+			NULL
+		},
+		&max_files_per_process,
+		1000, 25, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	/*
+	 * See also CheckRequiredParameterValues() if this parameter changes
+	 */
+	{
+		{"max_prepared_transactions", PGC_POSTMASTER, RESOURCES_MEM,
+			gettext_noop("Sets the maximum number of simultaneously prepared transactions."),
+			NULL
+		},
+		&max_prepared_xacts,
+		0, 0, MAX_BACKENDS,
+		NULL, NULL, NULL
+	},
+
+#ifdef LOCK_DEBUG
+	{
+		{"trace_lock_oidmin", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Sets the minimum OID of tables for tracking locks."),
+			gettext_noop("Is used to avoid output on system tables."),
+			GUC_NOT_IN_SAMPLE
+		},
+		&Trace_lock_oidmin,
+		FirstNormalObjectId, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"trace_lock_table", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Sets the OID of the table with unconditionally lock tracing."),
+			NULL,
+			GUC_NOT_IN_SAMPLE
+		},
+		&Trace_lock_table,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+#endif
+
+	{
+		{"statement_timeout", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the maximum allowed duration of any statement."),
+			gettext_noop("A value of 0 turns off the timeout."),
+			GUC_UNIT_MS
+		},
+		&StatementTimeout,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"lock_timeout", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the maximum allowed duration of any wait for a lock."),
+			gettext_noop("A value of 0 turns off the timeout."),
+			GUC_UNIT_MS
+		},
+		&LockTimeout,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"idle_in_transaction_session_timeout", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the maximum allowed duration of any idling transaction."),
+			gettext_noop("A value of 0 turns off the timeout."),
+			GUC_UNIT_MS
+		},
+		&IdleInTransactionSessionTimeout,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_freeze_min_age", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Minimum age at which VACUUM should freeze a table row."),
+			NULL
+		},
+		&vacuum_freeze_min_age,
+		50000000, 0, 1000000000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_freeze_table_age", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Age at which VACUUM should scan whole table to freeze tuples."),
+			NULL
+		},
+		&vacuum_freeze_table_age,
+		150000000, 0, 2000000000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_multixact_freeze_min_age", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Minimum age at which VACUUM should freeze a MultiXactId in a table row."),
+			NULL
+		},
+		&vacuum_multixact_freeze_min_age,
+		5000000, 0, 1000000000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_multixact_freeze_table_age", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Multixact age at which VACUUM should scan whole table to freeze tuples."),
+			NULL
+		},
+		&vacuum_multixact_freeze_table_age,
+		150000000, 0, 2000000000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"vacuum_defer_cleanup_age", PGC_SIGHUP, REPLICATION_MASTER,
+			gettext_noop("Number of transactions by which VACUUM and HOT cleanup should be deferred, if any."),
+			NULL
+		},
+		&vacuum_defer_cleanup_age,
+		0, 0, 1000000,
+		NULL, NULL, NULL
+	},
+
+	/*
+	 * See also CheckRequiredParameterValues() if this parameter changes
+	 */
+	{
+		{"max_locks_per_transaction", PGC_POSTMASTER, LOCK_MANAGEMENT,
+			gettext_noop("Sets the maximum number of locks per transaction."),
+			gettext_noop("The shared lock table is sized on the assumption that "
+						 "at most max_locks_per_transaction * max_connections distinct "
+						 "objects will need to be locked at any one time.")
+		},
+		&max_locks_per_xact,
+		64, 10, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_pred_locks_per_transaction", PGC_POSTMASTER, LOCK_MANAGEMENT,
+			gettext_noop("Sets the maximum number of predicate locks per transaction."),
+			gettext_noop("The shared predicate lock table is sized on the assumption that "
+						 "at most max_pred_locks_per_transaction * max_connections distinct "
+						 "objects will need to be locked at any one time.")
+		},
+		&max_predicate_locks_per_xact,
+		64, 10, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_pred_locks_per_relation", PGC_SIGHUP, LOCK_MANAGEMENT,
+			gettext_noop("Sets the maximum number of predicate-locked pages and tuples per relation."),
+			gettext_noop("If more than this total of pages and tuples in the same relation are locked "
+						 "by a connection, those locks are replaced by a relation-level lock.")
+		},
+		&max_predicate_locks_per_relation,
+		-2, INT_MIN, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_pred_locks_per_page", PGC_SIGHUP, LOCK_MANAGEMENT,
+			gettext_noop("Sets the maximum number of predicate-locked tuples per page."),
+			gettext_noop("If more than this number of tuples on the same page are locked "
+						 "by a connection, those locks are replaced by a page-level lock.")
+		},
+		&max_predicate_locks_per_page,
+		2, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"authentication_timeout", PGC_SIGHUP, CONN_AUTH_AUTH,
+			gettext_noop("Sets the maximum allowed time to complete client authentication."),
+			NULL,
+			GUC_UNIT_S
+		},
+		&AuthenticationTimeout,
+		60, 1, 600,
+		NULL, NULL, NULL
+	},
+
+	{
+		/* Not for general use */
+		{"pre_auth_delay", PGC_SIGHUP, DEVELOPER_OPTIONS,
+			gettext_noop("Waits N seconds on connection startup before authentication."),
+			gettext_noop("This allows attaching a debugger to the process."),
+			GUC_NOT_IN_SAMPLE | GUC_UNIT_S
+		},
+		&PreAuthDelay,
+		0, 0, 60,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_keep_segments", PGC_SIGHUP, REPLICATION_SENDING,
+			gettext_noop("Sets the number of WAL files held for standby servers."),
+			NULL
+		},
+		&wal_keep_segments,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"min_wal_size", PGC_SIGHUP, WAL_CHECKPOINTS,
+			gettext_noop("Sets the minimum size to shrink the WAL to."),
+			NULL,
+			GUC_UNIT_MB
+		},
+		&min_wal_size_mb,
+		DEFAULT_MIN_WAL_SEGS * (DEFAULT_XLOG_SEG_SIZE / (1024 * 1024)),
+		2, MAX_KILOBYTES,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_wal_size", PGC_SIGHUP, WAL_CHECKPOINTS,
+			gettext_noop("Sets the WAL size that triggers a checkpoint."),
+			NULL,
+			GUC_UNIT_MB
+		},
+		&max_wal_size_mb,
+		DEFAULT_MAX_WAL_SEGS * (DEFAULT_XLOG_SEG_SIZE / (1024 * 1024)),
+		2, MAX_KILOBYTES,
+		NULL, assign_max_wal_size, NULL
+	},
+
+	{
+		{"checkpoint_timeout", PGC_SIGHUP, WAL_CHECKPOINTS,
+			gettext_noop("Sets the maximum time between automatic WAL checkpoints."),
+			NULL,
+			GUC_UNIT_S
+		},
+		&CheckPointTimeout,
+		300, 30, 86400,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"checkpoint_warning", PGC_SIGHUP, WAL_CHECKPOINTS,
+			gettext_noop("Enables warnings if checkpoint segments are filled more "
+						 "frequently than this."),
+			gettext_noop("Write a message to the server log if checkpoints "
+						 "caused by the filling of checkpoint segment files happens more "
+						 "frequently than this number of seconds. Zero turns off the warning."),
+			GUC_UNIT_S
+		},
+		&CheckPointWarning,
+		30, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"checkpoint_flush_after", PGC_SIGHUP, WAL_CHECKPOINTS,
+			gettext_noop("Number of pages after which previously performed writes are flushed to disk."),
+			NULL,
+			GUC_UNIT_BLOCKS
+		},
+		&checkpoint_flush_after,
+		DEFAULT_CHECKPOINT_FLUSH_AFTER, 0, WRITEBACK_MAX_PENDING_FLUSHES,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_buffers", PGC_POSTMASTER, WAL_SETTINGS,
+			gettext_noop("Sets the number of disk-page buffers in shared memory for WAL."),
+			NULL,
+			GUC_UNIT_XBLOCKS
+		},
+		&XLOGbuffers,
+		-1, -1, (INT_MAX / XLOG_BLCKSZ),
+		check_wal_buffers, NULL, NULL
+	},
+
+	{
+		{"wal_writer_delay", PGC_SIGHUP, WAL_SETTINGS,
+			gettext_noop("Time between WAL flushes performed in the WAL writer."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&WalWriterDelay,
+		200, 1, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_writer_flush_after", PGC_SIGHUP, WAL_SETTINGS,
+			gettext_noop("Amount of WAL written out by WAL writer that triggers a flush."),
+			NULL,
+			GUC_UNIT_XBLOCKS
+		},
+		&WalWriterFlushAfter,
+		(1024 * 1024) / XLOG_BLCKSZ, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_wal_senders", PGC_POSTMASTER, REPLICATION_SENDING,
+			gettext_noop("Sets the maximum number of simultaneously running WAL sender processes."),
+			NULL
+		},
+		&max_wal_senders,
+		10, 0, MAX_BACKENDS,
+		check_max_wal_senders, NULL, NULL
+	},
+
+	{
+		/* see max_wal_senders */
+		{"max_replication_slots", PGC_POSTMASTER, REPLICATION_SENDING,
+			gettext_noop("Sets the maximum number of simultaneously defined replication slots."),
+			NULL
+		},
+		&max_replication_slots,
+		10, 0, MAX_BACKENDS /* XXX? */ ,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_sender_timeout", PGC_USERSET, REPLICATION_SENDING,
+			gettext_noop("Sets the maximum time to wait for WAL replication."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&wal_sender_timeout,
+		60 * 1000, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"commit_delay", PGC_SUSET, WAL_SETTINGS,
+			gettext_noop("Sets the delay in microseconds between transaction commit and "
+						 "flushing WAL to disk."),
+			NULL
+			/* we have no microseconds designation, so can't supply units here */
+		},
+		&CommitDelay,
+		0, 0, 100000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"commit_siblings", PGC_USERSET, WAL_SETTINGS,
+			gettext_noop("Sets the minimum concurrent open transactions before performing "
+						 "commit_delay."),
+			NULL
+		},
+		&CommitSiblings,
+		5, 0, 1000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"extra_float_digits", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the number of digits displayed for floating-point values."),
+			gettext_noop("This affects real, double precision, and geometric data types. "
+						 "A zero or negative parameter value is added to the standard "
+						 "number of digits (FLT_DIG or DBL_DIG as appropriate). "
+						 "Any value greater than zero selects precise output mode.")
+		},
+		&extra_float_digits,
+		1, -15, 3,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_min_duration_statement", PGC_SUSET, LOGGING_WHEN,
+			gettext_noop("Sets the minimum execution time above which "
+						 "statements will be logged."),
+			gettext_noop("Zero prints all queries. -1 turns this feature off."),
+			GUC_UNIT_MS
+		},
+		&log_min_duration_statement,
+		-1, -1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_autovacuum_min_duration", PGC_SIGHUP, LOGGING_WHAT,
+			gettext_noop("Sets the minimum execution time above which "
+						 "autovacuum actions will be logged."),
+			gettext_noop("Zero prints all actions. -1 turns autovacuum logging off."),
+			GUC_UNIT_MS
+		},
+		&Log_autovacuum_min_duration,
+		-1, -1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"bgwriter_delay", PGC_SIGHUP, RESOURCES_BGWRITER,
+			gettext_noop("Background writer sleep time between rounds."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&BgWriterDelay,
+		200, 10, 10000,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"bgwriter_lru_maxpages", PGC_SIGHUP, RESOURCES_BGWRITER,
+			gettext_noop("Background writer maximum number of LRU pages to flush per round."),
+			NULL
+		},
+		&bgwriter_lru_maxpages,
+		100, 0, INT_MAX / 2,	/* Same upper limit as shared_buffers */
+		NULL, NULL, NULL
+	},
+
+	{
+		{"bgwriter_flush_after", PGC_SIGHUP, RESOURCES_BGWRITER,
+			gettext_noop("Number of pages after which previously performed writes are flushed to disk."),
+			NULL,
+			GUC_UNIT_BLOCKS
+		},
+		&bgwriter_flush_after,
+		DEFAULT_BGWRITER_FLUSH_AFTER, 0, WRITEBACK_MAX_PENDING_FLUSHES,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"effective_io_concurrency",
+			PGC_USERSET,
+			RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Number of simultaneous requests that can be handled efficiently by the disk subsystem."),
+			gettext_noop("For RAID arrays, this should be approximately the number of drive spindles in the array."),
+			GUC_EXPLAIN
+		},
+		&effective_io_concurrency,
+#ifdef USE_PREFETCH
+		1,
+#else
+		0,
+#endif
+		0, MAX_IO_CONCURRENCY,
+		check_effective_io_concurrency, assign_effective_io_concurrency, NULL
+	},
+
+	{
+		{"backend_flush_after", PGC_USERSET, RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Number of pages after which previously performed writes are flushed to disk."),
+			NULL,
+			GUC_UNIT_BLOCKS
+		},
+		&backend_flush_after,
+		DEFAULT_BACKEND_FLUSH_AFTER, 0, WRITEBACK_MAX_PENDING_FLUSHES,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_worker_processes",
+			PGC_POSTMASTER,
+			RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Maximum number of concurrent worker processes."),
+			NULL,
+		},
+		&max_worker_processes,
+		8, 0, MAX_BACKENDS,
+		check_max_worker_processes, NULL, NULL
+	},
+
+	{
+		{"max_logical_replication_workers",
+			PGC_POSTMASTER,
+			REPLICATION_SUBSCRIBERS,
+			gettext_noop("Maximum number of logical replication worker processes."),
+			NULL,
+		},
+		&max_logical_replication_workers,
+		4, 0, MAX_BACKENDS,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_sync_workers_per_subscription",
+			PGC_SIGHUP,
+			REPLICATION_SUBSCRIBERS,
+			gettext_noop("Maximum number of table synchronization workers per subscription."),
+			NULL,
+		},
+		&max_sync_workers_per_subscription,
+		2, 0, MAX_BACKENDS,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_rotation_age", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Automatic log file rotation will occur after N minutes."),
+			NULL,
+			GUC_UNIT_MIN
+		},
+		&Log_RotationAge,
+		HOURS_PER_DAY * MINS_PER_HOUR, 0, INT_MAX / SECS_PER_MINUTE,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_rotation_size", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Automatic log file rotation will occur after N kilobytes."),
+			NULL,
+			GUC_UNIT_KB
+		},
+		&Log_RotationSize,
+		10 * 1024, 0, INT_MAX / 1024,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_function_args", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the maximum number of function arguments."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&max_function_args,
+		FUNC_MAX_ARGS, FUNC_MAX_ARGS, FUNC_MAX_ARGS,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_index_keys", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the maximum number of index keys."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&max_index_keys,
+		INDEX_MAX_KEYS, INDEX_MAX_KEYS, INDEX_MAX_KEYS,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_identifier_length", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the maximum identifier length."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&max_identifier_length,
+		NAMEDATALEN - 1, NAMEDATALEN - 1, NAMEDATALEN - 1,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"block_size", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the size of a disk block."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&block_size,
+		BLCKSZ, BLCKSZ, BLCKSZ,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"segment_size", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the number of pages per disk file."),
+			NULL,
+			GUC_UNIT_BLOCKS | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&segment_size,
+		RELSEG_SIZE, RELSEG_SIZE, RELSEG_SIZE,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_block_size", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the block size in the write ahead log."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&wal_block_size,
+		XLOG_BLCKSZ, XLOG_BLCKSZ, XLOG_BLCKSZ,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_retrieve_retry_interval", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Sets the time to wait before retrying to retrieve WAL "
+						 "after a failed attempt."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&wal_retrieve_retry_interval,
+		5000, 1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_segment_size", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the size of write ahead log segments."),
+			NULL,
+			GUC_UNIT_BYTE | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&wal_segment_size,
+		DEFAULT_XLOG_SEG_SIZE,
+		WalSegMinSize,
+		WalSegMaxSize,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"autovacuum_naptime", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Time to sleep between autovacuum runs."),
+			NULL,
+			GUC_UNIT_S
+		},
+		&autovacuum_naptime,
+		60, 1, INT_MAX / 1000,
+		NULL, NULL, NULL
+	},
+	{
+		{"autovacuum_vacuum_threshold", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Minimum number of tuple updates or deletes prior to vacuum."),
+			NULL
+		},
+		&autovacuum_vac_thresh,
+		50, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"autovacuum_analyze_threshold", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Minimum number of tuple inserts, updates, or deletes prior to analyze."),
+			NULL
+		},
+		&autovacuum_anl_thresh,
+		50, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		/* see varsup.c for why this is PGC_POSTMASTER not PGC_SIGHUP */
+		{"autovacuum_freeze_max_age", PGC_POSTMASTER, AUTOVACUUM,
+			gettext_noop("Age at which to autovacuum a table to prevent transaction ID wraparound."),
+			NULL
+		},
+		&autovacuum_freeze_max_age,
+		/* see pg_resetwal if you change the upper-limit value */
+		200000000, 100000, 2000000000,
+		NULL, NULL, NULL
+	},
+	{
+		/* see multixact.c for why this is PGC_POSTMASTER not PGC_SIGHUP */
+		{"autovacuum_multixact_freeze_max_age", PGC_POSTMASTER, AUTOVACUUM,
+			gettext_noop("Multixact age at which to autovacuum a table to prevent multixact wraparound."),
+			NULL
+		},
+		&autovacuum_multixact_freeze_max_age,
+		400000000, 10000, 2000000000,
+		NULL, NULL, NULL
+	},
+	{
+		/* see max_connections */
+		{"autovacuum_max_workers", PGC_POSTMASTER, AUTOVACUUM,
+			gettext_noop("Sets the maximum number of simultaneously running autovacuum worker processes."),
+			NULL
+		},
+		&autovacuum_max_workers,
+		3, 1, MAX_BACKENDS,
+		check_autovacuum_max_workers, NULL, NULL
+	},
+
+	{
+		{"max_parallel_maintenance_workers", PGC_USERSET, RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Sets the maximum number of parallel processes per maintenance operation."),
+			NULL
+		},
+		&max_parallel_maintenance_workers,
+		2, 0, 1024,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_parallel_workers_per_gather", PGC_USERSET, RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Sets the maximum number of parallel processes per executor node."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&max_parallel_workers_per_gather,
+		2, 0, MAX_PARALLEL_WORKER_LIMIT,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"max_parallel_workers", PGC_USERSET, RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Sets the maximum number of parallel workers that can be active at one time."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&max_parallel_workers,
+		8, 0, MAX_PARALLEL_WORKER_LIMIT,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"autovacuum_work_mem", PGC_SIGHUP, RESOURCES_MEM,
+			gettext_noop("Sets the maximum memory to be used by each autovacuum worker process."),
+			NULL,
+			GUC_UNIT_KB
+		},
+		&autovacuum_work_mem,
+		-1, -1, MAX_KILOBYTES,
+		check_autovacuum_work_mem, NULL, NULL
+	},
+
+	{
+		{"old_snapshot_threshold", PGC_POSTMASTER, RESOURCES_ASYNCHRONOUS,
+			gettext_noop("Time before a snapshot is too old to read pages changed after the snapshot was taken."),
+			gettext_noop("A value of -1 disables this feature."),
+			GUC_UNIT_MIN
+		},
+		&old_snapshot_threshold,
+		-1, -1, MINS_PER_HOUR * HOURS_PER_DAY * 60,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"tcp_keepalives_idle", PGC_USERSET, CLIENT_CONN_OTHER,
+			gettext_noop("Time between issuing TCP keepalives."),
+			gettext_noop("A value of 0 uses the system default."),
+			GUC_UNIT_S
+		},
+		&tcp_keepalives_idle,
+		0, 0, INT_MAX,
+		NULL, assign_tcp_keepalives_idle, show_tcp_keepalives_idle
+	},
+
+	{
+		{"tcp_keepalives_interval", PGC_USERSET, CLIENT_CONN_OTHER,
+			gettext_noop("Time between TCP keepalive retransmits."),
+			gettext_noop("A value of 0 uses the system default."),
+			GUC_UNIT_S
+		},
+		&tcp_keepalives_interval,
+		0, 0, INT_MAX,
+		NULL, assign_tcp_keepalives_interval, show_tcp_keepalives_interval
+	},
+
+	{
+		{"ssl_renegotiation_limit", PGC_USERSET, CONN_AUTH_SSL,
+			gettext_noop("SSL renegotiation is no longer supported; this can only be 0."),
+			NULL,
+			GUC_NO_SHOW_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE,
+		},
+		&ssl_renegotiation_limit,
+		0, 0, 0,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"tcp_keepalives_count", PGC_USERSET, CLIENT_CONN_OTHER,
+			gettext_noop("Maximum number of TCP keepalive retransmits."),
+			gettext_noop("This controls the number of consecutive keepalive retransmits that can be "
+						 "lost before a connection is considered dead. A value of 0 uses the "
+						 "system default."),
+		},
+		&tcp_keepalives_count,
+		0, 0, INT_MAX,
+		NULL, assign_tcp_keepalives_count, show_tcp_keepalives_count
+	},
+
+	{
+		{"gin_fuzzy_search_limit", PGC_USERSET, CLIENT_CONN_OTHER,
+			gettext_noop("Sets the maximum allowed result for exact search by GIN."),
+			NULL,
+			0
+		},
+		&GinFuzzySearchLimit,
+		0, 0, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"effective_cache_size", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's assumption about the total size of the data caches."),
+			gettext_noop("That is, the total size of the caches (kernel cache and shared buffers) used for PostgreSQL data files. "
+						 "This is measured in disk pages, which are normally 8 kB each."),
+			GUC_UNIT_BLOCKS | GUC_EXPLAIN,
+		},
+		&effective_cache_size,
+		DEFAULT_EFFECTIVE_CACHE_SIZE, 1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"min_parallel_table_scan_size", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the minimum amount of table data for a parallel scan."),
+			gettext_noop("If the planner estimates that it will read a number of table pages too small to reach this limit, a parallel scan will not be considered."),
+			GUC_UNIT_BLOCKS | GUC_EXPLAIN,
+		},
+		&min_parallel_table_scan_size,
+		(8 * 1024 * 1024) / BLCKSZ, 0, INT_MAX / 3,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"min_parallel_index_scan_size", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the minimum amount of index data for a parallel scan."),
+			gettext_noop("If the planner estimates that it will read a number of index pages too small to reach this limit, a parallel scan will not be considered."),
+			GUC_UNIT_BLOCKS | GUC_EXPLAIN,
+		},
+		&min_parallel_index_scan_size,
+		(512 * 1024) / BLCKSZ, 0, INT_MAX / 3,
+		NULL, NULL, NULL
+	},
+
+	{
+		/* Can't be set in postgresql.conf */
+		{"server_version_num", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the server version as an integer."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&server_version_num,
+		PG_VERSION_NUM, PG_VERSION_NUM, PG_VERSION_NUM,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_temp_files", PGC_SUSET, LOGGING_WHAT,
+			gettext_noop("Log the use of temporary files larger than this number of kilobytes."),
+			gettext_noop("Zero logs all files. The default is -1 (turning this feature off)."),
+			GUC_UNIT_KB
+		},
+		&log_temp_files,
+		-1, -1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"track_activity_query_size", PGC_POSTMASTER, RESOURCES_MEM,
+			gettext_noop("Sets the size reserved for pg_stat_activity.query, in bytes."),
+			NULL,
+			GUC_UNIT_BYTE
+		},
+		&pgstat_track_activity_query_size,
+		1024, 100, 102400,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"gin_pending_list_limit", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the maximum size of the pending list for GIN index."),
+			NULL,
+			GUC_UNIT_KB
+		},
+		&gin_pending_list_limit,
+		4096, 64, MAX_KILOBYTES,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"tcp_user_timeout", PGC_USERSET, CLIENT_CONN_OTHER,
+			gettext_noop("TCP user timeout."),
+			gettext_noop("A value of 0 uses the system default."),
+			GUC_UNIT_MS
+		},
+		&tcp_user_timeout,
+		0, 0, INT_MAX,
+		NULL, assign_tcp_user_timeout, show_tcp_user_timeout
+	},
+
+	/* End-of-list marker */
+	{
+		{NULL, 0, 0, NULL, NULL}, NULL, 0, 0, 0, NULL, NULL, NULL
+	}
+};
+
+
+static struct config_real ConfigureNamesReal[] =
+{
+	{
+		{"seq_page_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of a "
+						 "sequentially fetched disk page."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&seq_page_cost,
+		DEFAULT_SEQ_PAGE_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"random_page_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of a "
+						 "nonsequentially fetched disk page."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&random_page_cost,
+		DEFAULT_RANDOM_PAGE_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"cpu_tuple_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of "
+						 "processing each tuple (row)."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&cpu_tuple_cost,
+		DEFAULT_CPU_TUPLE_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"cpu_index_tuple_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of "
+						 "processing each index entry during an index scan."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&cpu_index_tuple_cost,
+		DEFAULT_CPU_INDEX_TUPLE_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"cpu_operator_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of "
+						 "processing each operator or function call."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&cpu_operator_cost,
+		DEFAULT_CPU_OPERATOR_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"parallel_tuple_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of "
+						 "passing each tuple (row) from worker to master backend."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&parallel_tuple_cost,
+		DEFAULT_PARALLEL_TUPLE_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+	{
+		{"parallel_setup_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Sets the planner's estimate of the cost of "
+						 "starting up worker processes for parallel query."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&parallel_setup_cost,
+		DEFAULT_PARALLEL_SETUP_COST, 0, DBL_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_above_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Perform JIT compilation if query is more expensive."),
+			gettext_noop("-1 disables JIT compilation."),
+			GUC_EXPLAIN
+		},
+		&jit_above_cost,
+		100000, -1, DBL_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_optimize_above_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Optimize JITed functions if query is more expensive."),
+			gettext_noop("-1 disables optimization."),
+			GUC_EXPLAIN
+		},
+		&jit_optimize_above_cost,
+		500000, -1, DBL_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"jit_inline_above_cost", PGC_USERSET, QUERY_TUNING_COST,
+			gettext_noop("Perform JIT inlining if query is more expensive."),
+			gettext_noop("-1 disables inlining."),
+			GUC_EXPLAIN
+		},
+		&jit_inline_above_cost,
+		500000, -1, DBL_MAX,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"cursor_tuple_fraction", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Sets the planner's estimate of the fraction of "
+						 "a cursor's rows that will be retrieved."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&cursor_tuple_fraction,
+		DEFAULT_CURSOR_TUPLE_FRACTION, 0.0, 1.0,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"geqo_selection_bias", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("GEQO: selective pressure within the population."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&Geqo_selection_bias,
+		DEFAULT_GEQO_SELECTION_BIAS,
+		MIN_GEQO_SELECTION_BIAS, MAX_GEQO_SELECTION_BIAS,
+		NULL, NULL, NULL
+	},
+	{
+		{"geqo_seed", PGC_USERSET, QUERY_TUNING_GEQO,
+			gettext_noop("GEQO: seed for random path selection."),
+			NULL,
+			GUC_EXPLAIN
+		},
+		&Geqo_seed,
+		0.0, 0.0, 1.0,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"bgwriter_lru_multiplier", PGC_SIGHUP, RESOURCES_BGWRITER,
+			gettext_noop("Multiple of the average buffer usage to free per round."),
+			NULL
+		},
+		&bgwriter_lru_multiplier,
+		2.0, 0.0, 10.0,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"seed", PGC_USERSET, UNGROUPED,
+			gettext_noop("Sets the seed for random-number generation."),
+			NULL,
+			GUC_NO_SHOW_ALL | GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&phony_random_seed,
+		0.0, -1.0, 1.0,
+		check_random_seed, assign_random_seed, show_random_seed
+	},
+
+	{
+		{"vacuum_cost_delay", PGC_USERSET, RESOURCES_VACUUM_DELAY,
+			gettext_noop("Vacuum cost delay in milliseconds."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&VacuumCostDelay,
+		0, 0, 100,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"autovacuum_vacuum_cost_delay", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Vacuum cost delay in milliseconds, for autovacuum."),
+			NULL,
+			GUC_UNIT_MS
+		},
+		&autovacuum_vac_cost_delay,
+		2, -1, 100,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"autovacuum_vacuum_scale_factor", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Number of tuple updates or deletes prior to vacuum as a fraction of reltuples."),
+			NULL
+		},
+		&autovacuum_vac_scale,
+		0.2, 0.0, 100.0,
+		NULL, NULL, NULL
+	},
+	{
+		{"autovacuum_analyze_scale_factor", PGC_SIGHUP, AUTOVACUUM,
+			gettext_noop("Number of tuple inserts, updates, or deletes prior to analyze as a fraction of reltuples."),
+			NULL
+		},
+		&autovacuum_anl_scale,
+		0.1, 0.0, 100.0,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"checkpoint_completion_target", PGC_SIGHUP, WAL_CHECKPOINTS,
+			gettext_noop("Time spent flushing dirty buffers during checkpoint, as fraction of checkpoint interval."),
+			NULL
+		},
+		&CheckPointCompletionTarget,
+		0.5, 0.0, 1.0,
+		NULL, assign_checkpoint_completion_target, NULL
+	},
+
+	{
+		{"vacuum_cleanup_index_scale_factor", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Number of tuple inserts prior to index cleanup as a fraction of reltuples."),
+			NULL
+		},
+		&vacuum_cleanup_index_scale_factor,
+		0.1, 0.0, 1e10,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_transaction_sample_rate", PGC_SUSET, LOGGING_WHEN,
+			gettext_noop("Set the fraction of transactions to log for new transactions."),
+			gettext_noop("Logs all statements from a fraction of transactions. "
+						 "Use a value between 0.0 (never log) and 1.0 (log all "
+						 "statements for all transactions).")
+		},
+		&log_xact_sample_rate,
+		0.0, 0.0, 1.0,
+		NULL, NULL, NULL
+	},
+
+	/* End-of-list marker */
+	{
+		{NULL, 0, 0, NULL, NULL}, NULL, 0.0, 0.0, 0.0, NULL, NULL, NULL
+	}
+};
+
+
+static struct config_string ConfigureNamesString[] =
+{
+	{
+		{"archive_command", PGC_SIGHUP, WAL_ARCHIVING,
+			gettext_noop("Sets the shell command that will be called to archive a WAL file."),
+			NULL
+		},
+		&XLogArchiveCommand,
+		"",
+		NULL, NULL, show_archive_command
+	},
+
+	{
+		{"restore_command", PGC_POSTMASTER, WAL_ARCHIVE_RECOVERY,
+			gettext_noop("Sets the shell command that will retrieve an archived WAL file."),
+			NULL
+		},
+		&recoveryRestoreCommand,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"archive_cleanup_command", PGC_SIGHUP, WAL_ARCHIVE_RECOVERY,
+			gettext_noop("Sets the shell command that will be executed at every restart point."),
+			NULL
+		},
+		&archiveCleanupCommand,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"recovery_end_command", PGC_SIGHUP, WAL_ARCHIVE_RECOVERY,
+			gettext_noop("Sets the shell command that will be executed once at the end of recovery."),
+			NULL
+		},
+		&recoveryEndCommand,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"recovery_target_timeline", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Specifies the timeline to recover into."),
+			NULL
+		},
+		&recovery_target_timeline_string,
+		"latest",
+		check_recovery_target_timeline, assign_recovery_target_timeline, NULL
+	},
+
+	{
+		{"recovery_target", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Set to \"immediate\" to end recovery as soon as a consistent state is reached."),
+			NULL
+		},
+		&recovery_target_string,
+		"",
+		check_recovery_target, assign_recovery_target, NULL
+	},
+	{
+		{"recovery_target_xid", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Sets the transaction ID up to which recovery will proceed."),
+			NULL
+		},
+		&recovery_target_xid_string,
+		"",
+		check_recovery_target_xid, assign_recovery_target_xid, NULL
+	},
+	{
+		{"recovery_target_time", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Sets the time stamp up to which recovery will proceed."),
+			NULL
+		},
+		&recovery_target_time_string,
+		"",
+		check_recovery_target_time, assign_recovery_target_time, NULL
+	},
+	{
+		{"recovery_target_name", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Sets the named restore point up to which recovery will proceed."),
+			NULL
+		},
+		&recovery_target_name_string,
+		"",
+		check_recovery_target_name, assign_recovery_target_name, NULL
+	},
+	{
+		{"recovery_target_lsn", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Sets the LSN of the write-ahead log location up to which recovery will proceed."),
+			NULL
+		},
+		&recovery_target_lsn_string,
+		"",
+		check_recovery_target_lsn, assign_recovery_target_lsn, NULL
+	},
+
+	{
+		{"promote_trigger_file", PGC_SIGHUP, REPLICATION_STANDBY,
+			gettext_noop("Specifies a file name whose presence ends recovery in the standby."),
+			NULL
+		},
+		&PromoteTriggerFile,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"primary_conninfo", PGC_POSTMASTER, REPLICATION_STANDBY,
+			gettext_noop("Sets the connection string to be used to connect to the sending server."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&PrimaryConnInfo,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"primary_slot_name", PGC_POSTMASTER, REPLICATION_STANDBY,
+			gettext_noop("Sets the name of the replication slot to use on the sending server."),
+			NULL
+		},
+		&PrimarySlotName,
+		"",
+		check_primary_slot_name, NULL, NULL
+	},
+
+	{
+		{"client_encoding", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the client's character set encoding."),
+			NULL,
+			GUC_IS_NAME | GUC_REPORT
+		},
+		&client_encoding_string,
+		"SQL_ASCII",
+		check_client_encoding, assign_client_encoding, NULL
+	},
+
+	{
+		{"log_line_prefix", PGC_SIGHUP, LOGGING_WHAT,
+			gettext_noop("Controls information prefixed to each log line."),
+			gettext_noop("If blank, no prefix is used.")
+		},
+		&Log_line_prefix,
+		"%m [%p] ",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_timezone", PGC_SIGHUP, LOGGING_WHAT,
+			gettext_noop("Sets the time zone to use in log messages."),
+			NULL
+		},
+		&log_timezone_string,
+		"GMT",
+		check_log_timezone, assign_log_timezone, show_log_timezone
+	},
+
+	{
+		{"DateStyle", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the display format for date and time values."),
+			gettext_noop("Also controls interpretation of ambiguous "
+						 "date inputs."),
+			GUC_LIST_INPUT | GUC_REPORT
+		},
+		&datestyle_string,
+		"ISO, MDY",
+		check_datestyle, assign_datestyle, NULL
+	},
+
+	{
+		{"default_table_access_method", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the default table access method for new tables."),
+			NULL,
+			GUC_IS_NAME
+		},
+		&default_table_access_method,
+		DEFAULT_TABLE_ACCESS_METHOD,
+		check_default_table_access_method, NULL, NULL
+	},
+
+	{
+		{"default_tablespace", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the default tablespace to create tables and indexes in."),
+			gettext_noop("An empty string selects the database's default tablespace."),
+			GUC_IS_NAME
+		},
+		&default_tablespace,
+		"",
+		check_default_tablespace, NULL, NULL
+	},
+
+	{
+		{"temp_tablespaces", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the tablespace(s) to use for temporary tables and sort files."),
+			NULL,
+			GUC_LIST_INPUT | GUC_LIST_QUOTE
+		},
+		&temp_tablespaces,
+		"",
+		check_temp_tablespaces, assign_temp_tablespaces, NULL
+	},
+
+	{
+		{"dynamic_library_path", PGC_SUSET, CLIENT_CONN_OTHER,
+			gettext_noop("Sets the path for dynamically loadable modules."),
+			gettext_noop("If a dynamically loadable module needs to be opened and "
+						 "the specified name does not have a directory component (i.e., the "
+						 "name does not contain a slash), the system will search this path for "
+						 "the specified file."),
+			GUC_SUPERUSER_ONLY
+		},
+		&Dynamic_library_path,
+		"$libdir",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"krb_server_keyfile", PGC_SIGHUP, CONN_AUTH_AUTH,
+			gettext_noop("Sets the location of the Kerberos server key file."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&pg_krb_server_keyfile,
+		PG_KRB_SRVTAB,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"bonjour_name", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the Bonjour service name."),
+			NULL
+		},
+		&bonjour_name,
+		"",
+		NULL, NULL, NULL
+	},
+
+	/* See main.c about why defaults for LC_foo are not all alike */
+
+	{
+		{"lc_collate", PGC_INTERNAL, CLIENT_CONN_LOCALE,
+			gettext_noop("Shows the collation order locale."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&locale_collate,
+		"C",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"lc_ctype", PGC_INTERNAL, CLIENT_CONN_LOCALE,
+			gettext_noop("Shows the character classification and case conversion locale."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&locale_ctype,
+		"C",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"lc_messages", PGC_SUSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the language in which messages are displayed."),
+			NULL
+		},
+		&locale_messages,
+		"",
+		check_locale_messages, assign_locale_messages, NULL
+	},
+
+	{
+		{"lc_monetary", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the locale for formatting monetary amounts."),
+			NULL
+		},
+		&locale_monetary,
+		"C",
+		check_locale_monetary, assign_locale_monetary, NULL
+	},
+
+	{
+		{"lc_numeric", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the locale for formatting numbers."),
+			NULL
+		},
+		&locale_numeric,
+		"C",
+		check_locale_numeric, assign_locale_numeric, NULL
+	},
+
+	{
+		{"lc_time", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the locale for formatting date and time values."),
+			NULL
+		},
+		&locale_time,
+		"C",
+		check_locale_time, assign_locale_time, NULL
+	},
+
+	{
+		{"session_preload_libraries", PGC_SUSET, CLIENT_CONN_PRELOAD,
+			gettext_noop("Lists shared libraries to preload into each backend."),
+			NULL,
+			GUC_LIST_INPUT | GUC_LIST_QUOTE | GUC_SUPERUSER_ONLY
+		},
+		&session_preload_libraries_string,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"shared_preload_libraries", PGC_POSTMASTER, CLIENT_CONN_PRELOAD,
+			gettext_noop("Lists shared libraries to preload into server."),
+			NULL,
+			GUC_LIST_INPUT | GUC_LIST_QUOTE | GUC_SUPERUSER_ONLY
+		},
+		&shared_preload_libraries_string,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"local_preload_libraries", PGC_USERSET, CLIENT_CONN_PRELOAD,
+			gettext_noop("Lists unprivileged shared libraries to preload into each backend."),
+			NULL,
+			GUC_LIST_INPUT | GUC_LIST_QUOTE
+		},
+		&local_preload_libraries_string,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"search_path", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the schema search order for names that are not schema-qualified."),
+			NULL,
+			GUC_LIST_INPUT | GUC_LIST_QUOTE | GUC_EXPLAIN
+		},
+		&namespace_search_path,
+		"\"$user\", public",
+		check_search_path, assign_search_path, NULL
+	},
+
+	{
+		/* Can't be set in postgresql.conf */
+		{"server_encoding", PGC_INTERNAL, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the server (database) character set encoding."),
+			NULL,
+			GUC_IS_NAME | GUC_REPORT | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&server_encoding_string,
+		"SQL_ASCII",
+		NULL, NULL, NULL
+	},
+
+	{
+		/* Can't be set in postgresql.conf */
+		{"server_version", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Shows the server version."),
+			NULL,
+			GUC_REPORT | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&server_version_string,
+		PG_VERSION,
+		NULL, NULL, NULL
+	},
+
+	{
+		/* Not for general use --- used by SET ROLE */
+		{"role", PGC_USERSET, UNGROUPED,
+			gettext_noop("Sets the current role."),
+			NULL,
+			GUC_IS_NAME | GUC_NO_SHOW_ALL | GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE | GUC_NOT_WHILE_SEC_REST
+		},
+		&role_string,
+		"none",
+		check_role, assign_role, show_role
+	},
+
+	{
+		/* Not for general use --- used by SET SESSION AUTHORIZATION */
+		{"session_authorization", PGC_USERSET, UNGROUPED,
+			gettext_noop("Sets the session user name."),
+			NULL,
+			GUC_IS_NAME | GUC_REPORT | GUC_NO_SHOW_ALL | GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE | GUC_NOT_WHILE_SEC_REST
+		},
+		&session_authorization_string,
+		NULL,
+		check_session_authorization, assign_session_authorization, NULL
+	},
+
+	{
+		{"log_destination", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Sets the destination for server log output."),
+			gettext_noop("Valid values are combinations of \"stderr\", "
+						 "\"syslog\", \"csvlog\", and \"eventlog\", "
+						 "depending on the platform."),
+			GUC_LIST_INPUT
+		},
+		&Log_destination_string,
+		"stderr",
+		check_log_destination, assign_log_destination, NULL
+	},
+	{
+		{"log_directory", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Sets the destination directory for log files."),
+			gettext_noop("Can be specified as relative to the data directory "
+						 "or as absolute path."),
+			GUC_SUPERUSER_ONLY
+		},
+		&Log_directory,
+		"log",
+		check_canonical_path, NULL, NULL
+	},
+	{
+		{"log_filename", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Sets the file name pattern for log files."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&Log_filename,
+		"postgresql-%Y-%m-%d_%H%M%S.log",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"syslog_ident", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Sets the program name used to identify PostgreSQL "
+						 "messages in syslog."),
+			NULL
+		},
+		&syslog_ident_str,
+		"postgres",
+		NULL, assign_syslog_ident, NULL
+	},
+
+	{
+		{"event_source", PGC_POSTMASTER, LOGGING_WHERE,
+			gettext_noop("Sets the application name used to identify "
+						 "PostgreSQL messages in the event log."),
+			NULL
+		},
+		&event_source,
+		DEFAULT_EVENT_SOURCE,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"TimeZone", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the time zone for displaying and interpreting time stamps."),
+			NULL,
+			GUC_REPORT
+		},
+		&timezone_string,
+		"GMT",
+		check_timezone, assign_timezone, show_timezone
+	},
+	{
+		{"timezone_abbreviations", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Selects a file of time zone abbreviations."),
+			NULL
+		},
+		&timezone_abbreviations_string,
+		NULL,
+		check_timezone_abbreviations, assign_timezone_abbreviations, NULL
+	},
+
+	{
+		{"unix_socket_group", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the owning group of the Unix-domain socket."),
+			gettext_noop("The owning user of the socket is always the user "
+						 "that starts the server.")
+		},
+		&Unix_socket_group,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"unix_socket_directories", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the directories where Unix-domain sockets will be created."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&Unix_socket_directories,
+#ifdef HAVE_UNIX_SOCKETS
+		DEFAULT_PGSOCKET_DIR,
+#else
+		"",
+#endif
+		NULL, NULL, NULL
+	},
+
+	{
+		{"listen_addresses", PGC_POSTMASTER, CONN_AUTH_SETTINGS,
+			gettext_noop("Sets the host name or IP address(es) to listen to."),
+			NULL,
+			GUC_LIST_INPUT
+		},
+		&ListenAddresses,
+		"localhost",
+		NULL, NULL, NULL
+	},
+
+	{
+		/*
+		 * Can't be set by ALTER SYSTEM as it can lead to recursive definition
+		 * of data_directory.
+		 */
+		{"data_directory", PGC_POSTMASTER, FILE_LOCATIONS,
+			gettext_noop("Sets the server's data directory."),
+			NULL,
+			GUC_SUPERUSER_ONLY | GUC_DISALLOW_IN_AUTO_FILE
+		},
+		&data_directory,
+		NULL,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"config_file", PGC_POSTMASTER, FILE_LOCATIONS,
+			gettext_noop("Sets the server's main configuration file."),
+			NULL,
+			GUC_DISALLOW_IN_FILE | GUC_SUPERUSER_ONLY
+		},
+		&ConfigFileName,
+		NULL,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"hba_file", PGC_POSTMASTER, FILE_LOCATIONS,
+			gettext_noop("Sets the server's \"hba\" configuration file."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&HbaFileName,
+		NULL,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ident_file", PGC_POSTMASTER, FILE_LOCATIONS,
+			gettext_noop("Sets the server's \"ident\" configuration file."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&IdentFileName,
+		NULL,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"external_pid_file", PGC_POSTMASTER, FILE_LOCATIONS,
+			gettext_noop("Writes the postmaster PID to the specified file."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&external_pid_file,
+		NULL,
+		check_canonical_path, NULL, NULL
+	},
+
+	{
+		{"ssl_library", PGC_INTERNAL, PRESET_OPTIONS,
+			gettext_noop("Name of the SSL library."),
+			NULL,
+			GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&ssl_library,
+#ifdef USE_SSL
+		"OpenSSL",
+#else
+		"",
+#endif
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_cert_file", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Location of the SSL server certificate file."),
+			NULL
+		},
+		&ssl_cert_file,
+		"server.crt",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_key_file", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Location of the SSL server private key file."),
+			NULL
+		},
+		&ssl_key_file,
+		"server.key",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_ca_file", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Location of the SSL certificate authority file."),
+			NULL
+		},
+		&ssl_ca_file,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_crl_file", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Location of the SSL certificate revocation list file."),
+			NULL
+		},
+		&ssl_crl_file,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"stats_temp_directory", PGC_SIGHUP, STATS_COLLECTOR,
+			gettext_noop("Writes temporary statistics files to the specified directory."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&pgstat_temp_directory,
+		PG_STAT_TMP_DIR,
+		check_canonical_path, assign_pgstat_temp_directory, NULL
+	},
+
+	{
+		{"synchronous_standby_names", PGC_SIGHUP, REPLICATION_MASTER,
+			gettext_noop("Number of synchronous standbys and list of names of potential synchronous ones."),
+			NULL,
+			GUC_LIST_INPUT
+		},
+		&SyncRepStandbyNames,
+		"",
+		check_synchronous_standby_names, assign_synchronous_standby_names, NULL
+	},
+
+	{
+		{"default_text_search_config", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets default text search configuration."),
+			NULL
+		},
+		&TSCurrentConfig,
+		"pg_catalog.simple",
+		check_TSCurrentConfig, assign_TSCurrentConfig, NULL
+	},
+
+	{
+		{"ssl_ciphers", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Sets the list of allowed SSL ciphers."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&SSLCipherSuites,
+#ifdef USE_SSL
+		"HIGH:MEDIUM:+3DES:!aNULL",
+#else
+		"none",
+#endif
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_ecdh_curve", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Sets the curve to use for ECDH."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&SSLECDHCurve,
+#ifdef USE_SSL
+		"prime256v1",
+#else
+		"none",
+#endif
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_dh_params_file", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Location of the SSL DH parameters file."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&ssl_dh_params_file,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_passphrase_command", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Command to obtain passphrases for SSL."),
+			NULL
+		},
+		&ssl_passphrase_command,
+		"",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"application_name", PGC_USERSET, LOGGING_WHAT,
+			gettext_noop("Sets the application name to be reported in statistics and logs."),
+			NULL,
+			GUC_IS_NAME | GUC_REPORT | GUC_NOT_IN_SAMPLE
+		},
+		&application_name,
+		"",
+		check_application_name, assign_application_name, NULL
+	},
+
+	{
+		{"cluster_name", PGC_POSTMASTER, PROCESS_TITLE,
+			gettext_noop("Sets the name of the cluster, which is included in the process title."),
+			NULL,
+			GUC_IS_NAME
+		},
+		&cluster_name,
+		"",
+		check_cluster_name, NULL, NULL
+	},
+
+	{
+		{"wal_consistency_checking", PGC_SUSET, DEVELOPER_OPTIONS,
+			gettext_noop("Sets the WAL resource managers for which WAL consistency checks are done."),
+			gettext_noop("Full-page images will be logged for all data blocks and cross-checked against the results of WAL replay."),
+			GUC_LIST_INPUT | GUC_NOT_IN_SAMPLE
+		},
+		&wal_consistency_checking_string,
+		"",
+		check_wal_consistency_checking, assign_wal_consistency_checking, NULL
+	},
+
+	{
+		{"jit_provider", PGC_POSTMASTER, CLIENT_CONN_PRELOAD,
+			gettext_noop("JIT provider to use."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&jit_provider,
+		"llvmjit",
+		NULL, NULL, NULL
+	},
+
+	{
+		{"restrict_nonsystem_relation_kind", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets relation kinds of non-system relation to restrict use"),
+			NULL,
+			GUC_LIST_INPUT | GUC_NOT_IN_SAMPLE
+		},
+		&restrict_nonsystem_relation_kind_string,
+		"",
+		check_restrict_nonsystem_relation_kind, assign_restrict_nonsystem_relation_kind, NULL
+	},
+
+	/* End-of-list marker */
+	{
+		{NULL, 0, 0, NULL, NULL}, NULL, NULL, NULL, NULL, NULL
+	}
+};
+
+
+static struct config_enum ConfigureNamesEnum[] =
+{
+	{
+		{"backslash_quote", PGC_USERSET, COMPAT_OPTIONS_PREVIOUS,
+			gettext_noop("Sets whether \"\\'\" is allowed in string literals."),
+			NULL
+		},
+		&backslash_quote,
+		BACKSLASH_QUOTE_SAFE_ENCODING, backslash_quote_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"bytea_output", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the output format for bytea."),
+			NULL
+		},
+		&bytea_output,
+		BYTEA_OUTPUT_HEX, bytea_output_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"client_min_messages", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the message levels that are sent to the client."),
+			gettext_noop("Each level includes all the levels that follow it. The later"
+						 " the level, the fewer messages are sent.")
+		},
+		&client_min_messages,
+		NOTICE, client_message_level_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"constraint_exclusion", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Enables the planner to use constraints to optimize queries."),
+			gettext_noop("Table scans will be skipped if their constraints"
+						 " guarantee that no rows match the query."),
+			GUC_EXPLAIN
+		},
+		&constraint_exclusion,
+		CONSTRAINT_EXCLUSION_PARTITION, constraint_exclusion_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"default_transaction_isolation", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the transaction isolation level of each new transaction."),
+			NULL
+		},
+		&DefaultXactIsoLevel,
+		XACT_READ_COMMITTED, isolation_level_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"transaction_isolation", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the current transaction's isolation level."),
+			NULL,
+			GUC_NO_RESET_ALL | GUC_NOT_IN_SAMPLE | GUC_DISALLOW_IN_FILE
+		},
+		&XactIsoLevel,
+		XACT_READ_COMMITTED, isolation_level_options,
+		check_XactIsoLevel, NULL, NULL
+	},
+
+	{
+		{"IntervalStyle", PGC_USERSET, CLIENT_CONN_LOCALE,
+			gettext_noop("Sets the display format for interval values."),
+			NULL,
+			GUC_REPORT
+		},
+		&IntervalStyle,
+		INTSTYLE_POSTGRES, intervalstyle_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_error_verbosity", PGC_SUSET, LOGGING_WHAT,
+			gettext_noop("Sets the verbosity of logged messages."),
+			NULL
+		},
+		&Log_error_verbosity,
+		PGERROR_DEFAULT, log_error_verbosity_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_min_messages", PGC_SUSET, LOGGING_WHEN,
+			gettext_noop("Sets the message levels that are logged."),
+			gettext_noop("Each level includes all the levels that follow it. The later"
+						 " the level, the fewer messages are sent.")
+		},
+		&log_min_messages,
+		WARNING, server_message_level_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_min_error_statement", PGC_SUSET, LOGGING_WHEN,
+			gettext_noop("Causes all statements generating error at or above this level to be logged."),
+			gettext_noop("Each level includes all the levels that follow it. The later"
+						 " the level, the fewer messages are sent.")
+		},
+		&log_min_error_statement,
+		ERROR, server_message_level_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"log_statement", PGC_SUSET, LOGGING_WHAT,
+			gettext_noop("Sets the type of statements logged."),
+			NULL
+		},
+		&log_statement,
+		LOGSTMT_NONE, log_statement_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"syslog_facility", PGC_SIGHUP, LOGGING_WHERE,
+			gettext_noop("Sets the syslog \"facility\" to be used when syslog enabled."),
+			NULL
+		},
+		&syslog_facility,
+#ifdef HAVE_SYSLOG
+		LOG_LOCAL0,
+#else
+		0,
+#endif
+		syslog_facility_options,
+		NULL, assign_syslog_facility, NULL
+	},
+
+	{
+		{"session_replication_role", PGC_SUSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets the session's behavior for triggers and rewrite rules."),
+			NULL
+		},
+		&SessionReplicationRole,
+		SESSION_REPLICATION_ROLE_ORIGIN, session_replication_role_options,
+		NULL, assign_session_replication_role, NULL
+	},
+
+	{
+		{"synchronous_commit", PGC_USERSET, WAL_SETTINGS,
+			gettext_noop("Sets the current transaction's synchronization level."),
+			NULL
+		},
+		&synchronous_commit,
+		SYNCHRONOUS_COMMIT_ON, synchronous_commit_options,
+		NULL, assign_synchronous_commit, NULL
+	},
+
+	{
+		{"archive_mode", PGC_POSTMASTER, WAL_ARCHIVING,
+			gettext_noop("Allows archiving of WAL files using archive_command."),
+			NULL
+		},
+		&XLogArchiveMode,
+		ARCHIVE_MODE_OFF, archive_mode_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"recovery_target_action", PGC_POSTMASTER, WAL_RECOVERY_TARGET,
+			gettext_noop("Sets the action to perform upon reaching the recovery target."),
+			NULL
+		},
+		&recoveryTargetAction,
+		RECOVERY_TARGET_ACTION_PAUSE, recovery_target_action_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"trace_recovery_messages", PGC_SIGHUP, DEVELOPER_OPTIONS,
+			gettext_noop("Enables logging of recovery-related debugging information."),
+			gettext_noop("Each level includes all the levels that follow it. The later"
+						 " the level, the fewer messages are sent.")
+		},
+		&trace_recovery_messages,
+
+		/*
+		 * client_message_level_options allows too many values, really, but
+		 * it's not worth having a separate options array for this.
+		 */
+		LOG, client_message_level_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"track_functions", PGC_SUSET, STATS_COLLECTOR,
+			gettext_noop("Collects function-level statistics on database activity."),
+			NULL
+		},
+		&pgstat_track_functions,
+		TRACK_FUNC_OFF, track_function_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_level", PGC_POSTMASTER, WAL_SETTINGS,
+			gettext_noop("Set the level of information written to the WAL."),
+			NULL
+		},
+		&wal_level,
+		WAL_LEVEL_REPLICA, wal_level_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"dynamic_shared_memory_type", PGC_POSTMASTER, RESOURCES_MEM,
+			gettext_noop("Selects the dynamic shared memory implementation used."),
+			NULL
+		},
+		&dynamic_shared_memory_type,
+		DEFAULT_DYNAMIC_SHARED_MEMORY_TYPE, dynamic_shared_memory_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"shared_memory_type", PGC_POSTMASTER, RESOURCES_MEM,
+			gettext_noop("Selects the shared memory implementation used for the main shared memory region."),
+			NULL
+		},
+		&shared_memory_type,
+		DEFAULT_SHARED_MEMORY_TYPE, shared_memory_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"wal_sync_method", PGC_SIGHUP, WAL_SETTINGS,
+			gettext_noop("Selects the method used for forcing WAL updates to disk."),
+			NULL
+		},
+		&sync_method,
+		DEFAULT_SYNC_METHOD, sync_method_options,
+		NULL, assign_xlog_sync_method, NULL
+	},
+
+	{
+		{"xmlbinary", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets how binary values are to be encoded in XML."),
+			NULL
+		},
+		&xmlbinary,
+		XMLBINARY_BASE64, xmlbinary_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"xmloption", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("Sets whether XML data in implicit parsing and serialization "
+						 "operations is to be considered as documents or content fragments."),
+			NULL
+		},
+		&xmloption,
+		XMLOPTION_CONTENT, xmloption_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"huge_pages", PGC_POSTMASTER, RESOURCES_MEM,
+			gettext_noop("Use of huge pages on Linux or Windows."),
+			NULL
+		},
+		&huge_pages,
+		HUGE_PAGES_TRY, huge_pages_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"force_parallel_mode", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Forces use of parallel query facilities."),
+			gettext_noop("If possible, run query using a parallel worker and with parallel restrictions."),
+			GUC_EXPLAIN
+		},
+		&force_parallel_mode,
+		FORCE_PARALLEL_OFF, force_parallel_mode_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"password_encryption", PGC_USERSET, CONN_AUTH_AUTH,
+			gettext_noop("Chooses the algorithm for encrypting passwords."),
+			NULL
+		},
+		&Password_encryption,
+		PASSWORD_TYPE_MD5, password_encryption_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"plan_cache_mode", PGC_USERSET, QUERY_TUNING_OTHER,
+			gettext_noop("Controls the planner's selection of custom or generic plan."),
+			gettext_noop("Prepared statements can have custom and generic plans, and the planner "
+						 "will attempt to choose which is better.  This can be set to override "
+						 "the default behavior."),
+			GUC_EXPLAIN
+		},
+		&plan_cache_mode,
+		PLAN_CACHE_MODE_AUTO, plan_cache_mode_options,
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_min_protocol_version", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Sets the minimum SSL/TLS protocol version to use."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&ssl_min_protocol_version,
+		PG_TLS1_VERSION,
+		ssl_protocol_versions_info + 1, /* don't allow PG_TLS_ANY */
+		NULL, NULL, NULL
+	},
+
+	{
+		{"ssl_max_protocol_version", PGC_SIGHUP, CONN_AUTH_SSL,
+			gettext_noop("Sets the maximum SSL/TLS protocol version to use."),
+			NULL,
+			GUC_SUPERUSER_ONLY
+		},
+		&ssl_max_protocol_version,
+		PG_TLS_ANY,
+		ssl_protocol_versions_info,
+		NULL, NULL, NULL
+	},
+
+	/* End-of-list marker */
+	{
+		{NULL, 0, 0, NULL, NULL}, NULL, 0, NULL, NULL, NULL, NULL
+	}
+};
+
+/******** end of options list ********/
+
+
+/*
+ * To allow continued support of obsolete names for GUC variables, we apply
+ * the following mappings to any unrecognized name.  Note that an old name
+ * should be mapped to a new one only if the new variable has very similar
+ * semantics to the old.
+ */
+static const char *const map_old_guc_names[] = {
+	"sort_mem", "work_mem",
+	"vacuum_mem", "maintenance_work_mem",
+	NULL
+};
+
+
+/*
+ * Actual lookup of variables is done through this single, sorted array.
+ */
+static struct config_generic **guc_variables;
+
+/* Current number of variables contained in the vector */
+static int	num_guc_variables;
+
+/* Vector capacity */
+static int	size_guc_variables;
+
+
+static bool guc_dirty;			/* true if need to do commit/abort work */
+
+static bool reporting_enabled;	/* true to enable GUC_REPORT */
+
+static int	GUCNestLevel = 0;	/* 1 when in main transaction */
+
+
+static int	guc_var_compare(const void *a, const void *b);
+static int	guc_name_compare(const char *namea, const char *nameb);
+static void InitializeGUCOptionsFromEnvironment(void);
+static void InitializeOneGUCOption(struct config_generic *gconf);
+static void push_old_value(struct config_generic *gconf, GucAction action);
+static void ReportGUCOption(struct config_generic *record);
+static void reapply_stacked_values(struct config_generic *variable,
+								   struct config_string *pHolder,
+								   GucStack *stack,
+								   const char *curvalue,
+								   GucContext curscontext, GucSource cursource);
+static void ShowGUCConfigOption(const char *name, DestReceiver *dest);
+static void ShowAllGUCConfig(DestReceiver *dest);
+static char *_ShowOption(struct config_generic *record, bool use_units);
+static bool validate_option_array_item(const char *name, const char *value,
+									   bool skipIfNoPermissions);
+static void write_auto_conf_file(int fd, const char *filename, ConfigVariable *head_p);
+static void replace_auto_config_value(ConfigVariable **head_p, ConfigVariable **tail_p,
+									  const char *name, const char *value);
+
+
+/*
+ * Some infrastructure for checking malloc/strdup/realloc calls
+ */
+static void *
+guc_malloc(int elevel, size_t size)
+{
+	void	   *data;
+
+	/* Avoid unportable behavior of malloc(0) */
+	if (size == 0)
+		size = 1;
+	data = malloc(size);
+	if (data == NULL)
+		ereport(elevel,
+				(errcode(ERRCODE_OUT_OF_MEMORY),
+				 errmsg("out of memory")));
+	return data;
+}
+
+static void *
+guc_realloc(int elevel, void *old, size_t size)
+{
+	void	   *data;
+
+	/* Avoid unportable behavior of realloc(NULL, 0) */
+	if (old == NULL && size == 0)
+		size = 1;
+	data = realloc(old, size);
+	if (data == NULL)
+		ereport(elevel,
+				(errcode(ERRCODE_OUT_OF_MEMORY),
+				 errmsg("out of memory")));
+	return data;
+}
+
+static char *
+guc_strdup(int elevel, const char *src)
+{
+	char	   *data;
+
+	data = strdup(src);
+	if (data == NULL)
+		ereport(elevel,
+				(errcode(ERRCODE_OUT_OF_MEMORY),
+				 errmsg("out of memory")));
+	return data;
+}
+
+
+/*
+ * Detect whether strval is referenced anywhere in a GUC string item
+ */
+static bool
+string_field_used(struct config_string *conf, char *strval)
+{
+	GucStack   *stack;
+
+	if (strval == *(conf->variable) ||
+		strval == conf->reset_val ||
+		strval == conf->boot_val)
+		return true;
+	for (stack = conf->gen.stack; stack; stack = stack->prev)
+	{
+		if (strval == stack->prior.val.stringval ||
+			strval == stack->masked.val.stringval)
+			return true;
+	}
+	return false;
+}
+
+/*
+ * Support for assigning to a field of a string GUC item.  Free the prior
+ * value if it's not referenced anywhere else in the item (including stacked
+ * states).
+ */
+static void
+set_string_field(struct config_string *conf, char **field, char *newval)
+{
+	char	   *oldval = *field;
+
+	/* Do the assignment */
+	*field = newval;
+
+	/* Free old value if it's not NULL and isn't referenced anymore */
+	if (oldval && !string_field_used(conf, oldval))
+		free(oldval);
+}
+
+/*
+ * Detect whether an "extra" struct is referenced anywhere in a GUC item
+ */
+static bool
+extra_field_used(struct config_generic *gconf, void *extra)
+{
+	GucStack   *stack;
+
+	if (extra == gconf->extra)
+		return true;
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+			if (extra == ((struct config_bool *) gconf)->reset_extra)
+				return true;
+			break;
+		case PGC_INT:
+			if (extra == ((struct config_int *) gconf)->reset_extra)
+				return true;
+			break;
+		case PGC_REAL:
+			if (extra == ((struct config_real *) gconf)->reset_extra)
+				return true;
+			break;
+		case PGC_STRING:
+			if (extra == ((struct config_string *) gconf)->reset_extra)
+				return true;
+			break;
+		case PGC_ENUM:
+			if (extra == ((struct config_enum *) gconf)->reset_extra)
+				return true;
+			break;
+	}
+	for (stack = gconf->stack; stack; stack = stack->prev)
+	{
+		if (extra == stack->prior.extra ||
+			extra == stack->masked.extra)
+			return true;
+	}
+
+	return false;
+}
+
+/*
+ * Support for assigning to an "extra" field of a GUC item.  Free the prior
+ * value if it's not referenced anywhere else in the item (including stacked
+ * states).
+ */
+static void
+set_extra_field(struct config_generic *gconf, void **field, void *newval)
+{
+	void	   *oldval = *field;
+
+	/* Do the assignment */
+	*field = newval;
+
+	/* Free old value if it's not NULL and isn't referenced anymore */
+	if (oldval && !extra_field_used(gconf, oldval))
+		free(oldval);
+}
+
+/*
+ * Support for copying a variable's active value into a stack entry.
+ * The "extra" field associated with the active value is copied, too.
+ *
+ * NB: be sure stringval and extra fields of a new stack entry are
+ * initialized to NULL before this is used, else we'll try to free() them.
+ */
+static void
+set_stack_value(struct config_generic *gconf, config_var_value *val)
+{
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+			val->val.boolval =
+				*((struct config_bool *) gconf)->variable;
+			break;
+		case PGC_INT:
+			val->val.intval =
+				*((struct config_int *) gconf)->variable;
+			break;
+		case PGC_REAL:
+			val->val.realval =
+				*((struct config_real *) gconf)->variable;
+			break;
+		case PGC_STRING:
+			set_string_field((struct config_string *) gconf,
+							 &(val->val.stringval),
+							 *((struct config_string *) gconf)->variable);
+			break;
+		case PGC_ENUM:
+			val->val.enumval =
+				*((struct config_enum *) gconf)->variable;
+			break;
+	}
+	set_extra_field(gconf, &(val->extra), gconf->extra);
+}
+
+/*
+ * Support for discarding a no-longer-needed value in a stack entry.
+ * The "extra" field associated with the stack entry is cleared, too.
+ */
+static void
+discard_stack_value(struct config_generic *gconf, config_var_value *val)
+{
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+		case PGC_INT:
+		case PGC_REAL:
+		case PGC_ENUM:
+			/* no need to do anything */
+			break;
+		case PGC_STRING:
+			set_string_field((struct config_string *) gconf,
+							 &(val->val.stringval),
+							 NULL);
+			break;
+	}
+	set_extra_field(gconf, &(val->extra), NULL);
+}
+
+
+/*
+ * Fetch the sorted array pointer (exported for help_config.c's use ONLY)
+ */
+struct config_generic **
+get_guc_variables(void)
+{
+	return guc_variables;
+}
+
+
+/*
+ * Build the sorted array.  This is split out so that it could be
+ * re-executed after startup (e.g., we could allow loadable modules to
+ * add vars, and then we'd need to re-sort).
+ */
+void
+build_guc_variables(void)
+{
+	int			size_vars;
+	int			num_vars = 0;
+	struct config_generic **guc_vars;
+	int			i;
+
+	for (i = 0; ConfigureNamesBool[i].gen.name; i++)
+	{
+		struct config_bool *conf = &ConfigureNamesBool[i];
+
+		/* Rather than requiring vartype to be filled in by hand, do this: */
+		conf->gen.vartype = PGC_BOOL;
+		num_vars++;
+	}
+
+	for (i = 0; ConfigureNamesInt[i].gen.name; i++)
+	{
+		struct config_int *conf = &ConfigureNamesInt[i];
+
+		conf->gen.vartype = PGC_INT;
+		num_vars++;
+	}
+
+	for (i = 0; ConfigureNamesReal[i].gen.name; i++)
+	{
+		struct config_real *conf = &ConfigureNamesReal[i];
+
+		conf->gen.vartype = PGC_REAL;
+		num_vars++;
+	}
+
+	for (i = 0; ConfigureNamesString[i].gen.name; i++)
+	{
+		struct config_string *conf = &ConfigureNamesString[i];
+
+		conf->gen.vartype = PGC_STRING;
+		num_vars++;
+	}
+
+	for (i = 0; ConfigureNamesEnum[i].gen.name; i++)
+	{
+		struct config_enum *conf = &ConfigureNamesEnum[i];
+
+		conf->gen.vartype = PGC_ENUM;
+		num_vars++;
+	}
+
+	/*
+	 * Create table with 20% slack
+	 */
+	size_vars = num_vars + num_vars / 4;
+
+	guc_vars = (struct config_generic **)
+		guc_malloc(FATAL, size_vars * sizeof(struct config_generic *));
+
+	num_vars = 0;
+
+	for (i = 0; ConfigureNamesBool[i].gen.name; i++)
+		guc_vars[num_vars++] = &ConfigureNamesBool[i].gen;
+
+	for (i = 0; ConfigureNamesInt[i].gen.name; i++)
+		guc_vars[num_vars++] = &ConfigureNamesInt[i].gen;
+
+	for (i = 0; ConfigureNamesReal[i].gen.name; i++)
+		guc_vars[num_vars++] = &ConfigureNamesReal[i].gen;
+
+	for (i = 0; ConfigureNamesString[i].gen.name; i++)
+		guc_vars[num_vars++] = &ConfigureNamesString[i].gen;
+
+	for (i = 0; ConfigureNamesEnum[i].gen.name; i++)
+		guc_vars[num_vars++] = &ConfigureNamesEnum[i].gen;
+
+	if (guc_variables)
+		free(guc_variables);
+	guc_variables = guc_vars;
+	num_guc_variables = num_vars;
+	size_guc_variables = size_vars;
+	qsort((void *) guc_variables, num_guc_variables,
+		  sizeof(struct config_generic *), guc_var_compare);
+}
+
+/*
+ * Add a new GUC variable to the list of known variables. The
+ * list is expanded if needed.
+ */
+static bool
+add_guc_variable(struct config_generic *var, int elevel)
+{
+	if (num_guc_variables + 1 >= size_guc_variables)
+	{
+		/*
+		 * Increase the vector by 25%
+		 */
+		int			size_vars = size_guc_variables + size_guc_variables / 4;
+		struct config_generic **guc_vars;
+
+		if (size_vars == 0)
+		{
+			size_vars = 100;
+			guc_vars = (struct config_generic **)
+				guc_malloc(elevel, size_vars * sizeof(struct config_generic *));
+		}
+		else
+		{
+			guc_vars = (struct config_generic **)
+				guc_realloc(elevel, guc_variables, size_vars * sizeof(struct config_generic *));
+		}
+
+		if (guc_vars == NULL)
+			return false;		/* out of memory */
+
+		guc_variables = guc_vars;
+		size_guc_variables = size_vars;
+	}
+	guc_variables[num_guc_variables++] = var;
+	qsort((void *) guc_variables, num_guc_variables,
+		  sizeof(struct config_generic *), guc_var_compare);
+	return true;
+}
+
+/*
+ * Create and add a placeholder variable for a custom variable name.
+ */
+static struct config_generic *
+add_placeholder_variable(const char *name, int elevel)
+{
+	size_t		sz = sizeof(struct config_string) + sizeof(char *);
+	struct config_string *var;
+	struct config_generic *gen;
+
+	var = (struct config_string *) guc_malloc(elevel, sz);
+	if (var == NULL)
+		return NULL;
+	memset(var, 0, sz);
+	gen = &var->gen;
+
+	gen->name = guc_strdup(elevel, name);
+	if (gen->name == NULL)
+	{
+		free(var);
+		return NULL;
+	}
+
+	gen->context = PGC_USERSET;
+	gen->group = CUSTOM_OPTIONS;
+	gen->short_desc = "GUC placeholder variable";
+	gen->flags = GUC_NO_SHOW_ALL | GUC_NOT_IN_SAMPLE | GUC_CUSTOM_PLACEHOLDER;
+	gen->vartype = PGC_STRING;
+
+	/*
+	 * The char* is allocated at the end of the struct since we have no
+	 * 'static' place to point to.  Note that the current value, as well as
+	 * the boot and reset values, start out NULL.
+	 */
+	var->variable = (char **) (var + 1);
+
+	if (!add_guc_variable((struct config_generic *) var, elevel))
+	{
+		free(unconstify(char *, gen->name));
+		free(var);
+		return NULL;
+	}
+
+	return gen;
+}
+
+/*
+ * Look up option NAME.  If it exists, return a pointer to its record,
+ * else return NULL.  If create_placeholders is true, we'll create a
+ * placeholder record for a valid-looking custom variable name.
+ */
+static struct config_generic *
+find_option(const char *name, bool create_placeholders, int elevel)
+{
+	const char **key = &name;
+	struct config_generic **res;
+	int			i;
+
+	Assert(name);
+
+	/*
+	 * By equating const char ** with struct config_generic *, we are assuming
+	 * the name field is first in config_generic.
+	 */
+	res = (struct config_generic **) bsearch((void *) &key,
+											 (void *) guc_variables,
+											 num_guc_variables,
+											 sizeof(struct config_generic *),
+											 guc_var_compare);
+	if (res)
+		return *res;
+
+	/*
+	 * See if the name is an obsolete name for a variable.  We assume that the
+	 * set of supported old names is short enough that a brute-force search is
+	 * the best way.
+	 */
+	for (i = 0; map_old_guc_names[i] != NULL; i += 2)
+	{
+		if (guc_name_compare(name, map_old_guc_names[i]) == 0)
+			return find_option(map_old_guc_names[i + 1], false, elevel);
+	}
+
+	if (create_placeholders)
+	{
+		/*
+		 * Check if the name is qualified, and if so, add a placeholder.
+		 */
+		if (strchr(name, GUC_QUALIFIER_SEPARATOR) != NULL)
+			return add_placeholder_variable(name, elevel);
+	}
+
+	/* Unknown name */
+	return NULL;
+}
+
+
+/*
+ * comparator for qsorting and bsearching guc_variables array
+ */
+static int
+guc_var_compare(const void *a, const void *b)
+{
+	const char *namea = **(const char **const *) a;
+	const char *nameb = **(const char **const *) b;
+
+	return guc_name_compare(namea, nameb);
+}
+
+/*
+ * the bare comparison function for GUC names
+ */
+static int
+guc_name_compare(const char *namea, const char *nameb)
+{
+	/*
+	 * The temptation to use strcasecmp() here must be resisted, because the
+	 * array ordering has to remain stable across setlocale() calls. So, build
+	 * our own with a simple ASCII-only downcasing.
+	 */
+	while (*namea && *nameb)
+	{
+		char		cha = *namea++;
+		char		chb = *nameb++;
+
+		if (cha >= 'A' && cha <= 'Z')
+			cha += 'a' - 'A';
+		if (chb >= 'A' && chb <= 'Z')
+			chb += 'a' - 'A';
+		if (cha != chb)
+			return cha - chb;
+	}
+	if (*namea)
+		return 1;				/* a is longer */
+	if (*nameb)
+		return -1;				/* b is longer */
+	return 0;
+}
+
+
+/*
+ * Initialize GUC options during program startup.
+ *
+ * Note that we cannot read the config file yet, since we have not yet
+ * processed command-line switches.
+ */
+void
+InitializeGUCOptions(void)
+{
+	int			i;
+
+	/*
+	 * Before log_line_prefix could possibly receive a nonempty setting, make
+	 * sure that timezone processing is minimally alive (see elog.c).
+	 */
+	pg_timezone_initialize();
+
+	/*
+	 * Build sorted array of all GUC variables.
+	 */
+	build_guc_variables();
+
+	/*
+	 * Load all variables with their compiled-in defaults, and initialize
+	 * status fields as needed.
+	 */
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		InitializeOneGUCOption(guc_variables[i]);
+	}
+
+	guc_dirty = false;
+
+	reporting_enabled = false;
+
+	/*
+	 * Prevent any attempt to override the transaction modes from
+	 * non-interactive sources.
+	 */
+	SetConfigOption("transaction_isolation", "read committed",
+					PGC_POSTMASTER, PGC_S_OVERRIDE);
+	SetConfigOption("transaction_read_only", "no",
+					PGC_POSTMASTER, PGC_S_OVERRIDE);
+	SetConfigOption("transaction_deferrable", "no",
+					PGC_POSTMASTER, PGC_S_OVERRIDE);
+
+	/*
+	 * For historical reasons, some GUC parameters can receive defaults from
+	 * environment variables.  Process those settings.
+	 */
+	InitializeGUCOptionsFromEnvironment();
+}
+
+/*
+ * Assign any GUC values that can come from the server's environment.
+ *
+ * This is called from InitializeGUCOptions, and also from ProcessConfigFile
+ * to deal with the possibility that a setting has been removed from
+ * postgresql.conf and should now get a value from the environment.
+ * (The latter is a kludge that should probably go away someday; if so,
+ * fold this back into InitializeGUCOptions.)
+ */
+static void
+InitializeGUCOptionsFromEnvironment(void)
+{
+	char	   *env;
+	long		stack_rlimit;
+
+	env = getenv("PGPORT");
+	if (env != NULL)
+		SetConfigOption("port", env, PGC_POSTMASTER, PGC_S_ENV_VAR);
+
+	env = getenv("PGDATESTYLE");
+	if (env != NULL)
+		SetConfigOption("datestyle", env, PGC_POSTMASTER, PGC_S_ENV_VAR);
+
+	env = getenv("PGCLIENTENCODING");
+	if (env != NULL)
+		SetConfigOption("client_encoding", env, PGC_POSTMASTER, PGC_S_ENV_VAR);
+
+	/*
+	 * rlimit isn't exactly an "environment variable", but it behaves about
+	 * the same.  If we can identify the platform stack depth rlimit, increase
+	 * default stack depth setting up to whatever is safe (but at most 2MB).
+	 */
+	stack_rlimit = get_stack_depth_rlimit();
+	if (stack_rlimit > 0)
+	{
+		long		new_limit = (stack_rlimit - STACK_DEPTH_SLOP) / 1024L;
+
+		if (new_limit > 100)
+		{
+			char		limbuf[16];
+
+			new_limit = Min(new_limit, 2048);
+			sprintf(limbuf, "%ld", new_limit);
+			SetConfigOption("max_stack_depth", limbuf,
+							PGC_POSTMASTER, PGC_S_ENV_VAR);
+		}
+	}
+}
+
+/*
+ * Initialize one GUC option variable to its compiled-in default.
+ *
+ * Note: the reason for calling check_hooks is not that we think the boot_val
+ * might fail, but that the hooks might wish to compute an "extra" struct.
+ */
+static void
+InitializeOneGUCOption(struct config_generic *gconf)
+{
+	gconf->status = 0;
+	gconf->source = PGC_S_DEFAULT;
+	gconf->reset_source = PGC_S_DEFAULT;
+	gconf->scontext = PGC_INTERNAL;
+	gconf->reset_scontext = PGC_INTERNAL;
+	gconf->stack = NULL;
+	gconf->extra = NULL;
+	gconf->sourcefile = NULL;
+	gconf->sourceline = 0;
+
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *conf = (struct config_bool *) gconf;
+				bool		newval = conf->boot_val;
+				void	   *extra = NULL;
+
+				if (!call_bool_check_hook(conf, &newval, &extra,
+										  PGC_S_DEFAULT, LOG))
+					elog(FATAL, "failed to initialize %s to %d",
+						 conf->gen.name, (int) newval);
+				if (conf->assign_hook)
+					conf->assign_hook(newval, extra);
+				*conf->variable = conf->reset_val = newval;
+				conf->gen.extra = conf->reset_extra = extra;
+				break;
+			}
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) gconf;
+				int			newval = conf->boot_val;
+				void	   *extra = NULL;
+
+				Assert(newval >= conf->min);
+				Assert(newval <= conf->max);
+				if (!call_int_check_hook(conf, &newval, &extra,
+										 PGC_S_DEFAULT, LOG))
+					elog(FATAL, "failed to initialize %s to %d",
+						 conf->gen.name, newval);
+				if (conf->assign_hook)
+					conf->assign_hook(newval, extra);
+				*conf->variable = conf->reset_val = newval;
+				conf->gen.extra = conf->reset_extra = extra;
+				break;
+			}
+		case PGC_REAL:
+			{
+				struct config_real *conf = (struct config_real *) gconf;
+				double		newval = conf->boot_val;
+				void	   *extra = NULL;
+
+				Assert(newval >= conf->min);
+				Assert(newval <= conf->max);
+				if (!call_real_check_hook(conf, &newval, &extra,
+										  PGC_S_DEFAULT, LOG))
+					elog(FATAL, "failed to initialize %s to %g",
+						 conf->gen.name, newval);
+				if (conf->assign_hook)
+					conf->assign_hook(newval, extra);
+				*conf->variable = conf->reset_val = newval;
+				conf->gen.extra = conf->reset_extra = extra;
+				break;
+			}
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) gconf;
+				char	   *newval;
+				void	   *extra = NULL;
+
+				/* non-NULL boot_val must always get strdup'd */
+				if (conf->boot_val != NULL)
+					newval = guc_strdup(FATAL, conf->boot_val);
+				else
+					newval = NULL;
+
+				if (!call_string_check_hook(conf, &newval, &extra,
+											PGC_S_DEFAULT, LOG))
+					elog(FATAL, "failed to initialize %s to \"%s\"",
+						 conf->gen.name, newval ? newval : "");
+				if (conf->assign_hook)
+					conf->assign_hook(newval, extra);
+				*conf->variable = conf->reset_val = newval;
+				conf->gen.extra = conf->reset_extra = extra;
+				break;
+			}
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) gconf;
+				int			newval = conf->boot_val;
+				void	   *extra = NULL;
+
+				if (!call_enum_check_hook(conf, &newval, &extra,
+										  PGC_S_DEFAULT, LOG))
+					elog(FATAL, "failed to initialize %s to %d",
+						 conf->gen.name, newval);
+				if (conf->assign_hook)
+					conf->assign_hook(newval, extra);
+				*conf->variable = conf->reset_val = newval;
+				conf->gen.extra = conf->reset_extra = extra;
+				break;
+			}
+	}
+}
+
+
+/*
+ * Select the configuration files and data directory to be used, and
+ * do the initial read of postgresql.conf.
+ *
+ * This is called after processing command-line switches.
+ *		userDoption is the -D switch value if any (NULL if unspecified).
+ *		progname is just for use in error messages.
+ *
+ * Returns true on success; on failure, prints a suitable error message
+ * to stderr and returns false.
+ */
+bool
+SelectConfigFiles(const char *userDoption, const char *progname)
+{
+	char	   *configdir;
+	char	   *fname;
+	struct stat stat_buf;
+
+	/* configdir is -D option, or $PGDATA if no -D */
+	if (userDoption)
+		configdir = make_absolute_path(userDoption);
+	else
+		configdir = make_absolute_path(getenv("PGDATA"));
+
+	if (configdir && stat(configdir, &stat_buf) != 0)
+	{
+		write_stderr("%s: could not access directory \"%s\": %s\n",
+					 progname,
+					 configdir,
+					 strerror(errno));
+		if (errno == ENOENT)
+			write_stderr("Run initdb or pg_basebackup to initialize a PostgreSQL data directory.\n");
+		return false;
+	}
+
+	/*
+	 * Find the configuration file: if config_file was specified on the
+	 * command line, use it, else use configdir/postgresql.conf.  In any case
+	 * ensure the result is an absolute path, so that it will be interpreted
+	 * the same way by future backends.
+	 */
+	if (ConfigFileName)
+		fname = make_absolute_path(ConfigFileName);
+	else if (configdir)
+	{
+		fname = guc_malloc(FATAL,
+						   strlen(configdir) + strlen(CONFIG_FILENAME) + 2);
+		sprintf(fname, "%s/%s", configdir, CONFIG_FILENAME);
+	}
+	else
+	{
+		write_stderr("%s does not know where to find the server configuration file.\n"
+					 "You must specify the --config-file or -D invocation "
+					 "option or set the PGDATA environment variable.\n",
+					 progname);
+		return false;
+	}
+
+	/*
+	 * Set the ConfigFileName GUC variable to its final value, ensuring that
+	 * it can't be overridden later.
+	 */
+	SetConfigOption("config_file", fname, PGC_POSTMASTER, PGC_S_OVERRIDE);
+	free(fname);
+
+	/*
+	 * Now read the config file for the first time.
+	 */
+	if (stat(ConfigFileName, &stat_buf) != 0)
+	{
+		write_stderr("%s: could not access the server configuration file \"%s\": %s\n",
+					 progname, ConfigFileName, strerror(errno));
+		free(configdir);
+		return false;
+	}
+
+	/*
+	 * Read the configuration file for the first time.  This time only the
+	 * data_directory parameter is picked up to determine the data directory,
+	 * so that we can read the PG_AUTOCONF_FILENAME file next time.
+	 */
+	ProcessConfigFile(PGC_POSTMASTER);
+
+	/*
+	 * If the data_directory GUC variable has been set, use that as DataDir;
+	 * otherwise use configdir if set; else punt.
+	 *
+	 * Note: SetDataDir will copy and absolute-ize its argument, so we don't
+	 * have to.
+	 */
+	if (data_directory)
+		SetDataDir(data_directory);
+	else if (configdir)
+		SetDataDir(configdir);
+	else
+	{
+		write_stderr("%s does not know where to find the database system data.\n"
+					 "This can be specified as \"data_directory\" in \"%s\", "
+					 "or by the -D invocation option, or by the "
+					 "PGDATA environment variable.\n",
+					 progname, ConfigFileName);
+		return false;
+	}
+
+	/*
+	 * Reflect the final DataDir value back into the data_directory GUC var.
+	 * (If you are wondering why we don't just make them a single variable,
+	 * it's because the EXEC_BACKEND case needs DataDir to be transmitted to
+	 * child backends specially.  XXX is that still true?  Given that we now
+	 * chdir to DataDir, EXEC_BACKEND can read the config file without knowing
+	 * DataDir in advance.)
+	 */
+	SetConfigOption("data_directory", DataDir, PGC_POSTMASTER, PGC_S_OVERRIDE);
+
+	/*
+	 * Now read the config file a second time, allowing any settings in the
+	 * PG_AUTOCONF_FILENAME file to take effect.  (This is pretty ugly, but
+	 * since we have to determine the DataDir before we can find the autoconf
+	 * file, the alternatives seem worse.)
+	 */
+	ProcessConfigFile(PGC_POSTMASTER);
+
+	/*
+	 * If timezone_abbreviations wasn't set in the configuration file, install
+	 * the default value.  We do it this way because we can't safely install a
+	 * "real" value until my_exec_path is set, which may not have happened
+	 * when InitializeGUCOptions runs, so the bootstrap default value cannot
+	 * be the real desired default.
+	 */
+	pg_timezone_abbrev_initialize();
+
+	/*
+	 * Figure out where pg_hba.conf is, and make sure the path is absolute.
+	 */
+	if (HbaFileName)
+		fname = make_absolute_path(HbaFileName);
+	else if (configdir)
+	{
+		fname = guc_malloc(FATAL,
+						   strlen(configdir) + strlen(HBA_FILENAME) + 2);
+		sprintf(fname, "%s/%s", configdir, HBA_FILENAME);
+	}
+	else
+	{
+		write_stderr("%s does not know where to find the \"hba\" configuration file.\n"
+					 "This can be specified as \"hba_file\" in \"%s\", "
+					 "or by the -D invocation option, or by the "
+					 "PGDATA environment variable.\n",
+					 progname, ConfigFileName);
+		return false;
+	}
+	SetConfigOption("hba_file", fname, PGC_POSTMASTER, PGC_S_OVERRIDE);
+	free(fname);
+
+	/*
+	 * Likewise for pg_ident.conf.
+	 */
+	if (IdentFileName)
+		fname = make_absolute_path(IdentFileName);
+	else if (configdir)
+	{
+		fname = guc_malloc(FATAL,
+						   strlen(configdir) + strlen(IDENT_FILENAME) + 2);
+		sprintf(fname, "%s/%s", configdir, IDENT_FILENAME);
+	}
+	else
+	{
+		write_stderr("%s does not know where to find the \"ident\" configuration file.\n"
+					 "This can be specified as \"ident_file\" in \"%s\", "
+					 "or by the -D invocation option, or by the "
+					 "PGDATA environment variable.\n",
+					 progname, ConfigFileName);
+		return false;
+	}
+	SetConfigOption("ident_file", fname, PGC_POSTMASTER, PGC_S_OVERRIDE);
+	free(fname);
+
+	free(configdir);
+
+	return true;
+}
+
+
+/*
+ * Reset all options to their saved default values (implements RESET ALL)
+ */
+void
+ResetAllOptions(void)
+{
+	int			i;
+
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		struct config_generic *gconf = guc_variables[i];
+
+		/* Don't reset non-SET-able values */
+		if (gconf->context != PGC_SUSET &&
+			gconf->context != PGC_USERSET)
+			continue;
+		/* Don't reset if special exclusion from RESET ALL */
+		if (gconf->flags & GUC_NO_RESET_ALL)
+			continue;
+		/* No need to reset if wasn't SET */
+		if (gconf->source <= PGC_S_OVERRIDE)
+			continue;
+
+		/* Save old value to support transaction abort */
+		push_old_value(gconf, GUC_ACTION_SET);
+
+		switch (gconf->vartype)
+		{
+			case PGC_BOOL:
+				{
+					struct config_bool *conf = (struct config_bool *) gconf;
+
+					if (conf->assign_hook)
+						conf->assign_hook(conf->reset_val,
+										  conf->reset_extra);
+					*conf->variable = conf->reset_val;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									conf->reset_extra);
+					break;
+				}
+			case PGC_INT:
+				{
+					struct config_int *conf = (struct config_int *) gconf;
+
+					if (conf->assign_hook)
+						conf->assign_hook(conf->reset_val,
+										  conf->reset_extra);
+					*conf->variable = conf->reset_val;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									conf->reset_extra);
+					break;
+				}
+			case PGC_REAL:
+				{
+					struct config_real *conf = (struct config_real *) gconf;
+
+					if (conf->assign_hook)
+						conf->assign_hook(conf->reset_val,
+										  conf->reset_extra);
+					*conf->variable = conf->reset_val;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									conf->reset_extra);
+					break;
+				}
+			case PGC_STRING:
+				{
+					struct config_string *conf = (struct config_string *) gconf;
+
+					if (conf->assign_hook)
+						conf->assign_hook(conf->reset_val,
+										  conf->reset_extra);
+					set_string_field(conf, conf->variable, conf->reset_val);
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									conf->reset_extra);
+					break;
+				}
+			case PGC_ENUM:
+				{
+					struct config_enum *conf = (struct config_enum *) gconf;
+
+					if (conf->assign_hook)
+						conf->assign_hook(conf->reset_val,
+										  conf->reset_extra);
+					*conf->variable = conf->reset_val;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									conf->reset_extra);
+					break;
+				}
+		}
+
+		gconf->source = gconf->reset_source;
+		gconf->scontext = gconf->reset_scontext;
+
+		if (gconf->flags & GUC_REPORT)
+			ReportGUCOption(gconf);
+	}
+}
+
+
+/*
+ * push_old_value
+ *		Push previous state during transactional assignment to a GUC variable.
+ */
+static void
+push_old_value(struct config_generic *gconf, GucAction action)
+{
+	GucStack   *stack;
+
+	/* If we're not inside a nest level, do nothing */
+	if (GUCNestLevel == 0)
+		return;
+
+	/* Do we already have a stack entry of the current nest level? */
+	stack = gconf->stack;
+	if (stack && stack->nest_level >= GUCNestLevel)
+	{
+		/* Yes, so adjust its state if necessary */
+		Assert(stack->nest_level == GUCNestLevel);
+		switch (action)
+		{
+			case GUC_ACTION_SET:
+				/* SET overrides any prior action at same nest level */
+				if (stack->state == GUC_SET_LOCAL)
+				{
+					/* must discard old masked value */
+					discard_stack_value(gconf, &stack->masked);
+				}
+				stack->state = GUC_SET;
+				break;
+			case GUC_ACTION_LOCAL:
+				if (stack->state == GUC_SET)
+				{
+					/* SET followed by SET LOCAL, remember SET's value */
+					stack->masked_scontext = gconf->scontext;
+					set_stack_value(gconf, &stack->masked);
+					stack->state = GUC_SET_LOCAL;
+				}
+				/* in all other cases, no change to stack entry */
+				break;
+			case GUC_ACTION_SAVE:
+				/* Could only have a prior SAVE of same variable */
+				Assert(stack->state == GUC_SAVE);
+				break;
+		}
+		Assert(guc_dirty);		/* must be set already */
+		return;
+	}
+
+	/*
+	 * Push a new stack entry
+	 *
+	 * We keep all the stack entries in TopTransactionContext for simplicity.
+	 */
+	stack = (GucStack *) MemoryContextAllocZero(TopTransactionContext,
+												sizeof(GucStack));
+
+	stack->prev = gconf->stack;
+	stack->nest_level = GUCNestLevel;
+	switch (action)
+	{
+		case GUC_ACTION_SET:
+			stack->state = GUC_SET;
+			break;
+		case GUC_ACTION_LOCAL:
+			stack->state = GUC_LOCAL;
+			break;
+		case GUC_ACTION_SAVE:
+			stack->state = GUC_SAVE;
+			break;
+	}
+	stack->source = gconf->source;
+	stack->scontext = gconf->scontext;
+	set_stack_value(gconf, &stack->prior);
+
+	gconf->stack = stack;
+
+	/* Ensure we remember to pop at end of xact */
+	guc_dirty = true;
+}
+
+
+/*
+ * Do GUC processing at main transaction start.
+ */
+void
+AtStart_GUC(void)
+{
+	/*
+	 * The nest level should be 0 between transactions; if it isn't, somebody
+	 * didn't call AtEOXact_GUC, or called it with the wrong nestLevel.  We
+	 * throw a warning but make no other effort to clean up.
+	 */
+	if (GUCNestLevel != 0)
+		elog(WARNING, "GUC nest level = %d at transaction start",
+			 GUCNestLevel);
+	GUCNestLevel = 1;
+}
+
+/*
+ * Enter a new nesting level for GUC values.  This is called at subtransaction
+ * start, and when entering a function that has proconfig settings, and in
+ * some other places where we want to set GUC variables transiently.
+ * NOTE we must not risk error here, else subtransaction start will be unhappy.
+ */
+int
+NewGUCNestLevel(void)
+{
+	return ++GUCNestLevel;
+}
+
+/*
+ * Do GUC processing at transaction or subtransaction commit or abort, or
+ * when exiting a function that has proconfig settings, or when undoing a
+ * transient assignment to some GUC variables.  (The name is thus a bit of
+ * a misnomer; perhaps it should be ExitGUCNestLevel or some such.)
+ * During abort, we discard all GUC settings that were applied at nesting
+ * levels >= nestLevel.  nestLevel == 1 corresponds to the main transaction.
+ */
+void
+AtEOXact_GUC(bool isCommit, int nestLevel)
+{
+	bool		still_dirty;
+	int			i;
+
+	/*
+	 * Note: it's possible to get here with GUCNestLevel == nestLevel-1 during
+	 * abort, if there is a failure during transaction start before
+	 * AtStart_GUC is called.
+	 */
+	Assert(nestLevel > 0 &&
+		   (nestLevel <= GUCNestLevel ||
+			(nestLevel == GUCNestLevel + 1 && !isCommit)));
+
+	/* Quick exit if nothing's changed in this transaction */
+	if (!guc_dirty)
+	{
+		GUCNestLevel = nestLevel - 1;
+		return;
+	}
+
+	still_dirty = false;
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		struct config_generic *gconf = guc_variables[i];
+		GucStack   *stack;
+
+		/*
+		 * Process and pop each stack entry within the nest level. To simplify
+		 * fmgr_security_definer() and other places that use GUC_ACTION_SAVE,
+		 * we allow failure exit from code that uses a local nest level to be
+		 * recovered at the surrounding transaction or subtransaction abort;
+		 * so there could be more than one stack entry to pop.
+		 */
+		while ((stack = gconf->stack) != NULL &&
+			   stack->nest_level >= nestLevel)
+		{
+			GucStack   *prev = stack->prev;
+			bool		restorePrior = false;
+			bool		restoreMasked = false;
+			bool		changed;
+
+			/*
+			 * In this next bit, if we don't set either restorePrior or
+			 * restoreMasked, we must "discard" any unwanted fields of the
+			 * stack entries to avoid leaking memory.  If we do set one of
+			 * those flags, unused fields will be cleaned up after restoring.
+			 */
+			if (!isCommit)		/* if abort, always restore prior value */
+				restorePrior = true;
+			else if (stack->state == GUC_SAVE)
+				restorePrior = true;
+			else if (stack->nest_level == 1)
+			{
+				/* transaction commit */
+				if (stack->state == GUC_SET_LOCAL)
+					restoreMasked = true;
+				else if (stack->state == GUC_SET)
+				{
+					/* we keep the current active value */
+					discard_stack_value(gconf, &stack->prior);
+				}
+				else			/* must be GUC_LOCAL */
+					restorePrior = true;
+			}
+			else if (prev == NULL ||
+					 prev->nest_level < stack->nest_level - 1)
+			{
+				/* decrement entry's level and do not pop it */
+				stack->nest_level--;
+				continue;
+			}
+			else
+			{
+				/*
+				 * We have to merge this stack entry into prev. See README for
+				 * discussion of this bit.
+				 */
+				switch (stack->state)
+				{
+					case GUC_SAVE:
+						Assert(false);	/* can't get here */
+						break;
+
+					case GUC_SET:
+						/* next level always becomes SET */
+						discard_stack_value(gconf, &stack->prior);
+						if (prev->state == GUC_SET_LOCAL)
+							discard_stack_value(gconf, &prev->masked);
+						prev->state = GUC_SET;
+						break;
+
+					case GUC_LOCAL:
+						if (prev->state == GUC_SET)
+						{
+							/* LOCAL migrates down */
+							prev->masked_scontext = stack->scontext;
+							prev->masked = stack->prior;
+							prev->state = GUC_SET_LOCAL;
+						}
+						else
+						{
+							/* else just forget this stack level */
+							discard_stack_value(gconf, &stack->prior);
+						}
+						break;
+
+					case GUC_SET_LOCAL:
+						/* prior state at this level no longer wanted */
+						discard_stack_value(gconf, &stack->prior);
+						/* copy down the masked state */
+						prev->masked_scontext = stack->masked_scontext;
+						if (prev->state == GUC_SET_LOCAL)
+							discard_stack_value(gconf, &prev->masked);
+						prev->masked = stack->masked;
+						prev->state = GUC_SET_LOCAL;
+						break;
+				}
+			}
+
+			changed = false;
+
+			if (restorePrior || restoreMasked)
+			{
+				/* Perform appropriate restoration of the stacked value */
+				config_var_value newvalue;
+				GucSource	newsource;
+				GucContext	newscontext;
+
+				if (restoreMasked)
+				{
+					newvalue = stack->masked;
+					newsource = PGC_S_SESSION;
+					newscontext = stack->masked_scontext;
+				}
+				else
+				{
+					newvalue = stack->prior;
+					newsource = stack->source;
+					newscontext = stack->scontext;
+				}
+
+				switch (gconf->vartype)
+				{
+					case PGC_BOOL:
+						{
+							struct config_bool *conf = (struct config_bool *) gconf;
+							bool		newval = newvalue.val.boolval;
+							void	   *newextra = newvalue.extra;
+
+							if (*conf->variable != newval ||
+								conf->gen.extra != newextra)
+							{
+								if (conf->assign_hook)
+									conf->assign_hook(newval, newextra);
+								*conf->variable = newval;
+								set_extra_field(&conf->gen, &conf->gen.extra,
+												newextra);
+								changed = true;
+							}
+							break;
+						}
+					case PGC_INT:
+						{
+							struct config_int *conf = (struct config_int *) gconf;
+							int			newval = newvalue.val.intval;
+							void	   *newextra = newvalue.extra;
+
+							if (*conf->variable != newval ||
+								conf->gen.extra != newextra)
+							{
+								if (conf->assign_hook)
+									conf->assign_hook(newval, newextra);
+								*conf->variable = newval;
+								set_extra_field(&conf->gen, &conf->gen.extra,
+												newextra);
+								changed = true;
+							}
+							break;
+						}
+					case PGC_REAL:
+						{
+							struct config_real *conf = (struct config_real *) gconf;
+							double		newval = newvalue.val.realval;
+							void	   *newextra = newvalue.extra;
+
+							if (*conf->variable != newval ||
+								conf->gen.extra != newextra)
+							{
+								if (conf->assign_hook)
+									conf->assign_hook(newval, newextra);
+								*conf->variable = newval;
+								set_extra_field(&conf->gen, &conf->gen.extra,
+												newextra);
+								changed = true;
+							}
+							break;
+						}
+					case PGC_STRING:
+						{
+							struct config_string *conf = (struct config_string *) gconf;
+							char	   *newval = newvalue.val.stringval;
+							void	   *newextra = newvalue.extra;
+
+							if (*conf->variable != newval ||
+								conf->gen.extra != newextra)
+							{
+								if (conf->assign_hook)
+									conf->assign_hook(newval, newextra);
+								set_string_field(conf, conf->variable, newval);
+								set_extra_field(&conf->gen, &conf->gen.extra,
+												newextra);
+								changed = true;
+							}
+
+							/*
+							 * Release stacked values if not used anymore. We
+							 * could use discard_stack_value() here, but since
+							 * we have type-specific code anyway, might as
+							 * well inline it.
+							 */
+							set_string_field(conf, &stack->prior.val.stringval, NULL);
+							set_string_field(conf, &stack->masked.val.stringval, NULL);
+							break;
+						}
+					case PGC_ENUM:
+						{
+							struct config_enum *conf = (struct config_enum *) gconf;
+							int			newval = newvalue.val.enumval;
+							void	   *newextra = newvalue.extra;
+
+							if (*conf->variable != newval ||
+								conf->gen.extra != newextra)
+							{
+								if (conf->assign_hook)
+									conf->assign_hook(newval, newextra);
+								*conf->variable = newval;
+								set_extra_field(&conf->gen, &conf->gen.extra,
+												newextra);
+								changed = true;
+							}
+							break;
+						}
+				}
+
+				/*
+				 * Release stacked extra values if not used anymore.
+				 */
+				set_extra_field(gconf, &(stack->prior.extra), NULL);
+				set_extra_field(gconf, &(stack->masked.extra), NULL);
+
+				/* And restore source information */
+				gconf->source = newsource;
+				gconf->scontext = newscontext;
+			}
+
+			/* Finish popping the state stack */
+			gconf->stack = prev;
+			pfree(stack);
+
+			/* Report new value if we changed it */
+			if (changed && (gconf->flags & GUC_REPORT))
+				ReportGUCOption(gconf);
+		}						/* end of stack-popping loop */
+
+		if (stack != NULL)
+			still_dirty = true;
+	}
+
+	/* If there are no remaining stack entries, we can reset guc_dirty */
+	guc_dirty = still_dirty;
+
+	/* Update nesting level */
+	GUCNestLevel = nestLevel - 1;
+}
+
+
+/*
+ * Start up automatic reporting of changes to variables marked GUC_REPORT.
+ * This is executed at completion of backend startup.
+ */
+void
+BeginReportingGUCOptions(void)
+{
+	int			i;
+
+	/*
+	 * Don't do anything unless talking to an interactive frontend of protocol
+	 * 3.0 or later.
+	 */
+	if (whereToSendOutput != DestRemote ||
+		PG_PROTOCOL_MAJOR(FrontendProtocol) < 3)
+		return;
+
+	reporting_enabled = true;
+
+	/* Transmit initial values of interesting variables */
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		struct config_generic *conf = guc_variables[i];
+
+		if (conf->flags & GUC_REPORT)
+			ReportGUCOption(conf);
+	}
+}
+
+/*
+ * ReportGUCOption: if appropriate, transmit option value to frontend
+ */
+static void
+ReportGUCOption(struct config_generic *record)
+{
+	if (reporting_enabled && (record->flags & GUC_REPORT))
+	{
+		char	   *val = _ShowOption(record, false);
+		StringInfoData msgbuf;
+
+		pq_beginmessage(&msgbuf, 'S');
+		pq_sendstring(&msgbuf, record->name);
+		pq_sendstring(&msgbuf, val);
+		pq_endmessage(&msgbuf);
+
+		pfree(val);
+	}
+}
+
+/*
+ * Convert a value from one of the human-friendly units ("kB", "min" etc.)
+ * to the given base unit.  'value' and 'unit' are the input value and unit
+ * to convert from (there can be trailing spaces in the unit string).
+ * The converted value is stored in *base_value.
+ * It's caller's responsibility to round off the converted value as necessary
+ * and check for out-of-range.
+ *
+ * Returns true on success, false if the input unit is not recognized.
+ */
+static bool
+convert_to_base_unit(double value, const char *unit,
+					 int base_unit, double *base_value)
+{
+	char		unitstr[MAX_UNIT_LEN + 1];
+	int			unitlen;
+	const unit_conversion *table;
+	int			i;
+
+	/* extract unit string to compare to table entries */
+	unitlen = 0;
+	while (*unit != '\0' && !isspace((unsigned char) *unit) &&
+		   unitlen < MAX_UNIT_LEN)
+		unitstr[unitlen++] = *(unit++);
+	unitstr[unitlen] = '\0';
+	/* allow whitespace after unit */
+	while (isspace((unsigned char) *unit))
+		unit++;
+	if (*unit != '\0')
+		return false;			/* unit too long, or garbage after it */
+
+	/* now search the appropriate table */
+	if (base_unit & GUC_UNIT_MEMORY)
+		table = memory_unit_conversion_table;
+	else
+		table = time_unit_conversion_table;
+
+	for (i = 0; *table[i].unit; i++)
+	{
+		if (base_unit == table[i].base_unit &&
+			strcmp(unitstr, table[i].unit) == 0)
+		{
+			double		cvalue = value * table[i].multiplier;
+
+			/*
+			 * If the user gave a fractional value such as "30.1GB", round it
+			 * off to the nearest multiple of the next smaller unit, if there
+			 * is one.
+			 */
+			if (*table[i + 1].unit &&
+				base_unit == table[i + 1].base_unit)
+				cvalue = rint(cvalue / table[i + 1].multiplier) *
+					table[i + 1].multiplier;
+
+			*base_value = cvalue;
+			return true;
+		}
+	}
+	return false;
+}
+
+/*
+ * Convert an integer value in some base unit to a human-friendly unit.
+ *
+ * The output unit is chosen so that it's the greatest unit that can represent
+ * the value without loss.  For example, if the base unit is GUC_UNIT_KB, 1024
+ * is converted to 1 MB, but 1025 is represented as 1025 kB.
+ */
+static void
+convert_int_from_base_unit(int64 base_value, int base_unit,
+						   int64 *value, const char **unit)
+{
+	const unit_conversion *table;
+	int			i;
+
+	*unit = NULL;
+
+	if (base_unit & GUC_UNIT_MEMORY)
+		table = memory_unit_conversion_table;
+	else
+		table = time_unit_conversion_table;
+
+	for (i = 0; *table[i].unit; i++)
+	{
+		if (base_unit == table[i].base_unit)
+		{
+			/*
+			 * Accept the first conversion that divides the value evenly.  We
+			 * assume that the conversions for each base unit are ordered from
+			 * greatest unit to the smallest!
+			 */
+			if (table[i].multiplier <= 1.0 ||
+				base_value % (int64) table[i].multiplier == 0)
+			{
+				*value = (int64) rint(base_value / table[i].multiplier);
+				*unit = table[i].unit;
+				break;
+			}
+		}
+	}
+
+	Assert(*unit != NULL);
+}
+
+/*
+ * Convert a floating-point value in some base unit to a human-friendly unit.
+ *
+ * Same as above, except we have to do the math a bit differently, and
+ * there's a possibility that we don't find any exact divisor.
+ */
+static void
+convert_real_from_base_unit(double base_value, int base_unit,
+							double *value, const char **unit)
+{
+	const unit_conversion *table;
+	int			i;
+
+	*unit = NULL;
+
+	if (base_unit & GUC_UNIT_MEMORY)
+		table = memory_unit_conversion_table;
+	else
+		table = time_unit_conversion_table;
+
+	for (i = 0; *table[i].unit; i++)
+	{
+		if (base_unit == table[i].base_unit)
+		{
+			/*
+			 * Accept the first conversion that divides the value evenly; or
+			 * if there is none, use the smallest (last) target unit.
+			 *
+			 * What we actually care about here is whether snprintf with "%g"
+			 * will print the value as an integer, so the obvious test of
+			 * "*value == rint(*value)" is too strict; roundoff error might
+			 * make us choose an unreasonably small unit.  As a compromise,
+			 * accept a divisor that is within 1e-8 of producing an integer.
+			 */
+			*value = base_value / table[i].multiplier;
+			*unit = table[i].unit;
+			if (*value > 0 &&
+				fabs((rint(*value) / *value) - 1.0) <= 1e-8)
+				break;
+		}
+	}
+
+	Assert(*unit != NULL);
+}
+
+/*
+ * Return the name of a GUC's base unit (e.g. "ms") given its flags.
+ * Return NULL if the GUC is unitless.
+ */
+static const char *
+get_config_unit_name(int flags)
+{
+	switch (flags & (GUC_UNIT_MEMORY | GUC_UNIT_TIME))
+	{
+		case 0:
+			return NULL;		/* GUC has no units */
+		case GUC_UNIT_BYTE:
+			return "B";
+		case GUC_UNIT_KB:
+			return "kB";
+		case GUC_UNIT_MB:
+			return "MB";
+		case GUC_UNIT_BLOCKS:
+			{
+				static char bbuf[8];
+
+				/* initialize if first time through */
+				if (bbuf[0] == '\0')
+					snprintf(bbuf, sizeof(bbuf), "%dkB", BLCKSZ / 1024);
+				return bbuf;
+			}
+		case GUC_UNIT_XBLOCKS:
+			{
+				static char xbuf[8];
+
+				/* initialize if first time through */
+				if (xbuf[0] == '\0')
+					snprintf(xbuf, sizeof(xbuf), "%dkB", XLOG_BLCKSZ / 1024);
+				return xbuf;
+			}
+		case GUC_UNIT_MS:
+			return "ms";
+		case GUC_UNIT_S:
+			return "s";
+		case GUC_UNIT_MIN:
+			return "min";
+		default:
+			elog(ERROR, "unrecognized GUC units value: %d",
+				 flags & (GUC_UNIT_MEMORY | GUC_UNIT_TIME));
+			return NULL;
+	}
+}
+
+
+/*
+ * Try to parse value as an integer.  The accepted formats are the
+ * usual decimal, octal, or hexadecimal formats, as well as floating-point
+ * formats (which will be rounded to integer after any units conversion).
+ * Optionally, the value can be followed by a unit name if "flags" indicates
+ * a unit is allowed.
+ *
+ * If the string parses okay, return true, else false.
+ * If okay and result is not NULL, return the value in *result.
+ * If not okay and hintmsg is not NULL, *hintmsg is set to a suitable
+ * HINT message, or NULL if no hint provided.
+ */
+bool
+parse_int(const char *value, int *result, int flags, const char **hintmsg)
+{
+	/*
+	 * We assume here that double is wide enough to represent any integer
+	 * value with adequate precision.
+	 */
+	double		val;
+	char	   *endptr;
+
+	/* To suppress compiler warnings, always set output params */
+	if (result)
+		*result = 0;
+	if (hintmsg)
+		*hintmsg = NULL;
+
+	/*
+	 * Try to parse as an integer (allowing octal or hex input).  If the
+	 * conversion stops at a decimal point or 'e', or overflows, re-parse as
+	 * float.  This should work fine as long as we have no unit names starting
+	 * with 'e'.  If we ever do, the test could be extended to check for a
+	 * sign or digit after 'e', but for now that's unnecessary.
+	 */
+	errno = 0;
+	val = strtol(value, &endptr, 0);
+	if (*endptr == '.' || *endptr == 'e' || *endptr == 'E' ||
+		errno == ERANGE)
+	{
+		errno = 0;
+		val = strtod(value, &endptr);
+	}
+
+	if (endptr == value || errno == ERANGE)
+		return false;			/* no HINT for these cases */
+
+	/* reject NaN (infinities will fail range check below) */
+	if (isnan(val))
+		return false;			/* treat same as syntax error; no HINT */
+
+	/* allow whitespace between number and unit */
+	while (isspace((unsigned char) *endptr))
+		endptr++;
+
+	/* Handle possible unit */
+	if (*endptr != '\0')
+	{
+		if ((flags & GUC_UNIT) == 0)
+			return false;		/* this setting does not accept a unit */
+
+		if (!convert_to_base_unit(val,
+								  endptr, (flags & GUC_UNIT),
+								  &val))
+		{
+			/* invalid unit, or garbage after the unit; set hint and fail. */
+			if (hintmsg)
+			{
+				if (flags & GUC_UNIT_MEMORY)
+					*hintmsg = memory_units_hint;
+				else
+					*hintmsg = time_units_hint;
+			}
+			return false;
+		}
+	}
+
+	/* Round to int, then check for overflow */
+	val = rint(val);
+
+	if (val > INT_MAX || val < INT_MIN)
+	{
+		if (hintmsg)
+			*hintmsg = gettext_noop("Value exceeds integer range.");
+		return false;
+	}
+
+	if (result)
+		*result = (int) val;
+	return true;
+}
+
+/*
+ * Try to parse value as a floating point number in the usual format.
+ * Optionally, the value can be followed by a unit name if "flags" indicates
+ * a unit is allowed.
+ *
+ * If the string parses okay, return true, else false.
+ * If okay and result is not NULL, return the value in *result.
+ * If not okay and hintmsg is not NULL, *hintmsg is set to a suitable
+ * HINT message, or NULL if no hint provided.
+ */
+bool
+parse_real(const char *value, double *result, int flags, const char **hintmsg)
+{
+	double		val;
+	char	   *endptr;
+
+	/* To suppress compiler warnings, always set output params */
+	if (result)
+		*result = 0;
+	if (hintmsg)
+		*hintmsg = NULL;
+
+	errno = 0;
+	val = strtod(value, &endptr);
+
+	if (endptr == value || errno == ERANGE)
+		return false;			/* no HINT for these cases */
+
+	/* reject NaN (infinities will fail range checks later) */
+	if (isnan(val))
+		return false;			/* treat same as syntax error; no HINT */
+
+	/* allow whitespace between number and unit */
+	while (isspace((unsigned char) *endptr))
+		endptr++;
+
+	/* Handle possible unit */
+	if (*endptr != '\0')
+	{
+		if ((flags & GUC_UNIT) == 0)
+			return false;		/* this setting does not accept a unit */
+
+		if (!convert_to_base_unit(val,
+								  endptr, (flags & GUC_UNIT),
+								  &val))
+		{
+			/* invalid unit, or garbage after the unit; set hint and fail. */
+			if (hintmsg)
+			{
+				if (flags & GUC_UNIT_MEMORY)
+					*hintmsg = memory_units_hint;
+				else
+					*hintmsg = time_units_hint;
+			}
+			return false;
+		}
+	}
+
+	if (result)
+		*result = val;
+	return true;
+}
+
+
+/*
+ * Lookup the name for an enum option with the selected value.
+ * Should only ever be called with known-valid values, so throws
+ * an elog(ERROR) if the enum option is not found.
+ *
+ * The returned string is a pointer to static data and not
+ * allocated for modification.
+ */
+const char *
+config_enum_lookup_by_value(struct config_enum *record, int val)
+{
+	const struct config_enum_entry *entry;
+
+	for (entry = record->options; entry && entry->name; entry++)
+	{
+		if (entry->val == val)
+			return entry->name;
+	}
+
+	elog(ERROR, "could not find enum option %d for %s",
+		 val, record->gen.name);
+	return NULL;				/* silence compiler */
+}
+
+
+/*
+ * Lookup the value for an enum option with the selected name
+ * (case-insensitive).
+ * If the enum option is found, sets the retval value and returns
+ * true. If it's not found, return false and retval is set to 0.
+ */
+bool
+config_enum_lookup_by_name(struct config_enum *record, const char *value,
+						   int *retval)
+{
+	const struct config_enum_entry *entry;
+
+	for (entry = record->options; entry && entry->name; entry++)
+	{
+		if (pg_strcasecmp(value, entry->name) == 0)
+		{
+			*retval = entry->val;
+			return true;
+		}
+	}
+
+	*retval = 0;
+	return false;
+}
+
+
+/*
+ * Return a list of all available options for an enum, excluding
+ * hidden ones, separated by the given separator.
+ * If prefix is non-NULL, it is added before the first enum value.
+ * If suffix is non-NULL, it is added to the end of the string.
+ */
+static char *
+config_enum_get_options(struct config_enum *record, const char *prefix,
+						const char *suffix, const char *separator)
+{
+	const struct config_enum_entry *entry;
+	StringInfoData retstr;
+	int			seplen;
+
+	initStringInfo(&retstr);
+	appendStringInfoString(&retstr, prefix);
+
+	seplen = strlen(separator);
+	for (entry = record->options; entry && entry->name; entry++)
+	{
+		if (!entry->hidden)
+		{
+			appendStringInfoString(&retstr, entry->name);
+			appendBinaryStringInfo(&retstr, separator, seplen);
+		}
+	}
+
+	/*
+	 * All the entries may have been hidden, leaving the string empty if no
+	 * prefix was given. This indicates a broken GUC setup, since there is no
+	 * use for an enum without any values, so we just check to make sure we
+	 * don't write to invalid memory instead of actually trying to do
+	 * something smart with it.
+	 */
+	if (retstr.len >= seplen)
+	{
+		/* Replace final separator */
+		retstr.data[retstr.len - seplen] = '\0';
+		retstr.len -= seplen;
+	}
+
+	appendStringInfoString(&retstr, suffix);
+
+	return retstr.data;
+}
+
+/*
+ * Parse and validate a proposed value for the specified configuration
+ * parameter.
+ *
+ * This does built-in checks (such as range limits for an integer parameter)
+ * and also calls any check hook the parameter may have.
+ *
+ * record: GUC variable's info record
+ * name: variable name (should match the record of course)
+ * value: proposed value, as a string
+ * source: identifies source of value (check hooks may need this)
+ * elevel: level to log any error reports at
+ * newval: on success, converted parameter value is returned here
+ * newextra: on success, receives any "extra" data returned by check hook
+ *	(caller must initialize *newextra to NULL)
+ *
+ * Returns true if OK, false if not (or throws error, if elevel >= ERROR)
+ */
+static bool
+parse_and_validate_value(struct config_generic *record,
+						 const char *name, const char *value,
+						 GucSource source, int elevel,
+						 union config_var_val *newval, void **newextra)
+{
+	switch (record->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *conf = (struct config_bool *) record;
+
+				if (!parse_bool(value, &newval->boolval))
+				{
+					ereport(elevel,
+							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+							 errmsg("parameter \"%s\" requires a Boolean value",
+									name)));
+					return false;
+				}
+
+				if (!call_bool_check_hook(conf, &newval->boolval, newextra,
+										  source, elevel))
+					return false;
+			}
+			break;
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) record;
+				const char *hintmsg;
+
+				if (!parse_int(value, &newval->intval,
+							   conf->gen.flags, &hintmsg))
+				{
+					ereport(elevel,
+							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+							 errmsg("invalid value for parameter \"%s\": \"%s\"",
+									name, value),
+							 hintmsg ? errhint("%s", _(hintmsg)) : 0));
+					return false;
+				}
+
+				if (newval->intval < conf->min || newval->intval > conf->max)
+				{
+					const char *unit = get_config_unit_name(conf->gen.flags);
+
+					ereport(elevel,
+							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+							 errmsg("%d%s%s is outside the valid range for parameter \"%s\" (%d .. %d)",
+									newval->intval,
+									unit ? " " : "",
+									unit ? unit : "",
+									name,
+									conf->min, conf->max)));
+					return false;
+				}
+
+				if (!call_int_check_hook(conf, &newval->intval, newextra,
+										 source, elevel))
+					return false;
+			}
+			break;
+		case PGC_REAL:
+			{
+				struct config_real *conf = (struct config_real *) record;
+				const char *hintmsg;
+
+				if (!parse_real(value, &newval->realval,
+								conf->gen.flags, &hintmsg))
+				{
+					ereport(elevel,
+							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+							 errmsg("invalid value for parameter \"%s\": \"%s\"",
+									name, value),
+							 hintmsg ? errhint("%s", _(hintmsg)) : 0));
+					return false;
+				}
+
+				if (newval->realval < conf->min || newval->realval > conf->max)
+				{
+					const char *unit = get_config_unit_name(conf->gen.flags);
+
+					ereport(elevel,
+							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+							 errmsg("%g%s%s is outside the valid range for parameter \"%s\" (%g .. %g)",
+									newval->realval,
+									unit ? " " : "",
+									unit ? unit : "",
+									name,
+									conf->min, conf->max)));
+					return false;
+				}
+
+				if (!call_real_check_hook(conf, &newval->realval, newextra,
+										  source, elevel))
+					return false;
+			}
+			break;
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) record;
+
+				/*
+				 * The value passed by the caller could be transient, so we
+				 * always strdup it.
+				 */
+				newval->stringval = guc_strdup(elevel, value);
+				if (newval->stringval == NULL)
+					return false;
+
+				/*
+				 * The only built-in "parsing" check we have is to apply
+				 * truncation if GUC_IS_NAME.
+				 */
+				if (conf->gen.flags & GUC_IS_NAME)
+					truncate_identifier(newval->stringval,
+										strlen(newval->stringval),
+										true);
+
+				if (!call_string_check_hook(conf, &newval->stringval, newextra,
+											source, elevel))
+				{
+					free(newval->stringval);
+					newval->stringval = NULL;
+					return false;
+				}
+			}
+			break;
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) record;
+
+				if (!config_enum_lookup_by_name(conf, value, &newval->enumval))
+				{
+					char	   *hintmsg;
+
+					hintmsg = config_enum_get_options(conf,
+													  "Available values: ",
+													  ".", ", ");
+
+					ereport(elevel,
+							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+							 errmsg("invalid value for parameter \"%s\": \"%s\"",
+									name, value),
+							 hintmsg ? errhint("%s", _(hintmsg)) : 0));
+
+					if (hintmsg)
+						pfree(hintmsg);
+					return false;
+				}
+
+				if (!call_enum_check_hook(conf, &newval->enumval, newextra,
+										  source, elevel))
+					return false;
+			}
+			break;
+	}
+
+	return true;
+}
+
+
+/*
+ * Sets option `name' to given value.
+ *
+ * The value should be a string, which will be parsed and converted to
+ * the appropriate data type.  The context and source parameters indicate
+ * in which context this function is being called, so that it can apply the
+ * access restrictions properly.
+ *
+ * If value is NULL, set the option to its default value (normally the
+ * reset_val, but if source == PGC_S_DEFAULT we instead use the boot_val).
+ *
+ * action indicates whether to set the value globally in the session, locally
+ * to the current top transaction, or just for the duration of a function call.
+ *
+ * If changeVal is false then don't really set the option but do all
+ * the checks to see if it would work.
+ *
+ * elevel should normally be passed as zero, allowing this function to make
+ * its standard choice of ereport level.  However some callers need to be
+ * able to override that choice; they should pass the ereport level to use.
+ *
+ * Return value:
+ *	+1: the value is valid and was successfully applied.
+ *	0:	the name or value is invalid (but see below).
+ *	-1: the value was not applied because of context, priority, or changeVal.
+ *
+ * If there is an error (non-existing option, invalid value) then an
+ * ereport(ERROR) is thrown *unless* this is called for a source for which
+ * we don't want an ERROR (currently, those are defaults, the config file,
+ * and per-database or per-user settings, as well as callers who specify
+ * a less-than-ERROR elevel).  In those cases we write a suitable error
+ * message via ereport() and return 0.
+ *
+ * See also SetConfigOption for an external interface.
+ */
+int
+set_config_option(const char *name, const char *value,
+				  GucContext context, GucSource source,
+				  GucAction action, bool changeVal, int elevel,
+				  bool is_reload)
+{
+	struct config_generic *record;
+	union config_var_val newval_union;
+	void	   *newextra = NULL;
+	bool		prohibitValueChange = false;
+	bool		makeDefault;
+
+	if (elevel == 0)
+	{
+		if (source == PGC_S_DEFAULT || source == PGC_S_FILE)
+		{
+			/*
+			 * To avoid cluttering the log, only the postmaster bleats loudly
+			 * about problems with the config file.
+			 */
+			elevel = IsUnderPostmaster ? DEBUG3 : LOG;
+		}
+		else if (source == PGC_S_GLOBAL ||
+				 source == PGC_S_DATABASE ||
+				 source == PGC_S_USER ||
+				 source == PGC_S_DATABASE_USER)
+			elevel = WARNING;
+		else
+			elevel = ERROR;
+	}
+
+	/*
+	 * GUC_ACTION_SAVE changes are acceptable during a parallel operation,
+	 * because the current worker will also pop the change.  We're probably
+	 * dealing with a function having a proconfig entry.  Only the function's
+	 * body should observe the change, and peer workers do not share in the
+	 * execution of a function call started by this worker.
+	 *
+	 * Other changes might need to affect other workers, so forbid them.
+	 */
+	if (IsInParallelMode() && changeVal && action != GUC_ACTION_SAVE)
+	{
+		ereport(elevel,
+				(errcode(ERRCODE_INVALID_TRANSACTION_STATE),
+				 errmsg("cannot set parameters during a parallel operation")));
+		return -1;
+	}
+
+	record = find_option(name, true, elevel);
+	if (record == NULL)
+	{
+		ereport(elevel,
+				(errcode(ERRCODE_UNDEFINED_OBJECT),
+				 errmsg("unrecognized configuration parameter \"%s\"", name)));
+		return 0;
+	}
+
+	/*
+	 * Check if the option can be set at this time. See guc.h for the precise
+	 * rules.
+	 */
+	switch (record->context)
+	{
+		case PGC_INTERNAL:
+			if (context != PGC_INTERNAL)
+			{
+				ereport(elevel,
+						(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+						 errmsg("parameter \"%s\" cannot be changed",
+								name)));
+				return 0;
+			}
+			break;
+		case PGC_POSTMASTER:
+			if (context == PGC_SIGHUP)
+			{
+				/*
+				 * We are re-reading a PGC_POSTMASTER variable from
+				 * postgresql.conf.  We can't change the setting, so we should
+				 * give a warning if the DBA tries to change it.  However,
+				 * because of variant formats, canonicalization by check
+				 * hooks, etc, we can't just compare the given string directly
+				 * to what's stored.  Set a flag to check below after we have
+				 * the final storable value.
+				 */
+				prohibitValueChange = true;
+			}
+			else if (context != PGC_POSTMASTER)
+			{
+				ereport(elevel,
+						(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+						 errmsg("parameter \"%s\" cannot be changed without restarting the server",
+								name)));
+				return 0;
+			}
+			break;
+		case PGC_SIGHUP:
+			if (context != PGC_SIGHUP && context != PGC_POSTMASTER)
+			{
+				ereport(elevel,
+						(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+						 errmsg("parameter \"%s\" cannot be changed now",
+								name)));
+				return 0;
+			}
+
+			/*
+			 * Hmm, the idea of the SIGHUP context is "ought to be global, but
+			 * can be changed after postmaster start". But there's nothing
+			 * that prevents a crafty administrator from sending SIGHUP
+			 * signals to individual backends only.
+			 */
+			break;
+		case PGC_SU_BACKEND:
+			/* Reject if we're connecting but user is not superuser */
+			if (context == PGC_BACKEND)
+			{
+				ereport(elevel,
+						(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+						 errmsg("permission denied to set parameter \"%s\"",
+								name)));
+				return 0;
+			}
+			/* fall through to process the same as PGC_BACKEND */
+			/* FALLTHROUGH */
+		case PGC_BACKEND:
+			if (context == PGC_SIGHUP)
+			{
+				/*
+				 * If a PGC_BACKEND or PGC_SU_BACKEND parameter is changed in
+				 * the config file, we want to accept the new value in the
+				 * postmaster (whence it will propagate to
+				 * subsequently-started backends), but ignore it in existing
+				 * backends.  This is a tad klugy, but necessary because we
+				 * don't re-read the config file during backend start.
+				 *
+				 * However, if changeVal is false then plow ahead anyway since
+				 * we are trying to find out if the value is potentially good,
+				 * not actually use it.
+				 *
+				 * In EXEC_BACKEND builds, this works differently: we load all
+				 * non-default settings from the CONFIG_EXEC_PARAMS file
+				 * during backend start.  In that case we must accept
+				 * PGC_SIGHUP settings, so as to have the same value as if
+				 * we'd forked from the postmaster.  This can also happen when
+				 * using RestoreGUCState() within a background worker that
+				 * needs to have the same settings as the user backend that
+				 * started it. is_reload will be true when either situation
+				 * applies.
+				 */
+				if (IsUnderPostmaster && changeVal && !is_reload)
+					return -1;
+			}
+			else if (context != PGC_POSTMASTER &&
+					 context != PGC_BACKEND &&
+					 context != PGC_SU_BACKEND &&
+					 source != PGC_S_CLIENT)
+			{
+				ereport(elevel,
+						(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+						 errmsg("parameter \"%s\" cannot be set after connection start",
+								name)));
+				return 0;
+			}
+			break;
+		case PGC_SUSET:
+			if (context == PGC_USERSET || context == PGC_BACKEND)
+			{
+				ereport(elevel,
+						(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+						 errmsg("permission denied to set parameter \"%s\"",
+								name)));
+				return 0;
+			}
+			break;
+		case PGC_USERSET:
+			/* always okay */
+			break;
+	}
+
+	/*
+	 * Disallow changing GUC_NOT_WHILE_SEC_REST values if we are inside a
+	 * security restriction context.  We can reject this regardless of the GUC
+	 * context or source, mainly because sources that it might be reasonable
+	 * to override for won't be seen while inside a function.
+	 *
+	 * Note: variables marked GUC_NOT_WHILE_SEC_REST should usually be marked
+	 * GUC_NO_RESET_ALL as well, because ResetAllOptions() doesn't check this.
+	 * An exception might be made if the reset value is assumed to be "safe".
+	 *
+	 * Note: this flag is currently used for "session_authorization" and
+	 * "role".  We need to prohibit changing these inside a local userid
+	 * context because when we exit it, GUC won't be notified, leaving things
+	 * out of sync.  (This could be fixed by forcing a new GUC nesting level,
+	 * but that would change behavior in possibly-undesirable ways.)  Also, we
+	 * prohibit changing these in a security-restricted operation because
+	 * otherwise RESET could be used to regain the session user's privileges.
+	 */
+	if (record->flags & GUC_NOT_WHILE_SEC_REST)
+	{
+		if (InLocalUserIdChange())
+		{
+			/*
+			 * Phrasing of this error message is historical, but it's the most
+			 * common case.
+			 */
+			ereport(elevel,
+					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+					 errmsg("cannot set parameter \"%s\" within security-definer function",
+							name)));
+			return 0;
+		}
+		if (InSecurityRestrictedOperation())
+		{
+			ereport(elevel,
+					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+					 errmsg("cannot set parameter \"%s\" within security-restricted operation",
+							name)));
+			return 0;
+		}
+	}
+
+	/*
+	 * Should we set reset/stacked values?	(If so, the behavior is not
+	 * transactional.)	This is done either when we get a default value from
+	 * the database's/user's/client's default settings or when we reset a
+	 * value to its default.
+	 */
+	makeDefault = changeVal && (source <= PGC_S_OVERRIDE) &&
+		((value != NULL) || source == PGC_S_DEFAULT);
+
+	/*
+	 * Ignore attempted set if overridden by previously processed setting.
+	 * However, if changeVal is false then plow ahead anyway since we are
+	 * trying to find out if the value is potentially good, not actually use
+	 * it. Also keep going if makeDefault is true, since we may want to set
+	 * the reset/stacked values even if we can't set the variable itself.
+	 */
+	if (record->source > source)
+	{
+		if (changeVal && !makeDefault)
+		{
+			elog(DEBUG3, "\"%s\": setting ignored because previous source is higher priority",
+				 name);
+			return -1;
+		}
+		changeVal = false;
+	}
+
+	/*
+	 * Evaluate value and set variable.
+	 */
+	switch (record->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *conf = (struct config_bool *) record;
+
+#define newval (newval_union.boolval)
+
+				if (value)
+				{
+					if (!parse_and_validate_value(record, name, value,
+												  source, elevel,
+												  &newval_union, &newextra))
+						return 0;
+				}
+				else if (source == PGC_S_DEFAULT)
+				{
+					newval = conf->boot_val;
+					if (!call_bool_check_hook(conf, &newval, &newextra,
+											  source, elevel))
+						return 0;
+				}
+				else
+				{
+					newval = conf->reset_val;
+					newextra = conf->reset_extra;
+					source = conf->gen.reset_source;
+					context = conf->gen.reset_scontext;
+				}
+
+				if (prohibitValueChange)
+				{
+					/* Release newextra, unless it's reset_extra */
+					if (newextra && !extra_field_used(&conf->gen, newextra))
+						free(newextra);
+
+					if (*conf->variable != newval)
+					{
+						record->status |= GUC_PENDING_RESTART;
+						ereport(elevel,
+								(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+								 errmsg("parameter \"%s\" cannot be changed without restarting the server",
+										name)));
+						return 0;
+					}
+					record->status &= ~GUC_PENDING_RESTART;
+					return -1;
+				}
+
+				if (changeVal)
+				{
+					/* Save old value to support transaction abort */
+					if (!makeDefault)
+						push_old_value(&conf->gen, action);
+
+					if (conf->assign_hook)
+						conf->assign_hook(newval, newextra);
+					*conf->variable = newval;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									newextra);
+					conf->gen.source = source;
+					conf->gen.scontext = context;
+				}
+				if (makeDefault)
+				{
+					GucStack   *stack;
+
+					if (conf->gen.reset_source <= source)
+					{
+						conf->reset_val = newval;
+						set_extra_field(&conf->gen, &conf->reset_extra,
+										newextra);
+						conf->gen.reset_source = source;
+						conf->gen.reset_scontext = context;
+					}
+					for (stack = conf->gen.stack; stack; stack = stack->prev)
+					{
+						if (stack->source <= source)
+						{
+							stack->prior.val.boolval = newval;
+							set_extra_field(&conf->gen, &stack->prior.extra,
+											newextra);
+							stack->source = source;
+							stack->scontext = context;
+						}
+					}
+				}
+
+				/* Perhaps we didn't install newextra anywhere */
+				if (newextra && !extra_field_used(&conf->gen, newextra))
+					free(newextra);
+				break;
+
+#undef newval
+			}
+
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) record;
+
+#define newval (newval_union.intval)
+
+				if (value)
+				{
+					if (!parse_and_validate_value(record, name, value,
+												  source, elevel,
+												  &newval_union, &newextra))
+						return 0;
+				}
+				else if (source == PGC_S_DEFAULT)
+				{
+					newval = conf->boot_val;
+					if (!call_int_check_hook(conf, &newval, &newextra,
+											 source, elevel))
+						return 0;
+				}
+				else
+				{
+					newval = conf->reset_val;
+					newextra = conf->reset_extra;
+					source = conf->gen.reset_source;
+					context = conf->gen.reset_scontext;
+				}
+
+				if (prohibitValueChange)
+				{
+					/* Release newextra, unless it's reset_extra */
+					if (newextra && !extra_field_used(&conf->gen, newextra))
+						free(newextra);
+
+					if (*conf->variable != newval)
+					{
+						record->status |= GUC_PENDING_RESTART;
+						ereport(elevel,
+								(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+								 errmsg("parameter \"%s\" cannot be changed without restarting the server",
+										name)));
+						return 0;
+					}
+					record->status &= ~GUC_PENDING_RESTART;
+					return -1;
+				}
+
+				if (changeVal)
+				{
+					/* Save old value to support transaction abort */
+					if (!makeDefault)
+						push_old_value(&conf->gen, action);
+
+					if (conf->assign_hook)
+						conf->assign_hook(newval, newextra);
+					*conf->variable = newval;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									newextra);
+					conf->gen.source = source;
+					conf->gen.scontext = context;
+				}
+				if (makeDefault)
+				{
+					GucStack   *stack;
+
+					if (conf->gen.reset_source <= source)
+					{
+						conf->reset_val = newval;
+						set_extra_field(&conf->gen, &conf->reset_extra,
+										newextra);
+						conf->gen.reset_source = source;
+						conf->gen.reset_scontext = context;
+					}
+					for (stack = conf->gen.stack; stack; stack = stack->prev)
+					{
+						if (stack->source <= source)
+						{
+							stack->prior.val.intval = newval;
+							set_extra_field(&conf->gen, &stack->prior.extra,
+											newextra);
+							stack->source = source;
+							stack->scontext = context;
+						}
+					}
+				}
+
+				/* Perhaps we didn't install newextra anywhere */
+				if (newextra && !extra_field_used(&conf->gen, newextra))
+					free(newextra);
+				break;
+
+#undef newval
+			}
+
+		case PGC_REAL:
+			{
+				struct config_real *conf = (struct config_real *) record;
+
+#define newval (newval_union.realval)
+
+				if (value)
+				{
+					if (!parse_and_validate_value(record, name, value,
+												  source, elevel,
+												  &newval_union, &newextra))
+						return 0;
+				}
+				else if (source == PGC_S_DEFAULT)
+				{
+					newval = conf->boot_val;
+					if (!call_real_check_hook(conf, &newval, &newextra,
+											  source, elevel))
+						return 0;
+				}
+				else
+				{
+					newval = conf->reset_val;
+					newextra = conf->reset_extra;
+					source = conf->gen.reset_source;
+					context = conf->gen.reset_scontext;
+				}
+
+				if (prohibitValueChange)
+				{
+					/* Release newextra, unless it's reset_extra */
+					if (newextra && !extra_field_used(&conf->gen, newextra))
+						free(newextra);
+
+					if (*conf->variable != newval)
+					{
+						record->status |= GUC_PENDING_RESTART;
+						ereport(elevel,
+								(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+								 errmsg("parameter \"%s\" cannot be changed without restarting the server",
+										name)));
+						return 0;
+					}
+					record->status &= ~GUC_PENDING_RESTART;
+					return -1;
+				}
+
+				if (changeVal)
+				{
+					/* Save old value to support transaction abort */
+					if (!makeDefault)
+						push_old_value(&conf->gen, action);
+
+					if (conf->assign_hook)
+						conf->assign_hook(newval, newextra);
+					*conf->variable = newval;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									newextra);
+					conf->gen.source = source;
+					conf->gen.scontext = context;
+				}
+				if (makeDefault)
+				{
+					GucStack   *stack;
+
+					if (conf->gen.reset_source <= source)
+					{
+						conf->reset_val = newval;
+						set_extra_field(&conf->gen, &conf->reset_extra,
+										newextra);
+						conf->gen.reset_source = source;
+						conf->gen.reset_scontext = context;
+					}
+					for (stack = conf->gen.stack; stack; stack = stack->prev)
+					{
+						if (stack->source <= source)
+						{
+							stack->prior.val.realval = newval;
+							set_extra_field(&conf->gen, &stack->prior.extra,
+											newextra);
+							stack->source = source;
+							stack->scontext = context;
+						}
+					}
+				}
+
+				/* Perhaps we didn't install newextra anywhere */
+				if (newextra && !extra_field_used(&conf->gen, newextra))
+					free(newextra);
+				break;
+
+#undef newval
+			}
+
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) record;
+
+#define newval (newval_union.stringval)
+
+				if (value)
+				{
+					if (!parse_and_validate_value(record, name, value,
+												  source, elevel,
+												  &newval_union, &newextra))
+						return 0;
+				}
+				else if (source == PGC_S_DEFAULT)
+				{
+					/* non-NULL boot_val must always get strdup'd */
+					if (conf->boot_val != NULL)
+					{
+						newval = guc_strdup(elevel, conf->boot_val);
+						if (newval == NULL)
+							return 0;
+					}
+					else
+						newval = NULL;
+
+					if (!call_string_check_hook(conf, &newval, &newextra,
+												source, elevel))
+					{
+						free(newval);
+						return 0;
+					}
+				}
+				else
+				{
+					/*
+					 * strdup not needed, since reset_val is already under
+					 * guc.c's control
+					 */
+					newval = conf->reset_val;
+					newextra = conf->reset_extra;
+					source = conf->gen.reset_source;
+					context = conf->gen.reset_scontext;
+				}
+
+				if (prohibitValueChange)
+				{
+					bool		newval_different;
+
+					/* newval shouldn't be NULL, so we're a bit sloppy here */
+					newval_different = (*conf->variable == NULL ||
+										newval == NULL ||
+										strcmp(*conf->variable, newval) != 0);
+
+					/* Release newval, unless it's reset_val */
+					if (newval && !string_field_used(conf, newval))
+						free(newval);
+					/* Release newextra, unless it's reset_extra */
+					if (newextra && !extra_field_used(&conf->gen, newextra))
+						free(newextra);
+
+					if (newval_different)
+					{
+						record->status |= GUC_PENDING_RESTART;
+						ereport(elevel,
+								(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+								 errmsg("parameter \"%s\" cannot be changed without restarting the server",
+										name)));
+						return 0;
+					}
+					record->status &= ~GUC_PENDING_RESTART;
+					return -1;
+				}
+
+				if (changeVal)
+				{
+					/* Save old value to support transaction abort */
+					if (!makeDefault)
+						push_old_value(&conf->gen, action);
+
+					if (conf->assign_hook)
+						conf->assign_hook(newval, newextra);
+					set_string_field(conf, conf->variable, newval);
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									newextra);
+					conf->gen.source = source;
+					conf->gen.scontext = context;
+				}
+
+				if (makeDefault)
+				{
+					GucStack   *stack;
+
+					if (conf->gen.reset_source <= source)
+					{
+						set_string_field(conf, &conf->reset_val, newval);
+						set_extra_field(&conf->gen, &conf->reset_extra,
+										newextra);
+						conf->gen.reset_source = source;
+						conf->gen.reset_scontext = context;
+					}
+					for (stack = conf->gen.stack; stack; stack = stack->prev)
+					{
+						if (stack->source <= source)
+						{
+							set_string_field(conf, &stack->prior.val.stringval,
+											 newval);
+							set_extra_field(&conf->gen, &stack->prior.extra,
+											newextra);
+							stack->source = source;
+							stack->scontext = context;
+						}
+					}
+				}
+
+				/* Perhaps we didn't install newval anywhere */
+				if (newval && !string_field_used(conf, newval))
+					free(newval);
+				/* Perhaps we didn't install newextra anywhere */
+				if (newextra && !extra_field_used(&conf->gen, newextra))
+					free(newextra);
+				break;
+
+#undef newval
+			}
+
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) record;
+
+#define newval (newval_union.enumval)
+
+				if (value)
+				{
+					if (!parse_and_validate_value(record, name, value,
+												  source, elevel,
+												  &newval_union, &newextra))
+						return 0;
+				}
+				else if (source == PGC_S_DEFAULT)
+				{
+					newval = conf->boot_val;
+					if (!call_enum_check_hook(conf, &newval, &newextra,
+											  source, elevel))
+						return 0;
+				}
+				else
+				{
+					newval = conf->reset_val;
+					newextra = conf->reset_extra;
+					source = conf->gen.reset_source;
+					context = conf->gen.reset_scontext;
+				}
+
+				if (prohibitValueChange)
+				{
+					/* Release newextra, unless it's reset_extra */
+					if (newextra && !extra_field_used(&conf->gen, newextra))
+						free(newextra);
+
+					if (*conf->variable != newval)
+					{
+						record->status |= GUC_PENDING_RESTART;
+						ereport(elevel,
+								(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+								 errmsg("parameter \"%s\" cannot be changed without restarting the server",
+										name)));
+						return 0;
+					}
+					record->status &= ~GUC_PENDING_RESTART;
+					return -1;
+				}
+
+				if (changeVal)
+				{
+					/* Save old value to support transaction abort */
+					if (!makeDefault)
+						push_old_value(&conf->gen, action);
+
+					if (conf->assign_hook)
+						conf->assign_hook(newval, newextra);
+					*conf->variable = newval;
+					set_extra_field(&conf->gen, &conf->gen.extra,
+									newextra);
+					conf->gen.source = source;
+					conf->gen.scontext = context;
+				}
+				if (makeDefault)
+				{
+					GucStack   *stack;
+
+					if (conf->gen.reset_source <= source)
+					{
+						conf->reset_val = newval;
+						set_extra_field(&conf->gen, &conf->reset_extra,
+										newextra);
+						conf->gen.reset_source = source;
+						conf->gen.reset_scontext = context;
+					}
+					for (stack = conf->gen.stack; stack; stack = stack->prev)
+					{
+						if (stack->source <= source)
+						{
+							stack->prior.val.enumval = newval;
+							set_extra_field(&conf->gen, &stack->prior.extra,
+											newextra);
+							stack->source = source;
+							stack->scontext = context;
+						}
+					}
+				}
+
+				/* Perhaps we didn't install newextra anywhere */
+				if (newextra && !extra_field_used(&conf->gen, newextra))
+					free(newextra);
+				break;
+
+#undef newval
+			}
+	}
+
+	if (changeVal && (record->flags & GUC_REPORT))
+		ReportGUCOption(record);
+
+	return changeVal ? 1 : -1;
+}
+
+
+/*
+ * Set the fields for source file and line number the setting came from.
+ */
+static void
+set_config_sourcefile(const char *name, char *sourcefile, int sourceline)
+{
+	struct config_generic *record;
+	int			elevel;
+
+	/*
+	 * To avoid cluttering the log, only the postmaster bleats loudly about
+	 * problems with the config file.
+	 */
+	elevel = IsUnderPostmaster ? DEBUG3 : LOG;
+
+	record = find_option(name, true, elevel);
+	/* should not happen */
+	if (record == NULL)
+		elog(ERROR, "unrecognized configuration parameter \"%s\"", name);
+
+	sourcefile = guc_strdup(elevel, sourcefile);
+	if (record->sourcefile)
+		free(record->sourcefile);
+	record->sourcefile = sourcefile;
+	record->sourceline = sourceline;
+}
+
+/*
+ * Set a config option to the given value.
+ *
+ * See also set_config_option; this is just the wrapper to be called from
+ * outside GUC.  (This function should be used when possible, because its API
+ * is more stable than set_config_option's.)
+ *
+ * Note: there is no support here for setting source file/line, as it
+ * is currently not needed.
+ */
+void
+SetConfigOption(const char *name, const char *value,
+				GucContext context, GucSource source)
+{
+	(void) set_config_option(name, value, context, source,
+							 GUC_ACTION_SET, true, 0, false);
+}
+
+
+
+/*
+ * Fetch the current value of the option `name', as a string.
+ *
+ * If the option doesn't exist, return NULL if missing_ok is true (NOTE that
+ * this cannot be distinguished from a string variable with a NULL value!),
+ * otherwise throw an ereport and don't return.
+ *
+ * If restrict_privileged is true, we also enforce that only superusers and
+ * members of the pg_read_all_settings role can see GUC_SUPERUSER_ONLY
+ * variables.  This should only be passed as true in user-driven calls.
+ *
+ * The string is *not* allocated for modification and is really only
+ * valid until the next call to configuration related functions.
+ */
+const char *
+GetConfigOption(const char *name, bool missing_ok, bool restrict_privileged)
+{
+	struct config_generic *record;
+	static char buffer[256];
+
+	record = find_option(name, false, ERROR);
+	if (record == NULL)
+	{
+		if (missing_ok)
+			return NULL;
+		ereport(ERROR,
+				(errcode(ERRCODE_UNDEFINED_OBJECT),
+				 errmsg("unrecognized configuration parameter \"%s\"",
+						name)));
+	}
+	if (restrict_privileged &&
+		(record->flags & GUC_SUPERUSER_ONLY) &&
+		!is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS))
+		ereport(ERROR,
+				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+				 errmsg("must be superuser or a member of pg_read_all_settings to examine \"%s\"",
+						name)));
+
+	switch (record->vartype)
+	{
+		case PGC_BOOL:
+			return *((struct config_bool *) record)->variable ? "on" : "off";
+
+		case PGC_INT:
+			snprintf(buffer, sizeof(buffer), "%d",
+					 *((struct config_int *) record)->variable);
+			return buffer;
+
+		case PGC_REAL:
+			snprintf(buffer, sizeof(buffer), "%g",
+					 *((struct config_real *) record)->variable);
+			return buffer;
+
+		case PGC_STRING:
+			return *((struct config_string *) record)->variable;
+
+		case PGC_ENUM:
+			return config_enum_lookup_by_value((struct config_enum *) record,
+											   *((struct config_enum *) record)->variable);
+	}
+	return NULL;
+}
+
+/*
+ * Get the RESET value associated with the given option.
+ *
+ * Note: this is not re-entrant, due to use of static result buffer;
+ * not to mention that a string variable could have its reset_val changed.
+ * Beware of assuming the result value is good for very long.
+ */
+const char *
+GetConfigOptionResetString(const char *name)
+{
+	struct config_generic *record;
+	static char buffer[256];
+
+	record = find_option(name, false, ERROR);
+	if (record == NULL)
+		ereport(ERROR,
+				(errcode(ERRCODE_UNDEFINED_OBJECT),
+				 errmsg("unrecognized configuration parameter \"%s\"", name)));
+	if ((record->flags & GUC_SUPERUSER_ONLY) &&
+		!is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS))
+		ereport(ERROR,
+				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+				 errmsg("must be superuser or a member of pg_read_all_settings to examine \"%s\"",
+						name)));
+
+	switch (record->vartype)
+	{
+		case PGC_BOOL:
+			return ((struct config_bool *) record)->reset_val ? "on" : "off";
+
+		case PGC_INT:
+			snprintf(buffer, sizeof(buffer), "%d",
+					 ((struct config_int *) record)->reset_val);
+			return buffer;
+
+		case PGC_REAL:
+			snprintf(buffer, sizeof(buffer), "%g",
+					 ((struct config_real *) record)->reset_val);
+			return buffer;
+
+		case PGC_STRING:
+			return ((struct config_string *) record)->reset_val;
+
+		case PGC_ENUM:
+			return config_enum_lookup_by_value((struct config_enum *) record,
+											   ((struct config_enum *) record)->reset_val);
+	}
+	return NULL;
+}
+
+/*
+ * Get the GUC flags associated with the given option.
+ *
+ * If the option doesn't exist, return 0 if missing_ok is true,
+ * otherwise throw an ereport and don't return.
+ */
+int
+GetConfigOptionFlags(const char *name, bool missing_ok)
+{
+	struct config_generic *record;
+
+	record = find_option(name, false, WARNING);
+	if (record == NULL)
+	{
+		if (missing_ok)
+			return 0;
+		ereport(ERROR,
+				(errcode(ERRCODE_UNDEFINED_OBJECT),
+				 errmsg("unrecognized configuration parameter \"%s\"",
+						name)));
+	}
+	return record->flags;
+}
+
+
+/*
+ * flatten_set_variable_args
+ *		Given a parsenode List as emitted by the grammar for SET,
+ *		convert to the flat string representation used by GUC.
+ *
+ * We need to be told the name of the variable the args are for, because
+ * the flattening rules vary (ugh).
+ *
+ * The result is NULL if args is NIL (i.e., SET ... TO DEFAULT), otherwise
+ * a palloc'd string.
+ */
+static char *
+flatten_set_variable_args(const char *name, List *args)
+{
+	struct config_generic *record;
+	int			flags;
+	StringInfoData buf;
+	ListCell   *l;
+
+	/* Fast path if just DEFAULT */
+	if (args == NIL)
+		return NULL;
+
+	/*
+	 * Get flags for the variable; if it's not known, use default flags.
+	 * (Caller might throw error later, but not our business to do so here.)
+	 */
+	record = find_option(name, false, WARNING);
+	if (record)
+		flags = record->flags;
+	else
+		flags = 0;
+
+	/* Complain if list input and non-list variable */
+	if ((flags & GUC_LIST_INPUT) == 0 &&
+		list_length(args) != 1)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("SET %s takes only one argument", name)));
+
+	initStringInfo(&buf);
+
+	/*
+	 * Each list member may be a plain A_Const node, or an A_Const within a
+	 * TypeCast; the latter case is supported only for ConstInterval arguments
+	 * (for SET TIME ZONE).
+	 */
+	foreach(l, args)
+	{
+		Node	   *arg = (Node *) lfirst(l);
+		char	   *val;
+		TypeName   *typeName = NULL;
+		A_Const    *con;
+
+		if (l != list_head(args))
+			appendStringInfoString(&buf, ", ");
+
+		if (IsA(arg, TypeCast))
+		{
+			TypeCast   *tc = (TypeCast *) arg;
+
+			arg = tc->arg;
+			typeName = tc->typeName;
+		}
+
+		if (!IsA(arg, A_Const))
+			elog(ERROR, "unrecognized node type: %d", (int) nodeTag(arg));
+		con = (A_Const *) arg;
+
+		switch (nodeTag(&con->val))
+		{
+			case T_Integer:
+				appendStringInfo(&buf, "%d", intVal(&con->val));
+				break;
+			case T_Float:
+				/* represented as a string, so just copy it */
+				appendStringInfoString(&buf, strVal(&con->val));
+				break;
+			case T_String:
+				val = strVal(&con->val);
+				if (typeName != NULL)
+				{
+					/*
+					 * Must be a ConstInterval argument for TIME ZONE. Coerce
+					 * to interval and back to normalize the value and account
+					 * for any typmod.
+					 */
+					Oid			typoid;
+					int32		typmod;
+					Datum		interval;
+					char	   *intervalout;
+
+					typenameTypeIdAndMod(NULL, typeName, &typoid, &typmod);
+					Assert(typoid == INTERVALOID);
+
+					interval =
+						DirectFunctionCall3(interval_in,
+											CStringGetDatum(val),
+											ObjectIdGetDatum(InvalidOid),
+											Int32GetDatum(typmod));
+
+					intervalout =
+						DatumGetCString(DirectFunctionCall1(interval_out,
+															interval));
+					appendStringInfo(&buf, "INTERVAL '%s'", intervalout);
+				}
+				else
+				{
+					/*
+					 * Plain string literal or identifier.  For quote mode,
+					 * quote it if it's not a vanilla identifier.
+					 */
+					if (flags & GUC_LIST_QUOTE)
+						appendStringInfoString(&buf, quote_identifier(val));
+					else
+						appendStringInfoString(&buf, val);
+				}
+				break;
+			default:
+				elog(ERROR, "unrecognized node type: %d",
+					 (int) nodeTag(&con->val));
+				break;
+		}
+	}
+
+	return buf.data;
+}
+
+/*
+ * Write updated configuration parameter values into a temporary file.
+ * This function traverses the list of parameters and quotes the string
+ * values before writing them.
+ */
+static void
+write_auto_conf_file(int fd, const char *filename, ConfigVariable *head)
+{
+	StringInfoData buf;
+	ConfigVariable *item;
+
+	initStringInfo(&buf);
+
+	/* Emit file header containing warning comment */
+	appendStringInfoString(&buf, "# Do not edit this file manually!\n");
+	appendStringInfoString(&buf, "# It will be overwritten by the ALTER SYSTEM command.\n");
+
+	errno = 0;
+	if (write(fd, buf.data, buf.len) != buf.len)
+	{
+		/* if write didn't set errno, assume problem is no disk space */
+		if (errno == 0)
+			errno = ENOSPC;
+		ereport(ERROR,
+				(errcode_for_file_access(),
+				 errmsg("could not write to file \"%s\": %m", filename)));
+	}
+
+	/* Emit each parameter, properly quoting the value */
+	for (item = head; item != NULL; item = item->next)
+	{
+		char	   *escaped;
+
+		resetStringInfo(&buf);
+
+		appendStringInfoString(&buf, item->name);
+		appendStringInfoString(&buf, " = '");
+
+		escaped = escape_single_quotes_ascii(item->value);
+		if (!escaped)
+			ereport(ERROR,
+					(errcode(ERRCODE_OUT_OF_MEMORY),
+					 errmsg("out of memory")));
+		appendStringInfoString(&buf, escaped);
+		free(escaped);
+
+		appendStringInfoString(&buf, "'\n");
+
+		errno = 0;
+		if (write(fd, buf.data, buf.len) != buf.len)
+		{
+			/* if write didn't set errno, assume problem is no disk space */
+			if (errno == 0)
+				errno = ENOSPC;
+			ereport(ERROR,
+					(errcode_for_file_access(),
+					 errmsg("could not write to file \"%s\": %m", filename)));
+		}
+	}
+
+	/* fsync before considering the write to be successful */
+	if (pg_fsync(fd) != 0)
+		ereport(ERROR,
+				(errcode_for_file_access(),
+				 errmsg("could not fsync file \"%s\": %m", filename)));
+
+	pfree(buf.data);
+}
+
+/*
+ * Update the given list of configuration parameters, adding, replacing
+ * or deleting the entry for item "name" (delete if "value" == NULL).
+ */
+static void
+replace_auto_config_value(ConfigVariable **head_p, ConfigVariable **tail_p,
+						  const char *name, const char *value)
+{
+	ConfigVariable *item,
+			   *next,
+			   *prev = NULL;
+
+	/*
+	 * Remove any existing match(es) for "name".  Normally there'd be at most
+	 * one, but if external tools have modified the config file, there could
+	 * be more.
+	 */
+	for (item = *head_p; item != NULL; item = next)
+	{
+		next = item->next;
+		if (guc_name_compare(item->name, name) == 0)
+		{
+			/* found a match, delete it */
+			if (prev)
+				prev->next = next;
+			else
+				*head_p = next;
+			if (next == NULL)
+				*tail_p = prev;
+
+			pfree(item->name);
+			pfree(item->value);
+			pfree(item->filename);
+			pfree(item);
+		}
+		else
+			prev = item;
+	}
+
+	/* Done if we're trying to delete it */
+	if (value == NULL)
+		return;
+
+	/* OK, append a new entry */
+	item = palloc(sizeof *item);
+	item->name = pstrdup(name);
+	item->value = pstrdup(value);
+	item->errmsg = NULL;
+	item->filename = pstrdup("");	/* new item has no location */
+	item->sourceline = 0;
+	item->ignore = false;
+	item->applied = false;
+	item->next = NULL;
+
+	if (*head_p == NULL)
+		*head_p = item;
+	else
+		(*tail_p)->next = item;
+	*tail_p = item;
+}
+
+
+/*
+ * Execute ALTER SYSTEM statement.
+ *
+ * Read the old PG_AUTOCONF_FILENAME file, merge in the new variable value,
+ * and write out an updated file.  If the command is ALTER SYSTEM RESET ALL,
+ * we can skip reading the old file and just write an empty file.
+ *
+ * An LWLock is used to serialize updates of the configuration file.
+ *
+ * In case of an error, we leave the original automatic
+ * configuration file (PG_AUTOCONF_FILENAME) intact.
+ */
+void
+AlterSystemSetConfigFile(AlterSystemStmt *altersysstmt)
+{
+	char	   *name;
+	char	   *value;
+	bool		resetall = false;
+	ConfigVariable *head = NULL;
+	ConfigVariable *tail = NULL;
+	volatile int Tmpfd;
+	char		AutoConfFileName[MAXPGPATH];
+	char		AutoConfTmpFileName[MAXPGPATH];
+
+	if (!superuser())
+		ereport(ERROR,
+				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+				 (errmsg("must be superuser to execute ALTER SYSTEM command"))));
+
+	/*
+	 * Extract statement arguments
+	 */
+	name = altersysstmt->setstmt->name;
+
+	switch (altersysstmt->setstmt->kind)
+	{
+		case VAR_SET_VALUE:
+			value = ExtractSetVariableArgs(altersysstmt->setstmt);
+			break;
+
+		case VAR_SET_DEFAULT:
+		case VAR_RESET:
+			value = NULL;
+			break;
+
+		case VAR_RESET_ALL:
+			value = NULL;
+			resetall = true;
+			break;
+
+		default:
+			elog(ERROR, "unrecognized alter system stmt type: %d",
+				 altersysstmt->setstmt->kind);
+			break;
+	}
+
+	/*
+	 * Unless it's RESET_ALL, validate the target variable and value
+	 */
+	if (!resetall)
+	{
+		struct config_generic *record;
+
+		record = find_option(name, false, ERROR);
+		if (record == NULL)
+			ereport(ERROR,
+					(errcode(ERRCODE_UNDEFINED_OBJECT),
+					 errmsg("unrecognized configuration parameter \"%s\"",
+							name)));
+
+		/*
+		 * Don't allow parameters that can't be set in configuration files to
+		 * be set in PG_AUTOCONF_FILENAME file.
+		 */
+		if ((record->context == PGC_INTERNAL) ||
+			(record->flags & GUC_DISALLOW_IN_FILE) ||
+			(record->flags & GUC_DISALLOW_IN_AUTO_FILE))
+			ereport(ERROR,
+					(errcode(ERRCODE_CANT_CHANGE_RUNTIME_PARAM),
+					 errmsg("parameter \"%s\" cannot be changed",
+							name)));
+
+		/*
+		 * If a value is specified, verify that it's sane.
+		 */
+		if (value)
+		{
+			union config_var_val newval;
+			void	   *newextra = NULL;
+
+			/* Check that it's acceptable for the indicated parameter */
+			if (!parse_and_validate_value(record, name, value,
+										  PGC_S_FILE, ERROR,
+										  &newval, &newextra))
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("invalid value for parameter \"%s\": \"%s\"",
+								name, value)));
+
+			if (record->vartype == PGC_STRING && newval.stringval != NULL)
+				free(newval.stringval);
+			if (newextra)
+				free(newextra);
+
+			/*
+			 * We must also reject values containing newlines, because the
+			 * grammar for config files doesn't support embedded newlines in
+			 * string literals.
+			 */
+			if (strchr(value, '\n'))
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("parameter value for ALTER SYSTEM must not contain a newline")));
+		}
+	}
+
+	/*
+	 * PG_AUTOCONF_FILENAME and its corresponding temporary file are always in
+	 * the data directory, so we can reference them by simple relative paths.
+	 */
+	snprintf(AutoConfFileName, sizeof(AutoConfFileName), "%s",
+			 PG_AUTOCONF_FILENAME);
+	snprintf(AutoConfTmpFileName, sizeof(AutoConfTmpFileName), "%s.%s",
+			 AutoConfFileName,
+			 "tmp");
+
+	/*
+	 * Only one backend is allowed to operate on PG_AUTOCONF_FILENAME at a
+	 * time.  Use AutoFileLock to ensure that.  We must hold the lock while
+	 * reading the old file contents.
+	 */
+	LWLockAcquire(AutoFileLock, LW_EXCLUSIVE);
+
+	/*
+	 * If we're going to reset everything, then no need to open or parse the
+	 * old file.  We'll just write out an empty list.
+	 */
+	if (!resetall)
+	{
+		struct stat st;
+
+		if (stat(AutoConfFileName, &st) == 0)
+		{
+			/* open old file PG_AUTOCONF_FILENAME */
+			FILE	   *infile;
+
+			infile = AllocateFile(AutoConfFileName, "r");
+			if (infile == NULL)
+				ereport(ERROR,
+						(errcode_for_file_access(),
+						 errmsg("could not open file \"%s\": %m",
+								AutoConfFileName)));
+
+			/* parse it */
+			if (!ParseConfigFp(infile, AutoConfFileName, 0, LOG, &head, &tail))
+				ereport(ERROR,
+						(errcode(ERRCODE_CONFIG_FILE_ERROR),
+						 errmsg("could not parse contents of file \"%s\"",
+								AutoConfFileName)));
+
+			FreeFile(infile);
+		}
+
+		/*
+		 * Now, replace any existing entry with the new value, or add it if
+		 * not present.
+		 */
+		replace_auto_config_value(&head, &tail, name, value);
+	}
+
+	/*
+	 * To ensure crash safety, first write the new file data to a temp file,
+	 * then atomically rename it into place.
+	 *
+	 * If there is a temp file left over due to a previous crash, it's okay to
+	 * truncate and reuse it.
+	 */
+	Tmpfd = BasicOpenFile(AutoConfTmpFileName,
+						  O_CREAT | O_RDWR | O_TRUNC);
+	if (Tmpfd < 0)
+		ereport(ERROR,
+				(errcode_for_file_access(),
+				 errmsg("could not open file \"%s\": %m",
+						AutoConfTmpFileName)));
+
+	/*
+	 * Use a TRY block to clean up the file if we fail.  Since we need a TRY
+	 * block anyway, OK to use BasicOpenFile rather than OpenTransientFile.
+	 */
+	PG_TRY();
+	{
+		/* Write and sync the new contents to the temporary file */
+		write_auto_conf_file(Tmpfd, AutoConfTmpFileName, head);
+
+		/* Close before renaming; may be required on some platforms */
+		close(Tmpfd);
+		Tmpfd = -1;
+
+		/*
+		 * As the rename is atomic operation, if any problem occurs after this
+		 * at worst it can lose the parameters set by last ALTER SYSTEM
+		 * command.
+		 */
+		durable_rename(AutoConfTmpFileName, AutoConfFileName, ERROR);
+	}
+	PG_CATCH();
+	{
+		/* Close file first, else unlink might fail on some platforms */
+		if (Tmpfd >= 0)
+			close(Tmpfd);
+
+		/* Unlink, but ignore any error */
+		(void) unlink(AutoConfTmpFileName);
+
+		PG_RE_THROW();
+	}
+	PG_END_TRY();
+
+	FreeConfigVariables(head);
+
+	LWLockRelease(AutoFileLock);
+}
+
+/*
+ * SET command
+ */
+void
+ExecSetVariableStmt(VariableSetStmt *stmt, bool isTopLevel)
+{
+	GucAction	action = stmt->is_local ? GUC_ACTION_LOCAL : GUC_ACTION_SET;
+
+	/*
+	 * Workers synchronize these parameters at the start of the parallel
+	 * operation; then, we block SET during the operation.
+	 */
+	if (IsInParallelMode())
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_TRANSACTION_STATE),
+				 errmsg("cannot set parameters during a parallel operation")));
+
+	switch (stmt->kind)
+	{
+		case VAR_SET_VALUE:
+		case VAR_SET_CURRENT:
+			if (stmt->is_local)
+				WarnNoTransactionBlock(isTopLevel, "SET LOCAL");
+			(void) set_config_option(stmt->name,
+									 ExtractSetVariableArgs(stmt),
+									 (superuser() ? PGC_SUSET : PGC_USERSET),
+									 PGC_S_SESSION,
+									 action, true, 0, false);
+			break;
+		case VAR_SET_MULTI:
+
+			/*
+			 * Special-case SQL syntaxes.  The TRANSACTION and SESSION
+			 * CHARACTERISTICS cases effectively set more than one variable
+			 * per statement.  TRANSACTION SNAPSHOT only takes one argument,
+			 * but we put it here anyway since it's a special case and not
+			 * related to any GUC variable.
+			 */
+			if (strcmp(stmt->name, "TRANSACTION") == 0)
+			{
+				ListCell   *head;
+
+				WarnNoTransactionBlock(isTopLevel, "SET TRANSACTION");
+
+				foreach(head, stmt->args)
+				{
+					DefElem    *item = (DefElem *) lfirst(head);
+
+					if (strcmp(item->defname, "transaction_isolation") == 0)
+						SetPGVariable("transaction_isolation",
+									  list_make1(item->arg), stmt->is_local);
+					else if (strcmp(item->defname, "transaction_read_only") == 0)
+						SetPGVariable("transaction_read_only",
+									  list_make1(item->arg), stmt->is_local);
+					else if (strcmp(item->defname, "transaction_deferrable") == 0)
+						SetPGVariable("transaction_deferrable",
+									  list_make1(item->arg), stmt->is_local);
+					else
+						elog(ERROR, "unexpected SET TRANSACTION element: %s",
+							 item->defname);
+				}
+			}
+			else if (strcmp(stmt->name, "SESSION CHARACTERISTICS") == 0)
+			{
+				ListCell   *head;
+
+				foreach(head, stmt->args)
+				{
+					DefElem    *item = (DefElem *) lfirst(head);
+
+					if (strcmp(item->defname, "transaction_isolation") == 0)
+						SetPGVariable("default_transaction_isolation",
+									  list_make1(item->arg), stmt->is_local);
+					else if (strcmp(item->defname, "transaction_read_only") == 0)
+						SetPGVariable("default_transaction_read_only",
+									  list_make1(item->arg), stmt->is_local);
+					else if (strcmp(item->defname, "transaction_deferrable") == 0)
+						SetPGVariable("default_transaction_deferrable",
+									  list_make1(item->arg), stmt->is_local);
+					else
+						elog(ERROR, "unexpected SET SESSION element: %s",
+							 item->defname);
+				}
+			}
+			else if (strcmp(stmt->name, "TRANSACTION SNAPSHOT") == 0)
+			{
+				A_Const    *con = linitial_node(A_Const, stmt->args);
+
+				if (stmt->is_local)
+					ereport(ERROR,
+							(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+							 errmsg("SET LOCAL TRANSACTION SNAPSHOT is not implemented")));
+
+				WarnNoTransactionBlock(isTopLevel, "SET TRANSACTION");
+				Assert(nodeTag(&con->val) == T_String);
+				ImportSnapshot(strVal(&con->val));
+			}
+			else
+				elog(ERROR, "unexpected SET MULTI element: %s",
+					 stmt->name);
+			break;
+		case VAR_SET_DEFAULT:
+			if (stmt->is_local)
+				WarnNoTransactionBlock(isTopLevel, "SET LOCAL");
+			/* fall through */
+		case VAR_RESET:
+			if (strcmp(stmt->name, "transaction_isolation") == 0)
+				WarnNoTransactionBlock(isTopLevel, "RESET TRANSACTION");
+
+			(void) set_config_option(stmt->name,
+									 NULL,
+									 (superuser() ? PGC_SUSET : PGC_USERSET),
+									 PGC_S_SESSION,
+									 action, true, 0, false);
+			break;
+		case VAR_RESET_ALL:
+			ResetAllOptions();
+			break;
+	}
+}
+
+/*
+ * Get the value to assign for a VariableSetStmt, or NULL if it's RESET.
+ * The result is palloc'd.
+ *
+ * This is exported for use by actions such as ALTER ROLE SET.
+ */
+char *
+ExtractSetVariableArgs(VariableSetStmt *stmt)
+{
+	switch (stmt->kind)
+	{
+		case VAR_SET_VALUE:
+			return flatten_set_variable_args(stmt->name, stmt->args);
+		case VAR_SET_CURRENT:
+			return GetConfigOptionByName(stmt->name, NULL, false);
+		default:
+			return NULL;
+	}
+}
+
+/*
+ * SetPGVariable - SET command exported as an easily-C-callable function.
+ *
+ * This provides access to SET TO value, as well as SET TO DEFAULT (expressed
+ * by passing args == NIL), but not SET FROM CURRENT functionality.
+ */
+void
+SetPGVariable(const char *name, List *args, bool is_local)
+{
+	char	   *argstring = flatten_set_variable_args(name, args);
+
+	/* Note SET DEFAULT (argstring == NULL) is equivalent to RESET */
+	(void) set_config_option(name,
+							 argstring,
+							 (superuser() ? PGC_SUSET : PGC_USERSET),
+							 PGC_S_SESSION,
+							 is_local ? GUC_ACTION_LOCAL : GUC_ACTION_SET,
+							 true, 0, false);
+}
+
+/*
+ * SET command wrapped as a SQL callable function.
+ */
+Datum
+set_config_by_name(PG_FUNCTION_ARGS)
+{
+	char	   *name;
+	char	   *value;
+	char	   *new_value;
+	bool		is_local;
+
+	if (PG_ARGISNULL(0))
+		ereport(ERROR,
+				(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),
+				 errmsg("SET requires parameter name")));
+
+	/* Get the GUC variable name */
+	name = TextDatumGetCString(PG_GETARG_DATUM(0));
+
+	/* Get the desired value or set to NULL for a reset request */
+	if (PG_ARGISNULL(1))
+		value = NULL;
+	else
+		value = TextDatumGetCString(PG_GETARG_DATUM(1));
+
+	/*
+	 * Get the desired state of is_local. Default to false if provided value
+	 * is NULL
+	 */
+	if (PG_ARGISNULL(2))
+		is_local = false;
+	else
+		is_local = PG_GETARG_BOOL(2);
+
+	/* Note SET DEFAULT (argstring == NULL) is equivalent to RESET */
+	(void) set_config_option(name,
+							 value,
+							 (superuser() ? PGC_SUSET : PGC_USERSET),
+							 PGC_S_SESSION,
+							 is_local ? GUC_ACTION_LOCAL : GUC_ACTION_SET,
+							 true, 0, false);
+
+	/* get the new current value */
+	new_value = GetConfigOptionByName(name, NULL, false);
+
+	/* Convert return string to text */
+	PG_RETURN_TEXT_P(cstring_to_text(new_value));
+}
+
+
+/*
+ * Common code for DefineCustomXXXVariable subroutines: allocate the
+ * new variable's config struct and fill in generic fields.
+ */
+static struct config_generic *
+init_custom_variable(const char *name,
+					 const char *short_desc,
+					 const char *long_desc,
+					 GucContext context,
+					 int flags,
+					 enum config_type type,
+					 size_t sz)
+{
+	struct config_generic *gen;
+
+	/*
+	 * Only allow custom PGC_POSTMASTER variables to be created during shared
+	 * library preload; any later than that, we can't ensure that the value
+	 * doesn't change after startup.  This is a fatal elog if it happens; just
+	 * erroring out isn't safe because we don't know what the calling loadable
+	 * module might already have hooked into.
+	 */
+	if (context == PGC_POSTMASTER &&
+		!process_shared_preload_libraries_in_progress)
+		elog(FATAL, "cannot create PGC_POSTMASTER variables after startup");
+
+	/*
+	 * We can't support custom GUC_LIST_QUOTE variables, because the wrong
+	 * things would happen if such a variable were set or pg_dump'd when the
+	 * defining extension isn't loaded.  Again, treat this as fatal because
+	 * the loadable module may be partly initialized already.
+	 */
+	if (flags & GUC_LIST_QUOTE)
+		elog(FATAL, "extensions cannot define GUC_LIST_QUOTE variables");
+
+	/*
+	 * Before pljava commit 398f3b876ed402bdaec8bc804f29e2be95c75139
+	 * (2015-12-15), two of that module's PGC_USERSET variables facilitated
+	 * trivial escalation to superuser privileges.  Restrict the variables to
+	 * protect sites that have yet to upgrade pljava.
+	 */
+	if (context == PGC_USERSET &&
+		(strcmp(name, "pljava.classpath") == 0 ||
+		 strcmp(name, "pljava.vmoptions") == 0))
+		context = PGC_SUSET;
+
+	gen = (struct config_generic *) guc_malloc(ERROR, sz);
+	memset(gen, 0, sz);
+
+	gen->name = guc_strdup(ERROR, name);
+	gen->context = context;
+	gen->group = CUSTOM_OPTIONS;
+	gen->short_desc = short_desc;
+	gen->long_desc = long_desc;
+	gen->flags = flags;
+	gen->vartype = type;
+
+	return gen;
+}
+
+/*
+ * Common code for DefineCustomXXXVariable subroutines: insert the new
+ * variable into the GUC variable array, replacing any placeholder.
+ */
+static void
+define_custom_variable(struct config_generic *variable)
+{
+	const char *name = variable->name;
+	const char **nameAddr = &name;
+	struct config_string *pHolder;
+	struct config_generic **res;
+
+	/*
+	 * See if there's a placeholder by the same name.
+	 */
+	res = (struct config_generic **) bsearch((void *) &nameAddr,
+											 (void *) guc_variables,
+											 num_guc_variables,
+											 sizeof(struct config_generic *),
+											 guc_var_compare);
+	if (res == NULL)
+	{
+		/*
+		 * No placeholder to replace, so we can just add it ... but first,
+		 * make sure it's initialized to its default value.
+		 */
+		InitializeOneGUCOption(variable);
+		add_guc_variable(variable, ERROR);
+		return;
+	}
+
+	/*
+	 * This better be a placeholder
+	 */
+	if (((*res)->flags & GUC_CUSTOM_PLACEHOLDER) == 0)
+		ereport(ERROR,
+				(errcode(ERRCODE_INTERNAL_ERROR),
+				 errmsg("attempt to redefine parameter \"%s\"", name)));
+
+	Assert((*res)->vartype == PGC_STRING);
+	pHolder = (struct config_string *) (*res);
+
+	/*
+	 * First, set the variable to its default value.  We must do this even
+	 * though we intend to immediately apply a new value, since it's possible
+	 * that the new value is invalid.
+	 */
+	InitializeOneGUCOption(variable);
+
+	/*
+	 * Replace the placeholder. We aren't changing the name, so no re-sorting
+	 * is necessary
+	 */
+	*res = variable;
+
+	/*
+	 * Assign the string value(s) stored in the placeholder to the real
+	 * variable.  Essentially, we need to duplicate all the active and stacked
+	 * values, but with appropriate validation and datatype adjustment.
+	 *
+	 * If an assignment fails, we report a WARNING and keep going.  We don't
+	 * want to throw ERROR for bad values, because it'd bollix the add-on
+	 * module that's presumably halfway through getting loaded.  In such cases
+	 * the default or previous state will become active instead.
+	 */
+
+	/* First, apply the reset value if any */
+	if (pHolder->reset_val)
+		(void) set_config_option(name, pHolder->reset_val,
+								 pHolder->gen.reset_scontext,
+								 pHolder->gen.reset_source,
+								 GUC_ACTION_SET, true, WARNING, false);
+	/* That should not have resulted in stacking anything */
+	Assert(variable->stack == NULL);
+
+	/* Now, apply current and stacked values, in the order they were stacked */
+	reapply_stacked_values(variable, pHolder, pHolder->gen.stack,
+						   *(pHolder->variable),
+						   pHolder->gen.scontext, pHolder->gen.source);
+
+	/* Also copy over any saved source-location information */
+	if (pHolder->gen.sourcefile)
+		set_config_sourcefile(name, pHolder->gen.sourcefile,
+							  pHolder->gen.sourceline);
+
+	/*
+	 * Free up as much as we conveniently can of the placeholder structure.
+	 * (This neglects any stack items, so it's possible for some memory to be
+	 * leaked.  Since this can only happen once per session per variable, it
+	 * doesn't seem worth spending much code on.)
+	 */
+	set_string_field(pHolder, pHolder->variable, NULL);
+	set_string_field(pHolder, &pHolder->reset_val, NULL);
+
+	free(pHolder);
+}
+
+/*
+ * Recursive subroutine for define_custom_variable: reapply non-reset values
+ *
+ * We recurse so that the values are applied in the same order as originally.
+ * At each recursion level, apply the upper-level value (passed in) in the
+ * fashion implied by the stack entry.
+ */
+static void
+reapply_stacked_values(struct config_generic *variable,
+					   struct config_string *pHolder,
+					   GucStack *stack,
+					   const char *curvalue,
+					   GucContext curscontext, GucSource cursource)
+{
+	const char *name = variable->name;
+	GucStack   *oldvarstack = variable->stack;
+
+	if (stack != NULL)
+	{
+		/* First, recurse, so that stack items are processed bottom to top */
+		reapply_stacked_values(variable, pHolder, stack->prev,
+							   stack->prior.val.stringval,
+							   stack->scontext, stack->source);
+
+		/* See how to apply the passed-in value */
+		switch (stack->state)
+		{
+			case GUC_SAVE:
+				(void) set_config_option(name, curvalue,
+										 curscontext, cursource,
+										 GUC_ACTION_SAVE, true,
+										 WARNING, false);
+				break;
+
+			case GUC_SET:
+				(void) set_config_option(name, curvalue,
+										 curscontext, cursource,
+										 GUC_ACTION_SET, true,
+										 WARNING, false);
+				break;
+
+			case GUC_LOCAL:
+				(void) set_config_option(name, curvalue,
+										 curscontext, cursource,
+										 GUC_ACTION_LOCAL, true,
+										 WARNING, false);
+				break;
+
+			case GUC_SET_LOCAL:
+				/* first, apply the masked value as SET */
+				(void) set_config_option(name, stack->masked.val.stringval,
+										 stack->masked_scontext, PGC_S_SESSION,
+										 GUC_ACTION_SET, true,
+										 WARNING, false);
+				/* then apply the current value as LOCAL */
+				(void) set_config_option(name, curvalue,
+										 curscontext, cursource,
+										 GUC_ACTION_LOCAL, true,
+										 WARNING, false);
+				break;
+		}
+
+		/* If we successfully made a stack entry, adjust its nest level */
+		if (variable->stack != oldvarstack)
+			variable->stack->nest_level = stack->nest_level;
+	}
+	else
+	{
+		/*
+		 * We are at the end of the stack.  If the active/previous value is
+		 * different from the reset value, it must represent a previously
+		 * committed session value.  Apply it, and then drop the stack entry
+		 * that set_config_option will have created under the impression that
+		 * this is to be just a transactional assignment.  (We leak the stack
+		 * entry.)
+		 */
+		if (curvalue != pHolder->reset_val ||
+			curscontext != pHolder->gen.reset_scontext ||
+			cursource != pHolder->gen.reset_source)
+		{
+			(void) set_config_option(name, curvalue,
+									 curscontext, cursource,
+									 GUC_ACTION_SET, true, WARNING, false);
+			variable->stack = NULL;
+		}
+	}
+}
+
+void
+DefineCustomBoolVariable(const char *name,
+						 const char *short_desc,
+						 const char *long_desc,
+						 bool *valueAddr,
+						 bool bootValue,
+						 GucContext context,
+						 int flags,
+						 GucBoolCheckHook check_hook,
+						 GucBoolAssignHook assign_hook,
+						 GucShowHook show_hook)
+{
+	struct config_bool *var;
+
+	var = (struct config_bool *)
+		init_custom_variable(name, short_desc, long_desc, context, flags,
+							 PGC_BOOL, sizeof(struct config_bool));
+	var->variable = valueAddr;
+	var->boot_val = bootValue;
+	var->reset_val = bootValue;
+	var->check_hook = check_hook;
+	var->assign_hook = assign_hook;
+	var->show_hook = show_hook;
+	define_custom_variable(&var->gen);
+}
+
+void
+DefineCustomIntVariable(const char *name,
+						const char *short_desc,
+						const char *long_desc,
+						int *valueAddr,
+						int bootValue,
+						int minValue,
+						int maxValue,
+						GucContext context,
+						int flags,
+						GucIntCheckHook check_hook,
+						GucIntAssignHook assign_hook,
+						GucShowHook show_hook)
+{
+	struct config_int *var;
+
+	var = (struct config_int *)
+		init_custom_variable(name, short_desc, long_desc, context, flags,
+							 PGC_INT, sizeof(struct config_int));
+	var->variable = valueAddr;
+	var->boot_val = bootValue;
+	var->reset_val = bootValue;
+	var->min = minValue;
+	var->max = maxValue;
+	var->check_hook = check_hook;
+	var->assign_hook = assign_hook;
+	var->show_hook = show_hook;
+	define_custom_variable(&var->gen);
+}
+
+void
+DefineCustomRealVariable(const char *name,
+						 const char *short_desc,
+						 const char *long_desc,
+						 double *valueAddr,
+						 double bootValue,
+						 double minValue,
+						 double maxValue,
+						 GucContext context,
+						 int flags,
+						 GucRealCheckHook check_hook,
+						 GucRealAssignHook assign_hook,
+						 GucShowHook show_hook)
+{
+	struct config_real *var;
+
+	var = (struct config_real *)
+		init_custom_variable(name, short_desc, long_desc, context, flags,
+							 PGC_REAL, sizeof(struct config_real));
+	var->variable = valueAddr;
+	var->boot_val = bootValue;
+	var->reset_val = bootValue;
+	var->min = minValue;
+	var->max = maxValue;
+	var->check_hook = check_hook;
+	var->assign_hook = assign_hook;
+	var->show_hook = show_hook;
+	define_custom_variable(&var->gen);
+}
+
+void
+DefineCustomStringVariable(const char *name,
+						   const char *short_desc,
+						   const char *long_desc,
+						   char **valueAddr,
+						   const char *bootValue,
+						   GucContext context,
+						   int flags,
+						   GucStringCheckHook check_hook,
+						   GucStringAssignHook assign_hook,
+						   GucShowHook show_hook)
+{
+	struct config_string *var;
+
+	var = (struct config_string *)
+		init_custom_variable(name, short_desc, long_desc, context, flags,
+							 PGC_STRING, sizeof(struct config_string));
+	var->variable = valueAddr;
+	var->boot_val = bootValue;
+	var->check_hook = check_hook;
+	var->assign_hook = assign_hook;
+	var->show_hook = show_hook;
+	define_custom_variable(&var->gen);
+}
+
+void
+DefineCustomEnumVariable(const char *name,
+						 const char *short_desc,
+						 const char *long_desc,
+						 int *valueAddr,
+						 int bootValue,
+						 const struct config_enum_entry *options,
+						 GucContext context,
+						 int flags,
+						 GucEnumCheckHook check_hook,
+						 GucEnumAssignHook assign_hook,
+						 GucShowHook show_hook)
+{
+	struct config_enum *var;
+
+	var = (struct config_enum *)
+		init_custom_variable(name, short_desc, long_desc, context, flags,
+							 PGC_ENUM, sizeof(struct config_enum));
+	var->variable = valueAddr;
+	var->boot_val = bootValue;
+	var->reset_val = bootValue;
+	var->options = options;
+	var->check_hook = check_hook;
+	var->assign_hook = assign_hook;
+	var->show_hook = show_hook;
+	define_custom_variable(&var->gen);
+}
+
+void
+EmitWarningsOnPlaceholders(const char *className)
+{
+	int			classLen = strlen(className);
+	int			i;
+
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		struct config_generic *var = guc_variables[i];
+
+		if ((var->flags & GUC_CUSTOM_PLACEHOLDER) != 0 &&
+			strncmp(className, var->name, classLen) == 0 &&
+			var->name[classLen] == GUC_QUALIFIER_SEPARATOR)
+		{
+			ereport(WARNING,
+					(errcode(ERRCODE_UNDEFINED_OBJECT),
+					 errmsg("unrecognized configuration parameter \"%s\"",
+							var->name)));
+		}
+	}
+}
+
+
+/*
+ * SHOW command
+ */
+void
+GetPGVariable(const char *name, DestReceiver *dest)
+{
+	if (guc_name_compare(name, "all") == 0)
+		ShowAllGUCConfig(dest);
+	else
+		ShowGUCConfigOption(name, dest);
+}
+
+TupleDesc
+GetPGVariableResultDesc(const char *name)
+{
+	TupleDesc	tupdesc;
+
+	if (guc_name_compare(name, "all") == 0)
+	{
+		/* need a tuple descriptor representing three TEXT columns */
+		tupdesc = CreateTemplateTupleDesc(3);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 1, "name",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 2, "setting",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 3, "description",
+						   TEXTOID, -1, 0);
+	}
+	else
+	{
+		const char *varname;
+
+		/* Get the canonical spelling of name */
+		(void) GetConfigOptionByName(name, &varname, false);
+
+		/* need a tuple descriptor representing a single TEXT column */
+		tupdesc = CreateTemplateTupleDesc(1);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 1, varname,
+						   TEXTOID, -1, 0);
+	}
+	return tupdesc;
+}
+
+
+/*
+ * SHOW command
+ */
+static void
+ShowGUCConfigOption(const char *name, DestReceiver *dest)
+{
+	TupOutputState *tstate;
+	TupleDesc	tupdesc;
+	const char *varname;
+	char	   *value;
+
+	/* Get the value and canonical spelling of name */
+	value = GetConfigOptionByName(name, &varname, false);
+
+	/* need a tuple descriptor representing a single TEXT column */
+	tupdesc = CreateTemplateTupleDesc(1);
+	TupleDescInitBuiltinEntry(tupdesc, (AttrNumber) 1, varname,
+							  TEXTOID, -1, 0);
+
+	/* prepare for projection of tuples */
+	tstate = begin_tup_output_tupdesc(dest, tupdesc, &TTSOpsVirtual);
+
+	/* Send it */
+	do_text_output_oneline(tstate, value);
+
+	end_tup_output(tstate);
+}
+
+/*
+ * SHOW ALL command
+ */
+static void
+ShowAllGUCConfig(DestReceiver *dest)
+{
+	int			i;
+	TupOutputState *tstate;
+	TupleDesc	tupdesc;
+	Datum		values[3];
+	bool		isnull[3] = {false, false, false};
+
+	/* need a tuple descriptor representing three TEXT columns */
+	tupdesc = CreateTemplateTupleDesc(3);
+	TupleDescInitBuiltinEntry(tupdesc, (AttrNumber) 1, "name",
+							  TEXTOID, -1, 0);
+	TupleDescInitBuiltinEntry(tupdesc, (AttrNumber) 2, "setting",
+							  TEXTOID, -1, 0);
+	TupleDescInitBuiltinEntry(tupdesc, (AttrNumber) 3, "description",
+							  TEXTOID, -1, 0);
+
+	/* prepare for projection of tuples */
+	tstate = begin_tup_output_tupdesc(dest, tupdesc, &TTSOpsVirtual);
+
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		struct config_generic *conf = guc_variables[i];
+		char	   *setting;
+
+		if ((conf->flags & GUC_NO_SHOW_ALL) ||
+			((conf->flags & GUC_SUPERUSER_ONLY) &&
+			 !is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS)))
+			continue;
+
+		/* assign to the values array */
+		values[0] = PointerGetDatum(cstring_to_text(conf->name));
+
+		setting = _ShowOption(conf, true);
+		if (setting)
+		{
+			values[1] = PointerGetDatum(cstring_to_text(setting));
+			isnull[1] = false;
+		}
+		else
+		{
+			values[1] = PointerGetDatum(NULL);
+			isnull[1] = true;
+		}
+
+		if (conf->short_desc)
+		{
+			values[2] = PointerGetDatum(cstring_to_text(conf->short_desc));
+			isnull[2] = false;
+		}
+		else
+		{
+			values[2] = PointerGetDatum(NULL);
+			isnull[2] = true;
+		}
+
+		/* send it to dest */
+		do_tup_output(tstate, values, isnull);
+
+		/* clean up */
+		pfree(DatumGetPointer(values[0]));
+		if (setting)
+		{
+			pfree(setting);
+			pfree(DatumGetPointer(values[1]));
+		}
+		if (conf->short_desc)
+			pfree(DatumGetPointer(values[2]));
+	}
+
+	end_tup_output(tstate);
+}
+
+/*
+ * Return an array of modified GUC options to show in EXPLAIN.
+ *
+ * We only report options related to query planning (marked with GUC_EXPLAIN),
+ * with values different from their built-in defaults.
+ */
+struct config_generic **
+get_explain_guc_options(int *num)
+{
+	struct config_generic **result;
+
+	*num = 0;
+
+	/*
+	 * While only a fraction of all the GUC variables are marked GUC_EXPLAIN,
+	 * it doesn't seem worth dynamically resizing this array.
+	 */
+	result = palloc(sizeof(struct config_generic *) * num_guc_variables);
+
+	for (int i = 0; i < num_guc_variables; i++)
+	{
+		bool		modified;
+		struct config_generic *conf = guc_variables[i];
+
+		/* return only parameters marked for inclusion in explain */
+		if (!(conf->flags & GUC_EXPLAIN))
+			continue;
+
+		/* return only options visible to the current user */
+		if ((conf->flags & GUC_NO_SHOW_ALL) ||
+			((conf->flags & GUC_SUPERUSER_ONLY) &&
+			 !is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS)))
+			continue;
+
+		/* return only options that are different from their boot values */
+		modified = false;
+
+		switch (conf->vartype)
+		{
+			case PGC_BOOL:
+				{
+					struct config_bool *lconf = (struct config_bool *) conf;
+
+					modified = (lconf->boot_val != *(lconf->variable));
+				}
+				break;
+
+			case PGC_INT:
+				{
+					struct config_int *lconf = (struct config_int *) conf;
+
+					modified = (lconf->boot_val != *(lconf->variable));
+				}
+				break;
+
+			case PGC_REAL:
+				{
+					struct config_real *lconf = (struct config_real *) conf;
+
+					modified = (lconf->boot_val != *(lconf->variable));
+				}
+				break;
+
+			case PGC_STRING:
+				{
+					struct config_string *lconf = (struct config_string *) conf;
+
+					if (lconf->boot_val == NULL &&
+						*lconf->variable == NULL)
+						modified = false;
+					else if (lconf->boot_val == NULL ||
+							 *lconf->variable == NULL)
+						modified = true;
+					else
+						modified = (strcmp(lconf->boot_val, *(lconf->variable)) != 0);
+				}
+				break;
+
+			case PGC_ENUM:
+				{
+					struct config_enum *lconf = (struct config_enum *) conf;
+
+					modified = (lconf->boot_val != *(lconf->variable));
+				}
+				break;
+
+			default:
+				elog(ERROR, "unexpected GUC type: %d", conf->vartype);
+		}
+
+		if (!modified)
+			continue;
+
+		/* OK, report it */
+		result[*num] = conf;
+		*num = *num + 1;
+	}
+
+	return result;
+}
+
+/*
+ * Return GUC variable value by name; optionally return canonical form of
+ * name.  If the GUC is unset, then throw an error unless missing_ok is true,
+ * in which case return NULL.  Return value is palloc'd (but *varname isn't).
+ */
+char *
+GetConfigOptionByName(const char *name, const char **varname, bool missing_ok)
+{
+	struct config_generic *record;
+
+	record = find_option(name, false, ERROR);
+	if (record == NULL)
+	{
+		if (missing_ok)
+		{
+			if (varname)
+				*varname = NULL;
+			return NULL;
+		}
+
+		ereport(ERROR,
+				(errcode(ERRCODE_UNDEFINED_OBJECT),
+				 errmsg("unrecognized configuration parameter \"%s\"", name)));
+	}
+
+	if ((record->flags & GUC_SUPERUSER_ONLY) &&
+		!is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS))
+		ereport(ERROR,
+				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+				 errmsg("must be superuser or a member of pg_read_all_settings to examine \"%s\"",
+						name)));
+
+	if (varname)
+		*varname = record->name;
+
+	return _ShowOption(record, true);
+}
+
+/*
+ * Return GUC variable value by variable number; optionally return canonical
+ * form of name.  Return value is palloc'd.
+ */
+void
+GetConfigOptionByNum(int varnum, const char **values, bool *noshow)
+{
+	char		buffer[256];
+	struct config_generic *conf;
+
+	/* check requested variable number valid */
+	Assert((varnum >= 0) && (varnum < num_guc_variables));
+
+	conf = guc_variables[varnum];
+
+	if (noshow)
+	{
+		if ((conf->flags & GUC_NO_SHOW_ALL) ||
+			((conf->flags & GUC_SUPERUSER_ONLY) &&
+			 !is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS)))
+			*noshow = true;
+		else
+			*noshow = false;
+	}
+
+	/* first get the generic attributes */
+
+	/* name */
+	values[0] = conf->name;
+
+	/* setting: use _ShowOption in order to avoid duplicating the logic */
+	values[1] = _ShowOption(conf, false);
+
+	/* unit, if any (NULL is fine) */
+	values[2] = get_config_unit_name(conf->flags);
+
+	/* group */
+	values[3] = _(config_group_names[conf->group]);
+
+	/* short_desc */
+	values[4] = conf->short_desc != NULL ? _(conf->short_desc) : NULL;
+
+	/* extra_desc */
+	values[5] = conf->long_desc != NULL ? _(conf->long_desc) : NULL;
+
+	/* context */
+	values[6] = GucContext_Names[conf->context];
+
+	/* vartype */
+	values[7] = config_type_names[conf->vartype];
+
+	/* source */
+	values[8] = GucSource_Names[conf->source];
+
+	/* now get the type specific attributes */
+	switch (conf->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *lconf = (struct config_bool *) conf;
+
+				/* min_val */
+				values[9] = NULL;
+
+				/* max_val */
+				values[10] = NULL;
+
+				/* enumvals */
+				values[11] = NULL;
+
+				/* boot_val */
+				values[12] = pstrdup(lconf->boot_val ? "on" : "off");
+
+				/* reset_val */
+				values[13] = pstrdup(lconf->reset_val ? "on" : "off");
+			}
+			break;
+
+		case PGC_INT:
+			{
+				struct config_int *lconf = (struct config_int *) conf;
+
+				/* min_val */
+				snprintf(buffer, sizeof(buffer), "%d", lconf->min);
+				values[9] = pstrdup(buffer);
+
+				/* max_val */
+				snprintf(buffer, sizeof(buffer), "%d", lconf->max);
+				values[10] = pstrdup(buffer);
+
+				/* enumvals */
+				values[11] = NULL;
+
+				/* boot_val */
+				snprintf(buffer, sizeof(buffer), "%d", lconf->boot_val);
+				values[12] = pstrdup(buffer);
+
+				/* reset_val */
+				snprintf(buffer, sizeof(buffer), "%d", lconf->reset_val);
+				values[13] = pstrdup(buffer);
+			}
+			break;
+
+		case PGC_REAL:
+			{
+				struct config_real *lconf = (struct config_real *) conf;
+
+				/* min_val */
+				snprintf(buffer, sizeof(buffer), "%g", lconf->min);
+				values[9] = pstrdup(buffer);
+
+				/* max_val */
+				snprintf(buffer, sizeof(buffer), "%g", lconf->max);
+				values[10] = pstrdup(buffer);
+
+				/* enumvals */
+				values[11] = NULL;
+
+				/* boot_val */
+				snprintf(buffer, sizeof(buffer), "%g", lconf->boot_val);
+				values[12] = pstrdup(buffer);
+
+				/* reset_val */
+				snprintf(buffer, sizeof(buffer), "%g", lconf->reset_val);
+				values[13] = pstrdup(buffer);
+			}
+			break;
+
+		case PGC_STRING:
+			{
+				struct config_string *lconf = (struct config_string *) conf;
+
+				/* min_val */
+				values[9] = NULL;
+
+				/* max_val */
+				values[10] = NULL;
+
+				/* enumvals */
+				values[11] = NULL;
+
+				/* boot_val */
+				if (lconf->boot_val == NULL)
+					values[12] = NULL;
+				else
+					values[12] = pstrdup(lconf->boot_val);
+
+				/* reset_val */
+				if (lconf->reset_val == NULL)
+					values[13] = NULL;
+				else
+					values[13] = pstrdup(lconf->reset_val);
+			}
+			break;
+
+		case PGC_ENUM:
+			{
+				struct config_enum *lconf = (struct config_enum *) conf;
+
+				/* min_val */
+				values[9] = NULL;
+
+				/* max_val */
+				values[10] = NULL;
+
+				/* enumvals */
+
+				/*
+				 * NOTE! enumvals with double quotes in them are not
+				 * supported!
+				 */
+				values[11] = config_enum_get_options((struct config_enum *) conf,
+													 "{\"", "\"}", "\",\"");
+
+				/* boot_val */
+				values[12] = pstrdup(config_enum_lookup_by_value(lconf,
+																 lconf->boot_val));
+
+				/* reset_val */
+				values[13] = pstrdup(config_enum_lookup_by_value(lconf,
+																 lconf->reset_val));
+			}
+			break;
+
+		default:
+			{
+				/*
+				 * should never get here, but in case we do, set 'em to NULL
+				 */
+
+				/* min_val */
+				values[9] = NULL;
+
+				/* max_val */
+				values[10] = NULL;
+
+				/* enumvals */
+				values[11] = NULL;
+
+				/* boot_val */
+				values[12] = NULL;
+
+				/* reset_val */
+				values[13] = NULL;
+			}
+			break;
+	}
+
+	/*
+	 * If the setting came from a config file, set the source location. For
+	 * security reasons, we don't show source file/line number for
+	 * insufficiently-privileged users.
+	 */
+	if (conf->source == PGC_S_FILE &&
+		is_member_of_role(GetUserId(), DEFAULT_ROLE_READ_ALL_SETTINGS))
+	{
+		values[14] = conf->sourcefile;
+		snprintf(buffer, sizeof(buffer), "%d", conf->sourceline);
+		values[15] = pstrdup(buffer);
+	}
+	else
+	{
+		values[14] = NULL;
+		values[15] = NULL;
+	}
+
+	values[16] = (conf->status & GUC_PENDING_RESTART) ? "t" : "f";
+}
+
+/*
+ * Return the total number of GUC variables
+ */
+int
+GetNumConfigOptions(void)
+{
+	return num_guc_variables;
+}
+
+/*
+ * show_config_by_name - equiv to SHOW X command but implemented as
+ * a function.
+ */
+Datum
+show_config_by_name(PG_FUNCTION_ARGS)
+{
+	char	   *varname = TextDatumGetCString(PG_GETARG_DATUM(0));
+	char	   *varval;
+
+	/* Get the value */
+	varval = GetConfigOptionByName(varname, NULL, false);
+
+	/* Convert to text */
+	PG_RETURN_TEXT_P(cstring_to_text(varval));
+}
+
+/*
+ * show_config_by_name_missing_ok - equiv to SHOW X command but implemented as
+ * a function.  If X does not exist, suppress the error and just return NULL
+ * if missing_ok is true.
+ */
+Datum
+show_config_by_name_missing_ok(PG_FUNCTION_ARGS)
+{
+	char	   *varname = TextDatumGetCString(PG_GETARG_DATUM(0));
+	bool		missing_ok = PG_GETARG_BOOL(1);
+	char	   *varval;
+
+	/* Get the value */
+	varval = GetConfigOptionByName(varname, NULL, missing_ok);
+
+	/* return NULL if no such variable */
+	if (varval == NULL)
+		PG_RETURN_NULL();
+
+	/* Convert to text */
+	PG_RETURN_TEXT_P(cstring_to_text(varval));
+}
+
+/*
+ * show_all_settings - equiv to SHOW ALL command but implemented as
+ * a Table Function.
+ */
+#define NUM_PG_SETTINGS_ATTS	17
+
+Datum
+show_all_settings(PG_FUNCTION_ARGS)
+{
+	FuncCallContext *funcctx;
+	TupleDesc	tupdesc;
+	int			call_cntr;
+	int			max_calls;
+	AttInMetadata *attinmeta;
+	MemoryContext oldcontext;
+
+	/* stuff done only on the first call of the function */
+	if (SRF_IS_FIRSTCALL())
+	{
+		/* create a function context for cross-call persistence */
+		funcctx = SRF_FIRSTCALL_INIT();
+
+		/*
+		 * switch to memory context appropriate for multiple function calls
+		 */
+		oldcontext = MemoryContextSwitchTo(funcctx->multi_call_memory_ctx);
+
+		/*
+		 * need a tuple descriptor representing NUM_PG_SETTINGS_ATTS columns
+		 * of the appropriate types
+		 */
+		tupdesc = CreateTemplateTupleDesc(NUM_PG_SETTINGS_ATTS);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 1, "name",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 2, "setting",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 3, "unit",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 4, "category",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 5, "short_desc",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 6, "extra_desc",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 7, "context",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 8, "vartype",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 9, "source",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 10, "min_val",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 11, "max_val",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 12, "enumvals",
+						   TEXTARRAYOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 13, "boot_val",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 14, "reset_val",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 15, "sourcefile",
+						   TEXTOID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 16, "sourceline",
+						   INT4OID, -1, 0);
+		TupleDescInitEntry(tupdesc, (AttrNumber) 17, "pending_restart",
+						   BOOLOID, -1, 0);
+
+		/*
+		 * Generate attribute metadata needed later to produce tuples from raw
+		 * C strings
+		 */
+		attinmeta = TupleDescGetAttInMetadata(tupdesc);
+		funcctx->attinmeta = attinmeta;
+
+		/* total number of tuples to be returned */
+		funcctx->max_calls = GetNumConfigOptions();
+
+		MemoryContextSwitchTo(oldcontext);
+	}
+
+	/* stuff done on every call of the function */
+	funcctx = SRF_PERCALL_SETUP();
+
+	call_cntr = funcctx->call_cntr;
+	max_calls = funcctx->max_calls;
+	attinmeta = funcctx->attinmeta;
+
+	if (call_cntr < max_calls)	/* do when there is more left to send */
+	{
+		char	   *values[NUM_PG_SETTINGS_ATTS];
+		bool		noshow;
+		HeapTuple	tuple;
+		Datum		result;
+
+		/*
+		 * Get the next visible GUC variable name and value
+		 */
+		do
+		{
+			GetConfigOptionByNum(call_cntr, (const char **) values, &noshow);
+			if (noshow)
+			{
+				/* bump the counter and get the next config setting */
+				call_cntr = ++funcctx->call_cntr;
+
+				/* make sure we haven't gone too far now */
+				if (call_cntr >= max_calls)
+					SRF_RETURN_DONE(funcctx);
+			}
+		} while (noshow);
+
+		/* build a tuple */
+		tuple = BuildTupleFromCStrings(attinmeta, values);
+
+		/* make the tuple into a datum */
+		result = HeapTupleGetDatum(tuple);
+
+		SRF_RETURN_NEXT(funcctx, result);
+	}
+	else
+	{
+		/* do when there is no more left */
+		SRF_RETURN_DONE(funcctx);
+	}
+}
+
+/*
+ * show_all_file_settings
+ *
+ * Returns a table of all parameter settings in all configuration files
+ * which includes the config file pathname, the line number, a sequence number
+ * indicating the order in which the settings were encountered, the parameter
+ * name and value, a bool showing if the value could be applied, and possibly
+ * an associated error message.  (For problems such as syntax errors, the
+ * parameter name/value might be NULL.)
+ *
+ * Note: no filtering is done here, instead we depend on the GRANT system
+ * to prevent unprivileged users from accessing this function or the view
+ * built on top of it.
+ */
+Datum
+show_all_file_settings(PG_FUNCTION_ARGS)
+{
+#define NUM_PG_FILE_SETTINGS_ATTS 7
+	ReturnSetInfo *rsinfo = (ReturnSetInfo *) fcinfo->resultinfo;
+	TupleDesc	tupdesc;
+	Tuplestorestate *tupstore;
+	ConfigVariable *conf;
+	int			seqno;
+	MemoryContext per_query_ctx;
+	MemoryContext oldcontext;
+
+	/* Check to see if caller supports us returning a tuplestore */
+	if (rsinfo == NULL || !IsA(rsinfo, ReturnSetInfo))
+		ereport(ERROR,
+				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+				 errmsg("set-valued function called in context that cannot accept a set")));
+	if (!(rsinfo->allowedModes & SFRM_Materialize))
+		ereport(ERROR,
+				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
+				 errmsg("materialize mode required, but it is not " \
+						"allowed in this context")));
+
+	/* Scan the config files using current context as workspace */
+	conf = ProcessConfigFileInternal(PGC_SIGHUP, false, DEBUG3);
+
+	/* Switch into long-lived context to construct returned data structures */
+	per_query_ctx = rsinfo->econtext->ecxt_per_query_memory;
+	oldcontext = MemoryContextSwitchTo(per_query_ctx);
+
+	/* Build a tuple descriptor for our result type */
+	tupdesc = CreateTemplateTupleDesc(NUM_PG_FILE_SETTINGS_ATTS);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 1, "sourcefile",
+					   TEXTOID, -1, 0);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 2, "sourceline",
+					   INT4OID, -1, 0);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 3, "seqno",
+					   INT4OID, -1, 0);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 4, "name",
+					   TEXTOID, -1, 0);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 5, "setting",
+					   TEXTOID, -1, 0);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 6, "applied",
+					   BOOLOID, -1, 0);
+	TupleDescInitEntry(tupdesc, (AttrNumber) 7, "error",
+					   TEXTOID, -1, 0);
+
+	/* Build a tuplestore to return our results in */
+	tupstore = tuplestore_begin_heap(true, false, work_mem);
+	rsinfo->returnMode = SFRM_Materialize;
+	rsinfo->setResult = tupstore;
+	rsinfo->setDesc = tupdesc;
+
+	/* The rest can be done in short-lived context */
+	MemoryContextSwitchTo(oldcontext);
+
+	/* Process the results and create a tuplestore */
+	for (seqno = 1; conf != NULL; conf = conf->next, seqno++)
+	{
+		Datum		values[NUM_PG_FILE_SETTINGS_ATTS];
+		bool		nulls[NUM_PG_FILE_SETTINGS_ATTS];
+
+		memset(values, 0, sizeof(values));
+		memset(nulls, 0, sizeof(nulls));
+
+		/* sourcefile */
+		if (conf->filename)
+			values[0] = PointerGetDatum(cstring_to_text(conf->filename));
+		else
+			nulls[0] = true;
+
+		/* sourceline (not meaningful if no sourcefile) */
+		if (conf->filename)
+			values[1] = Int32GetDatum(conf->sourceline);
+		else
+			nulls[1] = true;
+
+		/* seqno */
+		values[2] = Int32GetDatum(seqno);
+
+		/* name */
+		if (conf->name)
+			values[3] = PointerGetDatum(cstring_to_text(conf->name));
+		else
+			nulls[3] = true;
+
+		/* setting */
+		if (conf->value)
+			values[4] = PointerGetDatum(cstring_to_text(conf->value));
+		else
+			nulls[4] = true;
+
+		/* applied */
+		values[5] = BoolGetDatum(conf->applied);
+
+		/* error */
+		if (conf->errmsg)
+			values[6] = PointerGetDatum(cstring_to_text(conf->errmsg));
+		else
+			nulls[6] = true;
+
+		/* shove row into tuplestore */
+		tuplestore_putvalues(tupstore, tupdesc, values, nulls);
+	}
+
+	tuplestore_donestoring(tupstore);
+
+	return (Datum) 0;
+}
+
+static char *
+_ShowOption(struct config_generic *record, bool use_units)
+{
+	char		buffer[256];
+	const char *val;
+
+	switch (record->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *conf = (struct config_bool *) record;
+
+				if (conf->show_hook)
+					val = conf->show_hook();
+				else
+					val = *conf->variable ? "on" : "off";
+			}
+			break;
+
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) record;
+
+				if (conf->show_hook)
+					val = conf->show_hook();
+				else
+				{
+					/*
+					 * Use int64 arithmetic to avoid overflows in units
+					 * conversion.
+					 */
+					int64		result = *conf->variable;
+					const char *unit;
+
+					if (use_units && result > 0 && (record->flags & GUC_UNIT))
+						convert_int_from_base_unit(result,
+												   record->flags & GUC_UNIT,
+												   &result, &unit);
+					else
+						unit = "";
+
+					snprintf(buffer, sizeof(buffer), INT64_FORMAT "%s",
+							 result, unit);
+					val = buffer;
+				}
+			}
+			break;
+
+		case PGC_REAL:
+			{
+				struct config_real *conf = (struct config_real *) record;
+
+				if (conf->show_hook)
+					val = conf->show_hook();
+				else
+				{
+					double		result = *conf->variable;
+					const char *unit;
+
+					if (use_units && result > 0 && (record->flags & GUC_UNIT))
+						convert_real_from_base_unit(result,
+													record->flags & GUC_UNIT,
+													&result, &unit);
+					else
+						unit = "";
+
+					snprintf(buffer, sizeof(buffer), "%g%s",
+							 result, unit);
+					val = buffer;
+				}
+			}
+			break;
+
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) record;
+
+				if (conf->show_hook)
+					val = conf->show_hook();
+				else if (*conf->variable && **conf->variable)
+					val = *conf->variable;
+				else
+					val = "";
+			}
+			break;
+
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) record;
+
+				if (conf->show_hook)
+					val = conf->show_hook();
+				else
+					val = config_enum_lookup_by_value(conf, *conf->variable);
+			}
+			break;
+
+		default:
+			/* just to keep compiler quiet */
+			val = "???";
+			break;
+	}
+
+	return pstrdup(val);
+}
+
+
+#ifdef EXEC_BACKEND
+
+/*
+ *	These routines dump out all non-default GUC options into a binary
+ *	file that is read by all exec'ed backends.  The format is:
+ *
+ *		variable name, string, null terminated
+ *		variable value, string, null terminated
+ *		variable sourcefile, string, null terminated (empty if none)
+ *		variable sourceline, integer
+ *		variable source, integer
+ *		variable scontext, integer
+ */
+static void
+write_one_nondefault_variable(FILE *fp, struct config_generic *gconf)
+{
+	if (gconf->source == PGC_S_DEFAULT)
+		return;
+
+	fprintf(fp, "%s", gconf->name);
+	fputc(0, fp);
+
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *conf = (struct config_bool *) gconf;
+
+				if (*conf->variable)
+					fprintf(fp, "true");
+				else
+					fprintf(fp, "false");
+			}
+			break;
+
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) gconf;
+
+				fprintf(fp, "%d", *conf->variable);
+			}
+			break;
+
+		case PGC_REAL:
+			{
+				struct config_real *conf = (struct config_real *) gconf;
+
+				fprintf(fp, "%.17g", *conf->variable);
+			}
+			break;
+
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) gconf;
+
+				if (*conf->variable)
+					fprintf(fp, "%s", *conf->variable);
+			}
+			break;
+
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) gconf;
+
+				fprintf(fp, "%s",
+						config_enum_lookup_by_value(conf, *conf->variable));
+			}
+			break;
+	}
+
+	fputc(0, fp);
+
+	if (gconf->sourcefile)
+		fprintf(fp, "%s", gconf->sourcefile);
+	fputc(0, fp);
+
+	fwrite(&gconf->sourceline, 1, sizeof(gconf->sourceline), fp);
+	fwrite(&gconf->source, 1, sizeof(gconf->source), fp);
+	fwrite(&gconf->scontext, 1, sizeof(gconf->scontext), fp);
+}
+
+void
+write_nondefault_variables(GucContext context)
+{
+	int			elevel;
+	FILE	   *fp;
+	int			i;
+
+	Assert(context == PGC_POSTMASTER || context == PGC_SIGHUP);
+
+	elevel = (context == PGC_SIGHUP) ? LOG : ERROR;
+
+	/*
+	 * Open file
+	 */
+	fp = AllocateFile(CONFIG_EXEC_PARAMS_NEW, "w");
+	if (!fp)
+	{
+		ereport(elevel,
+				(errcode_for_file_access(),
+				 errmsg("could not write to file \"%s\": %m",
+						CONFIG_EXEC_PARAMS_NEW)));
+		return;
+	}
+
+	for (i = 0; i < num_guc_variables; i++)
+	{
+		write_one_nondefault_variable(fp, guc_variables[i]);
+	}
+
+	if (FreeFile(fp))
+	{
+		ereport(elevel,
+				(errcode_for_file_access(),
+				 errmsg("could not write to file \"%s\": %m",
+						CONFIG_EXEC_PARAMS_NEW)));
+		return;
+	}
+
+	/*
+	 * Put new file in place.  This could delay on Win32, but we don't hold
+	 * any exclusive locks.
+	 */
+	rename(CONFIG_EXEC_PARAMS_NEW, CONFIG_EXEC_PARAMS);
+}
+
+
+/*
+ *	Read string, including null byte from file
+ *
+ *	Return NULL on EOF and nothing read
+ */
+static char *
+read_string_with_null(FILE *fp)
+{
+	int			i = 0,
+				ch,
+				maxlen = 256;
+	char	   *str = NULL;
+
+	do
+	{
+		if ((ch = fgetc(fp)) == EOF)
+		{
+			if (i == 0)
+				return NULL;
+			else
+				elog(FATAL, "invalid format of exec config params file");
+		}
+		if (i == 0)
+			str = guc_malloc(FATAL, maxlen);
+		else if (i == maxlen)
+			str = guc_realloc(FATAL, str, maxlen *= 2);
+		str[i++] = ch;
+	} while (ch != 0);
+
+	return str;
+}
+
+
+/*
+ *	This routine loads a previous postmaster dump of its non-default
+ *	settings.
+ */
+void
+read_nondefault_variables(void)
+{
+	FILE	   *fp;
+	char	   *varname,
+			   *varvalue,
+			   *varsourcefile;
+	int			varsourceline;
+	GucSource	varsource;
+	GucContext	varscontext;
+
+	/*
+	 * Assert that PGC_BACKEND/PGC_SU_BACKEND case in set_config_option() will
+	 * do the right thing.
+	 */
+	Assert(IsInitProcessingMode());
+
+	/*
+	 * Open file
+	 */
+	fp = AllocateFile(CONFIG_EXEC_PARAMS, "r");
+	if (!fp)
+	{
+		/* File not found is fine */
+		if (errno != ENOENT)
+			ereport(FATAL,
+					(errcode_for_file_access(),
+					 errmsg("could not read from file \"%s\": %m",
+							CONFIG_EXEC_PARAMS)));
+		return;
+	}
+
+	for (;;)
+	{
+		struct config_generic *record;
+
+		if ((varname = read_string_with_null(fp)) == NULL)
+			break;
+
+		if ((record = find_option(varname, true, FATAL)) == NULL)
+			elog(FATAL, "failed to locate variable \"%s\" in exec config params file", varname);
+
+		if ((varvalue = read_string_with_null(fp)) == NULL)
+			elog(FATAL, "invalid format of exec config params file");
+		if ((varsourcefile = read_string_with_null(fp)) == NULL)
+			elog(FATAL, "invalid format of exec config params file");
+		if (fread(&varsourceline, 1, sizeof(varsourceline), fp) != sizeof(varsourceline))
+			elog(FATAL, "invalid format of exec config params file");
+		if (fread(&varsource, 1, sizeof(varsource), fp) != sizeof(varsource))
+			elog(FATAL, "invalid format of exec config params file");
+		if (fread(&varscontext, 1, sizeof(varscontext), fp) != sizeof(varscontext))
+			elog(FATAL, "invalid format of exec config params file");
+
+		(void) set_config_option(varname, varvalue,
+								 varscontext, varsource,
+								 GUC_ACTION_SET, true, 0, true);
+		if (varsourcefile[0])
+			set_config_sourcefile(varname, varsourcefile, varsourceline);
+
+		free(varname);
+		free(varvalue);
+		free(varsourcefile);
+	}
+
+	FreeFile(fp);
+}
+#endif							/* EXEC_BACKEND */
+
+/*
+ * can_skip_gucvar:
+ * When serializing, determine whether to skip this GUC.  When restoring, the
+ * negation of this test determines whether to restore the compiled-in default
+ * value before processing serialized values.
+ *
+ * A PGC_S_DEFAULT setting on the serialize side will typically match new
+ * postmaster children, but that can be false when got_SIGHUP == true and the
+ * pending configuration change modifies this setting.  Nonetheless, we omit
+ * PGC_S_DEFAULT settings from serialization and make up for that by restoring
+ * defaults before applying serialized values.
+ *
+ * PGC_POSTMASTER variables always have the same value in every child of a
+ * particular postmaster.  Most PGC_INTERNAL variables are compile-time
+ * constants; a few, like server_encoding and lc_ctype, are handled specially
+ * outside the serialize/restore procedure.  Therefore, SerializeGUCState()
+ * never sends these, and RestoreGUCState() never changes them.
+ *
+ * Role is a special variable in the sense that its current value can be an
+ * invalid value and there are multiple ways by which that can happen (like
+ * after setting the role, someone drops it).  So we handle it outside of
+ * serialize/restore machinery.
+ */
+static bool
+can_skip_gucvar(struct config_generic *gconf)
+{
+	return gconf->context == PGC_POSTMASTER ||
+		gconf->context == PGC_INTERNAL || gconf->source == PGC_S_DEFAULT ||
+		strcmp(gconf->name, "role") == 0;
+}
+
+/*
+ * estimate_variable_size:
+ *		Compute space needed for dumping the given GUC variable.
+ *
+ * It's OK to overestimate, but not to underestimate.
+ */
+static Size
+estimate_variable_size(struct config_generic *gconf)
+{
+	Size		size;
+	Size		valsize = 0;
+
+	if (can_skip_gucvar(gconf))
+		return 0;
+
+	/* Name, plus trailing zero byte. */
+	size = strlen(gconf->name) + 1;
+
+	/* Get the maximum display length of the GUC value. */
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+			{
+				valsize = 5;	/* max(strlen('true'), strlen('false')) */
+			}
+			break;
+
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) gconf;
+
+				/*
+				 * Instead of getting the exact display length, use max
+				 * length.  Also reduce the max length for typical ranges of
+				 * small values.  Maximum value is 2147483647, i.e. 10 chars.
+				 * Include one byte for sign.
+				 */
+				if (Abs(*conf->variable) < 1000)
+					valsize = 3 + 1;
+				else
+					valsize = 10 + 1;
+			}
+			break;
+
+		case PGC_REAL:
+			{
+				/*
+				 * We are going to print it with %e with REALTYPE_PRECISION
+				 * fractional digits.  Account for sign, leading digit,
+				 * decimal point, and exponent with up to 3 digits.  E.g.
+				 * -3.99329042340000021e+110
+				 */
+				valsize = 1 + 1 + 1 + REALTYPE_PRECISION + 5;
+			}
+			break;
+
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) gconf;
+
+				/*
+				 * If the value is NULL, we transmit it as an empty string.
+				 * Although this is not physically the same value, GUC
+				 * generally treats a NULL the same as empty string.
+				 */
+				if (*conf->variable)
+					valsize = strlen(*conf->variable);
+				else
+					valsize = 0;
+			}
+			break;
+
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) gconf;
+
+				valsize = strlen(config_enum_lookup_by_value(conf, *conf->variable));
+			}
+			break;
+	}
+
+	/* Allow space for terminating zero-byte for value */
+	size = add_size(size, valsize + 1);
+
+	if (gconf->sourcefile)
+		size = add_size(size, strlen(gconf->sourcefile));
+
+	/* Allow space for terminating zero-byte for sourcefile */
+	size = add_size(size, 1);
+
+	/* Include line whenever file is nonempty. */
+	if (gconf->sourcefile && gconf->sourcefile[0])
+		size = add_size(size, sizeof(gconf->sourceline));
+
+	size = add_size(size, sizeof(gconf->source));
+	size = add_size(size, sizeof(gconf->scontext));
+
+	return size;
+}
+
+/*
+ * EstimateGUCStateSpace:
+ * Returns the size needed to store the GUC state for the current process
+ */
+Size
+EstimateGUCStateSpace(void)
+{
+	Size		size;
+	int			i;
+
+	/* Add space reqd for saving the data size of the guc state */
+	size = sizeof(Size);
+
+	/* Add up the space needed for each GUC variable */
+	for (i = 0; i < num_guc_variables; i++)
+		size = add_size(size,
+						estimate_variable_size(guc_variables[i]));
+
+	return size;
+}
+
+/*
+ * do_serialize:
+ * Copies the formatted string into the destination.  Moves ahead the
+ * destination pointer, and decrements the maxbytes by that many bytes. If
+ * maxbytes is not sufficient to copy the string, error out.
+ */
+static void
+do_serialize(char **destptr, Size *maxbytes, const char *fmt,...)
+{
+	va_list		vargs;
+	int			n;
+
+	if (*maxbytes <= 0)
+		elog(ERROR, "not enough space to serialize GUC state");
+
+	va_start(vargs, fmt);
+	n = vsnprintf(*destptr, *maxbytes, fmt, vargs);
+	va_end(vargs);
+
+	if (n < 0)
+	{
+		/* Shouldn't happen. Better show errno description. */
+		elog(ERROR, "vsnprintf failed: %m with format string \"%s\"", fmt);
+	}
+	if (n >= *maxbytes)
+	{
+		/* This shouldn't happen either, really. */
+		elog(ERROR, "not enough space to serialize GUC state");
+	}
+
+	/* Shift the destptr ahead of the null terminator */
+	*destptr += n + 1;
+	*maxbytes -= n + 1;
+}
+
+/* Binary copy version of do_serialize() */
+static void
+do_serialize_binary(char **destptr, Size *maxbytes, void *val, Size valsize)
+{
+	if (valsize > *maxbytes)
+		elog(ERROR, "not enough space to serialize GUC state");
+
+	memcpy(*destptr, val, valsize);
+	*destptr += valsize;
+	*maxbytes -= valsize;
+}
+
+/*
+ * serialize_variable:
+ * Dumps name, value and other information of a GUC variable into destptr.
+ */
+static void
+serialize_variable(char **destptr, Size *maxbytes,
+				   struct config_generic *gconf)
+{
+	if (can_skip_gucvar(gconf))
+		return;
+
+	do_serialize(destptr, maxbytes, "%s", gconf->name);
+
+	switch (gconf->vartype)
+	{
+		case PGC_BOOL:
+			{
+				struct config_bool *conf = (struct config_bool *) gconf;
+
+				do_serialize(destptr, maxbytes,
+							 (*conf->variable ? "true" : "false"));
+			}
+			break;
+
+		case PGC_INT:
+			{
+				struct config_int *conf = (struct config_int *) gconf;
+
+				do_serialize(destptr, maxbytes, "%d", *conf->variable);
+			}
+			break;
+
+		case PGC_REAL:
+			{
+				struct config_real *conf = (struct config_real *) gconf;
+
+				do_serialize(destptr, maxbytes, "%.*e",
+							 REALTYPE_PRECISION, *conf->variable);
+			}
+			break;
+
+		case PGC_STRING:
+			{
+				struct config_string *conf = (struct config_string *) gconf;
+
+				/* NULL becomes empty string, see estimate_variable_size() */
+				do_serialize(destptr, maxbytes, "%s",
+							 *conf->variable ? *conf->variable : "");
+			}
+			break;
+
+		case PGC_ENUM:
+			{
+				struct config_enum *conf = (struct config_enum *) gconf;
+
+				do_serialize(destptr, maxbytes, "%s",
+							 config_enum_lookup_by_value(conf, *conf->variable));
+			}
+			break;
+	}
+
+	do_serialize(destptr, maxbytes, "%s",
+				 (gconf->sourcefile ? gconf->sourcefile : ""));
+
+	if (gconf->sourcefile && gconf->sourcefile[0])
+		do_serialize_binary(destptr, maxbytes, &gconf->sourceline,
+							sizeof(gconf->sourceline));
+
+	do_serialize_binary(destptr, maxbytes, &gconf->source,
+						sizeof(gconf->source));
+	do_serialize_binary(destptr, maxbytes, &gconf->scontext,
+						sizeof(gconf->scontext));
+}
+
+/*
+ * SerializeGUCState:
+ * Dumps the complete GUC state onto the memory location at start_address.
+ */
+void
+SerializeGUCState(Size maxsize, char *start_address)
+{
+	char	   *curptr;
+	Size		actual_size;
+	Size		bytes_left;
+	int			i;
+
+	/* Reserve space for saving the actual size of the guc state */
+	Assert(maxsize > sizeof(actual_size));
+	curptr = start_address + sizeof(actual_size);
+	bytes_left = maxsize - sizeof(actual_size);
+
+	for (i = 0; i < num_guc_variables; i++)
+		serialize_variable(&curptr, &bytes_left, guc_variables[i]);
+
+	/* Store actual size without assuming alignment of start_address. */
+	actual_size = maxsize - bytes_left - sizeof(actual_size);
+	memcpy(start_address, &actual_size, sizeof(actual_size));
+}
+
+/*
+ * read_gucstate:
+ * Actually it does not read anything, just returns the srcptr. But it does
+ * move the srcptr past the terminating zero byte, so that the caller is ready
+ * to read the next string.
+ */
+static char *
+read_gucstate(char **srcptr, char *srcend)
+{
+	char	   *retptr = *srcptr;
+	char	   *ptr;
+
+	if (*srcptr >= srcend)
+		elog(ERROR, "incomplete GUC state");
+
+	/* The string variables are all null terminated */
+	for (ptr = *srcptr; ptr < srcend && *ptr != '\0'; ptr++)
+		;
+
+	if (ptr >= srcend)
+		elog(ERROR, "could not find null terminator in GUC state");
+
+	/* Set the new position to the byte following the terminating NUL */
+	*srcptr = ptr + 1;
+
+	return retptr;
+}
+
+/* Binary read version of read_gucstate(). Copies into dest */
+static void
+read_gucstate_binary(char **srcptr, char *srcend, void *dest, Size size)
+{
+	if (*srcptr + size > srcend)
+		elog(ERROR, "incomplete GUC state");
+
+	memcpy(dest, *srcptr, size);
+	*srcptr += size;
+}
+
+/*
+ * Callback used to add a context message when reporting errors that occur
+ * while trying to restore GUCs in parallel workers.
+ */
+static void
+guc_restore_error_context_callback(void *arg)
+{
+	char	  **error_context_name_and_value = (char **) arg;
+
+	if (error_context_name_and_value)
+		errcontext("while setting parameter \"%s\" to \"%s\"",
+				   error_context_name_and_value[0],
+				   error_context_name_and_value[1]);
+}
+
+/*
+ * RestoreGUCState:
+ * Reads the GUC state at the specified address and updates the GUCs with the
+ * values read from the GUC state.
+ */
+void
+RestoreGUCState(void *gucstate)
+{
+	char	   *varname,
+			   *varvalue,
+			   *varsourcefile;
+	int			varsourceline;
+	GucSource	varsource;
+	GucContext	varscontext;
+	char	   *srcptr = (char *) gucstate;
+	char	   *srcend;
+	Size		len;
+	int			i;
+	ErrorContextCallback error_context_callback;
+
+	/* See comment at can_skip_gucvar(). */
+	for (i = 0; i < num_guc_variables; i++)
+		if (!can_skip_gucvar(guc_variables[i]))
+			InitializeOneGUCOption(guc_variables[i]);
+
+	/* First item is the length of the subsequent data */
+	memcpy(&len, gucstate, sizeof(len));
+
+	srcptr += sizeof(len);
+	srcend = srcptr + len;
+
+	/* If the GUC value check fails, we want errors to show useful context. */
+	error_context_callback.callback = guc_restore_error_context_callback;
+	error_context_callback.previous = error_context_stack;
+	error_context_callback.arg = NULL;
+	error_context_stack = &error_context_callback;
+
+	while (srcptr < srcend)
+	{
+		int			result;
+		char	   *error_context_name_and_value[2];
+
+		varname = read_gucstate(&srcptr, srcend);
+		varvalue = read_gucstate(&srcptr, srcend);
+		varsourcefile = read_gucstate(&srcptr, srcend);
+		if (varsourcefile[0])
+			read_gucstate_binary(&srcptr, srcend,
+								 &varsourceline, sizeof(varsourceline));
+		else
+			varsourceline = 0;
+		read_gucstate_binary(&srcptr, srcend,
+							 &varsource, sizeof(varsource));
+		read_gucstate_binary(&srcptr, srcend,
+							 &varscontext, sizeof(varscontext));
+
+		error_context_name_and_value[0] = varname;
+		error_context_name_and_value[1] = varvalue;
+		error_context_callback.arg = &error_context_name_and_value[0];
+		result = set_config_option(varname, varvalue, varscontext, varsource,
+								   GUC_ACTION_SET, true, ERROR, true);
+		if (result <= 0)
+			ereport(ERROR,
+					(errcode(ERRCODE_INTERNAL_ERROR),
+					 errmsg("parameter \"%s\" could not be set", varname)));
+		if (varsourcefile[0])
+			set_config_sourcefile(varname, varsourcefile, varsourceline);
+		error_context_callback.arg = NULL;
+	}
+
+	error_context_stack = error_context_callback.previous;
+}
+
+/*
+ * A little "long argument" simulation, although not quite GNU
+ * compliant. Takes a string of the form "some-option=some value" and
+ * returns name = "some_option" and value = "some value" in malloc'ed
+ * storage. Note that '-' is converted to '_' in the option name. If
+ * there is no '=' in the input string then value will be NULL.
+ */
+void
+ParseLongOption(const char *string, char **name, char **value)
+{
+	size_t		equal_pos;
+	char	   *cp;
+
+	AssertArg(string);
+	AssertArg(name);
+	AssertArg(value);
+
+	equal_pos = strcspn(string, "=");
+
+	if (string[equal_pos] == '=')
+	{
+		*name = guc_malloc(FATAL, equal_pos + 1);
+		strlcpy(*name, string, equal_pos + 1);
+
+		*value = guc_strdup(FATAL, &string[equal_pos + 1]);
+	}
+	else
+	{
+		/* no equal sign in string */
+		*name = guc_strdup(FATAL, string);
+		*value = NULL;
+	}
+
+	for (cp = *name; *cp; cp++)
+		if (*cp == '-')
+			*cp = '_';
+}
+
+
+/*
+ * Handle options fetched from pg_db_role_setting.setconfig,
+ * pg_proc.proconfig, etc.  Caller must specify proper context/source/action.
+ *
+ * The array parameter must be an array of TEXT (it must not be NULL).
+ */
+void
+ProcessGUCArray(ArrayType *array,
+				GucContext context, GucSource source, GucAction action)
+{
+	int			i;
+
+	Assert(array != NULL);
+	Assert(ARR_ELEMTYPE(array) == TEXTOID);
+	Assert(ARR_NDIM(array) == 1);
+	Assert(ARR_LBOUND(array)[0] == 1);
+
+	for (i = 1; i <= ARR_DIMS(array)[0]; i++)
+	{
+		Datum		d;
+		bool		isnull;
+		char	   *s;
+		char	   *name;
+		char	   *value;
+		char	   *namecopy;
+		char	   *valuecopy;
+
+		d = array_ref(array, 1, &i,
+					  -1 /* varlenarray */ ,
+					  -1 /* TEXT's typlen */ ,
+					  false /* TEXT's typbyval */ ,
+					  'i' /* TEXT's typalign */ ,
+					  &isnull);
+
+		if (isnull)
+			continue;
+
+		s = TextDatumGetCString(d);
+
+		ParseLongOption(s, &name, &value);
+		if (!value)
+		{
+			ereport(WARNING,
+					(errcode(ERRCODE_SYNTAX_ERROR),
+					 errmsg("could not parse setting for parameter \"%s\"",
+							name)));
+			free(name);
+			continue;
+		}
+
+		/* free malloc'd strings immediately to avoid leak upon error */
+		namecopy = pstrdup(name);
+		free(name);
+		valuecopy = pstrdup(value);
+		free(value);
+
+		(void) set_config_option(namecopy, valuecopy,
+								 context, source,
+								 action, true, 0, false);
+
+		pfree(namecopy);
+		pfree(valuecopy);
+		pfree(s);
+	}
+}
+
+
+/*
+ * Add an entry to an option array.  The array parameter may be NULL
+ * to indicate the current table entry is NULL.
+ */
+ArrayType *
+GUCArrayAdd(ArrayType *array, const char *name, const char *value)
+{
+	struct config_generic *record;
+	Datum		datum;
+	char	   *newval;
+	ArrayType  *a;
+
+	Assert(name);
+	Assert(value);
+
+	/* test if the option is valid and we're allowed to set it */
+	(void) validate_option_array_item(name, value, false);
+
+	/* normalize name (converts obsolete GUC names to modern spellings) */
+	record = find_option(name, false, WARNING);
+	if (record)
+		name = record->name;
+
+	/* build new item for array */
+	newval = psprintf("%s=%s", name, value);
+	datum = CStringGetTextDatum(newval);
+
+	if (array)
+	{
+		int			index;
+		bool		isnull;
+		int			i;
+
+		Assert(ARR_ELEMTYPE(array) == TEXTOID);
+		Assert(ARR_NDIM(array) == 1);
+		Assert(ARR_LBOUND(array)[0] == 1);
+
+		index = ARR_DIMS(array)[0] + 1; /* add after end */
+
+		for (i = 1; i <= ARR_DIMS(array)[0]; i++)
+		{
+			Datum		d;
+			char	   *current;
+
+			d = array_ref(array, 1, &i,
+						  -1 /* varlenarray */ ,
+						  -1 /* TEXT's typlen */ ,
+						  false /* TEXT's typbyval */ ,
+						  'i' /* TEXT's typalign */ ,
+						  &isnull);
+			if (isnull)
+				continue;
+			current = TextDatumGetCString(d);
+
+			/* check for match up through and including '=' */
+			if (strncmp(current, newval, strlen(name) + 1) == 0)
+			{
+				index = i;
+				break;
+			}
+		}
+
+		a = array_set(array, 1, &index,
+					  datum,
+					  false,
+					  -1 /* varlena array */ ,
+					  -1 /* TEXT's typlen */ ,
+					  false /* TEXT's typbyval */ ,
+					  'i' /* TEXT's typalign */ );
+	}
+	else
+		a = construct_array(&datum, 1,
+							TEXTOID,
+							-1, false, 'i');
+
+	return a;
+}
+
+
+/*
+ * Delete an entry from an option array.  The array parameter may be NULL
+ * to indicate the current table entry is NULL.  Also, if the return value
+ * is NULL then a null should be stored.
+ */
+ArrayType *
+GUCArrayDelete(ArrayType *array, const char *name)
+{
+	struct config_generic *record;
+	ArrayType  *newarray;
+	int			i;
+	int			index;
+
+	Assert(name);
+
+	/* test if the option is valid and we're allowed to set it */
+	(void) validate_option_array_item(name, NULL, false);
+
+	/* normalize name (converts obsolete GUC names to modern spellings) */
+	record = find_option(name, false, WARNING);
+	if (record)
+		name = record->name;
+
+	/* if array is currently null, then surely nothing to delete */
+	if (!array)
+		return NULL;
+
+	newarray = NULL;
+	index = 1;
+
+	for (i = 1; i <= ARR_DIMS(array)[0]; i++)
+	{
+		Datum		d;
+		char	   *val;
+		bool		isnull;
+
+		d = array_ref(array, 1, &i,
+					  -1 /* varlenarray */ ,
+					  -1 /* TEXT's typlen */ ,
+					  false /* TEXT's typbyval */ ,
+					  'i' /* TEXT's typalign */ ,
+					  &isnull);
+		if (isnull)
+			continue;
+		val = TextDatumGetCString(d);
+
+		/* ignore entry if it's what we want to delete */
+		if (strncmp(val, name, strlen(name)) == 0
+			&& val[strlen(name)] == '=')
+			continue;
+
+		/* else add it to the output array */
+		if (newarray)
+			newarray = array_set(newarray, 1, &index,
+								 d,
+								 false,
+								 -1 /* varlenarray */ ,
+								 -1 /* TEXT's typlen */ ,
+								 false /* TEXT's typbyval */ ,
+								 'i' /* TEXT's typalign */ );
+		else
+			newarray = construct_array(&d, 1,
+									   TEXTOID,
+									   -1, false, 'i');
+
+		index++;
+	}
+
+	return newarray;
+}
+
+
+/*
+ * Given a GUC array, delete all settings from it that our permission
+ * level allows: if superuser, delete them all; if regular user, only
+ * those that are PGC_USERSET
+ */
+ArrayType *
+GUCArrayReset(ArrayType *array)
+{
+	ArrayType  *newarray;
+	int			i;
+	int			index;
+
+	/* if array is currently null, nothing to do */
+	if (!array)
+		return NULL;
+
+	/* if we're superuser, we can delete everything, so just do it */
+	if (superuser())
+		return NULL;
+
+	newarray = NULL;
+	index = 1;
+
+	for (i = 1; i <= ARR_DIMS(array)[0]; i++)
+	{
+		Datum		d;
+		char	   *val;
+		char	   *eqsgn;
+		bool		isnull;
+
+		d = array_ref(array, 1, &i,
+					  -1 /* varlenarray */ ,
+					  -1 /* TEXT's typlen */ ,
+					  false /* TEXT's typbyval */ ,
+					  'i' /* TEXT's typalign */ ,
+					  &isnull);
+		if (isnull)
+			continue;
+		val = TextDatumGetCString(d);
+
+		eqsgn = strchr(val, '=');
+		*eqsgn = '\0';
+
+		/* skip if we have permission to delete it */
+		if (validate_option_array_item(val, NULL, true))
+			continue;
+
+		/* else add it to the output array */
+		if (newarray)
+			newarray = array_set(newarray, 1, &index,
+								 d,
+								 false,
+								 -1 /* varlenarray */ ,
+								 -1 /* TEXT's typlen */ ,
+								 false /* TEXT's typbyval */ ,
+								 'i' /* TEXT's typalign */ );
+		else
+			newarray = construct_array(&d, 1,
+									   TEXTOID,
+									   -1, false, 'i');
+
+		index++;
+		pfree(val);
+	}
+
+	return newarray;
+}
+
+/*
+ * Validate a proposed option setting for GUCArrayAdd/Delete/Reset.
+ *
+ * name is the option name.  value is the proposed value for the Add case,
+ * or NULL for the Delete/Reset cases.  If skipIfNoPermissions is true, it's
+ * not an error to have no permissions to set the option.
+ *
+ * Returns true if OK, false if skipIfNoPermissions is true and user does not
+ * have permission to change this option (all other error cases result in an
+ * error being thrown).
+ */
+static bool
+validate_option_array_item(const char *name, const char *value,
+						   bool skipIfNoPermissions)
+
+{
+	struct config_generic *gconf;
+
+	/*
+	 * There are three cases to consider:
+	 *
+	 * name is a known GUC variable.  Check the value normally, check
+	 * permissions normally (i.e., allow if variable is USERSET, or if it's
+	 * SUSET and user is superuser).
+	 *
+	 * name is not known, but exists or can be created as a placeholder (i.e.,
+	 * it has a prefixed name).  We allow this case if you're a superuser,
+	 * otherwise not.  Superusers are assumed to know what they're doing. We
+	 * can't allow it for other users, because when the placeholder is
+	 * resolved it might turn out to be a SUSET variable;
+	 * define_custom_variable assumes we checked that.
+	 *
+	 * name is not known and can't be created as a placeholder.  Throw error,
+	 * unless skipIfNoPermissions is true, in which case return false.
+	 */
+	gconf = find_option(name, true, WARNING);
+	if (!gconf)
+	{
+		/* not known, failed to make a placeholder */
+		if (skipIfNoPermissions)
+			return false;
+		ereport(ERROR,
+				(errcode(ERRCODE_UNDEFINED_OBJECT),
+				 errmsg("unrecognized configuration parameter \"%s\"",
+						name)));
+	}
+
+	if (gconf->flags & GUC_CUSTOM_PLACEHOLDER)
+	{
+		/*
+		 * We cannot do any meaningful check on the value, so only permissions
+		 * are useful to check.
+		 */
+		if (superuser())
+			return true;
+		if (skipIfNoPermissions)
+			return false;
+		ereport(ERROR,
+				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+				 errmsg("permission denied to set parameter \"%s\"", name)));
+	}
+
+	/* manual permissions check so we can avoid an error being thrown */
+	if (gconf->context == PGC_USERSET)
+		 /* ok */ ;
+	else if (gconf->context == PGC_SUSET && superuser())
+		 /* ok */ ;
+	else if (skipIfNoPermissions)
+		return false;
+	/* if a permissions error should be thrown, let set_config_option do it */
+
+	/* test for permissions and valid option value */
+	(void) set_config_option(name, value,
+							 superuser() ? PGC_SUSET : PGC_USERSET,
+							 PGC_S_TEST, GUC_ACTION_SET, false, 0, false);
+
+	return true;
+}
+
+
+/*
+ * Called by check_hooks that want to override the normal
+ * ERRCODE_INVALID_PARAMETER_VALUE SQLSTATE for check hook failures.
+ *
+ * Note that GUC_check_errmsg() etc are just macros that result in a direct
+ * assignment to the associated variables.  That is ugly, but forced by the
+ * limitations of C's macro mechanisms.
+ */
+void
+GUC_check_errcode(int sqlerrcode)
+{
+	GUC_check_errcode_value = sqlerrcode;
+}
+
+
+/*
+ * Convenience functions to manage calling a variable's check_hook.
+ * These mostly take care of the protocol for letting check hooks supply
+ * portions of the error report on failure.
+ */
+
+static bool
+call_bool_check_hook(struct config_bool *conf, bool *newval, void **extra,
+					 GucSource source, int elevel)
+{
+	/* Quick success if no hook */
+	if (!conf->check_hook)
+		return true;
+
+	/* Reset variables that might be set by hook */
+	GUC_check_errcode_value = ERRCODE_INVALID_PARAMETER_VALUE;
+	GUC_check_errmsg_string = NULL;
+	GUC_check_errdetail_string = NULL;
+	GUC_check_errhint_string = NULL;
+
+	if (!conf->check_hook(newval, extra, source))
+	{
+		ereport(elevel,
+				(errcode(GUC_check_errcode_value),
+				 GUC_check_errmsg_string ?
+				 errmsg_internal("%s", GUC_check_errmsg_string) :
+				 errmsg("invalid value for parameter \"%s\": %d",
+						conf->gen.name, (int) *newval),
+				 GUC_check_errdetail_string ?
+				 errdetail_internal("%s", GUC_check_errdetail_string) : 0,
+				 GUC_check_errhint_string ?
+				 errhint("%s", GUC_check_errhint_string) : 0));
+		/* Flush any strings created in ErrorContext */
+		FlushErrorState();
+		return false;
+	}
+
+	return true;
+}
+
+static bool
+call_int_check_hook(struct config_int *conf, int *newval, void **extra,
+					GucSource source, int elevel)
+{
+	/* Quick success if no hook */
+	if (!conf->check_hook)
+		return true;
+
+	/* Reset variables that might be set by hook */
+	GUC_check_errcode_value = ERRCODE_INVALID_PARAMETER_VALUE;
+	GUC_check_errmsg_string = NULL;
+	GUC_check_errdetail_string = NULL;
+	GUC_check_errhint_string = NULL;
+
+	if (!conf->check_hook(newval, extra, source))
+	{
+		ereport(elevel,
+				(errcode(GUC_check_errcode_value),
+				 GUC_check_errmsg_string ?
+				 errmsg_internal("%s", GUC_check_errmsg_string) :
+				 errmsg("invalid value for parameter \"%s\": %d",
+						conf->gen.name, *newval),
+				 GUC_check_errdetail_string ?
+				 errdetail_internal("%s", GUC_check_errdetail_string) : 0,
+				 GUC_check_errhint_string ?
+				 errhint("%s", GUC_check_errhint_string) : 0));
+		/* Flush any strings created in ErrorContext */
+		FlushErrorState();
+		return false;
+	}
+
+	return true;
+}
+
+static bool
+call_real_check_hook(struct config_real *conf, double *newval, void **extra,
+					 GucSource source, int elevel)
+{
+	/* Quick success if no hook */
+	if (!conf->check_hook)
+		return true;
+
+	/* Reset variables that might be set by hook */
+	GUC_check_errcode_value = ERRCODE_INVALID_PARAMETER_VALUE;
+	GUC_check_errmsg_string = NULL;
+	GUC_check_errdetail_string = NULL;
+	GUC_check_errhint_string = NULL;
+
+	if (!conf->check_hook(newval, extra, source))
+	{
+		ereport(elevel,
+				(errcode(GUC_check_errcode_value),
+				 GUC_check_errmsg_string ?
+				 errmsg_internal("%s", GUC_check_errmsg_string) :
+				 errmsg("invalid value for parameter \"%s\": %g",
+						conf->gen.name, *newval),
+				 GUC_check_errdetail_string ?
+				 errdetail_internal("%s", GUC_check_errdetail_string) : 0,
+				 GUC_check_errhint_string ?
+				 errhint("%s", GUC_check_errhint_string) : 0));
+		/* Flush any strings created in ErrorContext */
+		FlushErrorState();
+		return false;
+	}
+
+	return true;
+}
+
+static bool
+call_string_check_hook(struct config_string *conf, char **newval, void **extra,
+					   GucSource source, int elevel)
+{
+	volatile bool result = true;
+
+	/* Quick success if no hook */
+	if (!conf->check_hook)
+		return true;
+
+	/*
+	 * If elevel is ERROR, or if the check_hook itself throws an elog
+	 * (undesirable, but not always avoidable), make sure we don't leak the
+	 * already-malloc'd newval string.
+	 */
+	PG_TRY();
+	{
+		/* Reset variables that might be set by hook */
+		GUC_check_errcode_value = ERRCODE_INVALID_PARAMETER_VALUE;
+		GUC_check_errmsg_string = NULL;
+		GUC_check_errdetail_string = NULL;
+		GUC_check_errhint_string = NULL;
+
+		if (!conf->check_hook(newval, extra, source))
+		{
+			ereport(elevel,
+					(errcode(GUC_check_errcode_value),
+					 GUC_check_errmsg_string ?
+					 errmsg_internal("%s", GUC_check_errmsg_string) :
+					 errmsg("invalid value for parameter \"%s\": \"%s\"",
+							conf->gen.name, *newval ? *newval : ""),
+					 GUC_check_errdetail_string ?
+					 errdetail_internal("%s", GUC_check_errdetail_string) : 0,
+					 GUC_check_errhint_string ?
+					 errhint("%s", GUC_check_errhint_string) : 0));
+			/* Flush any strings created in ErrorContext */
+			FlushErrorState();
+			result = false;
+		}
+	}
+	PG_CATCH();
+	{
+		free(*newval);
+		PG_RE_THROW();
+	}
+	PG_END_TRY();
+
+	return result;
+}
+
+static bool
+call_enum_check_hook(struct config_enum *conf, int *newval, void **extra,
+					 GucSource source, int elevel)
+{
+	/* Quick success if no hook */
+	if (!conf->check_hook)
+		return true;
+
+	/* Reset variables that might be set by hook */
+	GUC_check_errcode_value = ERRCODE_INVALID_PARAMETER_VALUE;
+	GUC_check_errmsg_string = NULL;
+	GUC_check_errdetail_string = NULL;
+	GUC_check_errhint_string = NULL;
+
+	if (!conf->check_hook(newval, extra, source))
+	{
+		ereport(elevel,
+				(errcode(GUC_check_errcode_value),
+				 GUC_check_errmsg_string ?
+				 errmsg_internal("%s", GUC_check_errmsg_string) :
+				 errmsg("invalid value for parameter \"%s\": \"%s\"",
+						conf->gen.name,
+						config_enum_lookup_by_value(conf, *newval)),
+				 GUC_check_errdetail_string ?
+				 errdetail_internal("%s", GUC_check_errdetail_string) : 0,
+				 GUC_check_errhint_string ?
+				 errhint("%s", GUC_check_errhint_string) : 0));
+		/* Flush any strings created in ErrorContext */
+		FlushErrorState();
+		return false;
+	}
+
+	return true;
+}
+
+
+/*
+ * check_hook, assign_hook and show_hook subroutines
+ */
+
+static bool
+check_wal_consistency_checking(char **newval, void **extra, GucSource source)
+{
+	char	   *rawstring;
+	List	   *elemlist;
+	ListCell   *l;
+	bool		newwalconsistency[RM_MAX_ID + 1];
+
+	/* Initialize the array */
+	MemSet(newwalconsistency, 0, (RM_MAX_ID + 1) * sizeof(bool));
+
+	/* Need a modifiable copy of string */
+	rawstring = pstrdup(*newval);
+
+	/* Parse string into list of identifiers */
+	if (!SplitIdentifierString(rawstring, ',', &elemlist))
+	{
+		/* syntax error in list */
+		GUC_check_errdetail("List syntax is invalid.");
+		pfree(rawstring);
+		list_free(elemlist);
+		return false;
+	}
+
+	foreach(l, elemlist)
+	{
+		char	   *tok = (char *) lfirst(l);
+		bool		found = false;
+		RmgrId		rmid;
+
+		/* Check for 'all'. */
+		if (pg_strcasecmp(tok, "all") == 0)
+		{
+			for (rmid = 0; rmid <= RM_MAX_ID; rmid++)
+				if (RmgrTable[rmid].rm_mask != NULL)
+					newwalconsistency[rmid] = true;
+			found = true;
+		}
+		else
+		{
+			/*
+			 * Check if the token matches with any individual resource
+			 * manager.
+			 */
+			for (rmid = 0; rmid <= RM_MAX_ID; rmid++)
+			{
+				if (pg_strcasecmp(tok, RmgrTable[rmid].rm_name) == 0 &&
+					RmgrTable[rmid].rm_mask != NULL)
+				{
+					newwalconsistency[rmid] = true;
+					found = true;
+				}
+			}
+		}
+
+		/* If a valid resource manager is found, check for the next one. */
+		if (!found)
+		{
+			GUC_check_errdetail("Unrecognized key word: \"%s\".", tok);
+			pfree(rawstring);
+			list_free(elemlist);
+			return false;
+		}
+	}
+
+	pfree(rawstring);
+	list_free(elemlist);
+
+	/* assign new value */
+	*extra = guc_malloc(ERROR, (RM_MAX_ID + 1) * sizeof(bool));
+	memcpy(*extra, newwalconsistency, (RM_MAX_ID + 1) * sizeof(bool));
+	return true;
+}
+
+static void
+assign_wal_consistency_checking(const char *newval, void *extra)
+{
+	wal_consistency_checking = (bool *) extra;
+}
+
+static bool
+check_log_destination(char **newval, void **extra, GucSource source)
+{
+	char	   *rawstring;
+	List	   *elemlist;
+	ListCell   *l;
+	int			newlogdest = 0;
+	int		   *myextra;
+
+	/* Need a modifiable copy of string */
+	rawstring = pstrdup(*newval);
+
+	/* Parse string into list of identifiers */
+	if (!SplitIdentifierString(rawstring, ',', &elemlist))
+	{
+		/* syntax error in list */
+		GUC_check_errdetail("List syntax is invalid.");
+		pfree(rawstring);
+		list_free(elemlist);
+		return false;
+	}
+
+	foreach(l, elemlist)
+	{
+		char	   *tok = (char *) lfirst(l);
+
+		if (pg_strcasecmp(tok, "stderr") == 0)
+			newlogdest |= LOG_DESTINATION_STDERR;
+		else if (pg_strcasecmp(tok, "csvlog") == 0)
+			newlogdest |= LOG_DESTINATION_CSVLOG;
+#ifdef HAVE_SYSLOG
+		else if (pg_strcasecmp(tok, "syslog") == 0)
+			newlogdest |= LOG_DESTINATION_SYSLOG;
+#endif
+#ifdef WIN32
+		else if (pg_strcasecmp(tok, "eventlog") == 0)
+			newlogdest |= LOG_DESTINATION_EVENTLOG;
+#endif
+		else
+		{
+			GUC_check_errdetail("Unrecognized key word: \"%s\".", tok);
+			pfree(rawstring);
+			list_free(elemlist);
+			return false;
+		}
+	}
+
+	pfree(rawstring);
+	list_free(elemlist);
+
+	myextra = (int *) guc_malloc(ERROR, sizeof(int));
+	*myextra = newlogdest;
+	*extra = (void *) myextra;
+
+	return true;
+}
+
+static void
+assign_log_destination(const char *newval, void *extra)
+{
+	Log_destination = *((int *) extra);
+}
+
+static void
+assign_syslog_facility(int newval, void *extra)
+{
+#ifdef HAVE_SYSLOG
+	set_syslog_parameters(syslog_ident_str ? syslog_ident_str : "postgres",
+						  newval);
+#endif
+	/* Without syslog support, just ignore it */
+}
+
+static void
+assign_syslog_ident(const char *newval, void *extra)
+{
+#ifdef HAVE_SYSLOG
+	set_syslog_parameters(newval, syslog_facility);
+#endif
+	/* Without syslog support, it will always be set to "none", so ignore */
+}
+
+
+static void
+assign_session_replication_role(int newval, void *extra)
+{
+	/*
+	 * Must flush the plan cache when changing replication role; but don't
+	 * flush unnecessarily.
+	 */
+	if (SessionReplicationRole != newval)
+		ResetPlanCache();
+}
+
+static bool
+check_temp_buffers(int *newval, void **extra, GucSource source)
+{
+	/*
+	 * Once local buffers have been initialized, it's too late to change this.
+	 * However, if this is only a test call, allow it.
+	 */
+	if (source != PGC_S_TEST && NLocBuffer && NLocBuffer != *newval)
+	{
+		GUC_check_errdetail("\"temp_buffers\" cannot be changed after any temporary tables have been accessed in the session.");
+		return false;
+	}
+	return true;
+}
+
+static bool
+check_bonjour(bool *newval, void **extra, GucSource source)
+{
+#ifndef USE_BONJOUR
+	if (*newval)
+	{
+		GUC_check_errmsg("Bonjour is not supported by this build");
+		return false;
+	}
+#endif
+	return true;
+}
+
+static bool
+check_ssl(bool *newval, void **extra, GucSource source)
+{
+#ifndef USE_SSL
+	if (*newval)
+	{
+		GUC_check_errmsg("SSL is not supported by this build");
+		return false;
+	}
+#endif
+	return true;
+}
+
+static bool
+check_stage_log_stats(bool *newval, void **extra, GucSource source)
+{
+	if (*newval && log_statement_stats)
+	{
+		GUC_check_errdetail("Cannot enable parameter when \"log_statement_stats\" is true.");
+		return false;
+	}
+	return true;
+}
+
+static bool
+check_log_stats(bool *newval, void **extra, GucSource source)
+{
+	if (*newval &&
+		(log_parser_stats || log_planner_stats || log_executor_stats))
+	{
+		GUC_check_errdetail("Cannot enable \"log_statement_stats\" when "
+							"\"log_parser_stats\", \"log_planner_stats\", "
+							"or \"log_executor_stats\" is true.");
+		return false;
+	}
+	return true;
+}
+
+static bool
+check_canonical_path(char **newval, void **extra, GucSource source)
+{
+	/*
+	 * Since canonicalize_path never enlarges the string, we can just modify
+	 * newval in-place.  But watch out for NULL, which is the default value
+	 * for external_pid_file.
+	 */
+	if (*newval)
+		canonicalize_path(*newval);
+	return true;
+}
+
+static bool
+check_timezone_abbreviations(char **newval, void **extra, GucSource source)
+{
+	/*
+	 * The boot_val given above for timezone_abbreviations is NULL. When we
+	 * see this we just do nothing.  If this value isn't overridden from the
+	 * config file then pg_timezone_abbrev_initialize() will eventually
+	 * replace it with "Default".  This hack has two purposes: to avoid
+	 * wasting cycles loading values that might soon be overridden from the
+	 * config file, and to avoid trying to read the timezone abbrev files
+	 * during InitializeGUCOptions().  The latter doesn't work in an
+	 * EXEC_BACKEND subprocess because my_exec_path hasn't been set yet and so
+	 * we can't locate PGSHAREDIR.
+	 */
+	if (*newval == NULL)
+	{
+		Assert(source == PGC_S_DEFAULT);
+		return true;
+	}
+
+	/* OK, load the file and produce a malloc'd TimeZoneAbbrevTable */
+	*extra = load_tzoffsets(*newval);
+
+	/* tzparser.c returns NULL on failure, reporting via GUC_check_errmsg */
+	if (!*extra)
+		return false;
+
+	return true;
+}
+
+static void
+assign_timezone_abbreviations(const char *newval, void *extra)
+{
+	/* Do nothing for the boot_val default of NULL */
+	if (!extra)
+		return;
+
+	InstallTimeZoneAbbrevs((TimeZoneAbbrevTable *) extra);
+}
+
+/*
+ * pg_timezone_abbrev_initialize --- set default value if not done already
+ *
+ * This is called after initial loading of postgresql.conf.  If no
+ * timezone_abbreviations setting was found therein, select default.
+ * If a non-default value is already installed, nothing will happen.
+ *
+ * This can also be called from ProcessConfigFile to establish the default
+ * value after a postgresql.conf entry for it is removed.
+ */
+static void
+pg_timezone_abbrev_initialize(void)
+{
+	SetConfigOption("timezone_abbreviations", "Default",
+					PGC_POSTMASTER, PGC_S_DYNAMIC_DEFAULT);
+}
+
+static const char *
+show_archive_command(void)
+{
+	if (XLogArchivingActive())
+		return XLogArchiveCommand;
+	else
+		return "(disabled)";
+}
+
+static void
+assign_tcp_keepalives_idle(int newval, void *extra)
+{
+	/*
+	 * The kernel API provides no way to test a value without setting it; and
+	 * once we set it we might fail to unset it.  So there seems little point
+	 * in fully implementing the check-then-assign GUC API for these
+	 * variables.  Instead we just do the assignment on demand.  pqcomm.c
+	 * reports any problems via elog(LOG).
+	 *
+	 * This approach means that the GUC value might have little to do with the
+	 * actual kernel value, so we use a show_hook that retrieves the kernel
+	 * value rather than trusting GUC's copy.
+	 */
+	(void) pq_setkeepalivesidle(newval, MyProcPort);
+}
+
+static const char *
+show_tcp_keepalives_idle(void)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	static char nbuf[16];
+
+	snprintf(nbuf, sizeof(nbuf), "%d", pq_getkeepalivesidle(MyProcPort));
+	return nbuf;
+}
+
+static void
+assign_tcp_keepalives_interval(int newval, void *extra)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	(void) pq_setkeepalivesinterval(newval, MyProcPort);
+}
+
+static const char *
+show_tcp_keepalives_interval(void)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	static char nbuf[16];
+
+	snprintf(nbuf, sizeof(nbuf), "%d", pq_getkeepalivesinterval(MyProcPort));
+	return nbuf;
+}
+
+static void
+assign_tcp_keepalives_count(int newval, void *extra)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	(void) pq_setkeepalivescount(newval, MyProcPort);
+}
+
+static const char *
+show_tcp_keepalives_count(void)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	static char nbuf[16];
+
+	snprintf(nbuf, sizeof(nbuf), "%d", pq_getkeepalivescount(MyProcPort));
+	return nbuf;
+}
+
+static void
+assign_tcp_user_timeout(int newval, void *extra)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	(void) pq_settcpusertimeout(newval, MyProcPort);
+}
+
+static const char *
+show_tcp_user_timeout(void)
+{
+	/* See comments in assign_tcp_keepalives_idle */
+	static char nbuf[16];
+
+	snprintf(nbuf, sizeof(nbuf), "%d", pq_gettcpusertimeout(MyProcPort));
+	return nbuf;
+}
+
+static bool
+check_maxconnections(int *newval, void **extra, GucSource source)
+{
+	if (*newval + autovacuum_max_workers + 1 +
+		max_worker_processes + max_wal_senders > MAX_BACKENDS)
+		return false;
+	return true;
+}
+
+static bool
+check_autovacuum_max_workers(int *newval, void **extra, GucSource source)
+{
+	if (MaxConnections + *newval + 1 +
+		max_worker_processes + max_wal_senders > MAX_BACKENDS)
+		return false;
+	return true;
+}
+
+static bool
+check_max_wal_senders(int *newval, void **extra, GucSource source)
+{
+	if (MaxConnections + autovacuum_max_workers + 1 +
+		max_worker_processes + *newval > MAX_BACKENDS)
+		return false;
+	return true;
+}
+
+static bool
+check_autovacuum_work_mem(int *newval, void **extra, GucSource source)
+{
+	/*
+	 * -1 indicates fallback.
+	 *
+	 * If we haven't yet changed the boot_val default of -1, just let it be.
+	 * Autovacuum will look to maintenance_work_mem instead.
+	 */
+	if (*newval == -1)
+		return true;
+
+	/*
+	 * We clamp manually-set values to at least 1MB.  Since
+	 * maintenance_work_mem is always set to at least this value, do the same
+	 * here.
+	 */
+	if (*newval < 1024)
+		*newval = 1024;
+
+	return true;
+}
+
+static bool
+check_max_worker_processes(int *newval, void **extra, GucSource source)
+{
+	if (MaxConnections + autovacuum_max_workers + 1 +
+		*newval + max_wal_senders > MAX_BACKENDS)
+		return false;
+	return true;
+}
+
+static bool
+check_effective_io_concurrency(int *newval, void **extra, GucSource source)
+{
+#ifdef USE_PREFETCH
+	double		new_prefetch_pages;
+
+	if (ComputeIoConcurrency(*newval, &new_prefetch_pages))
+	{
+		int		   *myextra = (int *) guc_malloc(ERROR, sizeof(int));
+
+		*myextra = (int) rint(new_prefetch_pages);
+		*extra = (void *) myextra;
+
+		return true;
+	}
+	else
+		return false;
+#else
+	if (*newval != 0)
+	{
+		GUC_check_errdetail("effective_io_concurrency must be set to 0 on platforms that lack posix_fadvise().");
+		return false;
+	}
+	return true;
+#endif							/* USE_PREFETCH */
+}
+
+static void
+assign_effective_io_concurrency(int newval, void *extra)
+{
+#ifdef USE_PREFETCH
+	target_prefetch_pages = *((int *) extra);
+#endif							/* USE_PREFETCH */
+}
+
+static void
+assign_pgstat_temp_directory(const char *newval, void *extra)
+{
+	/* check_canonical_path already canonicalized newval for us */
+	char	   *dname;
+	char	   *tname;
+	char	   *fname;
+
+	/* directory */
+	dname = guc_malloc(ERROR, strlen(newval) + 1);	/* runtime dir */
+	sprintf(dname, "%s", newval);
+
+	/* global stats */
+	tname = guc_malloc(ERROR, strlen(newval) + 12); /* /global.tmp */
+	sprintf(tname, "%s/global.tmp", newval);
+	fname = guc_malloc(ERROR, strlen(newval) + 13); /* /global.stat */
+	sprintf(fname, "%s/global.stat", newval);
+
+	if (pgstat_stat_directory)
+		free(pgstat_stat_directory);
+	pgstat_stat_directory = dname;
+	if (pgstat_stat_tmpname)
+		free(pgstat_stat_tmpname);
+	pgstat_stat_tmpname = tname;
+	if (pgstat_stat_filename)
+		free(pgstat_stat_filename);
+	pgstat_stat_filename = fname;
+}
+
+static bool
+check_application_name(char **newval, void **extra, GucSource source)
+{
+	/* Only allow clean ASCII chars in the application name */
+	pg_clean_ascii(*newval);
+
+	return true;
+}
+
+static void
+assign_application_name(const char *newval, void *extra)
+{
+	/* Update the pg_stat_activity view */
+	pgstat_report_appname(newval);
+}
+
+static bool
+check_cluster_name(char **newval, void **extra, GucSource source)
+{
+	/* Only allow clean ASCII chars in the cluster name */
+	pg_clean_ascii(*newval);
+
+	return true;
+}
+
+static const char *
+show_unix_socket_permissions(void)
+{
+	static char buf[12];
+
+	snprintf(buf, sizeof(buf), "%04o", Unix_socket_permissions);
+	return buf;
+}
+
+static const char *
+show_log_file_mode(void)
+{
+	static char buf[12];
+
+	snprintf(buf, sizeof(buf), "%04o", Log_file_mode);
+	return buf;
+}
+
+static const char *
+show_data_directory_mode(void)
+{
+	static char buf[12];
+
+	snprintf(buf, sizeof(buf), "%04o", data_directory_mode);
+	return buf;
+}
+
+static bool
+check_recovery_target_timeline(char **newval, void **extra, GucSource source)
+{
+	RecoveryTargetTimeLineGoal rttg;
+	RecoveryTargetTimeLineGoal *myextra;
+
+	if (strcmp(*newval, "current") == 0)
+		rttg = RECOVERY_TARGET_TIMELINE_CONTROLFILE;
+	else if (strcmp(*newval, "latest") == 0)
+		rttg = RECOVERY_TARGET_TIMELINE_LATEST;
+	else
+	{
+		rttg = RECOVERY_TARGET_TIMELINE_NUMERIC;
+
+		errno = 0;
+		strtoul(*newval, NULL, 0);
+		if (errno == EINVAL || errno == ERANGE)
+		{
+			GUC_check_errdetail("recovery_target_timeline is not a valid number.");
+			return false;
+		}
+	}
+
+	myextra = (RecoveryTargetTimeLineGoal *) guc_malloc(ERROR, sizeof(RecoveryTargetTimeLineGoal));
+	*myextra = rttg;
+	*extra = (void *) myextra;
+
+	return true;
+}
+
+static void
+assign_recovery_target_timeline(const char *newval, void *extra)
+{
+	recoveryTargetTimeLineGoal = *((RecoveryTargetTimeLineGoal *) extra);
+	if (recoveryTargetTimeLineGoal == RECOVERY_TARGET_TIMELINE_NUMERIC)
+		recoveryTargetTLIRequested = (TimeLineID) strtoul(newval, NULL, 0);
+	else
+		recoveryTargetTLIRequested = 0;
+}
+
+/*
+ * Recovery target settings: Only one of the several recovery_target* settings
+ * may be set.  Setting a second one results in an error.  The global variable
+ * recoveryTarget tracks which kind of recovery target was chosen.  Other
+ * variables store the actual target value (for example a string or a xid).
+ * The assign functions of the parameters check whether a competing parameter
+ * was already set.  But we want to allow setting the same parameter multiple
+ * times.  We also want to allow unsetting a parameter and setting a different
+ * one, so we unset recoveryTarget when the parameter is set to an empty
+ * string.
+ */
+
+static void
+pg_attribute_noreturn()
+error_multiple_recovery_targets(void)
+{
+	ereport(ERROR,
+			(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+			 errmsg("multiple recovery targets specified"),
+			 errdetail("At most one of recovery_target, recovery_target_lsn, recovery_target_name, recovery_target_time, recovery_target_xid may be set.")));
+}
+
+static bool
+check_recovery_target(char **newval, void **extra, GucSource source)
+{
+	if (strcmp(*newval, "immediate") != 0 && strcmp(*newval, "") != 0)
+	{
+		GUC_check_errdetail("The only allowed value is \"immediate\".");
+		return false;
+	}
+	return true;
+}
+
+static void
+assign_recovery_target(const char *newval, void *extra)
+{
+	if (recoveryTarget != RECOVERY_TARGET_UNSET &&
+		recoveryTarget != RECOVERY_TARGET_IMMEDIATE)
+		error_multiple_recovery_targets();
+
+	if (newval && strcmp(newval, "") != 0)
+		recoveryTarget = RECOVERY_TARGET_IMMEDIATE;
+	else
+		recoveryTarget = RECOVERY_TARGET_UNSET;
+}
+
+static bool
+check_recovery_target_xid(char **newval, void **extra, GucSource source)
+{
+	if (strcmp(*newval, "") != 0)
+	{
+		TransactionId xid;
+		TransactionId *myextra;
+
+		errno = 0;
+		xid = (TransactionId) pg_strtouint64(*newval, NULL, 0);
+		if (errno == EINVAL || errno == ERANGE)
+			return false;
+
+		myextra = (TransactionId *) guc_malloc(ERROR, sizeof(TransactionId));
+		*myextra = xid;
+		*extra = (void *) myextra;
+	}
+	return true;
+}
+
+static void
+assign_recovery_target_xid(const char *newval, void *extra)
+{
+	if (recoveryTarget != RECOVERY_TARGET_UNSET &&
+		recoveryTarget != RECOVERY_TARGET_XID)
+		error_multiple_recovery_targets();
+
+	if (newval && strcmp(newval, "") != 0)
+	{
+		recoveryTarget = RECOVERY_TARGET_XID;
+		recoveryTargetXid = *((TransactionId *) extra);
+	}
+	else
+		recoveryTarget = RECOVERY_TARGET_UNSET;
+}
+
+/*
+ * The interpretation of the recovery_target_time string can depend on the
+ * time zone setting, so we need to wait until after all GUC processing is
+ * done before we can do the final parsing of the string.  This check function
+ * only does a parsing pass to catch syntax errors, but we store the string
+ * and parse it again when we need to use it.
+ */
+static bool
+check_recovery_target_time(char **newval, void **extra, GucSource source)
+{
+	if (strcmp(*newval, "") != 0)
+	{
+		/* reject some special values */
+		if (strcmp(*newval, "now") == 0 ||
+			strcmp(*newval, "today") == 0 ||
+			strcmp(*newval, "tomorrow") == 0 ||
+			strcmp(*newval, "yesterday") == 0)
+		{
+			return false;
+		}
+
+		/*
+		 * parse timestamp value (see also timestamptz_in())
+		 */
+		{
+			char	   *str = *newval;
+			fsec_t		fsec;
+			struct pg_tm tt,
+					   *tm = &tt;
+			int			tz;
+			int			dtype;
+			int			nf;
+			int			dterr;
+			char	   *field[MAXDATEFIELDS];
+			int			ftype[MAXDATEFIELDS];
+			char		workbuf[MAXDATELEN + MAXDATEFIELDS];
+			TimestampTz timestamp;
+
+			dterr = ParseDateTime(str, workbuf, sizeof(workbuf),
+								  field, ftype, MAXDATEFIELDS, &nf);
+			if (dterr == 0)
+				dterr = DecodeDateTime(field, ftype, nf, &dtype, tm, &fsec, &tz);
+			if (dterr != 0)
+				return false;
+			if (dtype != DTK_DATE)
+				return false;
+
+			if (tm2timestamp(tm, fsec, &tz, &timestamp) != 0)
+			{
+				GUC_check_errdetail("timestamp out of range: \"%s\"", str);
+				return false;
+			}
+		}
+	}
+	return true;
+}
+
+static void
+assign_recovery_target_time(const char *newval, void *extra)
+{
+	if (recoveryTarget != RECOVERY_TARGET_UNSET &&
+		recoveryTarget != RECOVERY_TARGET_TIME)
+		error_multiple_recovery_targets();
+
+	if (newval && strcmp(newval, "") != 0)
+		recoveryTarget = RECOVERY_TARGET_TIME;
+	else
+		recoveryTarget = RECOVERY_TARGET_UNSET;
+}
+
+static bool
+check_recovery_target_name(char **newval, void **extra, GucSource source)
+{
+	/* Use the value of newval directly */
+	if (strlen(*newval) >= MAXFNAMELEN)
+	{
+		GUC_check_errdetail("%s is too long (maximum %d characters).",
+							"recovery_target_name", MAXFNAMELEN - 1);
+		return false;
+	}
+	return true;
+}
+
+static void
+assign_recovery_target_name(const char *newval, void *extra)
+{
+	if (recoveryTarget != RECOVERY_TARGET_UNSET &&
+		recoveryTarget != RECOVERY_TARGET_NAME)
+		error_multiple_recovery_targets();
+
+	if (newval && strcmp(newval, "") != 0)
+	{
+		recoveryTarget = RECOVERY_TARGET_NAME;
+		recoveryTargetName = newval;
+	}
+	else
+		recoveryTarget = RECOVERY_TARGET_UNSET;
+}
+
+static bool
+check_recovery_target_lsn(char **newval, void **extra, GucSource source)
+{
+	if (strcmp(*newval, "") != 0)
+	{
+		XLogRecPtr	lsn;
+		XLogRecPtr *myextra;
+		bool		have_error = false;
+
+		lsn = pg_lsn_in_internal(*newval, &have_error);
+		if (have_error)
+			return false;
+
+		myextra = (XLogRecPtr *) guc_malloc(ERROR, sizeof(XLogRecPtr));
+		*myextra = lsn;
+		*extra = (void *) myextra;
+	}
+	return true;
+}
+
+static void
+assign_recovery_target_lsn(const char *newval, void *extra)
+{
+	if (recoveryTarget != RECOVERY_TARGET_UNSET &&
+		recoveryTarget != RECOVERY_TARGET_LSN)
+		error_multiple_recovery_targets();
+
+	if (newval && strcmp(newval, "") != 0)
+	{
+		recoveryTarget = RECOVERY_TARGET_LSN;
+		recoveryTargetLSN = *((XLogRecPtr *) extra);
+	}
+	else
+		recoveryTarget = RECOVERY_TARGET_UNSET;
+}
+
+static bool
+check_primary_slot_name(char **newval, void **extra, GucSource source)
+{
+	if (*newval && strcmp(*newval, "") != 0 &&
+		!ReplicationSlotValidateName(*newval, WARNING))
+		return false;
+
+	return true;
+}
+
+static bool
+check_default_with_oids(bool *newval, void **extra, GucSource source)
+{
+	if (*newval)
+	{
+		/* check the GUC's definition for an explanation */
+		GUC_check_errcode(ERRCODE_FEATURE_NOT_SUPPORTED);
+		GUC_check_errmsg("tables declared WITH OIDS are not supported");
+
+		return false;
+	}
+
+	return true;
+}
+
+#include "guc-file.c"
diff --git a/src/include/lero/lero_extension.h b/src/include/lero/lero_extension.h
new file mode 100644
index 0000000..c492a0d
--- /dev/null
+++ b/src/include/lero/lero_extension.h
@@ -0,0 +1,37 @@
+#include "postgres.h"
+#include "fmgr.h"
+#include "optimizer/paths.h"
+#include "nodes/plannodes.h"
+#include "nodes/pg_list.h"
+#include "utils/guc.h"
+
+#ifndef LERO_EXTENSION
+#define LERO_EXTENSION
+extern bool enable_lero;
+
+extern int lero_server_port;
+
+extern char *lero_server_host;
+
+typedef struct LeroPlan {
+	double *card;
+
+	PlannedStmt* plan;
+
+	double latency;
+} LeroPlan;
+
+typedef struct RelatedTable {
+    List *tables;
+} RelatedTable;
+
+extern void lero_pgsysml_set_joinrel_size_estimates(PlannerInfo *root, RelOptInfo *rel,
+						   RelOptInfo *outer_rel,
+						   RelOptInfo *inner_rel,
+						   SpecialJoinInfo *sjinfo,
+						   List *restrictlist);
+
+extern PlannedStmt* lero_pgsysml_hook_planner(Query *parse, const char *queryString,
+                                int cursorOptions,
+                                ParamListInfo boundParams);
+#endif
\ No newline at end of file
diff --git a/src/include/lero/utils.h b/src/include/lero/utils.h
new file mode 100644
index 0000000..f34d9e1
--- /dev/null
+++ b/src/include/lero/utils.h
@@ -0,0 +1,54 @@
+#include "yyjson.h"
+#include "postgres.h"
+#include "optimizer/paths.h"
+#include "nodes/pathnodes.h"
+#include "nodes/pg_list.h"
+#include "optimizer/planner.h"
+#include "parser/parsetree.h"
+#include "lero_extension.h"
+
+#ifndef LERO_UTILS
+#define LERO_UTILS
+
+// msg related
+#define MSG_TYPE "msg_type"
+#define MSG_INIT "init"
+#define MSG_PREDICT "guided_optimization"
+#define MSG_QUERY_ID "query_id"
+
+#define MSG_SCORE "latency"
+#define MSG_ERROR "error"
+#define MSG_FINISH "finish"
+#define MSG_END_FLAG "*LERO_END*"
+
+extern int 
+connect_to_server(const char* host, int port);
+
+extern void 
+write_all_to_socket(int conn_fd, const char *str);
+
+extern char*
+concat_str(char* a, char *b);
+
+extern char*
+send_and_receive_msg(int conn_fd, char* json_str);
+
+char*
+get_query_unique_id(const char *queryString);
+
+extern yyjson_doc*
+parse_json_str(const char* json);
+
+extern yyjson_mut_val*
+double_list_to_json_arr(double l[], int n, yyjson_mut_doc *json_doc);
+
+extern yyjson_mut_val*
+int_list_to_json_arr(int l[], int n, yyjson_mut_doc *json_doc);
+
+extern yyjson_mut_val*
+plan_to_json(PlannedStmt* stmt, Plan *plan, yyjson_mut_doc *json_doc);
+
+extern void 
+add_join_input_tables(PlannerInfo *root, Path *path, RelatedTable *related_table);
+
+#endif
\ No newline at end of file
diff --git a/src/include/lero/yyjson.h b/src/include/lero/yyjson.h
new file mode 100644
index 0000000..7235bcb
--- /dev/null
+++ b/src/include/lero/yyjson.h
@@ -0,0 +1,4221 @@
+/*==============================================================================
+ * Created by Yaoyuan on 2019/3/9.
+ * Copyright (C) 2019 Yaoyuan <ibireme@gmail.com>.
+ *
+ * Released under the MIT License:
+ * https://github.com/ibireme/yyjson/blob/master/LICENSE
+ *============================================================================*/
+
+#ifndef YYJSON_H
+#define YYJSON_H
+
+
+
+/*==============================================================================
+ * Header Files
+ *============================================================================*/
+
+#include <stdlib.h>
+#include <stddef.h>
+#include <limits.h>
+#include <string.h>
+#include <float.h>
+
+
+
+/*==============================================================================
+ * Version
+ *============================================================================*/
+
+#define YYJSON_VERSION_MAJOR  0
+#define YYJSON_VERSION_MINOR  4
+#define YYJSON_VERSION_PATCH  0
+#define YYJSON_VERSION_HEX    0x000400
+#define YYJSON_VERSION_STRING "0.4.0"
+
+
+
+/*==============================================================================
+ * Compile Flags
+ *============================================================================*/
+
+/* Define as 1 to disable JSON reader.
+   This may reduce binary size if you don't need JSON reader */
+#ifndef YYJSON_DISABLE_READER
+#endif
+
+/* Define as 1 to disable JSON writer.
+   This may reduce binary size if you don't need JSON writer */
+#ifndef YYJSON_DISABLE_WRITER
+#endif
+
+/* Define as 1 to disable the fast floating-point number conversion in yyjson,
+   and use libc's `strtod/snprintf` instead. This may reduce binary size,
+   but slow down floating-point reading and writing speed. */
+#ifndef YYJSON_DISABLE_FAST_FP_CONV
+#endif
+
+/* Define as 1 to disable non-standard JSON support at compile time:
+       Reading and writing inf/nan literal, such as 'NaN', '-Infinity'.
+       Single line and multiple line comments.
+       Single trailing comma at the end of an object or array.
+   This may also invalidate these options:
+       YYJSON_READ_ALLOW_INF_AND_NAN
+       YYJSON_READ_ALLOW_COMMENTS
+       YYJSON_READ_ALLOW_TRAILING_COMMAS
+       YYJSON_WRITE_ALLOW_INF_AND_NAN
+   This may reduce binary size, and increase performance slightly. */
+#ifndef YYJSON_DISABLE_NON_STANDARD
+#endif
+
+/* Define as 1 to disable unaligned memory access if target architecture does
+   not support unaligned memory access (such as some embedded processors).
+   If this value is not defined, yyjson will perform some automatic detection.
+   Wrong definition of this flag may cause performance degradation, but will not
+   cause runtime errors. */
+#ifndef YYJSON_DISABLE_UNALIGNED_MEMORY_ACCESS
+#endif
+
+/* Define as 1 to export symbols when build library as Windows DLL. */
+#ifndef YYJSON_EXPORTS
+#endif
+
+/* Define as 1 to import symbols when use library as Windows DLL. */
+#ifndef YYJSON_IMPORTS
+#endif
+
+/* Define as 1 to include <stdint.h> for compiler which doesn't support C99. */
+#ifndef YYJSON_HAS_STDINT_H
+#endif
+
+/* Define as 1 to include <stdbool.h> for compiler which doesn't support C99. */
+#ifndef YYJSON_HAS_STDBOOL_H
+#endif
+
+
+
+/*==============================================================================
+ * Compiler Macros
+ *============================================================================*/
+
+/* compiler version check (MSVC) */
+#ifdef _MSC_VER
+#   define YYJSON_MSC_VER _MSC_VER
+#else
+#   define YYJSON_MSC_VER 0
+#endif
+
+/* compiler version check (GCC) */
+#ifdef __GNUC__
+#   define YYJSON_GCC_VER __GNUC__
+#else
+#   define YYJSON_GCC_VER 0
+#endif
+
+/* C version check */
+#if defined(__STDC__) && (__STDC__ >= 1) && defined(__STDC_VERSION__)
+#   define YYJSON_STDC_VER __STDC_VERSION__
+#else
+#   define YYJSON_STDC_VER 0
+#endif
+
+/* C++ version check */
+#if defined(__cplusplus)
+#   define YYJSON_CPP_VER __cplusplus
+#else
+#   define YYJSON_CPP_VER 0
+#endif
+
+/* compiler builtin check (since gcc 10.0, clang 2.6, icc 2021) */
+#ifndef yyjson_has_builtin
+#   ifdef __has_builtin
+#       define yyjson_has_builtin(x) __has_builtin(x)
+#   else
+#       define yyjson_has_builtin(x) 0
+#   endif
+#endif
+
+/* compiler attribute check (since gcc 5.0, clang 2.9, icc 17) */
+#ifndef yyjson_has_attribute
+#   ifdef __has_attribute
+#       define yyjson_has_attribute(x) __has_attribute(x)
+#   else
+#       define yyjson_has_attribute(x) 0
+#   endif
+#endif
+
+/* include check (since gcc 5.0, clang 2.7, icc 16) */
+#ifndef yyjson_has_include
+#   ifdef __has_include
+#       define yyjson_has_include(x) __has_include(x)
+#   else
+#       define yyjson_has_include(x) 0
+#   endif
+#endif
+
+/* inline */
+#ifndef yyjson_inline
+#   if YYJSON_MSC_VER >= 1200
+#       define yyjson_inline __forceinline
+#   elif defined(_MSC_VER)
+#       define yyjson_inline __inline
+#   elif yyjson_has_attribute(always_inline) || YYJSON_GCC_VER >= 4
+#       define yyjson_inline __inline__ __attribute__((always_inline))
+#   elif defined(__clang__) || defined(__GNUC__)
+#       define yyjson_inline __inline__
+#   elif defined(__cplusplus) || YYJSON_STDC_VER >= 199901L
+#       define yyjson_inline inline
+#   else
+#       define yyjson_inline
+#   endif
+#endif
+
+/* noinline */
+#ifndef yyjson_noinline
+#   if YYJSON_MSC_VER >= 1400
+#       define yyjson_noinline __declspec(noinline)
+#   elif yyjson_has_attribute(noinline) || YYJSON_GCC_VER >= 4
+#       define yyjson_noinline __attribute__((noinline))
+#   else
+#       define yyjson_noinline
+#   endif
+#endif
+
+/* align */
+#ifndef yyjson_align
+#   if YYJSON_MSC_VER >= 1300
+#       define yyjson_align(x) __declspec(align(x))
+#   elif yyjson_has_attribute(aligned) || defined(__GNUC__)
+#       define yyjson_align(x) __attribute__((aligned(x)))
+#   elif YYJSON_CPP_VER >= 201103L
+#       define yyjson_align(x) alignas(x)
+#   else
+#       define yyjson_align(x)
+#   endif
+#endif
+
+/* likely */
+#ifndef yyjson_likely
+#   if yyjson_has_builtin(__builtin_expect) || YYJSON_GCC_VER >= 4
+#       define yyjson_likely(expr) __builtin_expect(!!(expr), 1)
+#   else
+#       define yyjson_likely(expr) (expr)
+#   endif
+#endif
+
+/* unlikely */
+#ifndef yyjson_unlikely
+#   if yyjson_has_builtin(__builtin_expect) || YYJSON_GCC_VER >= 4
+#       define yyjson_unlikely(expr) __builtin_expect(!!(expr), 0)
+#   else
+#       define yyjson_unlikely(expr) (expr)
+#   endif
+#endif
+
+/* function export */
+#ifndef yyjson_api
+#   if defined(_WIN32)
+#       if defined(YYJSON_EXPORTS) && YYJSON_EXPORTS
+#           define yyjson_api __declspec(dllexport)
+#       elif defined(YYJSON_IMPORTS) && YYJSON_IMPORTS
+#           define yyjson_api __declspec(dllimport)
+#       else
+#           define yyjson_api
+#       endif
+#   elif yyjson_has_attribute(visibility) || YYJSON_GCC_VER >= 4
+#       define yyjson_api __attribute__((visibility("default")))
+#   else
+#       define yyjson_api
+#   endif
+#endif
+
+/* inline function export */
+#ifndef yyjson_api_inline
+#   define yyjson_api_inline static yyjson_inline
+#endif
+
+/* stdint (C89 compatible) */
+#if (defined(YYJSON_HAS_STDINT_H) && YYJSON_HAS_STDINT_H) || \
+    YYJSON_MSC_VER >= 1600 || YYJSON_STDC_VER >= 199901L || \
+    defined(_STDINT_H) || defined(_STDINT_H_) || \
+    defined(__CLANG_STDINT_H) || defined(_STDINT_H_INCLUDED) || \
+    yyjson_has_include(<stdint.h>)
+#   include <stdint.h>
+#elif defined(_MSC_VER)
+#   if _MSC_VER < 1300
+        typedef signed char         int8_t;
+        typedef signed short        int16_t;
+        typedef signed int          int32_t;
+        typedef unsigned char       uint8_t;
+        typedef unsigned short      uint16_t;
+        typedef unsigned int        uint32_t;
+        typedef signed __int64      int64_t;
+        typedef unsigned __int64    uint64_t;
+#   else
+        typedef signed __int8       int8_t;
+        typedef signed __int16      int16_t;
+        typedef signed __int32      int32_t;
+        typedef unsigned __int8     uint8_t;
+        typedef unsigned __int16    uint16_t;
+        typedef unsigned __int32    uint32_t;
+        typedef signed __int64      int64_t;
+        typedef unsigned __int64    uint64_t;
+#   endif
+#else
+#   if UCHAR_MAX == 0xFFU
+        typedef signed char     int8_t;
+        typedef unsigned char   uint8_t;
+#   else
+#       error cannot find 8-bit integer type
+#   endif
+#   if USHRT_MAX == 0xFFFFU
+        typedef unsigned short  uint16_t;
+        typedef signed short    int16_t;
+#   elif UINT_MAX == 0xFFFFU
+        typedef unsigned int    uint16_t;
+        typedef signed int      int16_t;
+#   else
+#       error cannot find 16-bit integer type
+#   endif
+#   if UINT_MAX == 0xFFFFFFFFUL
+        typedef unsigned int    uint32_t;
+        typedef signed int      int32_t;
+#   elif ULONG_MAX == 0xFFFFFFFFUL
+        typedef unsigned long   uint32_t;
+        typedef signed long     int32_t;
+#   elif USHRT_MAX == 0xFFFFFFFFUL
+        typedef unsigned short  uint32_t;
+        typedef signed short    int32_t;
+#   else
+#       error cannot find 32-bit integer type
+#   endif
+#   if defined(__INT64_TYPE__) && defined(__UINT64_TYPE__)
+        typedef __INT64_TYPE__  int64_t;
+        typedef __UINT64_TYPE__ uint64_t;
+#   elif defined(__GNUC__) || defined(__clang__)
+#       if !defined(_SYS_TYPES_H) && !defined(__int8_t_defined)
+        __extension__ typedef long long             int64_t;
+#       endif
+        __extension__ typedef unsigned long long    uint64_t;
+#   elif defined(_LONG_LONG) || defined(__MWERKS__) || defined(_CRAYC) || \
+        defined(__SUNPRO_C) || defined(__SUNPRO_CC)
+        typedef long long           int64_t;
+        typedef unsigned long long  uint64_t;
+#   elif (defined(__BORLANDC__) && __BORLANDC__ > 0x460) || \
+        defined(__WATCOM_INT64__) || defined (__alpha) || defined (__DECC)
+        typedef __int64             int64_t;
+        typedef unsigned __int64    uint64_t;
+#   else
+#       error cannot find 64-bit integer type
+#   endif
+#endif
+
+/* stdbool (C89 compatible) */
+#if (defined(YYJSON_HAS_STDBOOL_H) && YYJSON_HAS_STDBOOL_H) || \
+    (yyjson_has_include(<stdbool.h>) && !defined(__STRICT_ANSI__)) || \
+    YYJSON_MSC_VER >= 1800 || YYJSON_STDC_VER >= 199901L
+#   include <stdbool.h>
+#elif !defined(__bool_true_false_are_defined)
+#   define __bool_true_false_are_defined 1
+#   if defined(__cplusplus)
+#       if defined(__GNUC__) && !defined(__STRICT_ANSI__)
+#           define _Bool bool
+#           if __cplusplus < 201103L
+#               define bool bool
+#               define false false
+#               define true true
+#           endif
+#       endif
+#   else
+#       define bool unsigned char
+#       define true 1
+#       define false 0
+#   endif
+#endif
+
+/* char bit check */
+#if defined(CHAR_BIT)
+#   if CHAR_BIT != 8
+#       error non 8-bit char is not supported
+#   endif
+#endif
+
+
+
+/*==============================================================================
+ * Compile Hint Begin
+ *============================================================================*/
+
+/* extern "C" begin */
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* warning suppress begin */
+#if defined(__clang__)
+#   pragma clang diagnostic push
+#   pragma clang diagnostic ignored "-Wunused-function"
+#   pragma clang diagnostic ignored "-Wunused-parameter"
+#elif defined(__GNUC__)
+#   if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6)
+#   pragma GCC diagnostic push
+#   endif
+#   pragma GCC diagnostic ignored "-Wunused-function"
+#   pragma GCC diagnostic ignored "-Wunused-parameter"
+#elif defined(_MSC_VER)
+#   pragma warning(push)
+#   pragma warning(disable:4800) /* 'int': forcing value to 'true' or 'false' */
+#endif
+
+/* version, same as YYJSON_VERSION_HEX */
+yyjson_api uint32_t yyjson_version(void);
+
+
+
+/*==============================================================================
+ * JSON Types
+ *============================================================================*/
+
+/** Type of JSON value (3 bit). */
+typedef uint8_t yyjson_type;
+#define YYJSON_TYPE_NONE        ((uint8_t)0)        /* _____000 */
+#define YYJSON_TYPE_NULL        ((uint8_t)2)        /* _____010 */
+#define YYJSON_TYPE_BOOL        ((uint8_t)3)        /* _____011 */
+#define YYJSON_TYPE_NUM         ((uint8_t)4)        /* _____100 */
+#define YYJSON_TYPE_STR         ((uint8_t)5)        /* _____101 */
+#define YYJSON_TYPE_ARR         ((uint8_t)6)        /* _____110 */
+#define YYJSON_TYPE_OBJ         ((uint8_t)7)        /* _____111 */
+
+/** Subtype of JSON value (2 bit). */
+typedef uint8_t yyjson_subtype;
+#define YYJSON_SUBTYPE_NONE     ((uint8_t)(0 << 3)) /* ___00___ */
+#define YYJSON_SUBTYPE_FALSE    ((uint8_t)(0 << 3)) /* ___00___ */
+#define YYJSON_SUBTYPE_TRUE     ((uint8_t)(1 << 3)) /* ___01___ */
+#define YYJSON_SUBTYPE_UINT     ((uint8_t)(0 << 3)) /* ___00___ */
+#define YYJSON_SUBTYPE_SINT     ((uint8_t)(1 << 3)) /* ___01___ */
+#define YYJSON_SUBTYPE_REAL     ((uint8_t)(2 << 3)) /* ___10___ */
+
+/** Mask and bits of JSON value. */
+#define YYJSON_TYPE_MASK        ((uint8_t)0x07)     /* _____111 */
+#define YYJSON_TYPE_BIT         ((uint8_t)3)
+#define YYJSON_SUBTYPE_MASK     ((uint8_t)0x18)     /* ___11___ */
+#define YYJSON_SUBTYPE_BIT      ((uint8_t)2)
+#define YYJSON_RESERVED_MASK    ((uint8_t)0xE0)     /* 111_____ */
+#define YYJSON_RESERVED_BIT     ((uint8_t)3)
+#define YYJSON_TAG_MASK         ((uint8_t)0xFF)     /* 11111111 */
+#define YYJSON_TAG_BIT          ((uint8_t)8)
+
+/** Padding size for JSON reader. */
+#define YYJSON_PADDING_SIZE     4
+
+
+
+/*==============================================================================
+ * Allocator
+ *============================================================================*/
+
+/**
+ A memory allocator.
+ 
+ Typically you don't need to use it, unless you want to customize your own
+ memory allocator.
+ */
+typedef struct yyjson_alc {
+    /* Same as libc's malloc(), should not be NULL. */
+    void *(*malloc)(void *ctx, size_t size);
+    /* Same as libc's realloc(), should not be NULL. */
+    void *(*realloc)(void *ctx, void *ptr, size_t size);
+    /* Same as libc's free(), should not be NULL. */
+    void (*free)(void *ctx, void *ptr);
+    /* A context for malloc/realloc/free, can be NULL. */
+    void *ctx;
+} yyjson_alc;
+
+/**
+ A pool allocator uses fixed length pre-allocated memory.
+ 
+ This allocator may used to avoid malloc()/memmove() calls.
+ The pre-allocated memory should be held by the caller. This is not
+ a general-purpose allocator, and should only be used to read or write
+ single JSON document.
+ 
+ Sample code (parse JSON with stack memory only):
+ 
+     char buf[65536];
+     yyjson_alc alc;
+     yyjson_alc_pool_init(&alc, buf, 65536);
+ 
+     const char *json = "{\"name\":\"Helvetica\",\"size\":14}"
+     yyjson_doc *doc = yyjson_read_opts(json, strlen(json), 0, &alc, NULL);
+     
+ */
+yyjson_api bool yyjson_alc_pool_init(yyjson_alc *alc, void *buf, size_t size);
+
+
+
+/*==============================================================================
+ * JSON Structure
+ *============================================================================*/
+
+/** An immutable JSON document. */
+typedef struct yyjson_doc yyjson_doc;
+
+/** An immutable JSON value. */
+typedef struct yyjson_val yyjson_val;
+
+/** A mutable JSON document. */
+typedef struct yyjson_mut_doc yyjson_mut_doc;
+
+/** A mutable JSON value. */
+typedef struct yyjson_mut_val yyjson_mut_val;
+
+
+
+/*==============================================================================
+ * JSON Reader API
+ *============================================================================*/
+
+/** Options for JSON reader. */
+typedef uint32_t yyjson_read_flag;
+
+/** Default option (RFC 8259 compliant):
+    - Read positive integer as uint64_t.
+    - Read negative integer as int64_t.
+    - Read floating-point number as double with correct rounding.
+    - Read integer which cannot fit in uint64_t or int64_t as double.
+    - Report error if real number is infinity.
+    - Report error if string contains invalid UTF-8 character or BOM.
+    - Report error on trailing commas, comments, inf and nan literals. */
+static const yyjson_read_flag YYJSON_READ_NOFLAG                = 0 << 0;
+
+/** Read the input data in-situ.
+    This option allows the reader to modify and use input data to store string
+    values, which can increase reading speed slightly.
+    The caller should hold the input data before free the document.
+    The input data must be padded by at least `YYJSON_PADDING_SIZE` byte.
+    For example: "[1,2]" should be "[1,2]\0\0\0\0", length should be 5. */
+static const yyjson_read_flag YYJSON_READ_INSITU                = 1 << 0;
+
+/** Stop when done instead of issues an error if there's additional content
+    after a JSON document. This option may used to parse small pieces of JSON
+    in larger data, such as NDJSON. */
+static const yyjson_read_flag YYJSON_READ_STOP_WHEN_DONE        = 1 << 1;
+
+/** Allow single trailing comma at the end of an object or array,
+    such as [1,2,3,] {"a":1,"b":2,}. */
+static const yyjson_read_flag YYJSON_READ_ALLOW_TRAILING_COMMAS = 1 << 2;
+
+/** Allow C-style single line and multiple line comments. */
+static const yyjson_read_flag YYJSON_READ_ALLOW_COMMENTS        = 1 << 3;
+
+/** Allow inf/nan number and literal, case-insensitive,
+    such as 1e999, NaN, inf, -Infinity. */
+static const yyjson_read_flag YYJSON_READ_ALLOW_INF_AND_NAN     = 1 << 4;
+
+
+
+/** Result code for JSON reader. */
+typedef uint32_t yyjson_read_code;
+
+/** Success, no error. */
+static const yyjson_read_code YYJSON_READ_SUCCESS                       = 0;
+
+/** Invalid parameter, such as NULL string or invalid file path. */
+static const yyjson_read_code YYJSON_READ_ERROR_INVALID_PARAMETER       = 1;
+
+/** Memory allocation failure occurs. */
+static const yyjson_read_code YYJSON_READ_ERROR_MEMORY_ALLOCATION       = 2;
+
+/** Input JSON string is empty. */
+static const yyjson_read_code YYJSON_READ_ERROR_EMPTY_CONTENT           = 3;
+
+/** Unexpected content after document, such as "[1]#". */
+static const yyjson_read_code YYJSON_READ_ERROR_UNEXPECTED_CONTENT      = 4;
+
+/** Unexpected ending, such as "[123". */
+static const yyjson_read_code YYJSON_READ_ERROR_UNEXPECTED_END          = 5;
+
+/** Unexpected character inside the document, such as "[#]". */
+static const yyjson_read_code YYJSON_READ_ERROR_UNEXPECTED_CHARACTER    = 6;
+
+/** Invalid JSON structure, such as "[1,]". */
+static const yyjson_read_code YYJSON_READ_ERROR_JSON_STRUCTURE          = 7;
+
+/** Invalid comment, such as unclosed multi-line comment. */
+static const yyjson_read_code YYJSON_READ_ERROR_INVALID_COMMENT         = 8;
+
+/** Invalid number, such as "123.e12", "000". */
+static const yyjson_read_code YYJSON_READ_ERROR_INVALID_NUMBER          = 9;
+
+/** Invalid string, such as invalid escaped character inside a string. */
+static const yyjson_read_code YYJSON_READ_ERROR_INVALID_STRING          = 10;
+
+/** Invalid JSON literal, such as "truu". */
+static const yyjson_read_code YYJSON_READ_ERROR_LITERAL                 = 11;
+
+/** Failed to open a file. */
+static const yyjson_read_code YYJSON_READ_ERROR_FILE_OPEN               = 12;
+
+/** Failed to read a file. */
+static const yyjson_read_code YYJSON_READ_ERROR_FILE_READ               = 13;
+
+/** Error information for JSON reader. */
+typedef struct yyjson_read_err {
+    /** Error code, see `yyjson_read_code` for all available values. */
+    yyjson_read_code code;
+    /** Short error message (NULL for success). */
+    const char *msg;
+    /** Error byte position for input data (0 for success). */
+    size_t pos;
+} yyjson_read_err;
+
+
+
+/**
+ Read JSON with options.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The `dat` is not modified by other threads.
+ 2. The `alc` is thread-safe or NULL.
+ 
+ @param dat The JSON data (UTF-8 without BOM).
+            If you pass NULL, you will get NULL result.
+            The data will not be modified without the flag `YYJSON_READ_INSITU`,
+            so you can pass a (const char *) string and case it to (char *) iff
+            you don't use the `YYJSON_READ_INSITU` flag.
+ 
+ @param len The JSON data's length.
+            If you pass 0, you will get NULL result.
+ 
+ @param flg The JSON read options.
+            You can combine multiple options using bitwise `|` operator.
+
+ @param alc The memory allocator used by JSON reader.
+            Pass NULL to use the libc's default allocator (thread-safe).
+ 
+ @param err A pointer to receive error information.
+            Pass NULL if you don't need error information.
+ 
+ @return    A new JSON document, or NULL if error occurs.
+            You should use yyjson_doc_free() to release it
+            when it's no longer needed.
+ */
+yyjson_api yyjson_doc *yyjson_read_opts(char *dat,
+                                        size_t len,
+                                        yyjson_read_flag flg,
+                                        const yyjson_alc *alc,
+                                        yyjson_read_err *err);
+
+/**
+ Read a JSON file.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The file is not modified by other threads.
+ 2. The `alc` is thread-safe or NULL.
+ 
+ @param path The JSON file's path.
+             If you pass an invalid path, you will get NULL result.
+ 
+ @param flg The JSON read options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param alc The memory allocator used by JSON reader.
+            Pass NULL to use the libc's default allocator (thread-safe).
+ 
+ @param err A pointer to receive error information.
+            Pass NULL if you don't need error information.
+ 
+ @return    A new JSON document, or NULL if error occurs.
+            You should use yyjson_doc_free() to release it
+            when it's no longer needed.
+ */
+yyjson_api yyjson_doc *yyjson_read_file(const char *path,
+                                        yyjson_read_flag flg,
+                                        const yyjson_alc *alc,
+                                        yyjson_read_err *err);
+
+/**
+ Read a JSON string.
+ 
+ This function is thread-safe.
+
+ @param dat The JSON string (UTF-8 without BOM).
+            If you pass NULL, you will get NULL result.
+ 
+ @param len The JSON data's length.
+            If you pass 0, you will get NULL result.
+ 
+ @param flg The JSON read options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @return    A new JSON document, or NULL if error occurs.
+            You should use yyjson_doc_free() to release it
+            when it's no longer needed.
+ */
+yyjson_api_inline yyjson_doc *yyjson_read(const char *dat,
+                                          size_t len,
+                                          yyjson_read_flag flg) {
+    flg &= ~YYJSON_READ_INSITU; /* const string cannot be modified */
+    return yyjson_read_opts((char *)dat, len, flg, NULL, NULL);
+}
+
+/**
+ Returns the size of maximum memory usage to read a JSON data.
+ You may use this value to avoid malloc() or calloc() call inside the reader
+ to get better performance, or read multiple JSON.
+ 
+ Sample code:
+ 
+     char *dat1, *dat2, *dat3; // JSON data
+     size_t len1, len2, len3; // JSON length
+     size_t max_len = max(len1, len2, len3);
+     yyjson_doc *doc;
+ 
+     // use one allocator for multiple JSON
+     size_t size = yyjson_read_max_memory_usage(max_len, 0);
+     void *buf = malloc(size);
+     yyjson_alc alc;
+     yyjson_alc_pool_init(&alc, buf, size);
+ 
+     // no more alloc() or realloc() call during reading
+     doc = yyjson_read_opts(dat1, len1, 0, &alc, NULL);
+     yyjson_doc_free(doc);
+     doc = yyjson_read_opts(dat2, len2, 0, &alc, NULL);
+     yyjson_doc_free(doc);
+     doc = yyjson_read_opts(dat3, len3, 0, &alc, NULL);
+     yyjson_doc_free(doc);
+ 
+     free(buf);
+    
+ @param len The JSON data's length.
+ @param flg The JSON read options.
+ @return The maximum memory size, or 0 if overflow.
+ */
+yyjson_api_inline size_t yyjson_read_max_memory_usage(size_t len,
+                                                      yyjson_read_flag flg) {
+    /*
+     1. The max value count is (json_size / 2 + 1),
+        for example: "[1,2,3,4]" size is 9, value count is 5.
+     2. Some broken JSON may cost more memory during reading, but fail at end,
+        for example: "[[[[[[[[".
+     3. yyjson use 16 bytes per value, see struct yyjson_val.
+     4. yyjson use dynamic memory with a growth factor of 1.5.
+     
+     The max memory size is (json_size / 2 * 16 * 1.5 + padding).
+     */
+    size_t mul = (size_t)12 + !(flg & YYJSON_READ_INSITU);
+    size_t pad = 256;
+    size_t max = (size_t)(~(size_t)0);
+    if (flg & YYJSON_READ_STOP_WHEN_DONE) len = len < 256 ? 256 : len;
+    if (len >= (max - pad - mul) / mul) return 0;
+    return len * mul + pad;
+}
+
+
+
+/*==============================================================================
+ * JSON Writer API
+ *============================================================================*/
+
+/** Options for JSON writer. */
+typedef uint32_t yyjson_write_flag;
+
+/** Default option:
+    - Write JSON minify.
+    - Report error on inf or nan number.
+    - Do not validate string encoding.
+    - Do not escape unicode or slash. */
+static const yyjson_write_flag YYJSON_WRITE_NOFLAG              = 0 << 0;
+
+/** Write JSON pretty with 4 space indent. */
+static const yyjson_write_flag YYJSON_WRITE_PRETTY              = 1 << 0;
+
+/** Escape unicode as `uXXXX`, make the output ASCII only. */
+static const yyjson_write_flag YYJSON_WRITE_ESCAPE_UNICODE      = 1 << 1;
+
+/** Escape '/' as '\/'. */
+static const yyjson_write_flag YYJSON_WRITE_ESCAPE_SLASHES      = 1 << 2;
+
+/** Write inf and nan number as 'Infinity' and 'NaN' literal (non-standard). */
+static const yyjson_write_flag YYJSON_WRITE_ALLOW_INF_AND_NAN   = 1 << 3;
+
+/** Write inf and nan number as null literal.
+    This flag will override `YYJSON_WRITE_ALLOW_INF_AND_NAN` flag. */
+static const yyjson_write_flag YYJSON_WRITE_INF_AND_NAN_AS_NULL = 1 << 4;
+
+
+
+/** Result code for JSON writer */
+typedef uint32_t yyjson_write_code;
+
+/** Success, no error. */
+static const yyjson_write_code YYJSON_WRITE_SUCCESS                     = 0;
+
+/** Invalid parameter, such as NULL document. */
+static const yyjson_write_code YYJSON_WRITE_ERROR_INVALID_PARAMETER     = 1;
+
+/** Memory allocation failure occurs. */
+static const yyjson_write_code YYJSON_WRITE_ERROR_MEMORY_ALLOCATION     = 2;
+
+/** Invalid value type in JSON document. */
+static const yyjson_write_code YYJSON_WRITE_ERROR_INVALID_VALUE_TYPE    = 3;
+
+/** NaN or Infinity number occurs. */
+static const yyjson_write_code YYJSON_WRITE_ERROR_NAN_OR_INF            = 4;
+
+/** Failed to open a file. */
+static const yyjson_write_code YYJSON_WRITE_ERROR_FILE_OPEN             = 5;
+
+/** Failed to write a file. */
+static const yyjson_write_code YYJSON_WRITE_ERROR_FILE_WRITE            = 6;
+
+/** Error information for JSON writer. */
+typedef struct yyjson_write_err {
+    /** Error code, see yyjson_write_code for all available values. */
+    yyjson_write_code code;
+    /** Short error message (NULL for success). */
+    const char *msg;
+} yyjson_write_err;
+
+
+
+/**
+ Write JSON with options.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The `alc` is thread-safe or NULL.
+
+ @param doc The JSON document.
+            If you pass NULL, you will get NULL result.
+ 
+ @param flg The JSON write options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param alc The memory allocator used by JSON writer.
+            Pass NULL to use the libc's default allocator (thread-safe).
+ 
+ @param len A pointer to receive output length in bytes.
+            Pass NULL if you don't need length information.
+
+ @param err A pointer to receive error information.
+            Pass NULL if you don't need error information.
+ 
+ @return    A new JSON string, or NULL if error occurs.
+            This string is encoded as UTF-8 with a null-terminator.
+            You should use free() or alc->free() to release it
+            when it's no longer needed.
+ */
+yyjson_api char *yyjson_write_opts(const yyjson_doc *doc,
+                                   yyjson_write_flag flg,
+                                   const yyjson_alc *alc,
+                                   size_t *len,
+                                   yyjson_write_err *err);
+
+/**
+ Write JSON file with options.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The file is not accessed by other threads.
+ 2. The `alc` is thread-safe or NULL.
+
+ @param path The JSON file's path.
+             If you pass an invalid path, you will get an error.
+             If the file is not empty, the content will be discarded.
+ 
+ @param doc The JSON document.
+            If you pass NULL or empty document, you will get an error.
+ 
+ @param flg The JSON write options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param alc The memory allocator used by JSON writer.
+            Pass NULL to use the libc's default allocator (thread-safe).
+ 
+ @param err A pointer to receive error information.
+            Pass NULL if you don't need error information.
+ 
+ @return    true for success, false for error.
+ */
+yyjson_api bool yyjson_write_file(const char *path,
+                                  const yyjson_doc *doc,
+                                  yyjson_write_flag flg,
+                                  const yyjson_alc *alc,
+                                  yyjson_write_err *err);
+
+/**
+ Write JSON.
+ 
+ This function is thread-safe.
+ 
+ @param doc The JSON document.
+            If you pass NULL, you will get NULL result.
+ 
+ @param flg The JSON write options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param len A pointer to receive output length in bytes.
+            Pass NULL if you don't need length information.
+ 
+ @return    A new JSON string, or NULL if error occurs.
+            This string is encoded as UTF-8 with a null-terminator.
+            You should use free() to release it when it's no longer needed.
+ */
+yyjson_api_inline char *yyjson_write(const yyjson_doc *doc,
+                                     yyjson_write_flag flg,
+                                     size_t *len) {
+    return yyjson_write_opts(doc, flg, NULL, len, NULL);
+}
+
+
+
+/**
+ Write JSON with options.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The `doc` is not modified by other threads.
+ 2. The `alc` is thread-safe or NULL.
+
+ @param doc The mutable JSON document.
+            If you pass NULL or empty document, you will get NULL result.
+ 
+ @param flg The JSON write options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param alc The memory allocator used by JSON writer.
+            Pass NULL to use the libc's default allocator (thread-safe).
+ 
+ @param len A pointer to receive output length in bytes.
+            Pass NULL if you don't need length information.
+
+ @param err A pointer to receive error information.
+            Pass NULL if you don't need error information.
+ 
+ @return    A new JSON string, or NULL if error occurs.
+            This string is encoded as UTF-8 with a null-terminator.
+            You should use free() or alc->free() to release it
+            when it's no longer needed.
+ */
+yyjson_api char *yyjson_mut_write_opts(const yyjson_mut_doc *doc,
+                                       yyjson_write_flag flg,
+                                       const yyjson_alc *alc,
+                                       size_t *len,
+                                       yyjson_write_err *err);
+
+/**
+ Write JSON file with options.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The file is not accessed by other threads.
+ 2. The `doc` is not modified by other threads.
+ 3. The `alc` is thread-safe or NULL.
+ 
+ @param path The JSON file's path.
+             If you pass an invalid path, you will get an error.
+             If the file is not empty, the content will be discarded.
+ 
+ @param doc The mutable JSON document.
+            If you pass NULL or empty document, you will get an error.
+ 
+ @param flg The JSON write options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param alc The memory allocator used by JSON writer.
+            Pass NULL to use the libc's default allocator (thread-safe).
+ 
+ @param err A pointer to receive error information.
+            Pass NULL if you don't need error information.
+ 
+ @return    true for success, false for error.
+ */
+yyjson_api bool yyjson_mut_write_file(const char *path,
+                                      const yyjson_mut_doc *doc,
+                                      yyjson_write_flag flg,
+                                      const yyjson_alc *alc,
+                                      yyjson_write_err *err);
+
+/**
+ Write JSON.
+ 
+ This function is thread-safe if you make sure that:
+ 1. The `doc` is not is not modified by other threads.
+
+ @param doc The JSON document.
+            If you pass NULL, you will get NULL result.
+ 
+ @param flg The JSON write options.
+            You can combine multiple options using bitwise `|` operator.
+ 
+ @param len A pointer to receive output length in bytes.
+            Pass NULL if you don't need length information.
+
+ @return    A new JSON string, or NULL if error occurs.
+            This string is encoded as UTF-8 with a null-terminator.
+            You should use free() or alc->free() to release it
+            when it's no longer needed.
+ */
+yyjson_api_inline char *yyjson_mut_write(const yyjson_mut_doc *doc,
+                                         yyjson_write_flag flg,
+                                         size_t *len) {
+    return yyjson_mut_write_opts(doc, flg, NULL, len, NULL);
+}
+
+
+
+/*==============================================================================
+ * JSON Document API
+ *============================================================================*/
+
+/** Returns the root value of this JSON document. */
+yyjson_api_inline yyjson_val *yyjson_doc_get_root(yyjson_doc *doc);
+
+/** Returns read size of input JSON data. */
+yyjson_api_inline size_t yyjson_doc_get_read_size(yyjson_doc *doc);
+
+/** Returns total value count in this JSON document. */
+yyjson_api_inline size_t yyjson_doc_get_val_count(yyjson_doc *doc);
+
+/** Release the JSON document and free the memory. */
+yyjson_api_inline void yyjson_doc_free(yyjson_doc *doc);
+
+
+
+/*==============================================================================
+ * JSON Value Type API
+ *============================================================================*/
+
+/** Returns whether the JSON value is null. */
+yyjson_api_inline bool yyjson_is_null(yyjson_val *val);
+
+/** Returns whether the JSON value is true. */
+yyjson_api_inline bool yyjson_is_true(yyjson_val *val);
+
+/** Returns whether the JSON value is false. */
+yyjson_api_inline bool yyjson_is_false(yyjson_val *val);
+
+/** Returns whether the JSON value is bool (true/false). */
+yyjson_api_inline bool yyjson_is_bool(yyjson_val *val);
+
+/** Returns whether the JSON value is unsigned integer (uint64_t). */
+yyjson_api_inline bool yyjson_is_uint(yyjson_val *val);
+
+/** Returns whether the JSON value is signed integer (int64_t). */
+yyjson_api_inline bool yyjson_is_sint(yyjson_val *val);
+
+/** Returns whether the JSON value is integer (uint64_t/int64_t). */
+yyjson_api_inline bool yyjson_is_int(yyjson_val *val);
+
+/** Returns whether the JSON value is real number (double). */
+yyjson_api_inline bool yyjson_is_real(yyjson_val *val);
+
+/** Returns whether the JSON value is number (uint64_t/int64_t/double). */
+yyjson_api_inline bool yyjson_is_num(yyjson_val *val);
+
+/** Returns whether the JSON value is string. */
+yyjson_api_inline bool yyjson_is_str(yyjson_val *val);
+
+/** Returns whether the JSON value is array. */
+yyjson_api_inline bool yyjson_is_arr(yyjson_val *val);
+
+/** Returns whether the JSON value is object. */
+yyjson_api_inline bool yyjson_is_obj(yyjson_val *val);
+
+/** Returns whether the JSON value is container (array/object). */
+yyjson_api_inline bool yyjson_is_ctn(yyjson_val *val);
+
+
+
+/*==============================================================================
+ * JSON Value Content API
+ *============================================================================*/
+
+/** Returns the JSON value's type. */
+yyjson_api_inline yyjson_type yyjson_get_type(yyjson_val *val);
+
+/** Returns the JSON value's subtype. */
+yyjson_api_inline yyjson_subtype yyjson_get_subtype(yyjson_val *val);
+
+/** Returns the JSON value's tag. */
+yyjson_api_inline uint8_t yyjson_get_tag(yyjson_val *val);
+
+/** Returns the JSON value's type description.
+    The return description should be one of these strings: "null", "string",
+    "array", "object", "true", "false", "uint", "sint", "real", "unknown". */
+yyjson_api_inline const char *yyjson_get_type_desc(yyjson_val *val);
+
+/** Returns the content if the value is bool, or false on error. */
+yyjson_api_inline bool yyjson_get_bool(yyjson_val *val);
+
+/** Returns the content if the value is integer, or 0 on error. */
+yyjson_api_inline uint64_t yyjson_get_uint(yyjson_val *val);
+
+/** Returns the content if the value is integer, or 0 on error. */
+yyjson_api_inline int64_t yyjson_get_sint(yyjson_val *val);
+
+/** Returns the content if the value is integer, or 0 on error. */
+yyjson_api_inline int yyjson_get_int(yyjson_val *val);
+
+/** Returns the content if the value is real number, or 0.0 on error. */
+yyjson_api_inline double yyjson_get_real(yyjson_val *val);
+
+/** Returns the content if the value is string, or NULL on error. */
+yyjson_api_inline const char *yyjson_get_str(yyjson_val *val);
+
+/** Returns the content length if the value is string, or 0 on error. */
+yyjson_api_inline size_t yyjson_get_len(yyjson_val *val);
+
+/** Returns whether the JSON value is equals to a string. */
+yyjson_api_inline bool yyjson_equals_str(yyjson_val *val, const char *str);
+
+/** Returns whether the JSON value is equals to a string. */
+yyjson_api_inline bool yyjson_equals_strn(yyjson_val *val, const char *str,
+                                          size_t len);
+
+
+
+/*==============================================================================
+ * JSON Array API
+ *============================================================================*/
+
+/** Returns the number of elements in this array, or 0 on error. */
+yyjson_api_inline size_t yyjson_arr_size(yyjson_val *arr);
+
+/** Returns the element at the specified position in this array,
+    or NULL if array is empty or the index is out of bounds.
+    @warning This function takes a linear search time if array is not flat. */
+yyjson_api_inline yyjson_val *yyjson_arr_get(yyjson_val *arr, size_t idx);
+
+/** Returns the first element of this array, or NULL if array is empty. */
+yyjson_api_inline yyjson_val *yyjson_arr_get_first(yyjson_val *arr);
+
+/** Returns the last element of this array, or NULL if array is empty.
+    @warning This function takes a linear search time if array is not flat. */
+yyjson_api_inline yyjson_val *yyjson_arr_get_last(yyjson_val *arr);
+
+
+
+/*==============================================================================
+ * JSON Array Iterator API
+ *============================================================================*/
+
+/**
+ A JSON array iterator.
+ 
+ Sample code:
+ 
+     yyjson_val *val;
+     yyjson_arr_iter iter;
+     yyjson_arr_iter_init(arr, &iter);
+     while ((val = yyjson_arr_iter_next(&iter))) {
+         print(val);
+     }
+ */
+typedef struct yyjson_arr_iter yyjson_arr_iter;
+
+/** Initialize an iterator for this array. */
+yyjson_api_inline bool yyjson_arr_iter_init(yyjson_val *arr,
+                                            yyjson_arr_iter *iter);
+
+/** Returns whether the iteration has more elements. */
+yyjson_api_inline bool yyjson_arr_iter_has_next(yyjson_arr_iter *iter);
+
+/** Returns the next element in the iteration, or NULL on end. */
+yyjson_api_inline yyjson_val *yyjson_arr_iter_next(yyjson_arr_iter *iter);
+
+/**
+ Macro for iterating over an array.
+ 
+ Sample code:
+ 
+     size_t idx, max;
+     yyjson_val *val;
+     yyjson_arr_foreach(arr, idx, max, val) {
+         print(idx, val);
+     }
+ */
+#define yyjson_arr_foreach(arr, idx, max, val) \
+    for ((idx) = 0, \
+        (max) = yyjson_arr_size(arr), \
+        (val) = yyjson_arr_get_first(arr); \
+        (idx) < (max); \
+        (idx)++, \
+        (val) = unsafe_yyjson_get_next(val))
+
+
+
+/*==============================================================================
+ * JSON Object API
+ *============================================================================*/
+
+/** Returns the number of key-value pairs in this object, or 0 on error. */
+yyjson_api_inline size_t yyjson_obj_size(yyjson_val *obj);
+
+/** Returns the value to which the specified key is mapped,
+    or NULL if this object contains no mapping for the key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_val *yyjson_obj_get(yyjson_val *obj, const char *key);
+
+/** Returns the value to which the specified key is mapped,
+    or NULL if this object contains no mapping for the key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_val *yyjson_obj_getn(yyjson_val *obj, const char *key,
+                                              size_t key_len);
+
+
+
+/*==============================================================================
+ * JSON Object Iterator API
+ *============================================================================*/
+
+/**
+ A JSON object iterator.
+ 
+ Sample code:
+ 
+     yyjson_val *key, *val;
+     yyjson_obj_iter iter;
+     yyjson_obj_iter_init(obj, &iter);
+     while ((key = yyjson_obj_iter_next(&iter))) {
+         val = yyjson_obj_iter_get_val(key);
+         print(key, val);
+     }
+ */
+typedef struct yyjson_obj_iter yyjson_obj_iter;
+
+/** Initialize an object iterator. */
+yyjson_api_inline bool yyjson_obj_iter_init(yyjson_val *obj,
+                                            yyjson_obj_iter *iter);
+
+/** Returns whether the iteration has more elements. */
+yyjson_api_inline bool yyjson_obj_iter_has_next(yyjson_obj_iter *iter);
+
+/** Returns the next key in the iteration, or NULL on end. */
+yyjson_api_inline yyjson_val *yyjson_obj_iter_next(yyjson_obj_iter *iter);
+
+/** Returns the value for key inside the iteration. */
+yyjson_api_inline yyjson_val *yyjson_obj_iter_get_val(yyjson_val *key);
+
+/**
+ Iterates to a specified key and returns the value.
+ If the key exists in the object, then the iterator will stop at the next key,
+ otherwise the iterator will not change and NULL is returned.
+ @warning This function takes a linear search time if the key is not nearby.
+ */
+yyjson_api_inline yyjson_val *yyjson_obj_iter_get(yyjson_obj_iter *iter,
+                                                  const char *key);
+
+/**
+ Iterates to a specified key and returns the value.
+ If the key exists in the object, then the iterator will stop at the next key,
+ otherwise the iterator will not change and NULL is returned.
+ @warning This function takes a linear search time if the key is not nearby.
+ */
+yyjson_api_inline yyjson_val *yyjson_obj_iter_getn(yyjson_obj_iter *iter,
+                                                   const char *key,
+                                                   size_t key_len);
+
+/**
+ Macro for iterating over an object.
+ 
+ Sample code:
+ 
+     size_t idx, max;
+     yyjson_val *key, *val;
+     yyjson_obj_foreach(obj, idx, max, key, val) {
+         print(key, val);
+     }
+ */
+#define yyjson_obj_foreach(obj, idx, max, key, val) \
+    for ((idx) = 0, \
+        (max) = yyjson_obj_size(obj), \
+        (key) = (obj) ? unsafe_yyjson_get_first(obj) : NULL, \
+        (val) = (key) + 1; \
+        (idx) < (max); \
+        (idx)++, \
+        (key) = unsafe_yyjson_get_next(val), \
+        (val) = (key) + 1)
+
+
+
+/*==============================================================================
+ * Mutable JSON Document API
+ *============================================================================*/
+
+/** Returns the root value of this JSON document. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_doc_get_root(yyjson_mut_doc *doc);
+
+/** Sets the root value of this JSON document. */
+yyjson_api_inline void yyjson_mut_doc_set_root(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *root);
+
+/** Delete the JSON document and free the memory. */
+yyjson_api void yyjson_mut_doc_free(yyjson_mut_doc *doc);
+
+/** Creates and returns a new mutable JSON document, returns NULL on error.
+    If allocator is NULL, the default allocator will be used. */
+yyjson_api yyjson_mut_doc *yyjson_mut_doc_new(const yyjson_alc *alc);
+
+/** Copies and returns a new mutable document from input, returns NULL on error.
+    This makes a `deep-copy` on the immutable document.
+    If allocator is NULL, the default allocator will be used. */
+yyjson_api yyjson_mut_doc *yyjson_doc_mut_copy(yyjson_doc *doc,
+                                               const yyjson_alc *alc);
+
+/** Copies and returns a new mutable value from input, returns NULL on error.
+    This makes a `deep-copy` on the immutable value.
+    The memory was managed by mutable document. */
+yyjson_api yyjson_mut_val *yyjson_val_mut_copy(yyjson_mut_doc *doc,
+                                               yyjson_val *val);
+
+
+
+/*==============================================================================
+ * Mutable JSON Value Type API
+ *============================================================================*/
+
+/** Returns whether the JSON value is null. */
+yyjson_api_inline bool yyjson_mut_is_null(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is true. */
+yyjson_api_inline bool yyjson_mut_is_true(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is false. */
+yyjson_api_inline bool yyjson_mut_is_false(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is bool (true/false). */
+yyjson_api_inline bool yyjson_mut_is_bool(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is unsigned integer (uint64_t). */
+yyjson_api_inline bool yyjson_mut_is_uint(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is signed integer (int64_t). */
+yyjson_api_inline bool yyjson_mut_is_sint(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is integer (uint64_t/int64_t). */
+yyjson_api_inline bool yyjson_mut_is_int(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is real number (double). */
+yyjson_api_inline bool yyjson_mut_is_real(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is number (uint/sint/real). */
+yyjson_api_inline bool yyjson_mut_is_num(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is string. */
+yyjson_api_inline bool yyjson_mut_is_str(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is array. */
+yyjson_api_inline bool yyjson_mut_is_arr(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is object. */
+yyjson_api_inline bool yyjson_mut_is_obj(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is container (array/object). */
+yyjson_api_inline bool yyjson_mut_is_ctn(yyjson_mut_val *val);
+
+
+
+/*==============================================================================
+ * Mutable JSON Value Content API
+ *============================================================================*/
+
+/** Returns the JSON value's type. */
+yyjson_api_inline yyjson_type yyjson_mut_get_type(yyjson_mut_val *val);
+
+/** Returns the JSON value's subtype. */
+yyjson_api_inline yyjson_subtype yyjson_mut_get_subtype(yyjson_mut_val *val);
+
+/** Returns the JSON value's tag. */
+yyjson_api_inline uint8_t yyjson_mut_get_tag(yyjson_mut_val *val);
+
+/** Returns the JSON value's type description.
+    The return description should be one of these strings: "null", "string",
+    "array", "object", "true", "false", "uint", "sint", "real", "unknown". */
+yyjson_api_inline const char *yyjson_mut_get_type_desc(yyjson_mut_val *val);
+
+/** Returns whether two JSON values are equal.
+    @warning This function takes a quadratic time. */
+yyjson_api bool yyjson_mut_equals(yyjson_mut_val *lhs,
+                                  yyjson_mut_val *rhs);
+
+/** Returns the content if the value is bool, or false on error. */
+yyjson_api_inline bool yyjson_mut_get_bool(yyjson_mut_val *val);
+
+/** Returns the content if the value is integer, or 0 on error. */
+yyjson_api_inline uint64_t yyjson_mut_get_uint(yyjson_mut_val *val);
+
+/** Returns the content if the value is integer, or 0 on error. */
+yyjson_api_inline int64_t yyjson_mut_get_sint(yyjson_mut_val *val);
+
+/** Returns the content if the value is integer, or 0 on error. */
+yyjson_api_inline int yyjson_mut_get_int(yyjson_mut_val *val);
+
+/** Returns the content if the value is real number, or 0.0 on error. */
+yyjson_api_inline double yyjson_mut_get_real(yyjson_mut_val *val);
+
+/** Returns the content if the value is string, or NULL on error. */
+yyjson_api_inline const char *yyjson_mut_get_str(yyjson_mut_val *val);
+
+/** Returns the content length if the value is string, or 0 on error. */
+yyjson_api_inline size_t yyjson_mut_get_len(yyjson_mut_val *val);
+
+/** Returns whether the JSON value is equals to a string. */
+yyjson_api_inline bool yyjson_mut_equals_str(yyjson_mut_val *val,
+                                             const char *str);
+
+/** Returns whether the JSON value is equals to a string. */
+yyjson_api_inline bool yyjson_mut_equals_strn(yyjson_mut_val *val,
+                                              const char *str, size_t len);
+
+
+
+/*==============================================================================
+ * Mutable JSON Value Creation API
+ *============================================================================*/
+
+/** Creates and returns a null value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_null(yyjson_mut_doc *doc);
+
+/** Creates and returns a true value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_true(yyjson_mut_doc *doc);
+
+/** Creates and returns a false value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_false(yyjson_mut_doc *doc);
+
+/** Creates and returns a bool value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_bool(yyjson_mut_doc *doc,
+                                                  bool val);
+
+/** Creates and returns an unsigned integer value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_uint(yyjson_mut_doc *doc,
+                                                  uint64_t num);
+
+/** Creates and returns a signed integer value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_sint(yyjson_mut_doc *doc,
+                                                  int64_t num);
+
+/** Creates and returns a signed integer value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_int(yyjson_mut_doc *doc,
+                                                 int64_t num);
+
+/** Creates and returns an real number value, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_real(yyjson_mut_doc *doc,
+                                                  double num);
+
+/** Creates and returns a string value, returns NULL on error.
+    The input value should be a valid UTF-8 encoded string with null-terminator.
+    @warning The input string is not copied. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_str(yyjson_mut_doc *doc,
+                                                 const char *str);
+
+/** Creates and returns a string value, returns NULL on error.
+    The input value should be a valid UTF-8 encoded string.
+    @warning The input string is not copied. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_strn(yyjson_mut_doc *doc,
+                                                  const char *str,
+                                                  size_t len);
+
+/** Creates and returns a string value, returns NULL on error.
+    The input value should be a valid UTF-8 encoded string with null-terminator.
+    The input string is copied and held by the document. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_strcpy(yyjson_mut_doc *doc,
+                                                    const char *str);
+
+/** Creates and returns a string value, returns NULL on error.
+    The input value should be a valid UTF-8 encoded string.
+    The input string is copied and held by the document. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_strncpy(yyjson_mut_doc *doc,
+                                                     const char *str,
+                                                     size_t len);
+
+
+
+/*==============================================================================
+ * Mutable JSON Array API
+ *============================================================================*/
+
+/** Returns the number of elements in this array. */
+yyjson_api_inline size_t yyjson_mut_arr_size(yyjson_mut_val *arr);
+
+/** Returns the element at the specified position in this array,
+    or NULL if array is empty or the index is out of bounds.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_get(yyjson_mut_val *arr,
+                                                     size_t idx);
+
+/** Returns the first element of this array, or NULL if array is empty. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_get_first(yyjson_mut_val *arr);
+
+/** Returns the last element of this array, or NULL if array is empty. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_get_last(yyjson_mut_val *arr);
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Iterator API
+ *============================================================================*/
+
+/**
+ A mutable JSON array iterator.
+ 
+ Sample code:
+ 
+     yyjson_mut_val *val;
+     yyjson_mut_arr_iter iter;
+     yyjson_mut_arr_iter_init(arr, &iter);
+     while ((val = yyjson_mut_arr_iter_next(&iter))) {
+         print(val);
+         if (val_is_unused(val)) {
+             yyjson_mut_arr_iter_remove(&iter);
+         }
+     }
+    
+ @warning You should not modify the array while enumerating through it,
+          but you can use yyjson_mut_arr_iter_remove() to remove current value.
+ */
+typedef struct yyjson_mut_arr_iter yyjson_mut_arr_iter;
+
+/** Initialize an iterator for this array. */
+yyjson_api_inline bool yyjson_mut_arr_iter_init(yyjson_mut_val *arr,
+                                                yyjson_mut_arr_iter *iter);
+
+/** Returns whether the iteration has more elements. */
+yyjson_api_inline bool yyjson_mut_arr_iter_has_next(yyjson_mut_arr_iter *iter);
+
+/** Returns the next element in the iteration, or NULL on end. */
+yyjson_api_inline
+yyjson_mut_val *yyjson_mut_arr_iter_next(yyjson_mut_arr_iter *iter);
+
+/** Removes and returns current element in the iteration. */
+yyjson_api_inline
+yyjson_mut_val *yyjson_mut_arr_iter_remove(yyjson_mut_arr_iter *iter);
+
+/**
+ Macro for iterating over an array.
+ 
+ Sample code:
+ 
+     size_t idx, max;
+     yyjson_mut_val *val;
+     yyjson_mut_arr_foreach(arr, idx, max, val) {
+         print(idx, val);
+     }
+ 
+ @warning You should not modify the array while enumerating through it.
+ */
+#define yyjson_mut_arr_foreach(arr, idx, max, val) \
+    for ((idx) = 0, \
+        (max) = yyjson_mut_arr_size(arr), \
+        (val) = yyjson_mut_arr_get_first(arr); \
+        (idx) < (max); \
+        (idx)++, \
+        (val) = (val)->next)
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Creation API
+ *============================================================================*/
+
+/** Creates and returns a mutable array, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr(yyjson_mut_doc *doc);
+
+/** Creates and returns a mutable array with bool. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_bool(
+    yyjson_mut_doc *doc, const bool *vals, size_t count);
+
+/** Creates and returns a mutable array with sint numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint(
+    yyjson_mut_doc *doc, const int64_t *vals, size_t count);
+
+/** Creates and returns a mutable array with uint numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint(
+    yyjson_mut_doc *doc, const uint64_t *vals, size_t count);
+
+/** Creates and returns a mutable array with real numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_real(
+    yyjson_mut_doc *doc, const double *vals, size_t count);
+
+/** Creates and returns a mutable array with int8 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint8(
+    yyjson_mut_doc *doc, const int8_t *vals, size_t count);
+
+/** Creates and returns a mutable array with int16 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint16(
+    yyjson_mut_doc *doc, const int16_t *vals, size_t count);
+
+/** Creates and returns a mutable array with int32 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint32(
+    yyjson_mut_doc *doc, const int32_t *vals, size_t count);
+
+/** Creates and returns a mutable array with int64 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint64(
+    yyjson_mut_doc *doc, const int64_t *vals, size_t count);
+
+/** Creates and returns a mutable array with uint8 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint8(
+    yyjson_mut_doc *doc, const uint8_t *vals, size_t count);
+
+/** Creates and returns a mutable array with uint16 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint16(
+    yyjson_mut_doc *doc, const uint16_t *vals, size_t count);
+
+/** Creates and returns a mutable array with uint32 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint32(
+    yyjson_mut_doc *doc, const uint32_t *vals, size_t count);
+
+/** Creates and returns a mutable array with uint64 numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint64(
+    yyjson_mut_doc *doc, const uint64_t *vals, size_t count);
+
+/** Creates and returns a mutable array with float numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_float(
+    yyjson_mut_doc *doc, const float *vals, size_t count);
+
+/** Creates and returns a mutable array with double numbers. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_double(
+    yyjson_mut_doc *doc, const double *vals, size_t count);
+
+/** Creates and returns a mutable array with strings (no copy).
+    The strings should be encoded as UTF-8 with null-terminator. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_str(
+    yyjson_mut_doc *doc, const char **vals, size_t count);
+
+/** Creates and returns a mutable array with strings (no copy).
+    The strings should be encoded as UTF-8. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_strn(
+    yyjson_mut_doc *doc, const char **vals, const size_t *lens, size_t count);
+
+/** Creates and returns a mutable array with strings (copied).
+    The strings should be encoded as UTF-8 with null-terminator. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_strcpy(
+    yyjson_mut_doc *doc, const char **vals, size_t count);
+
+/** Creates and returns a mutable array with strings (copied).
+    The strings should be encoded as UTF-8. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_strncpy(
+    yyjson_mut_doc *doc, const char **vals, const size_t *lens, size_t count);
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Modification API
+ *============================================================================*/
+
+/** Inserts a value into an array at a given index, returns false on error.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_arr_insert(yyjson_mut_val *arr,
+                                             yyjson_mut_val *val, size_t idx);
+
+/** Inserts a val at the end of the array, returns false on error. */
+yyjson_api_inline bool yyjson_mut_arr_append(yyjson_mut_val *arr,
+                                             yyjson_mut_val *val);
+
+/** Inserts a val at the head of the array, returns false on error. */
+yyjson_api_inline bool yyjson_mut_arr_prepend(yyjson_mut_val *arr,
+                                              yyjson_mut_val *val);
+
+/** Replaces a value at index and returns old value, returns NULL on error.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_replace(yyjson_mut_val *arr,
+                                                         size_t idx,
+                                                         yyjson_mut_val *val);
+
+/** Removes and returns a value at index, returns NULL on error.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_remove(yyjson_mut_val *arr,
+                                                        size_t idx);
+
+/** Removes and returns the first value in this array, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_remove_first(
+                                                        yyjson_mut_val *arr);
+
+/** Removes and returns the last value in this array, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_remove_last(
+                                                        yyjson_mut_val *arr);
+
+/** Removes all values within a specified range in the array.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_arr_remove_range(yyjson_mut_val *arr,
+                                                   size_t idx, size_t len);
+
+/** Removes all values in this array. */
+yyjson_api_inline bool yyjson_mut_arr_clear(yyjson_mut_val *arr);
+
+/** Rotates values in this array for the given number of times.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_arr_rotate(yyjson_mut_val *arr,
+                                             size_t idx);
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Modification Convenience API
+ *============================================================================*/
+
+/** Adds a value at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_val(yyjson_mut_val *arr,
+                                              yyjson_mut_val *val);
+
+/** Adds a null val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_null(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr);
+
+/** Adds a true val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_true(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr);
+
+/** Adds a false val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_false(yyjson_mut_doc *doc,
+                                                yyjson_mut_val *arr);
+
+/** Adds a bool val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_bool(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               bool val);
+
+/** Adds a uint val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_uint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               uint64_t num);
+
+/** Adds a sint val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_sint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               int64_t num);
+
+/** Adds an int val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_int(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *arr,
+                                              int64_t num);
+
+/** Adds a double val at the end of the array. */
+yyjson_api_inline bool yyjson_mut_arr_add_real(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               double num);
+
+/** Adds a string val at the end of the array (no copy).
+    The string should be encoded as UTF-8 with null-terminator. */
+yyjson_api_inline bool yyjson_mut_arr_add_str(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *arr,
+                                              const char *str);
+
+/** Adds a string val at the end of the array (no copy).
+    The strings should be encoded as UTF-8. */
+yyjson_api_inline bool yyjson_mut_arr_add_strn(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               const char *str,
+                                               size_t len);
+
+/** Adds a string val at the end of the array (copied).
+    The strings should be encoded as UTF-8 with null-terminator. */
+yyjson_api_inline bool yyjson_mut_arr_add_strcpy(yyjson_mut_doc *doc,
+                                                 yyjson_mut_val *arr,
+                                                 const char *str);
+
+/** Adds a string val at the end of the array (copied).
+    The strings should be encoded as UTF-8. */
+yyjson_api_inline bool yyjson_mut_arr_add_strncpy(yyjson_mut_doc *doc,
+                                                  yyjson_mut_val *arr,
+                                                  const char *str,
+                                                  size_t len);
+
+/** Creates and adds a new array at the end of the array.
+    Returns the new array, or NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_add_arr(yyjson_mut_doc *doc,
+                                                         yyjson_mut_val *arr);
+
+/** Creates and adds a new object at the end of the array.
+    Returns the new object, or NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_add_obj(yyjson_mut_doc *doc,
+                                                         yyjson_mut_val *arr);
+
+
+
+/*==============================================================================
+ * Mutable JSON Object API
+ *============================================================================*/
+
+/** Returns the number of key-value pair in this object. */
+yyjson_api_inline size_t yyjson_mut_obj_size(yyjson_mut_val *obj);
+
+/** Returns the value to which the specified key is mapped,
+    or NULL if this object contains no mapping for the key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_get(yyjson_mut_val *obj,
+                                                     const char *key_str);
+
+/** Returns the value to which the specified key is mapped,
+    or NULL if this object contains no mapping for the key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_getn(yyjson_mut_val *obj,
+                                                      const char *key_str,
+                                                      size_t key_len);
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Iterator API
+ *============================================================================*/
+
+/**
+ A mutable JSON object iterator.
+ 
+ Sample code:
+ 
+     yyjson_mut_val *key, *val;
+     yyjson_mut_obj_iter iter;
+     yyjson_mut_obj_iter_init(obj, &iter);
+     while ((key = yyjson_mut_obj_iter_next(&iter))) {
+         val = yyjson_mut_obj_iter_get_val(key);
+         print(key, val);
+         if (key_is_unused(key)) {
+             yyjson_mut_obj_iter_remove(&iter);
+         }
+     }
+ 
+ @warning You should not modify the object while enumerating through it,
+          but you can use yyjson_mut_obj_iter_remove() to remove current value.
+ */
+typedef struct yyjson_mut_obj_iter yyjson_mut_obj_iter;
+
+/** Initialize an object iterator. */
+yyjson_api_inline bool yyjson_mut_obj_iter_init(yyjson_mut_val *obj,
+                                                yyjson_mut_obj_iter *iter);
+
+/** Returns whether the iteration has more elements. */
+yyjson_api_inline bool yyjson_mut_obj_iter_has_next(yyjson_mut_obj_iter *iter);
+
+/** Returns the next key in the iteration, or NULL on end. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_next(
+                                                    yyjson_mut_obj_iter *iter);
+
+/** Returns the value for key inside the iteration. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_get_val(
+                                                    yyjson_mut_val *key);
+
+/** Removes and returns current key in the iteration, the value can be
+    accessed by key->next. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_remove(
+                                                    yyjson_mut_obj_iter *iter);
+
+/**
+ Iterates to a specified key and returns the value.
+ If the key exists in the object, then the iterator will stop at the next key,
+ otherwise the iterator will not change and NULL is returned.
+ @warning This function takes a linear search time if the key is not nearby.
+ */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_get(
+                                                    yyjson_mut_obj_iter *iter,
+                                                    const char *key);
+
+/**
+ Iterates to a specified key and returns the value.
+ If the key exists in the object, then the iterator will stop at the next key,
+ otherwise the iterator will not change and NULL is returned.
+ @warning This function takes a linear search time if the key is not nearby.
+ */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_getn(
+                                                    yyjson_mut_obj_iter *iter,
+                                                    const char *key,
+                                                    size_t key_len);
+
+/**
+ Macro for iterating over an object.
+ 
+ Sample code:
+ 
+     size_t idx, max;
+     yyjson_val *key, *val;
+     yyjson_obj_foreach(obj, idx, max, key, val) {
+         print(key, val);
+     }
+ 
+ @warning You should not modify the object while enumerating through it.
+ */
+#define yyjson_mut_obj_foreach(obj, idx, max, key, val) \
+    for ((idx) = 0, \
+        (max) = yyjson_mut_obj_size(obj), \
+        (key) = (max) ? ((yyjson_mut_val *)(obj)->uni.ptr)->next->next : NULL, \
+        (val) = (key) ? (key)->next : NULL; \
+        (idx) < (max); \
+        (idx)++, \
+        (key) = (val)->next, \
+        (val) = (key)->next)
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Creation API
+ *============================================================================*/
+
+/** Creates and returns a mutable object, returns NULL on error. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj(yyjson_mut_doc *doc);
+
+/** Creates and returns a mutable object with keys and values,
+    returns NULL on error. The keys and values are not copied.
+    The strings should be encoded as UTF-8 with null-terminator. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_with_str(yyjson_mut_doc *doc,
+                                                          const char **keys,
+                                                          const char **vals,
+                                                          size_t count);
+
+/** Creates and returns a mutable object with key-value pairs and pair count,
+    returns NULL on error. The keys and values are not copied.
+    The strings should be encoded as UTF-8 with null-terminator. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_with_kv(yyjson_mut_doc *doc,
+                                                         const char **kv_pairs,
+                                                         size_t pair_count);
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Modification API
+ *============================================================================*/
+
+/** Adds a key-value pair at the end of the object. The key must be a string.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add(yyjson_mut_val *obj,
+                                          yyjson_mut_val *key,
+                                          yyjson_mut_val *val);
+
+/** Adds a key-value pair to the object. The key must be a string.
+    This function may remove all key-value pairs for the given key before add.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_put(yyjson_mut_val *obj,
+                                          yyjson_mut_val *key,
+                                          yyjson_mut_val *val);
+
+/** Inserts a key-value pair to the object at the given position.
+    The key must be a string. This function allows duplicated key in one object.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_insert(yyjson_mut_val *obj,
+                                             yyjson_mut_val *key,
+                                             yyjson_mut_val *val,
+                                             size_t idx);
+
+/** Removes key-value pair from the object with given key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_remove(yyjson_mut_val *obj,
+                                             yyjson_mut_val *key);
+
+/** Removes all key-value pairs in this object. */
+yyjson_api_inline bool yyjson_mut_obj_clear(yyjson_mut_val *obj);
+
+/** Replaces value from the object with given key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_replace(yyjson_mut_val *obj,
+                                              yyjson_mut_val *key,
+                                              yyjson_mut_val *val);
+
+/** Rotates key-value pairs in the object for the given number of times.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_rotate(yyjson_mut_val *obj,
+                                             size_t idx);
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Modification Convenience API
+ *============================================================================*/
+
+/** Adds a null value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_null(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key);
+
+/** Adds a true value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_true(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key);
+
+/** Adds a false value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_false(yyjson_mut_doc *doc,
+                                                yyjson_mut_val *obj,
+                                                const char *key);
+
+/** Adds a bool value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_bool(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key, bool val);
+
+/** Adds a uint value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_uint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key, uint64_t val);
+
+/** Adds a sint value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_sint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key, int64_t val);
+
+/** Adds an int value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_int(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *obj,
+                                              const char *key, int64_t val);
+
+/** Adds a double value at the end of the object. The key is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_real(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key, double val);
+
+/** Adds a string value at the end of the object. The key/value is not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_str(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *obj,
+                                              const char *key, const char *val);
+
+/** Adds a string value at the end of the object.
+    The key and value are not copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_strn(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *key,
+                                               const char *val, size_t len);
+
+/** Adds a string value at the end of the object.
+    The key is not copied, but the value is copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_strcpy(yyjson_mut_doc *doc,
+                                                 yyjson_mut_val *obj,
+                                                 const char *key,
+                                                 const char *val);
+
+/** Adds a string value at the end of the object.
+    The key is not copied, but the value is copied.
+    This function allows duplicated key in one object. */
+yyjson_api_inline bool yyjson_mut_obj_add_strncpy(yyjson_mut_doc *doc,
+                                                  yyjson_mut_val *obj,
+                                                  const char *key,
+                                                  const char *val, size_t len);
+
+/** Removes all key-value pairs for the given key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_remove_str(yyjson_mut_val *obj,
+                                                 const char *key);
+
+/** Removes all key-value pairs for the given key.
+    @warning This function takes a linear search time. */
+yyjson_api_inline bool yyjson_mut_obj_remove_strn(yyjson_mut_val *obj,
+                                                  const char *key, size_t len);
+
+
+
+/*==============================================================================
+ * JSON Pointer API
+ *============================================================================*/
+
+/** Get a JSON value with JSON Pointer: https://tools.ietf.org/html/rfc6901
+    For example: "/users/0/uid".
+    Returns NULL if there's no matched value. */
+yyjson_api_inline yyjson_val *yyjson_get_pointer(yyjson_val *val,
+                                                 const char *pointer);
+
+/** Get a JSON value with JSON Pointer: https://tools.ietf.org/html/rfc6901
+    For example: "/users/0/uid".
+    Returns NULL if there's no matched value. */
+yyjson_api_inline yyjson_val *yyjson_doc_get_pointer(yyjson_doc *doc,
+                                                     const char *pointer);
+
+/** Get a JSON value with JSON Pointer: https://tools.ietf.org/html/rfc6901
+    For example: "/users/0/uid".
+    Returns NULL if there's no matched value. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_get_pointer(yyjson_mut_val *val,
+                                                         const char *pointer);
+
+/** Get a JSON value with JSON Pointer: https://tools.ietf.org/html/rfc6901
+    For example: "/users/0/uid".
+    Returns NULL if there's no matched value. */
+yyjson_api_inline yyjson_mut_val *yyjson_mut_doc_get_pointer(
+                                    yyjson_mut_doc *doc, const char *pointer);
+
+
+
+/*==============================================================================
+ * JSON Merge-Patch API
+ *============================================================================*/
+
+/** Creates and returns a merge-patched JSON value:
+    https://tools.ietf.org/html/rfc7386
+    Returns NULL if the patch could not be applied. */
+yyjson_api yyjson_mut_val *yyjson_merge_patch(yyjson_mut_doc *doc,
+                                              yyjson_val *orig,
+                                              yyjson_val *patch);
+
+
+
+/*==============================================================================
+ * JSON Structure (Implementation)
+ *============================================================================*/
+
+/* Payload of a JSON value (8 bytes). */
+typedef union yyjson_val_uni {
+    uint64_t    u64;
+    int64_t     i64;
+    double      f64;
+    const char *str;
+    void       *ptr;
+    size_t      ofs;
+} yyjson_val_uni;
+
+/* An immutable JSON value (16 bytes). */
+struct yyjson_val {
+    uint64_t tag;
+    yyjson_val_uni uni;
+};
+
+/* An immutable JSON Document. */
+struct yyjson_doc {
+    /* Root value of the document (nonnull). */
+    yyjson_val *root;
+    /* Allocator used by document (nonnull). */
+    yyjson_alc alc;
+    /* The total number of bytes read when parsing JSON (nonzero). */
+    size_t dat_read;
+    /* The total number of value read when parsing JSON (nonzero). */
+    size_t val_read;
+    /* The string pool used by JSON values (nullable). */
+    char *str_pool;
+};
+
+
+
+/*==============================================================================
+ * Unsafe JSON Value API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_type unsafe_yyjson_get_type(void *val) {
+    uint8_t tag = (uint8_t)((yyjson_val *)val)->tag;
+    return (yyjson_type)(tag & YYJSON_TYPE_MASK);
+}
+
+yyjson_api_inline yyjson_subtype unsafe_yyjson_get_subtype(void *val) {
+    uint8_t tag = (uint8_t)((yyjson_val *)val)->tag;
+    return (yyjson_subtype)(tag & YYJSON_SUBTYPE_MASK);
+}
+
+yyjson_api_inline uint8_t unsafe_yyjson_get_tag(void *val) {
+    uint8_t tag = (uint8_t)((yyjson_val *)val)->tag;
+    return (uint8_t)(tag & YYJSON_TAG_MASK);
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_null(void *val) {
+    return unsafe_yyjson_get_type(val) == YYJSON_TYPE_NULL;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_bool(void *val) {
+    return unsafe_yyjson_get_type(val) == YYJSON_TYPE_BOOL;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_num(void *val) {
+    return unsafe_yyjson_get_type(val) == YYJSON_TYPE_NUM;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_str(void *val) {
+    return unsafe_yyjson_get_type(val) == YYJSON_TYPE_STR;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_arr(void *val) {
+    return unsafe_yyjson_get_type(val) == YYJSON_TYPE_ARR;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_obj(void *val) {
+    return unsafe_yyjson_get_type(val) == YYJSON_TYPE_OBJ;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_ctn(void *val) {
+    uint8_t mask = YYJSON_TYPE_ARR & YYJSON_TYPE_OBJ;
+    return (unsafe_yyjson_get_tag(val) & mask) == mask;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_uint(void *val) {
+    const uint8_t patt = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+    return unsafe_yyjson_get_tag(val) == patt;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_sint(void *val) {
+    const uint8_t patt = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+    return unsafe_yyjson_get_tag(val) == patt;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_int(void *val) {
+    const uint8_t mask = YYJSON_TAG_MASK & (~YYJSON_SUBTYPE_SINT);
+    const uint8_t patt = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+    return (unsafe_yyjson_get_tag(val) & mask) == patt;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_real(void *val) {
+    const uint8_t patt = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+    return unsafe_yyjson_get_tag(val) == patt;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_true(void *val) {
+    const uint8_t patt = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE;
+    return unsafe_yyjson_get_tag(val) == patt;
+}
+
+yyjson_api_inline bool unsafe_yyjson_is_false(void *val) {
+    const uint8_t patt = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE;
+    return unsafe_yyjson_get_tag(val) == patt;
+}
+
+yyjson_api_inline bool unsafe_yyjson_arr_is_flat(yyjson_val *val) {
+    size_t ofs = val->uni.ofs;
+    size_t len = (size_t)(val->tag >> YYJSON_TAG_BIT);
+    return len * sizeof(yyjson_val) + sizeof(yyjson_val) == ofs;
+}
+
+yyjson_api_inline bool unsafe_yyjson_get_bool(void *val) {
+    uint8_t tag = unsafe_yyjson_get_tag(val);
+    return (bool)((tag & YYJSON_SUBTYPE_MASK) >> YYJSON_TYPE_BIT);
+}
+
+yyjson_api_inline uint64_t unsafe_yyjson_get_uint(void *val) {
+    return ((yyjson_val *)val)->uni.u64;
+}
+
+yyjson_api_inline int64_t unsafe_yyjson_get_sint(void *val) {
+    return ((yyjson_val *)val)->uni.i64;
+}
+
+yyjson_api_inline int unsafe_yyjson_get_int(void *val) {
+    return (int)((yyjson_val *)val)->uni.i64;
+}
+
+yyjson_api_inline double unsafe_yyjson_get_real(void *val) {
+    return ((yyjson_val *)val)->uni.f64;
+}
+
+yyjson_api_inline const char *unsafe_yyjson_get_str(void *val) {
+    return ((yyjson_val *)val)->uni.str;
+}
+
+yyjson_api_inline size_t unsafe_yyjson_get_len(void *val) {
+    return (size_t)(((yyjson_val *)val)->tag >> YYJSON_TAG_BIT);
+}
+
+yyjson_api_inline void unsafe_yyjson_set_len(void *val, size_t len) {
+    uint64_t tag = ((yyjson_val *)val)->tag & YYJSON_TAG_MASK;
+    tag |= (uint64_t)len << YYJSON_TAG_BIT;
+    ((yyjson_val *)val)->tag = tag;
+}
+
+yyjson_api_inline yyjson_val *unsafe_yyjson_get_first(yyjson_val *ctn) {
+    return ctn + 1;
+}
+
+yyjson_api_inline yyjson_val *unsafe_yyjson_get_next(yyjson_val *val) {
+    bool is_ctn = unsafe_yyjson_is_ctn(val);
+    size_t ctn_ofs = val->uni.ofs;
+    size_t ofs = (is_ctn ? ctn_ofs : sizeof(yyjson_val));
+    return (yyjson_val *)(void *)((uint8_t *)val + ofs);
+}
+
+yyjson_api_inline bool unsafe_yyjson_equals_strn(void *val, const char *str,
+                                                 size_t len) {
+    uint64_t tag = ((uint64_t)len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+    return ((yyjson_val *)val)->tag == tag &&
+    memcmp(((yyjson_val *)val)->uni.str, str, len) == 0;
+}
+
+yyjson_api_inline bool unsafe_yyjson_equals_str(void *val, const char *str) {
+    return unsafe_yyjson_equals_strn(val, str, strlen(str));
+}
+
+
+
+/*==============================================================================
+ * JSON Document API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_val *yyjson_doc_get_root(yyjson_doc *doc) {
+    return doc ? doc->root : NULL;
+}
+
+yyjson_api_inline size_t yyjson_doc_get_read_size(yyjson_doc *doc) {
+    return doc ? doc->dat_read : 0;
+}
+
+yyjson_api_inline size_t yyjson_doc_get_val_count(yyjson_doc *doc) {
+    return doc ? doc->val_read : 0;
+}
+
+yyjson_api_inline void yyjson_doc_free(yyjson_doc *doc) {
+    if (doc) {
+        yyjson_alc alc = doc->alc;
+        if (doc->str_pool) alc.free(alc.ctx, doc->str_pool);
+        alc.free(alc.ctx, doc);
+    }
+}
+
+
+
+/*==============================================================================
+ * JSON Value Type API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline bool yyjson_is_null(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_null(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_true(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_true(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_false(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_false(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_bool(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_bool(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_uint(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_uint(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_sint(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_sint(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_int(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_int(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_real(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_real(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_num(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_num(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_str(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_str(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_arr(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_arr(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_obj(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_obj(val) : false;
+}
+
+yyjson_api_inline bool yyjson_is_ctn(yyjson_val *val) {
+    return val ? unsafe_yyjson_is_ctn(val) : false;
+}
+
+
+
+/*==============================================================================
+ * JSON Value Content API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_type yyjson_get_type(yyjson_val *val) {
+    return val ? unsafe_yyjson_get_type(val) : YYJSON_TYPE_NONE;
+}
+
+yyjson_api_inline yyjson_subtype yyjson_get_subtype(yyjson_val *val) {
+    return val ? unsafe_yyjson_get_subtype(val) : YYJSON_SUBTYPE_NONE;
+}
+
+yyjson_api_inline uint8_t yyjson_get_tag(yyjson_val *val) {
+    return val ? unsafe_yyjson_get_tag(val) : 0;
+}
+
+yyjson_api_inline const char *yyjson_get_type_desc(yyjson_val *val) {
+    switch (yyjson_get_tag(val)) {
+        case YYJSON_TYPE_NULL | YYJSON_SUBTYPE_NONE:  return "null";
+        case YYJSON_TYPE_STR  | YYJSON_SUBTYPE_NONE:  return "string";
+        case YYJSON_TYPE_ARR  | YYJSON_SUBTYPE_NONE:  return "array";
+        case YYJSON_TYPE_OBJ  | YYJSON_SUBTYPE_NONE:  return "object";
+        case YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE:  return "true";
+        case YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE: return "false";
+        case YYJSON_TYPE_NUM  | YYJSON_SUBTYPE_UINT:  return "uint";
+        case YYJSON_TYPE_NUM  | YYJSON_SUBTYPE_SINT:  return "sint";
+        case YYJSON_TYPE_NUM  | YYJSON_SUBTYPE_REAL:  return "real";
+        default:                                      return "unknown";
+    }
+}
+
+yyjson_api_inline bool yyjson_get_bool(yyjson_val *val) {
+    return yyjson_is_bool(val) ? unsafe_yyjson_get_bool(val) : false;
+}
+
+yyjson_api_inline uint64_t yyjson_get_uint(yyjson_val *val) {
+    return yyjson_is_int(val) ? unsafe_yyjson_get_uint(val) : 0;
+}
+
+yyjson_api_inline int64_t yyjson_get_sint(yyjson_val *val) {
+    return yyjson_is_int(val) ? unsafe_yyjson_get_sint(val) : 0;
+}
+
+yyjson_api_inline int yyjson_get_int(yyjson_val *val) {
+    return yyjson_is_int(val) ? unsafe_yyjson_get_int(val) : 0;
+}
+
+yyjson_api_inline double yyjson_get_real(yyjson_val *val) {
+    return yyjson_is_real(val) ? unsafe_yyjson_get_real(val) : 0.0;
+}
+
+yyjson_api_inline const char *yyjson_get_str(yyjson_val *val) {
+    return yyjson_is_str(val) ? unsafe_yyjson_get_str(val) : NULL;
+}
+
+yyjson_api_inline size_t yyjson_get_len(yyjson_val *val) {
+    return yyjson_is_str(val) ? unsafe_yyjson_get_len(val) : 0;
+}
+
+yyjson_api_inline bool yyjson_equals_str(yyjson_val *val, const char *str) {
+    if (yyjson_likely(val && str)) {
+        return unsafe_yyjson_equals_str(val, str);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_equals_strn(yyjson_val *val, const char *str,
+                                          size_t len) {
+    if (yyjson_likely(val && str)) {
+        return unsafe_yyjson_equals_strn(val, str, len);
+    }
+    return false;
+}
+
+
+
+/*==============================================================================
+ * JSON Array API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline size_t yyjson_arr_size(yyjson_val *arr) {
+    return yyjson_is_arr(arr) ? unsafe_yyjson_get_len(arr) : 0;
+}
+
+yyjson_api_inline yyjson_val *yyjson_arr_get(yyjson_val *arr, size_t idx) {
+    if (yyjson_likely(yyjson_is_arr(arr))) {
+        if (yyjson_likely(unsafe_yyjson_get_len(arr) > idx)) {
+            yyjson_val *val = unsafe_yyjson_get_first(arr);
+            if (unsafe_yyjson_arr_is_flat(arr)) {
+                return val + idx;
+            } else {
+                while (idx-- > 0) val = unsafe_yyjson_get_next(val);
+                return val;
+            }
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_val *yyjson_arr_get_first(yyjson_val *arr) {
+    if (yyjson_likely(yyjson_is_arr(arr))) {
+        if (yyjson_likely(unsafe_yyjson_get_len(arr) > 0)) {
+            return unsafe_yyjson_get_first(arr);
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_val *yyjson_arr_get_last(yyjson_val *arr) {
+    if (yyjson_likely(yyjson_is_arr(arr))) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (yyjson_likely(len > 0)) {
+            yyjson_val *val = unsafe_yyjson_get_first(arr);
+            if (unsafe_yyjson_arr_is_flat(arr)) {
+                return val + (len - 1);
+            } else {
+                while (len-- > 1) val = unsafe_yyjson_get_next(val);
+                return val;
+            }
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * JSON Array Iterator API (Implementation)
+ *============================================================================*/
+
+struct yyjson_arr_iter {
+    size_t idx;
+    size_t max;
+    yyjson_val *cur;
+};
+
+yyjson_api_inline bool yyjson_arr_iter_init(yyjson_val *arr,
+                                            yyjson_arr_iter *iter) {
+    if (yyjson_likely(yyjson_is_arr(arr) && iter)) {
+        iter->idx = 0;
+        iter->max = unsafe_yyjson_get_len(arr);
+        iter->cur = unsafe_yyjson_get_first(arr);
+        return true;
+    }
+    if (iter) memset(iter, 0, sizeof(yyjson_arr_iter));
+    return false;
+}
+
+yyjson_api_inline bool yyjson_arr_iter_has_next(yyjson_arr_iter *iter) {
+    return iter ? iter->idx < iter->max : false;
+}
+
+yyjson_api_inline yyjson_val *yyjson_arr_iter_next(yyjson_arr_iter *iter) {
+    yyjson_val *val;
+    if (iter && iter->idx < iter->max) {
+        val = iter->cur;
+        iter->cur = unsafe_yyjson_get_next(val);
+        iter->idx++;
+        return val;
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * JSON Object API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline size_t yyjson_obj_size(yyjson_val *obj) {
+    return yyjson_is_obj(obj) ? unsafe_yyjson_get_len(obj) : 0;
+}
+
+yyjson_api_inline yyjson_val *yyjson_obj_get(yyjson_val *obj,
+                                             const char *key_str) {
+    return yyjson_obj_getn(obj, key_str, key_str ? strlen(key_str) : 0);
+}
+
+yyjson_api_inline yyjson_val *yyjson_obj_getn(yyjson_val *obj,
+                                              const char *key_str,
+                                              size_t key_len) {
+    uint64_t tag = (((uint64_t)key_len) << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+    if (yyjson_likely(yyjson_is_obj(obj) && key_str)) {
+        size_t len = unsafe_yyjson_get_len(obj);
+        yyjson_val *key = unsafe_yyjson_get_first(obj);
+        while (len-- > 0) {
+            if (key->tag == tag &&
+                memcmp(key->uni.ptr, key_str, key_len) == 0) {
+                return key + 1;
+            }
+            key = unsafe_yyjson_get_next(key + 1);
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * JSON Object Iterator API (Implementation)
+ *============================================================================*/
+
+struct yyjson_obj_iter {
+    size_t idx;
+    size_t max;
+    yyjson_val *cur;
+    yyjson_val *obj;
+};
+
+yyjson_api_inline bool yyjson_obj_iter_init(yyjson_val *obj,
+                                            yyjson_obj_iter *iter) {
+    if (yyjson_likely(yyjson_is_obj(obj) && iter)) {
+        iter->idx = 0;
+        iter->max = unsafe_yyjson_get_len(obj);
+        iter->cur = unsafe_yyjson_get_first(obj);
+        iter->obj = obj;
+        return true;
+    }
+    if (iter) {
+        iter->idx = 0;
+        iter->max = 0;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_obj_iter_has_next(yyjson_obj_iter *iter) {
+    return iter ? iter->idx < iter->max : false;
+}
+
+yyjson_api_inline yyjson_val *yyjson_obj_iter_next(yyjson_obj_iter *iter) {
+    if (iter && iter->idx < iter->max) {
+        yyjson_val *key = iter->cur;
+        iter->idx++;
+        iter->cur = unsafe_yyjson_get_next(key + 1);
+        return key;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_val *yyjson_obj_iter_get_val(yyjson_val *key) {
+    return key + 1;
+}
+
+yyjson_api_inline yyjson_val *yyjson_obj_iter_get(yyjson_obj_iter *iter,
+                                                  const char *key) {
+    return yyjson_obj_iter_getn(iter, key, key ? strlen(key) : 0);
+}
+
+yyjson_api_inline yyjson_val *yyjson_obj_iter_getn(yyjson_obj_iter *iter,
+                                                   const char *key,
+                                                   size_t key_len) {
+    if (iter && key) {
+        size_t idx = iter->idx;
+        size_t max = iter->max;
+        yyjson_val *cur = iter->cur;
+        if (yyjson_unlikely(idx == max)) {
+            idx = 0;
+            cur = unsafe_yyjson_get_first(iter->obj);
+        }
+        while (idx++ < max) {
+            yyjson_val *next = unsafe_yyjson_get_next(cur + 1);
+            if (unsafe_yyjson_get_len(cur) == key_len &&
+                memcmp(cur->uni.str, key, key_len) == 0) {
+                iter->idx = idx;
+                iter->cur = next;
+                return cur + 1;
+            }
+            cur = next;
+            if (idx == iter->max && iter->idx < iter->max) {
+                idx = 0;
+                max = iter->idx;
+                cur = unsafe_yyjson_get_first(iter->obj);
+            }
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Structure (Implementation)
+ *============================================================================*/
+
+/*
+ Mutable JSON value, 24 bytes.
+ The 'tag' and 'uni' field is same as immutable value.
+ The 'next' field links all elements inside the container to be a cycle.
+ */
+struct yyjson_mut_val {
+    uint64_t tag;
+    yyjson_val_uni uni;
+    yyjson_mut_val *next;
+};
+
+typedef struct yyjson_str_chunk {
+    struct yyjson_str_chunk *next;
+    /* flexible array member here */
+} yyjson_str_chunk;
+
+typedef struct yyjson_str_pool {
+    char *cur; /* cursor inside current chunk */
+    char *end; /* the end of current chunk */
+    size_t chunk_size; /* chunk size in bytes while creating new chunk */
+    size_t chunk_size_max; /* maximum chunk size in bytes */
+    yyjson_str_chunk *chunks; /* a linked list of chunks, nullable */
+} yyjson_str_pool;
+
+typedef struct yyjson_val_chunk {
+    struct yyjson_val_chunk *next;
+    /* flexible array member here */
+} yyjson_val_chunk;
+
+typedef struct yyjson_val_pool {
+    yyjson_mut_val *cur; /* cursor inside current chunk */
+    yyjson_mut_val *end; /* the end of current chunk */
+    size_t chunk_size; /* chunk size in bytes while creating new chunk */
+    size_t chunk_size_max; /* maximum chunk size in bytes */
+    yyjson_val_chunk *chunks; /* a linked list of chunks, nullable */
+} yyjson_val_pool;
+
+struct yyjson_mut_doc {
+    yyjson_mut_val *root; /* root value of the JSON document, nullable */
+    yyjson_alc alc; /* a valid allocator, nonnull */
+    yyjson_str_pool str_pool; /* string memory holder */
+    yyjson_val_pool val_pool; /* value memory holder */
+};
+
+/* Ensures the capacity to at least equal to the specified byte length. */
+yyjson_api bool unsafe_yyjson_str_pool_grow(yyjson_str_pool *pool,
+                                            yyjson_alc *alc, size_t len);
+
+/* Ensures the capacity to at least equal to the specified value count. */
+yyjson_api bool unsafe_yyjson_val_pool_grow(yyjson_val_pool *pool,
+                                            yyjson_alc *alc, size_t count);
+
+yyjson_api_inline char *unsafe_yyjson_mut_strncpy(yyjson_mut_doc *doc,
+                                                  const char *str, size_t len) {
+    char *mem;
+    yyjson_alc *alc = &doc->alc;
+    yyjson_str_pool *pool = &doc->str_pool;
+    
+    if (!str) return NULL;
+    if (yyjson_unlikely((size_t)(pool->end - pool->cur) <= len)) {
+        if (yyjson_unlikely(!unsafe_yyjson_str_pool_grow(pool, alc, len + 1))) {
+            return NULL;
+        }
+    }
+    
+    mem = pool->cur;
+    pool->cur = mem + len + 1;
+    memcpy((void *)mem, (const void *)str, len);
+    mem[len] = '\0';
+    return mem;
+}
+
+yyjson_api_inline yyjson_mut_val *unsafe_yyjson_mut_val(yyjson_mut_doc *doc,
+                                                        size_t count) {
+    yyjson_mut_val *val;
+    yyjson_alc *alc = &doc->alc;
+    yyjson_val_pool *pool = &doc->val_pool;
+    if (yyjson_unlikely((size_t)(pool->end - pool->cur) < count)) {
+        if (yyjson_unlikely(!unsafe_yyjson_val_pool_grow(pool, alc, count))) {
+            return NULL;
+        }
+    }
+    
+    val = pool->cur;
+    pool->cur += count;
+    return val;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Document API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_doc_get_root(yyjson_mut_doc *doc) {
+    return doc ? doc->root : NULL;
+}
+
+yyjson_api_inline void yyjson_mut_doc_set_root(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *root) {
+    if (doc) doc->root = root;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Value Type API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline bool yyjson_mut_is_null(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_null(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_true(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_true(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_false(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_false(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_bool(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_bool(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_uint(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_uint(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_sint(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_sint(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_int(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_int(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_real(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_real(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_num(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_num(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_str(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_str(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_arr(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_arr(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_obj(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_obj(val) : false;
+}
+
+yyjson_api_inline bool yyjson_mut_is_ctn(yyjson_mut_val *val) {
+    return val ? unsafe_yyjson_is_ctn(val) : false;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Value Content API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_type yyjson_mut_get_type(yyjson_mut_val *val) {
+    return yyjson_get_type((yyjson_val *)val);
+}
+
+yyjson_api_inline yyjson_subtype yyjson_mut_get_subtype(yyjson_mut_val *val) {
+    return yyjson_get_subtype((yyjson_val *)val);
+}
+
+yyjson_api_inline uint8_t yyjson_mut_get_tag(yyjson_mut_val *val) {
+    return yyjson_get_tag((yyjson_val *)val);
+}
+
+yyjson_api_inline const char *yyjson_mut_get_type_desc(yyjson_mut_val *val) {
+    return yyjson_get_type_desc((yyjson_val *)val);
+}
+
+yyjson_api_inline bool yyjson_mut_get_bool(yyjson_mut_val *val) {
+    return yyjson_get_bool((yyjson_val *)val);
+}
+
+yyjson_api_inline uint64_t yyjson_mut_get_uint(yyjson_mut_val *val) {
+    return yyjson_get_uint((yyjson_val *)val);
+}
+
+yyjson_api_inline int64_t yyjson_mut_get_sint(yyjson_mut_val *val) {
+    return yyjson_get_sint((yyjson_val *)val);
+}
+
+yyjson_api_inline int yyjson_mut_get_int(yyjson_mut_val *val) {
+    return yyjson_get_int((yyjson_val *)val);
+}
+
+yyjson_api_inline double yyjson_mut_get_real(yyjson_mut_val *val) {
+    return yyjson_get_real((yyjson_val *)val);
+}
+
+yyjson_api_inline const char *yyjson_mut_get_str(yyjson_mut_val *val) {
+    return yyjson_get_str((yyjson_val *)val);
+}
+
+yyjson_api_inline size_t yyjson_mut_get_len(yyjson_mut_val *val) {
+    return yyjson_get_len((yyjson_val *)val);
+}
+
+yyjson_api_inline bool yyjson_mut_equals_str(yyjson_mut_val *val,
+                                             const char *str) {
+    return yyjson_equals_str((yyjson_val *)val, str);
+}
+
+yyjson_api_inline bool yyjson_mut_equals_strn(yyjson_mut_val *val,
+                                              const char *str, size_t len) {
+    return yyjson_equals_strn((yyjson_val *)val, str, len);
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Value Creation API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_null(yyjson_mut_doc *doc) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_NULL | YYJSON_SUBTYPE_NONE;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_true(yyjson_mut_doc *doc) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_false(yyjson_mut_doc *doc) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_bool(yyjson_mut_doc *doc,
+                                                  bool _val) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_BOOL | (uint8_t)((uint8_t)_val << 3);
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_uint(yyjson_mut_doc *doc,
+                                                  uint64_t num) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+            val->uni.u64 = num;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_sint(yyjson_mut_doc *doc,
+                                                  int64_t num) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+            val->uni.i64 = num;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_int(yyjson_mut_doc *doc,
+                                                 int64_t num) {
+    return yyjson_mut_sint(doc, num);
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_real(yyjson_mut_doc *doc,
+                                                  double num) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+            val->uni.f64 = num;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_str(yyjson_mut_doc *doc,
+                                                 const char *str) {
+    if (yyjson_likely(str)) return yyjson_mut_strn(doc, str, strlen(str));
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_strn(yyjson_mut_doc *doc,
+                                                  const char *str,
+                                                  size_t len) {
+    if (yyjson_likely(doc && str)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = ((uint64_t)len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+            val->uni.str = str;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_strcpy(yyjson_mut_doc *doc,
+                                                    const char *str) {
+    if (yyjson_likely(str)) return yyjson_mut_strncpy(doc, str, strlen(str));
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_strncpy(yyjson_mut_doc *doc,
+                                                     const char *str,
+                                                     size_t len) {
+    if (yyjson_likely(doc && str)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        char *new_str = unsafe_yyjson_mut_strncpy(doc, str, len);
+        if (yyjson_likely(val && new_str)) {
+            val->tag = ((uint64_t)len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+            val->uni.str = new_str;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Array API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline size_t yyjson_mut_arr_size(yyjson_mut_val *arr) {
+    return yyjson_mut_is_arr(arr) ? unsafe_yyjson_get_len(arr) : 0;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_get(yyjson_mut_val *arr,
+                                                     size_t idx) {
+    if (yyjson_likely(idx < yyjson_mut_arr_size(arr))) {
+        yyjson_mut_val *val = (yyjson_mut_val *)arr->uni.ptr;
+        while (idx-- > 0) val = val->next;
+        return val->next;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_get_first(
+    yyjson_mut_val *arr) {
+    if (yyjson_likely(yyjson_mut_arr_size(arr) > 0)) {
+        return ((yyjson_mut_val *)arr->uni.ptr)->next;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_get_last(yyjson_mut_val *arr) {
+    if (yyjson_likely(yyjson_mut_arr_size(arr) > 0)) {
+        return ((yyjson_mut_val *)arr->uni.ptr);
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Iterator API (Implementation)
+ *============================================================================*/
+
+struct yyjson_mut_arr_iter {
+    size_t idx;
+    size_t max;
+    yyjson_mut_val *cur;
+    yyjson_mut_val *pre;
+    yyjson_mut_val *arr;
+};
+
+yyjson_api_inline bool yyjson_mut_arr_iter_init(yyjson_mut_val *arr,
+                                                yyjson_mut_arr_iter *iter) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr) && iter)) {
+        iter->idx = 0;
+        iter->max = unsafe_yyjson_get_len(arr);
+        iter->cur = iter->max ? (yyjson_mut_val *)arr->uni.ptr : NULL;
+        iter->pre = NULL;
+        iter->arr = arr;
+        return true;
+    }
+    if (iter) memset(iter, 0, sizeof(yyjson_mut_arr_iter));
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_iter_has_next(yyjson_mut_arr_iter *iter) {
+    return iter ? iter->idx < iter->max : false;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_iter_next(
+    yyjson_mut_arr_iter *iter) {
+    if (iter && iter->idx < iter->max) {
+        yyjson_mut_val *val = iter->cur;
+        iter->pre = val;
+        iter->cur = val->next;
+        iter->idx++;
+        return iter->cur;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_iter_remove(
+    yyjson_mut_arr_iter *iter) {
+    if (yyjson_likely(iter && 0 < iter->idx && iter->idx <= iter->max)) {
+        yyjson_mut_val *prev = iter->pre;
+        yyjson_mut_val *cur = iter->cur;
+        yyjson_mut_val *next = cur->next;
+        if (yyjson_unlikely(iter->idx == iter->max)) iter->arr->uni.ptr = prev;
+        iter->idx--;
+        iter->max--;
+        unsafe_yyjson_set_len(iter->arr, iter->max);
+        prev->next = next;
+        iter->cur = next;
+        return cur;
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Creation API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr(yyjson_mut_doc *doc) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_ARR | YYJSON_SUBTYPE_NONE;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+#define yyjson_mut_arr_with_func(func) \
+    if (yyjson_likely(doc && ((0 < count && count < \
+        (~(size_t)0) / sizeof(yyjson_mut_val) && vals) || count == 0))) { \
+        yyjson_mut_val *arr = unsafe_yyjson_mut_val(doc, 1 + count); \
+        if (yyjson_likely(arr)) { \
+            arr->tag = ((uint64_t)count << YYJSON_TAG_BIT) | YYJSON_TYPE_ARR; \
+            if (count > 0) { \
+                size_t i; \
+                for (i = 0; i < count; i++) { \
+                    yyjson_mut_val *val = arr + i + 1; \
+                    func \
+                    val->next = val + 1; \
+                } \
+                arr[count].next = arr + 1; \
+                arr->uni.ptr = arr + count; \
+            } \
+            return arr; \
+        } \
+    } \
+    return NULL
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_bool(
+    yyjson_mut_doc *doc, const bool *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_BOOL | (uint8_t)((uint8_t)vals[i] << 3);
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint(
+    yyjson_mut_doc *doc, const int64_t *vals, size_t count) {
+    return yyjson_mut_arr_with_sint64(doc, vals, count);
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint(
+    yyjson_mut_doc *doc, const uint64_t *vals, size_t count) {
+    return yyjson_mut_arr_with_uint64(doc, vals, count);
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_real(
+    yyjson_mut_doc *doc, const double *vals, size_t count) {
+    return yyjson_mut_arr_with_double(doc, vals, count);
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint8(
+    yyjson_mut_doc *doc, const int8_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+        val->uni.i64 = (int64_t)vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint16(
+    yyjson_mut_doc *doc, const int16_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+        val->uni.i64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint32(
+    yyjson_mut_doc *doc, const int32_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+        val->uni.i64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_sint64(
+    yyjson_mut_doc *doc, const int64_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+        val->uni.i64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint8(
+    yyjson_mut_doc *doc, const uint8_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+        val->uni.u64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint16(
+    yyjson_mut_doc *doc, const uint16_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+        val->uni.u64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint32(
+    yyjson_mut_doc *doc, const uint32_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+        val->uni.u64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_uint64(
+    yyjson_mut_doc *doc, const uint64_t *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+        val->uni.u64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_float(
+    yyjson_mut_doc *doc, const float *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+        val->uni.f64 = (double)vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_double(
+    yyjson_mut_doc *doc, const double *vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+        val->uni.f64 = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_str(
+    yyjson_mut_doc *doc, const char **vals, size_t count) {
+    yyjson_mut_arr_with_func({
+        uint64_t len = (uint64_t)strlen(vals[i]);
+        val->tag = (len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_strn(
+    yyjson_mut_doc *doc, const char **vals, const size_t *lens, size_t count) {
+    if (yyjson_unlikely(count > 0 && !lens)) return NULL;
+    yyjson_mut_arr_with_func({
+        val->tag = ((uint64_t)lens[i] << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = vals[i];
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_strcpy(
+    yyjson_mut_doc *doc, const char **vals, size_t count) {
+    size_t len;
+    const char *str;
+    yyjson_mut_arr_with_func({
+        str = vals[i];
+        if (!str) return NULL;
+        len = strlen(str);
+        val->tag = ((uint64_t)len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = unsafe_yyjson_mut_strncpy(doc, str, len);
+        if (!val->uni.str) return NULL;
+    });
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_with_strncpy(
+    yyjson_mut_doc *doc, const char **vals, const size_t *lens, size_t count) {
+    size_t len;
+    const char *str;
+    if (yyjson_unlikely(count > 0 && !lens)) return NULL;
+    yyjson_mut_arr_with_func({
+        str = vals[i];
+        len = lens[i];
+        val->tag = ((uint64_t)len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = unsafe_yyjson_mut_strncpy(doc, str, len);
+        if (!val->uni.str) return NULL;
+    });
+}
+
+#undef yyjson_mut_arr_with_func
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Modification API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline bool yyjson_mut_arr_insert(yyjson_mut_val *arr,
+                                             yyjson_mut_val *val, size_t idx) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr) && val)) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (yyjson_likely(idx <= len)) {
+            unsafe_yyjson_set_len(arr, len + 1);
+            if (len == 0) {
+                val->next = val;
+                arr->uni.ptr = val;
+            } else {
+                yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+                yyjson_mut_val *next = prev->next;
+                if (idx == len) {
+                    prev->next = val;
+                    val->next = next;
+                    arr->uni.ptr = val;
+                } else {
+                    while (idx-- > 0) {
+                        prev = next;
+                        next = next->next;
+                    }
+                    prev->next = val;
+                    val->next = next;
+                }
+            }
+            return true;
+        }
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_append(yyjson_mut_val *arr,
+                                             yyjson_mut_val *val) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr) && val)) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        unsafe_yyjson_set_len(arr, len + 1);
+        if (len == 0) {
+            val->next = val;
+        } else {
+            yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+            yyjson_mut_val *next = prev->next;
+            prev->next = val;
+            val->next = next;
+        }
+        arr->uni.ptr = val;
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_prepend(yyjson_mut_val *arr,
+                                              yyjson_mut_val *val) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr) && val)) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        unsafe_yyjson_set_len(arr, len + 1);
+        if (len == 0) {
+            val->next = val;
+            arr->uni.ptr = val;
+        } else {
+            yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+            yyjson_mut_val *next = prev->next;
+            prev->next = val;
+            val->next = next;
+        }
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_replace(yyjson_mut_val *arr,
+                                                         size_t idx,
+                                                         yyjson_mut_val *val) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr) && val)) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (yyjson_likely(idx < len)) {
+            if (yyjson_likely(len > 1)) {
+                yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+                yyjson_mut_val *next = prev->next;
+                while (idx-- > 0) {
+                    prev = next;
+                    next = next->next;
+                }
+                prev->next = val;
+                val->next = next->next;
+                if ((void *)next == arr->uni.ptr) arr->uni.ptr = val;
+                return next;
+            } else {
+                yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+                val->next = val;
+                arr->uni.ptr = val;
+                return prev;
+            };
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_remove(yyjson_mut_val *arr,
+                                                        size_t idx) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr))) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (yyjson_likely(idx < len)) {
+            unsafe_yyjson_set_len(arr, len - 1);
+            if (yyjson_likely(len > 1)) {
+                yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+                yyjson_mut_val *next = prev->next;
+                while (idx-- > 0) {
+                    prev = next;
+                    next = next->next;
+                }
+                prev->next = next->next;
+                if ((void *)next == arr->uni.ptr) arr->uni.ptr = prev;
+                return next;
+            } else {
+                return ((yyjson_mut_val *)arr->uni.ptr);
+            }
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_remove_first(
+    yyjson_mut_val *arr) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr))) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (len > 1) {
+            yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+            yyjson_mut_val *next = prev->next;
+            prev->next = next->next;
+            unsafe_yyjson_set_len(arr, len - 1);
+            return next;
+        } else if (len == 1) {
+            yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+            unsafe_yyjson_set_len(arr, 0);
+            return prev;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_remove_last(
+    yyjson_mut_val *arr) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr))) {
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (yyjson_likely(len > 1)) {
+            yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+            yyjson_mut_val *next = prev->next;
+            unsafe_yyjson_set_len(arr, len - 1);
+            while (--len > 0) prev = prev->next;
+            prev->next = next;
+            next = (yyjson_mut_val *)arr->uni.ptr;
+            arr->uni.ptr = prev;
+            return next;
+        } else if (len == 1) {
+            yyjson_mut_val *prev = ((yyjson_mut_val *)arr->uni.ptr);
+            unsafe_yyjson_set_len(arr, 0);
+            return prev;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_remove_range(yyjson_mut_val *arr,
+                                                   size_t _idx, size_t _len) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *prev, *next;
+        bool tail_removed;
+        size_t len = unsafe_yyjson_get_len(arr);
+        if (yyjson_unlikely(_idx + _len > len)) return false;
+        if (yyjson_unlikely(_len == 0)) return true;
+        unsafe_yyjson_set_len(arr, len - _len);
+        if (yyjson_unlikely(len == _len)) return true;
+        tail_removed = (_idx + _len == len);
+        prev = ((yyjson_mut_val *)arr->uni.ptr);
+        while (_idx-- > 0) prev = prev->next;
+        next = prev->next;
+        while (_len-- > 0) next = next->next;
+        prev->next = next;
+        if (yyjson_unlikely(tail_removed)) arr->uni.ptr = prev;
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_clear(yyjson_mut_val *arr) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr))) {
+        unsafe_yyjson_set_len(arr, 0);
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_rotate(yyjson_mut_val *arr,
+                                             size_t idx) {
+    if (yyjson_likely(yyjson_mut_is_arr(arr) &&
+                      unsafe_yyjson_get_len(arr) > idx)) {
+        yyjson_mut_val *val = (yyjson_mut_val *)arr->uni.ptr;
+        while (idx-- > 0) val = val->next;
+        arr->uni.ptr = (void *)val;
+        return true;
+    }
+    return false;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Array Modification Convenience API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline bool yyjson_mut_arr_add_val(yyjson_mut_val *arr,
+                                              yyjson_mut_val *val) {
+    return yyjson_mut_arr_append(arr, val);
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_null(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_null(doc);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_true(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_true(doc);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_false(yyjson_mut_doc *doc,
+                                                yyjson_mut_val *arr) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_false(doc);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_bool(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               bool _val) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_bool(doc, _val);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_uint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               uint64_t num) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_uint(doc, num);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_sint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               int64_t num) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_sint(doc, num);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_int(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *arr,
+                                              int64_t num) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_sint(doc, num);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_real(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               double num) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_real(doc, num);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_str(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *arr,
+                                              const char *str) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_str(doc, str);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_strn(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *arr,
+                                               const char *str, size_t len) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_strn(doc, str, len);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_strcpy(yyjson_mut_doc *doc,
+                                                 yyjson_mut_val *arr,
+                                                 const char *str) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_strcpy(doc, str);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_arr_add_strncpy(yyjson_mut_doc *doc,
+                                                  yyjson_mut_val *arr,
+                                                  const char *str, size_t len) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_strncpy(doc, str, len);
+        return yyjson_mut_arr_append(arr, val);
+    }
+    return false;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_add_arr(yyjson_mut_doc *doc,
+                                                         yyjson_mut_val *arr) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_arr(doc);
+        return yyjson_mut_arr_append(arr, val) ? val : NULL;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_arr_add_obj(yyjson_mut_doc *doc,
+                                                         yyjson_mut_val *arr) {
+    if (yyjson_likely(doc && yyjson_mut_is_arr(arr))) {
+        yyjson_mut_val *val = yyjson_mut_obj(doc);
+        return yyjson_mut_arr_append(arr, val) ? val : NULL;
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Object API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline size_t yyjson_mut_obj_size(yyjson_mut_val *obj) {
+    return yyjson_mut_is_obj(obj) ? unsafe_yyjson_get_len(obj) : 0;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_get(yyjson_mut_val *obj,
+                                                     const char *key_str) {
+    return yyjson_mut_obj_getn(obj, key_str, key_str ? strlen(key_str) : 0);
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_getn(yyjson_mut_val *obj,
+                                                      const char *key_str,
+                                                      size_t key_len) {
+    uint64_t tag = (((uint64_t)key_len) << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+    size_t len = yyjson_mut_obj_size(obj);
+    if (yyjson_likely(len && key_str)) {
+        yyjson_mut_val *key = ((yyjson_mut_val *)obj->uni.ptr)->next->next;
+        while (len-- > 0) {
+            if (key->tag == tag &&
+                memcmp(key->uni.ptr, key_str, key_len) == 0) {
+                return key->next;
+            }
+            key = key->next->next;
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Iterator API (Implementation)
+ *============================================================================*/
+
+struct yyjson_mut_obj_iter {
+    size_t idx;
+    size_t max;
+    yyjson_mut_val *cur;
+    yyjson_mut_val *pre;
+    yyjson_mut_val *obj;
+};
+
+yyjson_api_inline bool yyjson_mut_obj_iter_init(yyjson_mut_val *obj,
+                                                yyjson_mut_obj_iter *iter) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) && iter)) {
+        iter->idx = 0;
+        iter->max = unsafe_yyjson_get_len(obj);
+        iter->cur = iter->max ? (yyjson_mut_val *)obj->uni.ptr : NULL;
+        iter->pre = NULL;
+        iter->obj = obj;
+        return true;
+    }
+    if (iter) memset(iter, 0, sizeof(yyjson_mut_obj_iter));
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_iter_has_next(yyjson_mut_obj_iter *iter) {
+    return iter ? iter->idx < iter->max : false;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_next(
+    yyjson_mut_obj_iter *iter) {
+    if (iter && iter->idx < iter->max) {
+        yyjson_mut_val *key = iter->cur;
+        iter->pre = key;
+        iter->cur = key->next->next;
+        iter->idx++;
+        return iter->cur;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_get_val(
+    yyjson_mut_val *key) {
+    return key->next;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_remove(
+    yyjson_mut_obj_iter *iter) {
+    if (yyjson_likely(iter && 0 < iter->idx && iter->idx <= iter->max)) {
+        yyjson_mut_val *prev = iter->pre;
+        yyjson_mut_val *cur = iter->cur;
+        yyjson_mut_val *next = cur->next->next;
+        if (yyjson_unlikely(iter->idx == iter->max)) iter->obj->uni.ptr = prev;
+        iter->idx--;
+        iter->max--;
+        unsafe_yyjson_set_len(iter->obj, iter->max);
+        prev->next->next = next;
+        iter->cur = next;
+        return cur;
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_get(
+    yyjson_mut_obj_iter *iter, const char *key) {
+    return yyjson_mut_obj_iter_getn(iter, key, key ? strlen(key) : 0);
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_iter_getn(
+    yyjson_mut_obj_iter *iter, const char *key, size_t key_len) {
+    if (iter && key) {
+        size_t idx = 0;
+        size_t max = iter->max;
+        yyjson_mut_val *pre, *cur = iter->cur;
+        while (idx++ < max) {
+            pre = cur;
+            cur = cur->next->next;
+            if (unsafe_yyjson_get_len(cur) == key_len &&
+                memcmp(cur->uni.str, key, key_len) == 0) {
+                iter->idx += idx;
+                if (iter->idx > max) iter->idx -= max + 1;
+                iter->pre = pre;
+                iter->cur = cur;
+                return cur->next;
+            }
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Creation API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj(yyjson_mut_doc *doc) {
+    if (yyjson_likely(doc)) {
+        yyjson_mut_val *val = unsafe_yyjson_mut_val(doc, 1);
+        if (yyjson_likely(val)) {
+            val->tag = YYJSON_TYPE_OBJ | YYJSON_SUBTYPE_NONE;
+            return val;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_with_str(yyjson_mut_doc *doc,
+                                                          const char **keys,
+                                                          const char **vals,
+                                                          size_t count) {
+    if (yyjson_likely(doc && ((count > 0 && keys && vals) || (count == 0)))) {
+        yyjson_mut_val *obj = unsafe_yyjson_mut_val(doc, 1 + count * 2);
+        if (yyjson_likely(obj)) {
+            obj->tag = ((uint64_t)count << YYJSON_TAG_BIT) | YYJSON_TYPE_OBJ;
+            if (count > 0) {
+                size_t i;
+                for (i = 0; i < count; i++) {
+                    yyjson_mut_val *key = obj + (i * 2 + 1);
+                    yyjson_mut_val *val = obj + (i * 2 + 2);
+                    uint64_t key_len = (uint64_t)strlen(keys[i]);
+                    uint64_t val_len = (uint64_t)strlen(vals[i]);
+                    key->tag = (key_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+                    val->tag = (val_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+                    key->uni.str = keys[i];
+                    val->uni.str = vals[i];
+                    key->next = val;
+                    val->next = val + 1;
+                }
+                obj[count * 2].next = obj + 1;
+                obj->uni.ptr = obj + (count * 2 - 1);
+            }
+            return obj;
+        }
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_obj_with_kv(yyjson_mut_doc *doc,
+                                                         const char **pairs,
+                                                         size_t count) {
+    if (yyjson_likely(doc && ((count > 0 && pairs) || (count == 0)))) {
+        yyjson_mut_val *obj = unsafe_yyjson_mut_val(doc, 1 + count * 2);
+        if (yyjson_likely(obj)) {
+            obj->tag = ((uint64_t)count << YYJSON_TAG_BIT) | YYJSON_TYPE_OBJ;
+            if (count > 0) {
+                size_t i;
+                for (i = 0; i < count; i++) {
+                    yyjson_mut_val *key = obj + (i * 2 + 1);
+                    yyjson_mut_val *val = obj + (i * 2 + 2);
+                    const char *key_str = pairs[i * 2 + 0];
+                    const char *val_str = pairs[i * 2 + 1];
+                    uint64_t key_len = (uint64_t)strlen(key_str);
+                    uint64_t val_len = (uint64_t)strlen(val_str);
+                    key->tag = (key_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+                    val->tag = (val_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+                    key->uni.str = key_str;
+                    val->uni.str = val_str;
+                    key->next = val;
+                    val->next = val + 1;
+                }
+                obj[count * 2].next = obj + 1;
+                obj->uni.ptr = obj + (count * 2 - 1);
+            }
+            return obj;
+        }
+    }
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Modification API (Implementation)
+ *============================================================================*/
+
+yyjson_api_inline void unsafe_yyjson_mut_obj_add(yyjson_mut_val *obj,
+                                                 yyjson_mut_val *key,
+                                                 yyjson_mut_val *val,
+                                                 size_t len) {
+    if (yyjson_likely(len)) {
+        yyjson_mut_val *prev_val = ((yyjson_mut_val *)obj->uni.ptr)->next;
+        yyjson_mut_val *next_key = prev_val->next;
+        prev_val->next = key;
+        val->next = next_key;
+    } else {
+        val->next = key;
+    }
+    key->next = val;
+    obj->uni.ptr = (void *)key;
+    unsafe_yyjson_set_len(obj, len + 1);
+}
+
+yyjson_api_inline void unsafe_yyjson_mut_obj_remove(yyjson_mut_val *obj,
+                                                    const char *key,
+                                                    size_t key_len,
+                                                    uint64_t key_tag) {
+    size_t obj_len = unsafe_yyjson_get_len(obj);
+    if (obj_len) {
+        yyjson_mut_val *pre_key = (yyjson_mut_val *)obj->uni.ptr;
+        yyjson_mut_val *cur_key = pre_key->next->next;
+        size_t i;
+        for (i = 0; i < obj_len; i++) {
+            if (key_tag == cur_key->tag &&
+                memcmp(key, cur_key->uni.ptr, key_len) == 0) {
+                cur_key = cur_key->next->next;
+                pre_key->next->next = cur_key;
+                if (i + 1 == obj_len) obj->uni.ptr = pre_key;
+                i--;
+                obj_len--;
+            } else {
+                pre_key = cur_key;
+                cur_key = cur_key->next->next;
+            }
+        }
+        unsafe_yyjson_set_len(obj, obj_len);
+    }
+}
+
+yyjson_api_inline bool unsafe_yyjson_mut_obj_replace(yyjson_mut_val *obj,
+                                                     yyjson_mut_val *key,
+                                                     yyjson_mut_val *val) {
+    size_t key_len = unsafe_yyjson_get_len(key);
+    size_t obj_len = unsafe_yyjson_get_len(obj);
+    if (obj_len) {
+        yyjson_mut_val *pre_key = (yyjson_mut_val *)obj->uni.ptr;
+        yyjson_mut_val *cur_key = pre_key->next->next;
+        size_t i;
+        for (i = 0; i < obj_len; i++) {
+            if (key->tag == cur_key->tag &&
+            memcmp(key->uni.str, cur_key->uni.ptr, key_len) == 0) {
+                size_t cpy_len = sizeof(*key) - sizeof(key->next);
+                yyjson_mut_val tmp;
+                memcpy(&tmp, cur_key, cpy_len);
+                memcpy(cur_key, key, cpy_len);
+                memcpy(key, &tmp, cpy_len);
+
+                memcpy(&tmp, cur_key->next, cpy_len);
+                memcpy(cur_key->next, val, cpy_len);
+                memcpy(val, &tmp, cpy_len);
+                return true;
+            } else {
+                pre_key = cur_key;
+                cur_key = cur_key->next->next;
+            }
+        }
+    }
+    return false;
+}
+
+yyjson_api_inline void unsafe_yyjson_mut_obj_rotate(yyjson_mut_val *obj,
+                                                    size_t idx) {
+    yyjson_mut_val *key = (yyjson_mut_val *)obj->uni.ptr;
+    while (idx-- > 0) key = key->next->next;
+    obj->uni.ptr = (void *)key;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add(yyjson_mut_val *obj,
+                                          yyjson_mut_val *key,
+                                          yyjson_mut_val *val) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) &&
+                      yyjson_mut_is_str(key) && val)) {
+        unsafe_yyjson_mut_obj_add(obj, key, val, unsafe_yyjson_get_len(obj));
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_put(yyjson_mut_val *obj,
+                                          yyjson_mut_val *key,
+                                          yyjson_mut_val *val) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) &&
+                      yyjson_mut_is_str(key))) {
+        unsafe_yyjson_mut_obj_remove(obj, key->uni.str,
+                                     unsafe_yyjson_get_len(key), key->tag);
+        if (yyjson_likely(val)) {
+            unsafe_yyjson_mut_obj_add(obj, key, val,
+                                      unsafe_yyjson_get_len(obj));
+        }
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_insert(yyjson_mut_val *obj,
+                                             yyjson_mut_val *key,
+                                             yyjson_mut_val *val,
+                                             size_t idx) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) &&
+                      yyjson_mut_is_str(key) && val)) {
+        size_t len = unsafe_yyjson_get_len(obj);
+        if (yyjson_likely(len >= idx)) {
+            if (len > idx) {
+                void *ptr = obj->uni.ptr;
+                unsafe_yyjson_mut_obj_rotate(obj, idx);
+                unsafe_yyjson_mut_obj_add(obj, key, val, len);
+                obj->uni.ptr = ptr;
+            } else {
+                unsafe_yyjson_mut_obj_add(obj, key, val, len);
+            }
+            return true;
+        }
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_remove(yyjson_mut_val *obj,
+                                             yyjson_mut_val *key) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) && yyjson_mut_is_str(key))) {
+        unsafe_yyjson_mut_obj_remove(obj, key->uni.str,
+                                     unsafe_yyjson_get_len(key), key->tag);
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_clear(yyjson_mut_val *obj) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj))) {
+        unsafe_yyjson_set_len(obj, 0);
+        return true;
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_replace(yyjson_mut_val *obj,
+                                              yyjson_mut_val *key,
+                                              yyjson_mut_val *val) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) &&
+    yyjson_mut_is_str(key) && val)) {
+        return unsafe_yyjson_mut_obj_replace(obj, key, val);
+    }
+    return false;
+}
+
+yyjson_api_inline bool yyjson_mut_obj_rotate(yyjson_mut_val *obj,
+                                             size_t idx) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) &&
+                      unsafe_yyjson_get_len(obj) > idx)) {
+        unsafe_yyjson_mut_obj_rotate(obj, idx);
+        return true;
+    }
+    return false;
+}
+
+
+
+/*==============================================================================
+ * Mutable JSON Object Modification Convenience API (Implementation)
+ *============================================================================*/
+
+#define yyjson_mut_obj_add_func(func) \
+    if (yyjson_likely(doc && yyjson_mut_is_obj(obj) && _key)) { \
+        yyjson_mut_val *key = unsafe_yyjson_mut_val(doc, 2); \
+        if (yyjson_likely(key)) { \
+            size_t len = unsafe_yyjson_get_len(obj); \
+            yyjson_mut_val *val = key + 1; \
+            key->tag = YYJSON_TYPE_STR | YYJSON_SUBTYPE_NONE; \
+            key->tag |= (uint64_t)strlen(_key) << YYJSON_TAG_BIT; \
+            key->uni.str = _key; \
+            func \
+            unsafe_yyjson_mut_obj_add(obj, key, val, len); \
+            return true; \
+        } \
+    } \
+    return false
+
+yyjson_api_inline bool yyjson_mut_obj_add_null(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_NULL | YYJSON_SUBTYPE_NONE;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_true(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_false(yyjson_mut_doc *doc,
+                                                yyjson_mut_val *obj,
+                                                const char *_key) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_bool(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key,
+                                               bool _val) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_BOOL | (uint8_t)((uint8_t)(_val) << 3);
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_uint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key,
+                                               uint64_t _val) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT;
+        val->uni.u64 = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_sint(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key,
+                                               int64_t _val) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+        val->uni.i64 = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_int(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *obj,
+                                              const char *_key,
+                                              int64_t _val) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT;
+        val->uni.i64 = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_real(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key,
+                                               double _val) {
+    yyjson_mut_obj_add_func({
+        val->tag = YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL;
+        val->uni.f64 = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_str(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *obj,
+                                              const char *_key,
+                                              const char *_val) {
+    if (yyjson_unlikely(!_val)) return false;
+    yyjson_mut_obj_add_func({
+        val->tag = ((uint64_t)strlen(_val) << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_strn(yyjson_mut_doc *doc,
+                                               yyjson_mut_val *obj,
+                                               const char *_key,
+                                               const char *_val,
+                                               size_t _len) {
+    if (yyjson_unlikely(!_val)) return false;
+    yyjson_mut_obj_add_func({
+        val->tag = ((uint64_t)_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+        val->uni.str = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_strcpy(yyjson_mut_doc *doc,
+                                                 yyjson_mut_val *obj,
+                                                 const char *_key,
+                                                 const char *_val) {
+    if (yyjson_unlikely(!_val)) return false;
+    yyjson_mut_obj_add_func({
+        size_t _len = strlen(_val);
+        val->uni.str = unsafe_yyjson_mut_strncpy(doc, _val, _len);
+        if (yyjson_unlikely(!val->uni.str)) return false;
+        val->tag = ((uint64_t)_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_strncpy(yyjson_mut_doc *doc,
+                                                  yyjson_mut_val *obj,
+                                                  const char *_key,
+                                                  const char *_val,
+                                                  size_t _len) {
+    if (yyjson_unlikely(!_val)) return false;
+    yyjson_mut_obj_add_func({
+        val->uni.str = unsafe_yyjson_mut_strncpy(doc, _val, _len);
+        if (yyjson_unlikely(!val->uni.str)) return false;
+        val->tag = ((uint64_t)_len << YYJSON_TAG_BIT) | YYJSON_TYPE_STR;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_add_val(yyjson_mut_doc *doc,
+                                              yyjson_mut_val *obj,
+                                              const char *_key,
+                                              yyjson_mut_val *_val) {
+    if (yyjson_unlikely(!_val)) return false;
+    yyjson_mut_obj_add_func({
+        val = _val;
+    });
+}
+
+yyjson_api_inline bool yyjson_mut_obj_remove_str(yyjson_mut_val *obj,
+                                                 const char *key) {
+    return yyjson_mut_obj_remove_strn(obj, key, key ? strlen(key) : 0);
+}
+
+yyjson_api_inline bool yyjson_mut_obj_remove_strn(yyjson_mut_val *obj,
+                                                  const char *_key,
+                                                  size_t _len) {
+    if (yyjson_likely(yyjson_mut_is_obj(obj) && _key)) {
+        yyjson_mut_val *key;
+        yyjson_mut_obj_iter iter;
+        yyjson_mut_obj_iter_init(obj, &iter);
+        while ((key = yyjson_mut_obj_iter_next(&iter)) != NULL) {
+            if (unsafe_yyjson_get_len(key) == _len &&
+                memcmp(key->uni.str, _key, _len) == 0) {
+                yyjson_mut_obj_iter_remove(&iter);
+            }
+        }
+        return true;
+    }
+    return false;
+}
+
+
+
+/*==============================================================================
+ * JSON Pointer API (Implementation)
+ *============================================================================*/
+
+yyjson_api yyjson_val *unsafe_yyjson_get_pointer(yyjson_val *val,
+                                                 const char *ptr,
+                                                 size_t len);
+
+yyjson_api yyjson_mut_val *unsafe_yyjson_mut_get_pointer(yyjson_mut_val *val,
+                                                         const char *ptr,
+                                                         size_t len);
+
+yyjson_api_inline yyjson_val *yyjson_get_pointer(yyjson_val *val,
+                                                 const char *ptr) {
+    if (val && ptr) {
+        if (*ptr == '\0') return val;
+        if (*ptr != '/') return NULL;
+        return unsafe_yyjson_get_pointer(val, ptr, strlen(ptr));
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_val *yyjson_doc_get_pointer(yyjson_doc *doc,
+                                                     const char *ptr) {
+    if (doc) return yyjson_get_pointer(doc->root, ptr);
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_get_pointer(yyjson_mut_val *val,
+                                                         const char *ptr) {
+    if (val && ptr) {
+        if (*ptr == '\0') return val;
+        if (*ptr != '/') return NULL;
+        return unsafe_yyjson_mut_get_pointer(val, ptr, strlen(ptr));
+    }
+    return NULL;
+}
+
+yyjson_api_inline yyjson_mut_val *yyjson_mut_doc_get_pointer(
+    yyjson_mut_doc *doc, const char *ptr) {
+    if (doc) return yyjson_mut_get_pointer(doc->root, ptr);
+    return NULL;
+}
+
+
+
+/*==============================================================================
+ * Compiler Hint End
+ *============================================================================*/
+
+#if defined(__clang__)
+#   pragma clang diagnostic pop
+#elif defined(__GNUC__)
+#   if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6)
+#   pragma GCC diagnostic pop
+#   endif
+#elif defined(_MSC_VER)
+#   pragma warning(pop)
+#endif /* warning suppress end */
+
+#ifdef __cplusplus
+}
+#endif /* extern "C" end */
+
+#endif /* YYJSON_H */
diff --git a/src/include/optimizer/cost.h b/src/include/optimizer/cost.h
index b3d0b4f..005f38f 100644
--- a/src/include/optimizer/cost.h
+++ b/src/include/optimizer/cost.h
@@ -66,6 +66,10 @@ extern PGDLLIMPORT bool enable_parallel_hash;
 extern PGDLLIMPORT bool enable_partition_pruning;
 extern PGDLLIMPORT int constraint_exclusion;
 
+/** Lero Extension */
+extern char* lero_joinest_fname;
+/** ==== Lero Extension ==== */
+
 extern double index_pages_fetched(double tuples_fetched, BlockNumber pages,
 								  double index_pages, PlannerInfo *root);
 extern void cost_seqscan(Path *path, PlannerInfo *root, RelOptInfo *baserel,
diff --git a/src/include/optimizer/cost.h.orig b/src/include/optimizer/cost.h.orig
new file mode 100644
index 0000000..b3d0b4f
--- /dev/null
+++ b/src/include/optimizer/cost.h.orig
@@ -0,0 +1,200 @@
+/*-------------------------------------------------------------------------
+ *
+ * cost.h
+ *	  prototypes for costsize.c and clausesel.c.
+ *
+ *
+ * Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group
+ * Portions Copyright (c) 1994, Regents of the University of California
+ *
+ * src/include/optimizer/cost.h
+ *
+ *-------------------------------------------------------------------------
+ */
+#ifndef COST_H
+#define COST_H
+
+#include "nodes/pathnodes.h"
+#include "nodes/plannodes.h"
+
+
+/* defaults for costsize.c's Cost parameters */
+/* NB: cost-estimation code should use the variables, not these constants! */
+/* If you change these, update backend/utils/misc/postgresql.sample.conf */
+#define DEFAULT_SEQ_PAGE_COST  1.0
+#define DEFAULT_RANDOM_PAGE_COST  4.0
+#define DEFAULT_CPU_TUPLE_COST	0.01
+#define DEFAULT_CPU_INDEX_TUPLE_COST 0.005
+#define DEFAULT_CPU_OPERATOR_COST  0.0025
+#define DEFAULT_PARALLEL_TUPLE_COST 0.1
+#define DEFAULT_PARALLEL_SETUP_COST  1000.0
+
+#define DEFAULT_EFFECTIVE_CACHE_SIZE  524288	/* measured in pages */
+
+typedef enum
+{
+	CONSTRAINT_EXCLUSION_OFF,	/* do not use c_e */
+	CONSTRAINT_EXCLUSION_ON,	/* apply c_e to all rels */
+	CONSTRAINT_EXCLUSION_PARTITION	/* apply c_e to otherrels only */
+}			ConstraintExclusionType;
+
+
+/*
+ * prototypes for costsize.c
+ *	  routines to compute costs and sizes
+ */
+
+/* parameter variables and flags (see also optimizer.h) */
+extern PGDLLIMPORT Cost disable_cost;
+extern PGDLLIMPORT int max_parallel_workers_per_gather;
+extern PGDLLIMPORT bool enable_seqscan;
+extern PGDLLIMPORT bool enable_indexscan;
+extern PGDLLIMPORT bool enable_indexonlyscan;
+extern PGDLLIMPORT bool enable_bitmapscan;
+extern PGDLLIMPORT bool enable_tidscan;
+extern PGDLLIMPORT bool enable_sort;
+extern PGDLLIMPORT bool enable_hashagg;
+extern PGDLLIMPORT bool enable_nestloop;
+extern PGDLLIMPORT bool enable_material;
+extern PGDLLIMPORT bool enable_mergejoin;
+extern PGDLLIMPORT bool enable_hashjoin;
+extern PGDLLIMPORT bool enable_gathermerge;
+extern PGDLLIMPORT bool enable_partitionwise_join;
+extern PGDLLIMPORT bool enable_partitionwise_aggregate;
+extern PGDLLIMPORT bool enable_parallel_append;
+extern PGDLLIMPORT bool enable_parallel_hash;
+extern PGDLLIMPORT bool enable_partition_pruning;
+extern PGDLLIMPORT int constraint_exclusion;
+
+extern double index_pages_fetched(double tuples_fetched, BlockNumber pages,
+								  double index_pages, PlannerInfo *root);
+extern void cost_seqscan(Path *path, PlannerInfo *root, RelOptInfo *baserel,
+						 ParamPathInfo *param_info);
+extern void cost_samplescan(Path *path, PlannerInfo *root, RelOptInfo *baserel,
+							ParamPathInfo *param_info);
+extern void cost_index(IndexPath *path, PlannerInfo *root,
+					   double loop_count, bool partial_path);
+extern void cost_bitmap_heap_scan(Path *path, PlannerInfo *root, RelOptInfo *baserel,
+								  ParamPathInfo *param_info,
+								  Path *bitmapqual, double loop_count);
+extern void cost_bitmap_and_node(BitmapAndPath *path, PlannerInfo *root);
+extern void cost_bitmap_or_node(BitmapOrPath *path, PlannerInfo *root);
+extern void cost_bitmap_tree_node(Path *path, Cost *cost, Selectivity *selec);
+extern void cost_tidscan(Path *path, PlannerInfo *root,
+						 RelOptInfo *baserel, List *tidquals, ParamPathInfo *param_info);
+extern void cost_subqueryscan(SubqueryScanPath *path, PlannerInfo *root,
+							  RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_functionscan(Path *path, PlannerInfo *root,
+							  RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_valuesscan(Path *path, PlannerInfo *root,
+							RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_tablefuncscan(Path *path, PlannerInfo *root,
+							   RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_ctescan(Path *path, PlannerInfo *root,
+						 RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_namedtuplestorescan(Path *path, PlannerInfo *root,
+									 RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_resultscan(Path *path, PlannerInfo *root,
+							RelOptInfo *baserel, ParamPathInfo *param_info);
+extern void cost_recursive_union(Path *runion, Path *nrterm, Path *rterm);
+extern void cost_sort(Path *path, PlannerInfo *root,
+					  List *pathkeys, Cost input_cost, double tuples, int width,
+					  Cost comparison_cost, int sort_mem,
+					  double limit_tuples);
+extern void cost_append(AppendPath *path);
+extern void cost_merge_append(Path *path, PlannerInfo *root,
+							  List *pathkeys, int n_streams,
+							  Cost input_startup_cost, Cost input_total_cost,
+							  double tuples);
+extern void cost_material(Path *path,
+						  Cost input_startup_cost, Cost input_total_cost,
+						  double tuples, int width);
+extern void cost_agg(Path *path, PlannerInfo *root,
+					 AggStrategy aggstrategy, const AggClauseCosts *aggcosts,
+					 int numGroupCols, double numGroups,
+					 List *quals,
+					 Cost input_startup_cost, Cost input_total_cost,
+					 double input_tuples);
+extern void cost_windowagg(Path *path, PlannerInfo *root,
+						   List *windowFuncs, int numPartCols, int numOrderCols,
+						   Cost input_startup_cost, Cost input_total_cost,
+						   double input_tuples);
+extern void cost_group(Path *path, PlannerInfo *root,
+					   int numGroupCols, double numGroups,
+					   List *quals,
+					   Cost input_startup_cost, Cost input_total_cost,
+					   double input_tuples);
+extern void initial_cost_nestloop(PlannerInfo *root,
+								  JoinCostWorkspace *workspace,
+								  JoinType jointype,
+								  Path *outer_path, Path *inner_path,
+								  JoinPathExtraData *extra);
+extern void final_cost_nestloop(PlannerInfo *root, NestPath *path,
+								JoinCostWorkspace *workspace,
+								JoinPathExtraData *extra);
+extern void initial_cost_mergejoin(PlannerInfo *root,
+								   JoinCostWorkspace *workspace,
+								   JoinType jointype,
+								   List *mergeclauses,
+								   Path *outer_path, Path *inner_path,
+								   List *outersortkeys, List *innersortkeys,
+								   JoinPathExtraData *extra);
+extern void final_cost_mergejoin(PlannerInfo *root, MergePath *path,
+								 JoinCostWorkspace *workspace,
+								 JoinPathExtraData *extra);
+extern void initial_cost_hashjoin(PlannerInfo *root,
+								  JoinCostWorkspace *workspace,
+								  JoinType jointype,
+								  List *hashclauses,
+								  Path *outer_path, Path *inner_path,
+								  JoinPathExtraData *extra,
+								  bool parallel_hash);
+extern void final_cost_hashjoin(PlannerInfo *root, HashPath *path,
+								JoinCostWorkspace *workspace,
+								JoinPathExtraData *extra);
+extern void cost_gather(GatherPath *path, PlannerInfo *root,
+						RelOptInfo *baserel, ParamPathInfo *param_info, double *rows);
+extern void cost_gather_merge(GatherMergePath *path, PlannerInfo *root,
+							  RelOptInfo *rel, ParamPathInfo *param_info,
+							  Cost input_startup_cost, Cost input_total_cost,
+							  double *rows);
+extern void cost_subplan(PlannerInfo *root, SubPlan *subplan, Plan *plan);
+extern void cost_qual_eval(QualCost *cost, List *quals, PlannerInfo *root);
+extern void cost_qual_eval_node(QualCost *cost, Node *qual, PlannerInfo *root);
+extern void compute_semi_anti_join_factors(PlannerInfo *root,
+										   RelOptInfo *joinrel,
+										   RelOptInfo *outerrel,
+										   RelOptInfo *innerrel,
+										   JoinType jointype,
+										   SpecialJoinInfo *sjinfo,
+										   List *restrictlist,
+										   SemiAntiJoinFactors *semifactors);
+extern void set_baserel_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern double get_parameterized_baserel_size(PlannerInfo *root,
+											 RelOptInfo *rel,
+											 List *param_clauses);
+extern double get_parameterized_joinrel_size(PlannerInfo *root,
+											 RelOptInfo *rel,
+											 Path *outer_path,
+											 Path *inner_path,
+											 SpecialJoinInfo *sjinfo,
+											 List *restrict_clauses);
+extern void set_joinrel_size_estimates(PlannerInfo *root, RelOptInfo *rel,
+									   RelOptInfo *outer_rel,
+									   RelOptInfo *inner_rel,
+									   SpecialJoinInfo *sjinfo,
+									   List *restrictlist);
+extern void set_subquery_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern void set_function_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern void set_values_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern void set_cte_size_estimates(PlannerInfo *root, RelOptInfo *rel,
+								   double cte_rows);
+extern void set_tablefunc_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern void set_namedtuplestore_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern void set_result_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern void set_foreign_size_estimates(PlannerInfo *root, RelOptInfo *rel);
+extern PathTarget *set_pathtarget_cost_width(PlannerInfo *root, PathTarget *target);
+extern double compute_bitmap_pages(PlannerInfo *root, RelOptInfo *baserel,
+								   Path *bitmapqual, int loop_count, Cost *cost, double *tuple);
+
+#endif							/* COST_H */
diff --git a/src/include/optimizer/optimizer.h b/src/include/optimizer/optimizer.h
index d16b79d..922c227 100644
--- a/src/include/optimizer/optimizer.h
+++ b/src/include/optimizer/optimizer.h
@@ -104,7 +104,8 @@ typedef enum
 extern int	force_parallel_mode;
 extern bool parallel_leader_participation;
 
-extern struct PlannedStmt *planner(Query *parse, int cursorOptions,
+extern struct PlannedStmt *planner(Query *parse, const char *query_string, 
+								   int cursorOptions,
 								   struct ParamListInfoData *boundParams);
 
 extern Expr *expression_planner(Expr *expr);
diff --git a/src/include/optimizer/planner.h b/src/include/optimizer/planner.h
index 8d30b94..3faed58 100644
--- a/src/include/optimizer/planner.h
+++ b/src/include/optimizer/planner.h
@@ -23,7 +23,7 @@
 
 
 /* Hook for plugins to get control in planner() */
-typedef PlannedStmt *(*planner_hook_type) (Query *parse,
+typedef PlannedStmt *(*planner_hook_type) (Query *parse, const char *query_string,
 										   int cursorOptions,
 										   ParamListInfo boundParams);
 extern PGDLLIMPORT planner_hook_type planner_hook;
@@ -37,8 +37,8 @@ typedef void (*create_upper_paths_hook_type) (PlannerInfo *root,
 extern PGDLLIMPORT create_upper_paths_hook_type create_upper_paths_hook;
 
 
-extern PlannedStmt *standard_planner(Query *parse, int cursorOptions,
-									 ParamListInfo boundParams);
+extern PlannedStmt *standard_planner(Query *parse, const char *query_string, 
+									 int cursorOptions, ParamListInfo boundParams);
 
 extern PlannerInfo *subquery_planner(PlannerGlobal *glob, Query *parse,
 									 PlannerInfo *parent_root,
diff --git a/src/include/tcop/tcopprot.h b/src/include/tcop/tcopprot.h
index 7e99f41..f514fbe 100644
--- a/src/include/tcop/tcopprot.h
+++ b/src/include/tcop/tcopprot.h
@@ -58,9 +58,11 @@ extern List *pg_analyze_and_rewrite_params(RawStmt *parsetree,
 										   ParserSetupHook parserSetup,
 										   void *parserSetupArg,
 										   QueryEnvironment *queryEnv);
-extern PlannedStmt *pg_plan_query(Query *querytree, int cursorOptions,
+extern PlannedStmt *pg_plan_query(Query *querytree, const char *query_string, 
+								  int cursorOptions,
 								  ParamListInfo boundParams);
-extern List *pg_plan_queries(List *querytrees, int cursorOptions,
+extern List *pg_plan_queries(List *querytrees, const char *query_string, 
+							 int cursorOptions,
 							 ParamListInfo boundParams);
 
 extern bool check_max_stack_depth(int *newval, void **extra, GucSource source);
